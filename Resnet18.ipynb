{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from conf import global_settings as settings\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from utils import  get_training_dataloader, get_test_dataloader, WarmUpLR, \\\n",
    "    most_recent_folder, most_recent_weights, last_epoch, best_acc_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Basic Block for resnet 18 and resnet 34\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #BasicBlock and BottleNeck block\n",
    "    #have different output size\n",
    "    #we use class attribute expansion\n",
    "    #to distinct\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        #residual function\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "        )\n",
    "\n",
    "        #shortcut\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        #the shortcut output dimension is not the same with residual function\n",
    "        #use 1*1 convolution to match the dimension\n",
    "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, num_block, num_classes=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True))\n",
    "        #we use a different inputsize than the original paper\n",
    "        #so conv2_x's stride is 1\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
    "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        \"\"\"make resnet layers(by layer i didnt mean this 'layer' was the\n",
    "        same as a neuron netowork layer, ex. conv layer), one layer may\n",
    "        contain more than one residual block\n",
    "\n",
    "        Args:\n",
    "            block: block type, basic block or bottle neck block\n",
    "            out_channels: output depth channel number of this layer\n",
    "            num_blocks: how many blocks per layer\n",
    "            stride: the stride of the first block of this layer\n",
    "\n",
    "        Return:\n",
    "            return a resnet layer\n",
    "        \"\"\"\n",
    "\n",
    "        # we have num_block blocks per layer, the first block\n",
    "        # could be 1 or 2, other blocks would always be 1\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2_x(output)\n",
    "        output = self.conv3_x(output)\n",
    "        output = self.conv4_x(output)\n",
    "        output = self.conv5_x(output)\n",
    "        output = self.avg_pool(output)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def resnet18():\n",
    "    \"\"\" return a ResNet 18 object\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "\n",
    "    start = time.time()\n",
    "    net.train()\n",
    "    for batch_index, (images, labels) in enumerate(cifar100_training_loader):\n",
    "\n",
    "        if settings.GPU:\n",
    "            labels = labels.cuda()\n",
    "            images = images.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        n_iter = (epoch - 1) * len(cifar100_training_loader) + batch_index + 1\n",
    "\n",
    "        last_layer = list(net.children())[-1]\n",
    "        for name, para in last_layer.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                writer.add_scalar('LastLayerGradients/grad_norm2_weights', para.grad.norm(), n_iter)\n",
    "            if 'bias' in name:\n",
    "                writer.add_scalar('LastLayerGradients/grad_norm2_bias', para.grad.norm(), n_iter)\n",
    "\n",
    "        print('Training Epoch: {epoch} [{trained_samples}/{total_samples}]\\tLoss: {:0.4f}\\tLR: {:0.6f}'.format(\n",
    "            loss.item(),\n",
    "            optimizer.param_groups[0]['lr'],\n",
    "            epoch=epoch,\n",
    "            trained_samples=batch_index * settings.BATCH_SIZE + len(images),\n",
    "            total_samples=len(cifar100_training_loader.dataset)\n",
    "        ))\n",
    "\n",
    "        #update training loss for each iteration\n",
    "        writer.add_scalar('Train/loss', loss.item(), n_iter)\n",
    "\n",
    "        if epoch <= settings.BATCH_SIZE:\n",
    "            warmup_scheduler.step()\n",
    "\n",
    "    for name, param in net.named_parameters():\n",
    "        layer, attr = os.path.splitext(name)\n",
    "        attr = attr[1:]\n",
    "        writer.add_histogram(\"{}/{}\".format(layer, attr), param, epoch)\n",
    "\n",
    "    finish = time.time()\n",
    "\n",
    "    print('epoch {} training time consumed: {:.2f}s'.format(epoch, finish - start))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_training(epoch=0, tb=True):\n",
    "\n",
    "    start = time.time()\n",
    "    net.eval()\n",
    "\n",
    "    test_loss = 0.0 # cost function error\n",
    "    correct = 0.0\n",
    "\n",
    "    for (images, labels) in cifar100_test_loader:\n",
    "\n",
    "        if settings.GPU:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        outputs = net(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, preds = outputs.max(1)\n",
    "        correct += preds.eq(labels).sum()\n",
    "\n",
    "    finish = time.time()\n",
    "    if settings.GPU:\n",
    "        print('GPU INFO.....')\n",
    "        print(torch.cuda.memory_summary(), end='')\n",
    "    print('Evaluating Network.....')\n",
    "    print('Test set: Epoch: {}, Average loss: {:.4f}, Accuracy: {:.4f}, Time consumed:{:.2f}s'.format(\n",
    "        epoch,\n",
    "        test_loss / len(cifar100_test_loader.dataset),\n",
    "        correct.float() / len(cifar100_test_loader.dataset),\n",
    "        finish - start\n",
    "    ))\n",
    "    print()\n",
    "\n",
    "    #add informations to tensorboard\n",
    "    if tb:\n",
    "        writer.add_scalar('Test/Average loss', test_loss / len(cifar100_test_loader.dataset), epoch)\n",
    "        writer.add_scalar('Test/Accuracy', correct.float() / len(cifar100_test_loader.dataset), epoch)\n",
    "\n",
    "    return correct.float() / len(cifar100_test_loader.dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_training_dataloader(mean, std, batch_size=16, num_workers=2, shuffle=True):\n",
    "    \"\"\" return training dataloader\n",
    "    Args:\n",
    "        mean: mean of cifar100 training dataset\n",
    "        std: std of cifar100 training dataset\n",
    "        path: path to cifar100 training python dataset\n",
    "        batch_size: dataloader batchsize\n",
    "        num_workers: dataloader num_works\n",
    "        shuffle: whether to shuffle\n",
    "    Returns: train_data_loader:torch dataloader object\n",
    "    \"\"\"\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        #transforms.ToPILImage(),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    #cifar100_training = CIFAR100Train(path, transform=transform_train)\n",
    "    cifar100_training = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "    cifar100_training_loader = DataLoader(\n",
    "        cifar100_training, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n",
    "\n",
    "    return cifar100_training_loader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def get_test_dataloader(mean, std, batch_size=16, num_workers=2, shuffle=True):\n",
    "    \"\"\" return training dataloader\n",
    "    Args:\n",
    "        mean: mean of cifar100 test dataset\n",
    "        std: std of cifar100 test dataset\n",
    "        path: path to cifar100 test python dataset\n",
    "        batch_size: dataloader batchsize\n",
    "        num_workers: dataloader num_works\n",
    "        shuffle: whether to shuffle\n",
    "    Returns: cifar100_test_loader:torch dataloader object\n",
    "    \"\"\"\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    #cifar100_test = CIFAR100Test(path, transform=transform_test)\n",
    "    cifar100_test = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "    cifar100_test_loader = DataLoader(\n",
    "        cifar100_test, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)\n",
    "\n",
    "    return cifar100_test_loader\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n训练阶段\\n'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "训练阶段\n",
    "'''\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "net=resnet18()\n",
    "if settings.GPU:\n",
    "    net=net.cuda()\n",
    "#data preprocessing:\n",
    "cifar100_training_loader = get_training_dataloader(\n",
    "    settings.CIFAR100_TRAIN_MEAN,\n",
    "    settings.CIFAR100_TRAIN_STD,\n",
    "    num_workers=4,\n",
    "    batch_size=settings.BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "cifar100_test_loader = get_test_dataloader(\n",
    "    settings.CIFAR100_TRAIN_MEAN,\n",
    "    settings.CIFAR100_TRAIN_STD,\n",
    "    num_workers=4,\n",
    "    batch_size=settings.BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=settings.LR, momentum=0.9, weight_decay=5e-4)\n",
    "train_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=settings.MILESTONES, gamma=0.2) #learning rate decay\n",
    "iter_per_epoch = len(cifar100_training_loader)\n",
    "warmup_scheduler = WarmUpLR(optimizer, iter_per_epoch * settings.WARMUP)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resume_epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [11]\u001B[0m, in \u001B[0;36m<cell line: 48>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     50\u001B[0m     train_scheduler\u001B[38;5;241m.\u001B[39mstep(epoch)\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m settings\u001B[38;5;241m.\u001B[39mWARMUP:\n\u001B[1;32m---> 52\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mresume_epoch\u001B[49m:\n\u001B[0;32m     53\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m     54\u001B[0m train(epoch)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'resume_epoch' is not defined"
     ]
    }
   ],
   "source": [
    "if settings.RESUME:\n",
    "    recent_folder = most_recent_folder(os.path.join(settings.CHECKPOINT_PATH, settings.NET), fmt=settings.DATE_FORMAT)\n",
    "    if not recent_folder:\n",
    "        raise Exception('no recent folder were found')\n",
    "    checkpoint_path = os.path.join(settings.CHECKPOINT_PATH, settings.NET, recent_folder)\n",
    "else:\n",
    "    checkpoint_path = os.path.join(settings.CHECKPOINT_PATH, settings.NET, settings.TIME_NOW)\n",
    "\n",
    "#use tensorboard\n",
    "if not os.path.exists(settings.LOG_DIR):\n",
    "    os.mkdir(settings.LOG_DIR)\n",
    "\n",
    "#since tensorboard can't overwrite old values\n",
    "#so the only way is to create a new tensorboard log\n",
    "writer = SummaryWriter(log_dir=os.path.join(\n",
    "        settings.LOG_DIR, settings.NET, settings.TIME_NOW))\n",
    "input_tensor = torch.Tensor(1, 3, 32, 32)\n",
    "if settings.GPU:\n",
    "    input_tensor = input_tensor.cuda()\n",
    "writer.add_graph(net, input_tensor)\n",
    "\n",
    "#create checkpoint folder to save model\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_path = os.path.join(checkpoint_path, '{net}-{epoch}-{type}.pth')\n",
    "\n",
    "best_acc = 0.0\n",
    "if settings.RESUME:\n",
    "    best_weights = best_acc_weights(os.path.join(settings.CHECKPOINT_PATH, settings.NET, recent_folder))\n",
    "    if best_weights:\n",
    "        weights_path = os.path.join(settings.CHECKPOINT_PATH, settings.NET, recent_folder, best_weights)\n",
    "        print('found best acc weights file:{}'.format(weights_path))\n",
    "        print('load best training file to test acc...')\n",
    "        net.load_state_dict(torch.load(weights_path))\n",
    "        best_acc = eval_training(tb=False)\n",
    "        print('best acc is {:0.2f}'.format(best_acc))\n",
    "\n",
    "    recent_weights_file = most_recent_weights(os.path.join(settings.CHECKPOINT_PATH, settings.NET, recent_folder))\n",
    "    if not recent_weights_file:\n",
    "        raise Exception('no recent weights file were found')\n",
    "    weights_path = os.path.join(settings.CHECKPOINT_PATH, settings.NET, recent_folder, recent_weights_file)\n",
    "    print('loading weights file {} to resume training.....'.format(weights_path))\n",
    "    net.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "    resume_epoch = last_epoch(os.path.join(settings.CHECKPOINT_PATH, settings.NET, recent_folder))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [128/50000]\tLoss: 4.7416\tLR: 0.000000\n",
      "Training Epoch: 1 [256/50000]\tLoss: 4.7507\tLR: 0.000256\n",
      "Training Epoch: 1 [384/50000]\tLoss: 4.7553\tLR: 0.000512\n",
      "Training Epoch: 1 [512/50000]\tLoss: 4.7575\tLR: 0.000767\n",
      "Training Epoch: 1 [640/50000]\tLoss: 4.7439\tLR: 0.001023\n",
      "Training Epoch: 1 [768/50000]\tLoss: 4.7048\tLR: 0.001279\n",
      "Training Epoch: 1 [896/50000]\tLoss: 4.7390\tLR: 0.001535\n",
      "Training Epoch: 1 [1024/50000]\tLoss: 4.7267\tLR: 0.001790\n",
      "Training Epoch: 1 [1152/50000]\tLoss: 4.6013\tLR: 0.002046\n",
      "Training Epoch: 1 [1280/50000]\tLoss: 4.6719\tLR: 0.002302\n",
      "Training Epoch: 1 [1408/50000]\tLoss: 4.6062\tLR: 0.002558\n",
      "Training Epoch: 1 [1536/50000]\tLoss: 4.6495\tLR: 0.002813\n",
      "Training Epoch: 1 [1664/50000]\tLoss: 4.5117\tLR: 0.003069\n",
      "Training Epoch: 1 [1792/50000]\tLoss: 4.6379\tLR: 0.003325\n",
      "Training Epoch: 1 [1920/50000]\tLoss: 4.5186\tLR: 0.003581\n",
      "Training Epoch: 1 [2048/50000]\tLoss: 4.5133\tLR: 0.003836\n",
      "Training Epoch: 1 [2176/50000]\tLoss: 4.5414\tLR: 0.004092\n",
      "Training Epoch: 1 [2304/50000]\tLoss: 4.4152\tLR: 0.004348\n",
      "Training Epoch: 1 [2432/50000]\tLoss: 4.4881\tLR: 0.004604\n",
      "Training Epoch: 1 [2560/50000]\tLoss: 4.4669\tLR: 0.004859\n",
      "Training Epoch: 1 [2688/50000]\tLoss: 4.4389\tLR: 0.005115\n",
      "Training Epoch: 1 [2816/50000]\tLoss: 4.4711\tLR: 0.005371\n",
      "Training Epoch: 1 [2944/50000]\tLoss: 4.3286\tLR: 0.005627\n",
      "Training Epoch: 1 [3072/50000]\tLoss: 4.4513\tLR: 0.005882\n",
      "Training Epoch: 1 [3200/50000]\tLoss: 4.3969\tLR: 0.006138\n",
      "Training Epoch: 1 [3328/50000]\tLoss: 4.4627\tLR: 0.006394\n",
      "Training Epoch: 1 [3456/50000]\tLoss: 4.3669\tLR: 0.006650\n",
      "Training Epoch: 1 [3584/50000]\tLoss: 4.4137\tLR: 0.006905\n",
      "Training Epoch: 1 [3712/50000]\tLoss: 4.5118\tLR: 0.007161\n",
      "Training Epoch: 1 [3840/50000]\tLoss: 4.4220\tLR: 0.007417\n",
      "Training Epoch: 1 [3968/50000]\tLoss: 4.4331\tLR: 0.007673\n",
      "Training Epoch: 1 [4096/50000]\tLoss: 4.4156\tLR: 0.007928\n",
      "Training Epoch: 1 [4224/50000]\tLoss: 4.3575\tLR: 0.008184\n",
      "Training Epoch: 1 [4352/50000]\tLoss: 4.3659\tLR: 0.008440\n",
      "Training Epoch: 1 [4480/50000]\tLoss: 4.3213\tLR: 0.008696\n",
      "Training Epoch: 1 [4608/50000]\tLoss: 4.4053\tLR: 0.008951\n",
      "Training Epoch: 1 [4736/50000]\tLoss: 4.3322\tLR: 0.009207\n",
      "Training Epoch: 1 [4864/50000]\tLoss: 4.3602\tLR: 0.009463\n",
      "Training Epoch: 1 [4992/50000]\tLoss: 4.3231\tLR: 0.009719\n",
      "Training Epoch: 1 [5120/50000]\tLoss: 4.2610\tLR: 0.009974\n",
      "Training Epoch: 1 [5248/50000]\tLoss: 4.2641\tLR: 0.010230\n",
      "Training Epoch: 1 [5376/50000]\tLoss: 4.3066\tLR: 0.010486\n",
      "Training Epoch: 1 [5504/50000]\tLoss: 4.1762\tLR: 0.010742\n",
      "Training Epoch: 1 [5632/50000]\tLoss: 4.1603\tLR: 0.010997\n",
      "Training Epoch: 1 [5760/50000]\tLoss: 4.4326\tLR: 0.011253\n",
      "Training Epoch: 1 [5888/50000]\tLoss: 4.1960\tLR: 0.011509\n",
      "Training Epoch: 1 [6016/50000]\tLoss: 4.1034\tLR: 0.011765\n",
      "Training Epoch: 1 [6144/50000]\tLoss: 4.1219\tLR: 0.012020\n",
      "Training Epoch: 1 [6272/50000]\tLoss: 4.0866\tLR: 0.012276\n",
      "Training Epoch: 1 [6400/50000]\tLoss: 4.1371\tLR: 0.012532\n",
      "Training Epoch: 1 [6528/50000]\tLoss: 4.3022\tLR: 0.012788\n",
      "Training Epoch: 1 [6656/50000]\tLoss: 4.2387\tLR: 0.013043\n",
      "Training Epoch: 1 [6784/50000]\tLoss: 4.2563\tLR: 0.013299\n",
      "Training Epoch: 1 [6912/50000]\tLoss: 4.2236\tLR: 0.013555\n",
      "Training Epoch: 1 [7040/50000]\tLoss: 4.1563\tLR: 0.013811\n",
      "Training Epoch: 1 [7168/50000]\tLoss: 4.1914\tLR: 0.014066\n",
      "Training Epoch: 1 [7296/50000]\tLoss: 4.1911\tLR: 0.014322\n",
      "Training Epoch: 1 [7424/50000]\tLoss: 4.1178\tLR: 0.014578\n",
      "Training Epoch: 1 [7552/50000]\tLoss: 4.3457\tLR: 0.014834\n",
      "Training Epoch: 1 [7680/50000]\tLoss: 4.0025\tLR: 0.015090\n",
      "Training Epoch: 1 [7808/50000]\tLoss: 4.2445\tLR: 0.015345\n",
      "Training Epoch: 1 [7936/50000]\tLoss: 4.1724\tLR: 0.015601\n",
      "Training Epoch: 1 [8064/50000]\tLoss: 4.1491\tLR: 0.015857\n",
      "Training Epoch: 1 [8192/50000]\tLoss: 4.0215\tLR: 0.016113\n",
      "Training Epoch: 1 [8320/50000]\tLoss: 3.8833\tLR: 0.016368\n",
      "Training Epoch: 1 [8448/50000]\tLoss: 4.1844\tLR: 0.016624\n",
      "Training Epoch: 1 [8576/50000]\tLoss: 4.1033\tLR: 0.016880\n",
      "Training Epoch: 1 [8704/50000]\tLoss: 4.2611\tLR: 0.017136\n",
      "Training Epoch: 1 [8832/50000]\tLoss: 4.3477\tLR: 0.017391\n",
      "Training Epoch: 1 [8960/50000]\tLoss: 4.0179\tLR: 0.017647\n",
      "Training Epoch: 1 [9088/50000]\tLoss: 4.2224\tLR: 0.017903\n",
      "Training Epoch: 1 [9216/50000]\tLoss: 4.1768\tLR: 0.018159\n",
      "Training Epoch: 1 [9344/50000]\tLoss: 4.2663\tLR: 0.018414\n",
      "Training Epoch: 1 [9472/50000]\tLoss: 4.3697\tLR: 0.018670\n",
      "Training Epoch: 1 [9600/50000]\tLoss: 4.0921\tLR: 0.018926\n",
      "Training Epoch: 1 [9728/50000]\tLoss: 4.2749\tLR: 0.019182\n",
      "Training Epoch: 1 [9856/50000]\tLoss: 3.9546\tLR: 0.019437\n",
      "Training Epoch: 1 [9984/50000]\tLoss: 4.0068\tLR: 0.019693\n",
      "Training Epoch: 1 [10112/50000]\tLoss: 4.3575\tLR: 0.019949\n",
      "Training Epoch: 1 [10240/50000]\tLoss: 4.3017\tLR: 0.020205\n",
      "Training Epoch: 1 [10368/50000]\tLoss: 4.2234\tLR: 0.020460\n",
      "Training Epoch: 1 [10496/50000]\tLoss: 4.2433\tLR: 0.020716\n",
      "Training Epoch: 1 [10624/50000]\tLoss: 4.2446\tLR: 0.020972\n",
      "Training Epoch: 1 [10752/50000]\tLoss: 4.0904\tLR: 0.021228\n",
      "Training Epoch: 1 [10880/50000]\tLoss: 4.1581\tLR: 0.021483\n",
      "Training Epoch: 1 [11008/50000]\tLoss: 4.1139\tLR: 0.021739\n",
      "Training Epoch: 1 [11136/50000]\tLoss: 4.2511\tLR: 0.021995\n",
      "Training Epoch: 1 [11264/50000]\tLoss: 4.0251\tLR: 0.022251\n",
      "Training Epoch: 1 [11392/50000]\tLoss: 4.1293\tLR: 0.022506\n",
      "Training Epoch: 1 [11520/50000]\tLoss: 3.9112\tLR: 0.022762\n",
      "Training Epoch: 1 [11648/50000]\tLoss: 4.3104\tLR: 0.023018\n",
      "Training Epoch: 1 [11776/50000]\tLoss: 4.2503\tLR: 0.023274\n",
      "Training Epoch: 1 [11904/50000]\tLoss: 4.0248\tLR: 0.023529\n",
      "Training Epoch: 1 [12032/50000]\tLoss: 4.0006\tLR: 0.023785\n",
      "Training Epoch: 1 [12160/50000]\tLoss: 4.0509\tLR: 0.024041\n",
      "Training Epoch: 1 [12288/50000]\tLoss: 4.0217\tLR: 0.024297\n",
      "Training Epoch: 1 [12416/50000]\tLoss: 4.1484\tLR: 0.024552\n",
      "Training Epoch: 1 [12544/50000]\tLoss: 4.0969\tLR: 0.024808\n",
      "Training Epoch: 1 [12672/50000]\tLoss: 4.2731\tLR: 0.025064\n",
      "Training Epoch: 1 [12800/50000]\tLoss: 4.0635\tLR: 0.025320\n",
      "Training Epoch: 1 [12928/50000]\tLoss: 4.0420\tLR: 0.025575\n",
      "Training Epoch: 1 [13056/50000]\tLoss: 3.9759\tLR: 0.025831\n",
      "Training Epoch: 1 [13184/50000]\tLoss: 3.9934\tLR: 0.026087\n",
      "Training Epoch: 1 [13312/50000]\tLoss: 3.8491\tLR: 0.026343\n",
      "Training Epoch: 1 [13440/50000]\tLoss: 4.0289\tLR: 0.026598\n",
      "Training Epoch: 1 [13568/50000]\tLoss: 3.9801\tLR: 0.026854\n",
      "Training Epoch: 1 [13696/50000]\tLoss: 4.3723\tLR: 0.027110\n",
      "Training Epoch: 1 [13824/50000]\tLoss: 3.9181\tLR: 0.027366\n",
      "Training Epoch: 1 [13952/50000]\tLoss: 4.1283\tLR: 0.027621\n",
      "Training Epoch: 1 [14080/50000]\tLoss: 3.9353\tLR: 0.027877\n",
      "Training Epoch: 1 [14208/50000]\tLoss: 4.0970\tLR: 0.028133\n",
      "Training Epoch: 1 [14336/50000]\tLoss: 4.1968\tLR: 0.028389\n",
      "Training Epoch: 1 [14464/50000]\tLoss: 3.9837\tLR: 0.028645\n",
      "Training Epoch: 1 [14592/50000]\tLoss: 4.1329\tLR: 0.028900\n",
      "Training Epoch: 1 [14720/50000]\tLoss: 3.9886\tLR: 0.029156\n",
      "Training Epoch: 1 [14848/50000]\tLoss: 3.9628\tLR: 0.029412\n",
      "Training Epoch: 1 [14976/50000]\tLoss: 4.3138\tLR: 0.029668\n",
      "Training Epoch: 1 [15104/50000]\tLoss: 4.0243\tLR: 0.029923\n",
      "Training Epoch: 1 [15232/50000]\tLoss: 3.9016\tLR: 0.030179\n",
      "Training Epoch: 1 [15360/50000]\tLoss: 3.9880\tLR: 0.030435\n",
      "Training Epoch: 1 [15488/50000]\tLoss: 4.1579\tLR: 0.030691\n",
      "Training Epoch: 1 [15616/50000]\tLoss: 4.0958\tLR: 0.030946\n",
      "Training Epoch: 1 [15744/50000]\tLoss: 3.9233\tLR: 0.031202\n",
      "Training Epoch: 1 [15872/50000]\tLoss: 4.0028\tLR: 0.031458\n",
      "Training Epoch: 1 [16000/50000]\tLoss: 3.9988\tLR: 0.031714\n",
      "Training Epoch: 1 [16128/50000]\tLoss: 4.1499\tLR: 0.031969\n",
      "Training Epoch: 1 [16256/50000]\tLoss: 4.1453\tLR: 0.032225\n",
      "Training Epoch: 1 [16384/50000]\tLoss: 3.9548\tLR: 0.032481\n",
      "Training Epoch: 1 [16512/50000]\tLoss: 4.0092\tLR: 0.032737\n",
      "Training Epoch: 1 [16640/50000]\tLoss: 4.0563\tLR: 0.032992\n",
      "Training Epoch: 1 [16768/50000]\tLoss: 3.8166\tLR: 0.033248\n",
      "Training Epoch: 1 [16896/50000]\tLoss: 3.9232\tLR: 0.033504\n",
      "Training Epoch: 1 [17024/50000]\tLoss: 4.0836\tLR: 0.033760\n",
      "Training Epoch: 1 [17152/50000]\tLoss: 4.0504\tLR: 0.034015\n",
      "Training Epoch: 1 [17280/50000]\tLoss: 4.0015\tLR: 0.034271\n",
      "Training Epoch: 1 [17408/50000]\tLoss: 3.9223\tLR: 0.034527\n",
      "Training Epoch: 1 [17536/50000]\tLoss: 3.6737\tLR: 0.034783\n",
      "Training Epoch: 1 [17664/50000]\tLoss: 4.0964\tLR: 0.035038\n",
      "Training Epoch: 1 [17792/50000]\tLoss: 3.9985\tLR: 0.035294\n",
      "Training Epoch: 1 [17920/50000]\tLoss: 4.3177\tLR: 0.035550\n",
      "Training Epoch: 1 [18048/50000]\tLoss: 3.9286\tLR: 0.035806\n",
      "Training Epoch: 1 [18176/50000]\tLoss: 3.9419\tLR: 0.036061\n",
      "Training Epoch: 1 [18304/50000]\tLoss: 3.8006\tLR: 0.036317\n",
      "Training Epoch: 1 [18432/50000]\tLoss: 3.8526\tLR: 0.036573\n",
      "Training Epoch: 1 [18560/50000]\tLoss: 3.7865\tLR: 0.036829\n",
      "Training Epoch: 1 [18688/50000]\tLoss: 4.4315\tLR: 0.037084\n",
      "Training Epoch: 1 [18816/50000]\tLoss: 4.0446\tLR: 0.037340\n",
      "Training Epoch: 1 [18944/50000]\tLoss: 3.9839\tLR: 0.037596\n",
      "Training Epoch: 1 [19072/50000]\tLoss: 3.9264\tLR: 0.037852\n",
      "Training Epoch: 1 [19200/50000]\tLoss: 4.0799\tLR: 0.038107\n",
      "Training Epoch: 1 [19328/50000]\tLoss: 4.0893\tLR: 0.038363\n",
      "Training Epoch: 1 [19456/50000]\tLoss: 4.0916\tLR: 0.038619\n",
      "Training Epoch: 1 [19584/50000]\tLoss: 3.8611\tLR: 0.038875\n",
      "Training Epoch: 1 [19712/50000]\tLoss: 3.9326\tLR: 0.039130\n",
      "Training Epoch: 1 [19840/50000]\tLoss: 3.9025\tLR: 0.039386\n",
      "Training Epoch: 1 [19968/50000]\tLoss: 3.8108\tLR: 0.039642\n",
      "Training Epoch: 1 [20096/50000]\tLoss: 3.9539\tLR: 0.039898\n",
      "Training Epoch: 1 [20224/50000]\tLoss: 3.9839\tLR: 0.040153\n",
      "Training Epoch: 1 [20352/50000]\tLoss: 3.8945\tLR: 0.040409\n",
      "Training Epoch: 1 [20480/50000]\tLoss: 3.9207\tLR: 0.040665\n",
      "Training Epoch: 1 [20608/50000]\tLoss: 4.0203\tLR: 0.040921\n",
      "Training Epoch: 1 [20736/50000]\tLoss: 4.0471\tLR: 0.041176\n",
      "Training Epoch: 1 [20864/50000]\tLoss: 3.9289\tLR: 0.041432\n",
      "Training Epoch: 1 [20992/50000]\tLoss: 3.9840\tLR: 0.041688\n",
      "Training Epoch: 1 [21120/50000]\tLoss: 3.9699\tLR: 0.041944\n",
      "Training Epoch: 1 [21248/50000]\tLoss: 3.9369\tLR: 0.042199\n",
      "Training Epoch: 1 [21376/50000]\tLoss: 4.0151\tLR: 0.042455\n",
      "Training Epoch: 1 [21504/50000]\tLoss: 3.8293\tLR: 0.042711\n",
      "Training Epoch: 1 [21632/50000]\tLoss: 3.6727\tLR: 0.042967\n",
      "Training Epoch: 1 [21760/50000]\tLoss: 3.9299\tLR: 0.043223\n",
      "Training Epoch: 1 [21888/50000]\tLoss: 3.8375\tLR: 0.043478\n",
      "Training Epoch: 1 [22016/50000]\tLoss: 4.1334\tLR: 0.043734\n",
      "Training Epoch: 1 [22144/50000]\tLoss: 3.7854\tLR: 0.043990\n",
      "Training Epoch: 1 [22272/50000]\tLoss: 4.0541\tLR: 0.044246\n",
      "Training Epoch: 1 [22400/50000]\tLoss: 3.8555\tLR: 0.044501\n",
      "Training Epoch: 1 [22528/50000]\tLoss: 3.7753\tLR: 0.044757\n",
      "Training Epoch: 1 [22656/50000]\tLoss: 3.7033\tLR: 0.045013\n",
      "Training Epoch: 1 [22784/50000]\tLoss: 4.1272\tLR: 0.045269\n",
      "Training Epoch: 1 [22912/50000]\tLoss: 3.8664\tLR: 0.045524\n",
      "Training Epoch: 1 [23040/50000]\tLoss: 3.9333\tLR: 0.045780\n",
      "Training Epoch: 1 [23168/50000]\tLoss: 3.8815\tLR: 0.046036\n",
      "Training Epoch: 1 [23296/50000]\tLoss: 3.8117\tLR: 0.046292\n",
      "Training Epoch: 1 [23424/50000]\tLoss: 3.8726\tLR: 0.046547\n",
      "Training Epoch: 1 [23552/50000]\tLoss: 3.7878\tLR: 0.046803\n",
      "Training Epoch: 1 [23680/50000]\tLoss: 3.5951\tLR: 0.047059\n",
      "Training Epoch: 1 [23808/50000]\tLoss: 3.9563\tLR: 0.047315\n",
      "Training Epoch: 1 [23936/50000]\tLoss: 4.0826\tLR: 0.047570\n",
      "Training Epoch: 1 [24064/50000]\tLoss: 3.8259\tLR: 0.047826\n",
      "Training Epoch: 1 [24192/50000]\tLoss: 3.9922\tLR: 0.048082\n",
      "Training Epoch: 1 [24320/50000]\tLoss: 3.7543\tLR: 0.048338\n",
      "Training Epoch: 1 [24448/50000]\tLoss: 3.9082\tLR: 0.048593\n",
      "Training Epoch: 1 [24576/50000]\tLoss: 3.8101\tLR: 0.048849\n",
      "Training Epoch: 1 [24704/50000]\tLoss: 3.9437\tLR: 0.049105\n",
      "Training Epoch: 1 [24832/50000]\tLoss: 4.3707\tLR: 0.049361\n",
      "Training Epoch: 1 [24960/50000]\tLoss: 4.0036\tLR: 0.049616\n",
      "Training Epoch: 1 [25088/50000]\tLoss: 3.8689\tLR: 0.049872\n",
      "Training Epoch: 1 [25216/50000]\tLoss: 4.1412\tLR: 0.050128\n",
      "Training Epoch: 1 [25344/50000]\tLoss: 3.8052\tLR: 0.050384\n",
      "Training Epoch: 1 [25472/50000]\tLoss: 3.7077\tLR: 0.050639\n",
      "Training Epoch: 1 [25600/50000]\tLoss: 3.8916\tLR: 0.050895\n",
      "Training Epoch: 1 [25728/50000]\tLoss: 4.0345\tLR: 0.051151\n",
      "Training Epoch: 1 [25856/50000]\tLoss: 3.9372\tLR: 0.051407\n",
      "Training Epoch: 1 [25984/50000]\tLoss: 4.1656\tLR: 0.051662\n",
      "Training Epoch: 1 [26112/50000]\tLoss: 3.9202\tLR: 0.051918\n",
      "Training Epoch: 1 [26240/50000]\tLoss: 3.7589\tLR: 0.052174\n",
      "Training Epoch: 1 [26368/50000]\tLoss: 3.9910\tLR: 0.052430\n",
      "Training Epoch: 1 [26496/50000]\tLoss: 3.7422\tLR: 0.052685\n",
      "Training Epoch: 1 [26624/50000]\tLoss: 3.9505\tLR: 0.052941\n",
      "Training Epoch: 1 [26752/50000]\tLoss: 3.9593\tLR: 0.053197\n",
      "Training Epoch: 1 [26880/50000]\tLoss: 3.8374\tLR: 0.053453\n",
      "Training Epoch: 1 [27008/50000]\tLoss: 3.8077\tLR: 0.053708\n",
      "Training Epoch: 1 [27136/50000]\tLoss: 3.9043\tLR: 0.053964\n",
      "Training Epoch: 1 [27264/50000]\tLoss: 3.8795\tLR: 0.054220\n",
      "Training Epoch: 1 [27392/50000]\tLoss: 3.8492\tLR: 0.054476\n",
      "Training Epoch: 1 [27520/50000]\tLoss: 3.8798\tLR: 0.054731\n",
      "Training Epoch: 1 [27648/50000]\tLoss: 3.8057\tLR: 0.054987\n",
      "Training Epoch: 1 [27776/50000]\tLoss: 3.8402\tLR: 0.055243\n",
      "Training Epoch: 1 [27904/50000]\tLoss: 3.7793\tLR: 0.055499\n",
      "Training Epoch: 1 [28032/50000]\tLoss: 3.9037\tLR: 0.055754\n",
      "Training Epoch: 1 [28160/50000]\tLoss: 3.9404\tLR: 0.056010\n",
      "Training Epoch: 1 [28288/50000]\tLoss: 3.6452\tLR: 0.056266\n",
      "Training Epoch: 1 [28416/50000]\tLoss: 3.8734\tLR: 0.056522\n",
      "Training Epoch: 1 [28544/50000]\tLoss: 3.9383\tLR: 0.056777\n",
      "Training Epoch: 1 [28672/50000]\tLoss: 3.6675\tLR: 0.057033\n",
      "Training Epoch: 1 [28800/50000]\tLoss: 3.9645\tLR: 0.057289\n",
      "Training Epoch: 1 [28928/50000]\tLoss: 3.5677\tLR: 0.057545\n",
      "Training Epoch: 1 [29056/50000]\tLoss: 3.7235\tLR: 0.057801\n",
      "Training Epoch: 1 [29184/50000]\tLoss: 3.8966\tLR: 0.058056\n",
      "Training Epoch: 1 [29312/50000]\tLoss: 3.8390\tLR: 0.058312\n",
      "Training Epoch: 1 [29440/50000]\tLoss: 3.7753\tLR: 0.058568\n",
      "Training Epoch: 1 [29568/50000]\tLoss: 3.8095\tLR: 0.058824\n",
      "Training Epoch: 1 [29696/50000]\tLoss: 3.8389\tLR: 0.059079\n",
      "Training Epoch: 1 [29824/50000]\tLoss: 3.9392\tLR: 0.059335\n",
      "Training Epoch: 1 [29952/50000]\tLoss: 3.7513\tLR: 0.059591\n",
      "Training Epoch: 1 [30080/50000]\tLoss: 3.8054\tLR: 0.059847\n",
      "Training Epoch: 1 [30208/50000]\tLoss: 3.7743\tLR: 0.060102\n",
      "Training Epoch: 1 [30336/50000]\tLoss: 3.7498\tLR: 0.060358\n",
      "Training Epoch: 1 [30464/50000]\tLoss: 3.8753\tLR: 0.060614\n",
      "Training Epoch: 1 [30592/50000]\tLoss: 3.8211\tLR: 0.060870\n",
      "Training Epoch: 1 [30720/50000]\tLoss: 3.8481\tLR: 0.061125\n",
      "Training Epoch: 1 [30848/50000]\tLoss: 3.6630\tLR: 0.061381\n",
      "Training Epoch: 1 [30976/50000]\tLoss: 3.9851\tLR: 0.061637\n",
      "Training Epoch: 1 [31104/50000]\tLoss: 3.8436\tLR: 0.061893\n",
      "Training Epoch: 1 [31232/50000]\tLoss: 3.9038\tLR: 0.062148\n",
      "Training Epoch: 1 [31360/50000]\tLoss: 3.8304\tLR: 0.062404\n",
      "Training Epoch: 1 [31488/50000]\tLoss: 3.8071\tLR: 0.062660\n",
      "Training Epoch: 1 [31616/50000]\tLoss: 3.8314\tLR: 0.062916\n",
      "Training Epoch: 1 [31744/50000]\tLoss: 3.9307\tLR: 0.063171\n",
      "Training Epoch: 1 [31872/50000]\tLoss: 3.8889\tLR: 0.063427\n",
      "Training Epoch: 1 [32000/50000]\tLoss: 3.7625\tLR: 0.063683\n",
      "Training Epoch: 1 [32128/50000]\tLoss: 3.8250\tLR: 0.063939\n",
      "Training Epoch: 1 [32256/50000]\tLoss: 3.7520\tLR: 0.064194\n",
      "Training Epoch: 1 [32384/50000]\tLoss: 3.7837\tLR: 0.064450\n",
      "Training Epoch: 1 [32512/50000]\tLoss: 3.7508\tLR: 0.064706\n",
      "Training Epoch: 1 [32640/50000]\tLoss: 3.8242\tLR: 0.064962\n",
      "Training Epoch: 1 [32768/50000]\tLoss: 3.7687\tLR: 0.065217\n",
      "Training Epoch: 1 [32896/50000]\tLoss: 3.7335\tLR: 0.065473\n",
      "Training Epoch: 1 [33024/50000]\tLoss: 3.6020\tLR: 0.065729\n",
      "Training Epoch: 1 [33152/50000]\tLoss: 3.6842\tLR: 0.065985\n",
      "Training Epoch: 1 [33280/50000]\tLoss: 3.8328\tLR: 0.066240\n",
      "Training Epoch: 1 [33408/50000]\tLoss: 3.7149\tLR: 0.066496\n",
      "Training Epoch: 1 [33536/50000]\tLoss: 3.7683\tLR: 0.066752\n",
      "Training Epoch: 1 [33664/50000]\tLoss: 3.8103\tLR: 0.067008\n",
      "Training Epoch: 1 [33792/50000]\tLoss: 3.6025\tLR: 0.067263\n",
      "Training Epoch: 1 [33920/50000]\tLoss: 3.6395\tLR: 0.067519\n",
      "Training Epoch: 1 [34048/50000]\tLoss: 3.6081\tLR: 0.067775\n",
      "Training Epoch: 1 [34176/50000]\tLoss: 3.7968\tLR: 0.068031\n",
      "Training Epoch: 1 [34304/50000]\tLoss: 3.6334\tLR: 0.068286\n",
      "Training Epoch: 1 [34432/50000]\tLoss: 3.6192\tLR: 0.068542\n",
      "Training Epoch: 1 [34560/50000]\tLoss: 3.9346\tLR: 0.068798\n",
      "Training Epoch: 1 [34688/50000]\tLoss: 3.5656\tLR: 0.069054\n",
      "Training Epoch: 1 [34816/50000]\tLoss: 3.6776\tLR: 0.069309\n",
      "Training Epoch: 1 [34944/50000]\tLoss: 3.3151\tLR: 0.069565\n",
      "Training Epoch: 1 [35072/50000]\tLoss: 3.7701\tLR: 0.069821\n",
      "Training Epoch: 1 [35200/50000]\tLoss: 3.8566\tLR: 0.070077\n",
      "Training Epoch: 1 [35328/50000]\tLoss: 3.6708\tLR: 0.070332\n",
      "Training Epoch: 1 [35456/50000]\tLoss: 3.6070\tLR: 0.070588\n",
      "Training Epoch: 1 [35584/50000]\tLoss: 3.6821\tLR: 0.070844\n",
      "Training Epoch: 1 [35712/50000]\tLoss: 3.6428\tLR: 0.071100\n",
      "Training Epoch: 1 [35840/50000]\tLoss: 3.6123\tLR: 0.071355\n",
      "Training Epoch: 1 [35968/50000]\tLoss: 3.7784\tLR: 0.071611\n",
      "Training Epoch: 1 [36096/50000]\tLoss: 3.5587\tLR: 0.071867\n",
      "Training Epoch: 1 [36224/50000]\tLoss: 3.6677\tLR: 0.072123\n",
      "Training Epoch: 1 [36352/50000]\tLoss: 3.5300\tLR: 0.072379\n",
      "Training Epoch: 1 [36480/50000]\tLoss: 3.6877\tLR: 0.072634\n",
      "Training Epoch: 1 [36608/50000]\tLoss: 3.6994\tLR: 0.072890\n",
      "Training Epoch: 1 [36736/50000]\tLoss: 3.6280\tLR: 0.073146\n",
      "Training Epoch: 1 [36864/50000]\tLoss: 3.8025\tLR: 0.073402\n",
      "Training Epoch: 1 [36992/50000]\tLoss: 3.8130\tLR: 0.073657\n",
      "Training Epoch: 1 [37120/50000]\tLoss: 3.6468\tLR: 0.073913\n",
      "Training Epoch: 1 [37248/50000]\tLoss: 3.6163\tLR: 0.074169\n",
      "Training Epoch: 1 [37376/50000]\tLoss: 3.6819\tLR: 0.074425\n",
      "Training Epoch: 1 [37504/50000]\tLoss: 3.7998\tLR: 0.074680\n",
      "Training Epoch: 1 [37632/50000]\tLoss: 3.7870\tLR: 0.074936\n",
      "Training Epoch: 1 [37760/50000]\tLoss: 3.6201\tLR: 0.075192\n",
      "Training Epoch: 1 [37888/50000]\tLoss: 3.5441\tLR: 0.075448\n",
      "Training Epoch: 1 [38016/50000]\tLoss: 3.9608\tLR: 0.075703\n",
      "Training Epoch: 1 [38144/50000]\tLoss: 4.0122\tLR: 0.075959\n",
      "Training Epoch: 1 [38272/50000]\tLoss: 3.7107\tLR: 0.076215\n",
      "Training Epoch: 1 [38400/50000]\tLoss: 3.7273\tLR: 0.076471\n",
      "Training Epoch: 1 [38528/50000]\tLoss: 3.6112\tLR: 0.076726\n",
      "Training Epoch: 1 [38656/50000]\tLoss: 3.6339\tLR: 0.076982\n",
      "Training Epoch: 1 [38784/50000]\tLoss: 3.5245\tLR: 0.077238\n",
      "Training Epoch: 1 [38912/50000]\tLoss: 3.6854\tLR: 0.077494\n",
      "Training Epoch: 1 [39040/50000]\tLoss: 3.9458\tLR: 0.077749\n",
      "Training Epoch: 1 [39168/50000]\tLoss: 3.6169\tLR: 0.078005\n",
      "Training Epoch: 1 [39296/50000]\tLoss: 3.7907\tLR: 0.078261\n",
      "Training Epoch: 1 [39424/50000]\tLoss: 3.6688\tLR: 0.078517\n",
      "Training Epoch: 1 [39552/50000]\tLoss: 3.6019\tLR: 0.078772\n",
      "Training Epoch: 1 [39680/50000]\tLoss: 3.4873\tLR: 0.079028\n",
      "Training Epoch: 1 [39808/50000]\tLoss: 3.6091\tLR: 0.079284\n",
      "Training Epoch: 1 [39936/50000]\tLoss: 3.9220\tLR: 0.079540\n",
      "Training Epoch: 1 [40064/50000]\tLoss: 3.6859\tLR: 0.079795\n",
      "Training Epoch: 1 [40192/50000]\tLoss: 3.7885\tLR: 0.080051\n",
      "Training Epoch: 1 [40320/50000]\tLoss: 3.5201\tLR: 0.080307\n",
      "Training Epoch: 1 [40448/50000]\tLoss: 3.8530\tLR: 0.080563\n",
      "Training Epoch: 1 [40576/50000]\tLoss: 3.6502\tLR: 0.080818\n",
      "Training Epoch: 1 [40704/50000]\tLoss: 3.7097\tLR: 0.081074\n",
      "Training Epoch: 1 [40832/50000]\tLoss: 3.5946\tLR: 0.081330\n",
      "Training Epoch: 1 [40960/50000]\tLoss: 3.7507\tLR: 0.081586\n",
      "Training Epoch: 1 [41088/50000]\tLoss: 3.8346\tLR: 0.081841\n",
      "Training Epoch: 1 [41216/50000]\tLoss: 3.5667\tLR: 0.082097\n",
      "Training Epoch: 1 [41344/50000]\tLoss: 3.5280\tLR: 0.082353\n",
      "Training Epoch: 1 [41472/50000]\tLoss: 3.6147\tLR: 0.082609\n",
      "Training Epoch: 1 [41600/50000]\tLoss: 3.4894\tLR: 0.082864\n",
      "Training Epoch: 1 [41728/50000]\tLoss: 3.6209\tLR: 0.083120\n",
      "Training Epoch: 1 [41856/50000]\tLoss: 3.3788\tLR: 0.083376\n",
      "Training Epoch: 1 [41984/50000]\tLoss: 3.7290\tLR: 0.083632\n",
      "Training Epoch: 1 [42112/50000]\tLoss: 3.6404\tLR: 0.083887\n",
      "Training Epoch: 1 [42240/50000]\tLoss: 3.4748\tLR: 0.084143\n",
      "Training Epoch: 1 [42368/50000]\tLoss: 3.6622\tLR: 0.084399\n",
      "Training Epoch: 1 [42496/50000]\tLoss: 3.5606\tLR: 0.084655\n",
      "Training Epoch: 1 [42624/50000]\tLoss: 3.7729\tLR: 0.084910\n",
      "Training Epoch: 1 [42752/50000]\tLoss: 3.4422\tLR: 0.085166\n",
      "Training Epoch: 1 [42880/50000]\tLoss: 3.5148\tLR: 0.085422\n",
      "Training Epoch: 1 [43008/50000]\tLoss: 3.4798\tLR: 0.085678\n",
      "Training Epoch: 1 [43136/50000]\tLoss: 3.6587\tLR: 0.085934\n",
      "Training Epoch: 1 [43264/50000]\tLoss: 3.6867\tLR: 0.086189\n",
      "Training Epoch: 1 [43392/50000]\tLoss: 3.4594\tLR: 0.086445\n",
      "Training Epoch: 1 [43520/50000]\tLoss: 3.5230\tLR: 0.086701\n",
      "Training Epoch: 1 [43648/50000]\tLoss: 3.5088\tLR: 0.086957\n",
      "Training Epoch: 1 [43776/50000]\tLoss: 3.6915\tLR: 0.087212\n",
      "Training Epoch: 1 [43904/50000]\tLoss: 3.5611\tLR: 0.087468\n",
      "Training Epoch: 1 [44032/50000]\tLoss: 3.5651\tLR: 0.087724\n",
      "Training Epoch: 1 [44160/50000]\tLoss: 3.6367\tLR: 0.087980\n",
      "Training Epoch: 1 [44288/50000]\tLoss: 3.8016\tLR: 0.088235\n",
      "Training Epoch: 1 [44416/50000]\tLoss: 3.7344\tLR: 0.088491\n",
      "Training Epoch: 1 [44544/50000]\tLoss: 3.8626\tLR: 0.088747\n",
      "Training Epoch: 1 [44672/50000]\tLoss: 3.6295\tLR: 0.089003\n",
      "Training Epoch: 1 [44800/50000]\tLoss: 3.7721\tLR: 0.089258\n",
      "Training Epoch: 1 [44928/50000]\tLoss: 3.6247\tLR: 0.089514\n",
      "Training Epoch: 1 [45056/50000]\tLoss: 3.5339\tLR: 0.089770\n",
      "Training Epoch: 1 [45184/50000]\tLoss: 3.6680\tLR: 0.090026\n",
      "Training Epoch: 1 [45312/50000]\tLoss: 3.6034\tLR: 0.090281\n",
      "Training Epoch: 1 [45440/50000]\tLoss: 3.4277\tLR: 0.090537\n",
      "Training Epoch: 1 [45568/50000]\tLoss: 3.6341\tLR: 0.090793\n",
      "Training Epoch: 1 [45696/50000]\tLoss: 3.6461\tLR: 0.091049\n",
      "Training Epoch: 1 [45824/50000]\tLoss: 3.5599\tLR: 0.091304\n",
      "Training Epoch: 1 [45952/50000]\tLoss: 3.4817\tLR: 0.091560\n",
      "Training Epoch: 1 [46080/50000]\tLoss: 3.5602\tLR: 0.091816\n",
      "Training Epoch: 1 [46208/50000]\tLoss: 3.6364\tLR: 0.092072\n",
      "Training Epoch: 1 [46336/50000]\tLoss: 3.3520\tLR: 0.092327\n",
      "Training Epoch: 1 [46464/50000]\tLoss: 3.4604\tLR: 0.092583\n",
      "Training Epoch: 1 [46592/50000]\tLoss: 3.4799\tLR: 0.092839\n",
      "Training Epoch: 1 [46720/50000]\tLoss: 3.6628\tLR: 0.093095\n",
      "Training Epoch: 1 [46848/50000]\tLoss: 3.6647\tLR: 0.093350\n",
      "Training Epoch: 1 [46976/50000]\tLoss: 3.6767\tLR: 0.093606\n",
      "Training Epoch: 1 [47104/50000]\tLoss: 3.5723\tLR: 0.093862\n",
      "Training Epoch: 1 [47232/50000]\tLoss: 3.6088\tLR: 0.094118\n",
      "Training Epoch: 1 [47360/50000]\tLoss: 3.4408\tLR: 0.094373\n",
      "Training Epoch: 1 [47488/50000]\tLoss: 3.4945\tLR: 0.094629\n",
      "Training Epoch: 1 [47616/50000]\tLoss: 3.6847\tLR: 0.094885\n",
      "Training Epoch: 1 [47744/50000]\tLoss: 3.3746\tLR: 0.095141\n",
      "Training Epoch: 1 [47872/50000]\tLoss: 3.5401\tLR: 0.095396\n",
      "Training Epoch: 1 [48000/50000]\tLoss: 3.4741\tLR: 0.095652\n",
      "Training Epoch: 1 [48128/50000]\tLoss: 3.9272\tLR: 0.095908\n",
      "Training Epoch: 1 [48256/50000]\tLoss: 3.6859\tLR: 0.096164\n",
      "Training Epoch: 1 [48384/50000]\tLoss: 3.6854\tLR: 0.096419\n",
      "Training Epoch: 1 [48512/50000]\tLoss: 3.5735\tLR: 0.096675\n",
      "Training Epoch: 1 [48640/50000]\tLoss: 3.4197\tLR: 0.096931\n",
      "Training Epoch: 1 [48768/50000]\tLoss: 3.4898\tLR: 0.097187\n",
      "Training Epoch: 1 [48896/50000]\tLoss: 3.4354\tLR: 0.097442\n",
      "Training Epoch: 1 [49024/50000]\tLoss: 3.5832\tLR: 0.097698\n",
      "Training Epoch: 1 [49152/50000]\tLoss: 3.5104\tLR: 0.097954\n",
      "Training Epoch: 1 [49280/50000]\tLoss: 3.3393\tLR: 0.098210\n",
      "Training Epoch: 1 [49408/50000]\tLoss: 3.4934\tLR: 0.098465\n",
      "Training Epoch: 1 [49536/50000]\tLoss: 3.4270\tLR: 0.098721\n",
      "Training Epoch: 1 [49664/50000]\tLoss: 3.5278\tLR: 0.098977\n",
      "Training Epoch: 1 [49792/50000]\tLoss: 3.6553\tLR: 0.099233\n",
      "Training Epoch: 1 [49920/50000]\tLoss: 3.6611\tLR: 0.099488\n",
      "Training Epoch: 1 [50000/50000]\tLoss: 3.5501\tLR: 0.099744\n",
      "epoch 1 training time consumed: 491.37s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132781 KB |     854 MB |    1403 GB |    1403 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |    1399 GB |    1399 GB |\n",
      "|       from small pool |   10797 KB |      13 MB |       4 GB |       4 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132781 KB |     854 MB |    1403 GB |    1403 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |    1399 GB |    1399 GB |\n",
      "|       from small pool |   10797 KB |      13 MB |       4 GB |       4 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |    1380 GB |    1380 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |    1375 GB |    1375 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |       4 GB |       4 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     254    |     334    |  149217    |  148963    |\n",
      "|       from large pool |      24    |      65    |   63433    |   63409    |\n",
      "|       from small pool |     230    |     273    |   85784    |   85554    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     254    |     334    |  149217    |  148963    |\n",
      "|       from large pool |      24    |      65    |   63433    |   63409    |\n",
      "|       from small pool |     230    |     273    |   85784    |   85554    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      38    |      45    |   86291    |   86253    |\n",
      "|       from large pool |      10    |      23    |   30486    |   30476    |\n",
      "|       from small pool |      28    |      35    |   55805    |   55777    |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 1, Average loss: 0.0289, Accuracy: 0.1399, Time consumed:30.97s\n",
      "\n",
      "Training Epoch: 2 [128/50000]\tLoss: 3.3893\tLR: 0.100000\n",
      "Training Epoch: 2 [256/50000]\tLoss: 3.5129\tLR: 0.100256\n",
      "Training Epoch: 2 [384/50000]\tLoss: 3.6243\tLR: 0.100512\n",
      "Training Epoch: 2 [512/50000]\tLoss: 3.3488\tLR: 0.100767\n",
      "Training Epoch: 2 [640/50000]\tLoss: 3.3014\tLR: 0.101023\n",
      "Training Epoch: 2 [768/50000]\tLoss: 3.3868\tLR: 0.101279\n",
      "Training Epoch: 2 [896/50000]\tLoss: 3.3015\tLR: 0.101535\n",
      "Training Epoch: 2 [1024/50000]\tLoss: 3.3061\tLR: 0.101790\n",
      "Training Epoch: 2 [1152/50000]\tLoss: 3.6984\tLR: 0.102046\n",
      "Training Epoch: 2 [1280/50000]\tLoss: 3.3515\tLR: 0.102302\n",
      "Training Epoch: 2 [1408/50000]\tLoss: 3.3390\tLR: 0.102558\n",
      "Training Epoch: 2 [1536/50000]\tLoss: 3.5501\tLR: 0.102813\n",
      "Training Epoch: 2 [1664/50000]\tLoss: 3.6247\tLR: 0.103069\n",
      "Training Epoch: 2 [1792/50000]\tLoss: 3.8955\tLR: 0.103325\n",
      "Training Epoch: 2 [1920/50000]\tLoss: 3.4633\tLR: 0.103581\n",
      "Training Epoch: 2 [2048/50000]\tLoss: 3.5824\tLR: 0.103836\n",
      "Training Epoch: 2 [2176/50000]\tLoss: 3.5699\tLR: 0.104092\n",
      "Training Epoch: 2 [2304/50000]\tLoss: 3.5741\tLR: 0.104348\n",
      "Training Epoch: 2 [2432/50000]\tLoss: 3.9161\tLR: 0.104604\n",
      "Training Epoch: 2 [2560/50000]\tLoss: 3.4691\tLR: 0.104859\n",
      "Training Epoch: 2 [2688/50000]\tLoss: 3.5640\tLR: 0.105115\n",
      "Training Epoch: 2 [2816/50000]\tLoss: 3.5729\tLR: 0.105371\n",
      "Training Epoch: 2 [2944/50000]\tLoss: 3.4661\tLR: 0.105627\n",
      "Training Epoch: 2 [3072/50000]\tLoss: 3.4850\tLR: 0.105882\n",
      "Training Epoch: 2 [3200/50000]\tLoss: 3.3014\tLR: 0.106138\n",
      "Training Epoch: 2 [3328/50000]\tLoss: 3.4987\tLR: 0.106394\n",
      "Training Epoch: 2 [3456/50000]\tLoss: 3.4609\tLR: 0.106650\n",
      "Training Epoch: 2 [3584/50000]\tLoss: 3.5663\tLR: 0.106905\n",
      "Training Epoch: 2 [3712/50000]\tLoss: 3.4013\tLR: 0.107161\n",
      "Training Epoch: 2 [3840/50000]\tLoss: 3.3690\tLR: 0.107417\n",
      "Training Epoch: 2 [3968/50000]\tLoss: 3.4334\tLR: 0.107673\n",
      "Training Epoch: 2 [4096/50000]\tLoss: 3.2142\tLR: 0.107928\n",
      "Training Epoch: 2 [4224/50000]\tLoss: 3.4110\tLR: 0.108184\n",
      "Training Epoch: 2 [4352/50000]\tLoss: 3.4610\tLR: 0.108440\n",
      "Training Epoch: 2 [4480/50000]\tLoss: 3.5257\tLR: 0.108696\n",
      "Training Epoch: 2 [4608/50000]\tLoss: 3.3225\tLR: 0.108951\n",
      "Training Epoch: 2 [4736/50000]\tLoss: 3.6591\tLR: 0.109207\n",
      "Training Epoch: 2 [4864/50000]\tLoss: 3.3040\tLR: 0.109463\n",
      "Training Epoch: 2 [4992/50000]\tLoss: 3.4309\tLR: 0.109719\n",
      "Training Epoch: 2 [5120/50000]\tLoss: 3.5238\tLR: 0.109974\n",
      "Training Epoch: 2 [5248/50000]\tLoss: 3.4264\tLR: 0.110230\n",
      "Training Epoch: 2 [5376/50000]\tLoss: 3.4745\tLR: 0.110486\n",
      "Training Epoch: 2 [5504/50000]\tLoss: 3.4880\tLR: 0.110742\n",
      "Training Epoch: 2 [5632/50000]\tLoss: 3.4137\tLR: 0.110997\n",
      "Training Epoch: 2 [5760/50000]\tLoss: 3.5448\tLR: 0.111253\n",
      "Training Epoch: 2 [5888/50000]\tLoss: 3.5970\tLR: 0.111509\n",
      "Training Epoch: 2 [6016/50000]\tLoss: 3.3765\tLR: 0.111765\n",
      "Training Epoch: 2 [6144/50000]\tLoss: 3.5252\tLR: 0.112020\n",
      "Training Epoch: 2 [6272/50000]\tLoss: 3.4938\tLR: 0.112276\n",
      "Training Epoch: 2 [6400/50000]\tLoss: 3.5439\tLR: 0.112532\n",
      "Training Epoch: 2 [6528/50000]\tLoss: 3.4542\tLR: 0.112788\n",
      "Training Epoch: 2 [6656/50000]\tLoss: 3.6101\tLR: 0.113043\n",
      "Training Epoch: 2 [6784/50000]\tLoss: 3.3449\tLR: 0.113299\n",
      "Training Epoch: 2 [6912/50000]\tLoss: 3.4056\tLR: 0.113555\n",
      "Training Epoch: 2 [7040/50000]\tLoss: 3.5320\tLR: 0.113811\n",
      "Training Epoch: 2 [7168/50000]\tLoss: 3.4629\tLR: 0.114066\n",
      "Training Epoch: 2 [7296/50000]\tLoss: 3.3777\tLR: 0.114322\n",
      "Training Epoch: 2 [7424/50000]\tLoss: 3.4653\tLR: 0.114578\n",
      "Training Epoch: 2 [7552/50000]\tLoss: 3.6356\tLR: 0.114834\n",
      "Training Epoch: 2 [7680/50000]\tLoss: 3.4425\tLR: 0.115090\n",
      "Training Epoch: 2 [7808/50000]\tLoss: 3.4538\tLR: 0.115345\n",
      "Training Epoch: 2 [7936/50000]\tLoss: 3.4578\tLR: 0.115601\n",
      "Training Epoch: 2 [8064/50000]\tLoss: 3.4597\tLR: 0.115857\n",
      "Training Epoch: 2 [8192/50000]\tLoss: 3.5400\tLR: 0.116113\n",
      "Training Epoch: 2 [8320/50000]\tLoss: 3.6272\tLR: 0.116368\n",
      "Training Epoch: 2 [8448/50000]\tLoss: 3.6194\tLR: 0.116624\n",
      "Training Epoch: 2 [8576/50000]\tLoss: 3.3594\tLR: 0.116880\n",
      "Training Epoch: 2 [8704/50000]\tLoss: 3.7912\tLR: 0.117136\n",
      "Training Epoch: 2 [8832/50000]\tLoss: 3.5675\tLR: 0.117391\n",
      "Training Epoch: 2 [8960/50000]\tLoss: 3.5059\tLR: 0.117647\n",
      "Training Epoch: 2 [9088/50000]\tLoss: 3.6138\tLR: 0.117903\n",
      "Training Epoch: 2 [9216/50000]\tLoss: 3.3798\tLR: 0.118159\n",
      "Training Epoch: 2 [9344/50000]\tLoss: 3.3384\tLR: 0.118414\n",
      "Training Epoch: 2 [9472/50000]\tLoss: 3.3187\tLR: 0.118670\n",
      "Training Epoch: 2 [9600/50000]\tLoss: 3.5427\tLR: 0.118926\n",
      "Training Epoch: 2 [9728/50000]\tLoss: 3.5523\tLR: 0.119182\n",
      "Training Epoch: 2 [9856/50000]\tLoss: 3.3431\tLR: 0.119437\n",
      "Training Epoch: 2 [9984/50000]\tLoss: 3.2516\tLR: 0.119693\n",
      "Training Epoch: 2 [10112/50000]\tLoss: 3.5327\tLR: 0.119949\n",
      "Training Epoch: 2 [10240/50000]\tLoss: 3.1253\tLR: 0.120205\n",
      "Training Epoch: 2 [10368/50000]\tLoss: 3.2616\tLR: 0.120460\n",
      "Training Epoch: 2 [10496/50000]\tLoss: 3.5055\tLR: 0.120716\n",
      "Training Epoch: 2 [10624/50000]\tLoss: 3.5008\tLR: 0.120972\n",
      "Training Epoch: 2 [10752/50000]\tLoss: 3.5251\tLR: 0.121228\n",
      "Training Epoch: 2 [10880/50000]\tLoss: 3.1868\tLR: 0.121483\n",
      "Training Epoch: 2 [11008/50000]\tLoss: 3.4578\tLR: 0.121739\n",
      "Training Epoch: 2 [11136/50000]\tLoss: 3.3371\tLR: 0.121995\n",
      "Training Epoch: 2 [11264/50000]\tLoss: 3.3601\tLR: 0.122251\n",
      "Training Epoch: 2 [11392/50000]\tLoss: 3.4970\tLR: 0.122506\n",
      "Training Epoch: 2 [11520/50000]\tLoss: 3.3402\tLR: 0.122762\n",
      "Training Epoch: 2 [11648/50000]\tLoss: 3.4320\tLR: 0.123018\n",
      "Training Epoch: 2 [11776/50000]\tLoss: 3.6268\tLR: 0.123274\n",
      "Training Epoch: 2 [11904/50000]\tLoss: 3.3661\tLR: 0.123529\n",
      "Training Epoch: 2 [12032/50000]\tLoss: 3.6264\tLR: 0.123785\n",
      "Training Epoch: 2 [12160/50000]\tLoss: 3.4793\tLR: 0.124041\n",
      "Training Epoch: 2 [12288/50000]\tLoss: 3.4705\tLR: 0.124297\n",
      "Training Epoch: 2 [12416/50000]\tLoss: 3.3326\tLR: 0.124552\n",
      "Training Epoch: 2 [12544/50000]\tLoss: 3.3579\tLR: 0.124808\n",
      "Training Epoch: 2 [12672/50000]\tLoss: 3.5213\tLR: 0.125064\n",
      "Training Epoch: 2 [12800/50000]\tLoss: 3.4085\tLR: 0.125320\n",
      "Training Epoch: 2 [12928/50000]\tLoss: 3.4002\tLR: 0.125575\n",
      "Training Epoch: 2 [13056/50000]\tLoss: 3.1717\tLR: 0.125831\n",
      "Training Epoch: 2 [13184/50000]\tLoss: 3.4622\tLR: 0.126087\n",
      "Training Epoch: 2 [13312/50000]\tLoss: 3.5466\tLR: 0.126343\n",
      "Training Epoch: 2 [13440/50000]\tLoss: 3.3883\tLR: 0.126598\n",
      "Training Epoch: 2 [13568/50000]\tLoss: 3.5085\tLR: 0.126854\n",
      "Training Epoch: 2 [13696/50000]\tLoss: 3.7002\tLR: 0.127110\n",
      "Training Epoch: 2 [13824/50000]\tLoss: 3.4412\tLR: 0.127366\n",
      "Training Epoch: 2 [13952/50000]\tLoss: 3.4217\tLR: 0.127621\n",
      "Training Epoch: 2 [14080/50000]\tLoss: 3.6353\tLR: 0.127877\n",
      "Training Epoch: 2 [14208/50000]\tLoss: 3.2799\tLR: 0.128133\n",
      "Training Epoch: 2 [14336/50000]\tLoss: 3.3576\tLR: 0.128389\n",
      "Training Epoch: 2 [14464/50000]\tLoss: 3.3384\tLR: 0.128645\n",
      "Training Epoch: 2 [14592/50000]\tLoss: 3.3937\tLR: 0.128900\n",
      "Training Epoch: 2 [14720/50000]\tLoss: 3.3428\tLR: 0.129156\n",
      "Training Epoch: 2 [14848/50000]\tLoss: 3.3971\tLR: 0.129412\n",
      "Training Epoch: 2 [14976/50000]\tLoss: 3.5456\tLR: 0.129668\n",
      "Training Epoch: 2 [15104/50000]\tLoss: 3.5436\tLR: 0.129923\n",
      "Training Epoch: 2 [15232/50000]\tLoss: 3.5482\tLR: 0.130179\n",
      "Training Epoch: 2 [15360/50000]\tLoss: 3.4154\tLR: 0.130435\n",
      "Training Epoch: 2 [15488/50000]\tLoss: 3.4528\tLR: 0.130691\n",
      "Training Epoch: 2 [15616/50000]\tLoss: 3.2771\tLR: 0.130946\n",
      "Training Epoch: 2 [15744/50000]\tLoss: 3.4425\tLR: 0.131202\n",
      "Training Epoch: 2 [15872/50000]\tLoss: 3.3095\tLR: 0.131458\n",
      "Training Epoch: 2 [16000/50000]\tLoss: 3.2177\tLR: 0.131714\n",
      "Training Epoch: 2 [16128/50000]\tLoss: 3.3704\tLR: 0.131969\n",
      "Training Epoch: 2 [16256/50000]\tLoss: 3.3620\tLR: 0.132225\n",
      "Training Epoch: 2 [16384/50000]\tLoss: 3.5468\tLR: 0.132481\n",
      "Training Epoch: 2 [16512/50000]\tLoss: 3.4407\tLR: 0.132737\n",
      "Training Epoch: 2 [16640/50000]\tLoss: 3.6730\tLR: 0.132992\n",
      "Training Epoch: 2 [16768/50000]\tLoss: 3.4639\tLR: 0.133248\n",
      "Training Epoch: 2 [16896/50000]\tLoss: 3.2095\tLR: 0.133504\n",
      "Training Epoch: 2 [17024/50000]\tLoss: 3.3595\tLR: 0.133760\n",
      "Training Epoch: 2 [17152/50000]\tLoss: 3.6634\tLR: 0.134015\n",
      "Training Epoch: 2 [17280/50000]\tLoss: 3.6350\tLR: 0.134271\n",
      "Training Epoch: 2 [17408/50000]\tLoss: 3.4373\tLR: 0.134527\n",
      "Training Epoch: 2 [17536/50000]\tLoss: 3.3230\tLR: 0.134783\n",
      "Training Epoch: 2 [17664/50000]\tLoss: 3.3317\tLR: 0.135038\n",
      "Training Epoch: 2 [17792/50000]\tLoss: 3.3425\tLR: 0.135294\n",
      "Training Epoch: 2 [17920/50000]\tLoss: 3.3917\tLR: 0.135550\n",
      "Training Epoch: 2 [18048/50000]\tLoss: 3.6254\tLR: 0.135806\n",
      "Training Epoch: 2 [18176/50000]\tLoss: 3.2961\tLR: 0.136061\n",
      "Training Epoch: 2 [18304/50000]\tLoss: 3.6206\tLR: 0.136317\n",
      "Training Epoch: 2 [18432/50000]\tLoss: 3.3508\tLR: 0.136573\n",
      "Training Epoch: 2 [18560/50000]\tLoss: 3.4360\tLR: 0.136829\n",
      "Training Epoch: 2 [18688/50000]\tLoss: 3.3719\tLR: 0.137084\n",
      "Training Epoch: 2 [18816/50000]\tLoss: 3.4977\tLR: 0.137340\n",
      "Training Epoch: 2 [18944/50000]\tLoss: 3.5331\tLR: 0.137596\n",
      "Training Epoch: 2 [19072/50000]\tLoss: 3.3965\tLR: 0.137852\n",
      "Training Epoch: 2 [19200/50000]\tLoss: 3.2975\tLR: 0.138107\n",
      "Training Epoch: 2 [19328/50000]\tLoss: 3.2266\tLR: 0.138363\n",
      "Training Epoch: 2 [19456/50000]\tLoss: 3.3966\tLR: 0.138619\n",
      "Training Epoch: 2 [19584/50000]\tLoss: 3.1890\tLR: 0.138875\n",
      "Training Epoch: 2 [19712/50000]\tLoss: 3.1647\tLR: 0.139130\n",
      "Training Epoch: 2 [19840/50000]\tLoss: 3.1223\tLR: 0.139386\n",
      "Training Epoch: 2 [19968/50000]\tLoss: 3.3392\tLR: 0.139642\n",
      "Training Epoch: 2 [20096/50000]\tLoss: 3.3851\tLR: 0.139898\n",
      "Training Epoch: 2 [20224/50000]\tLoss: 3.4749\tLR: 0.140153\n",
      "Training Epoch: 2 [20352/50000]\tLoss: 3.3289\tLR: 0.140409\n",
      "Training Epoch: 2 [20480/50000]\tLoss: 3.2946\tLR: 0.140665\n",
      "Training Epoch: 2 [20608/50000]\tLoss: 3.1756\tLR: 0.140921\n",
      "Training Epoch: 2 [20736/50000]\tLoss: 3.4413\tLR: 0.141176\n",
      "Training Epoch: 2 [20864/50000]\tLoss: 3.4255\tLR: 0.141432\n",
      "Training Epoch: 2 [20992/50000]\tLoss: 3.2734\tLR: 0.141688\n",
      "Training Epoch: 2 [21120/50000]\tLoss: 3.2978\tLR: 0.141944\n",
      "Training Epoch: 2 [21248/50000]\tLoss: 3.2473\tLR: 0.142199\n",
      "Training Epoch: 2 [21376/50000]\tLoss: 3.0558\tLR: 0.142455\n",
      "Training Epoch: 2 [21504/50000]\tLoss: 3.4135\tLR: 0.142711\n",
      "Training Epoch: 2 [21632/50000]\tLoss: 3.5017\tLR: 0.142967\n",
      "Training Epoch: 2 [21760/50000]\tLoss: 3.4229\tLR: 0.143223\n",
      "Training Epoch: 2 [21888/50000]\tLoss: 3.2861\tLR: 0.143478\n",
      "Training Epoch: 2 [22016/50000]\tLoss: 3.6464\tLR: 0.143734\n",
      "Training Epoch: 2 [22144/50000]\tLoss: 3.2526\tLR: 0.143990\n",
      "Training Epoch: 2 [22272/50000]\tLoss: 3.4417\tLR: 0.144246\n",
      "Training Epoch: 2 [22400/50000]\tLoss: 3.3982\tLR: 0.144501\n",
      "Training Epoch: 2 [22528/50000]\tLoss: 3.4788\tLR: 0.144757\n",
      "Training Epoch: 2 [22656/50000]\tLoss: 3.4330\tLR: 0.145013\n",
      "Training Epoch: 2 [22784/50000]\tLoss: 3.4674\tLR: 0.145269\n",
      "Training Epoch: 2 [22912/50000]\tLoss: 3.4990\tLR: 0.145524\n",
      "Training Epoch: 2 [23040/50000]\tLoss: 3.3983\tLR: 0.145780\n",
      "Training Epoch: 2 [23168/50000]\tLoss: 3.2405\tLR: 0.146036\n",
      "Training Epoch: 2 [23296/50000]\tLoss: 3.3503\tLR: 0.146292\n",
      "Training Epoch: 2 [23424/50000]\tLoss: 3.3683\tLR: 0.146547\n",
      "Training Epoch: 2 [23552/50000]\tLoss: 3.5512\tLR: 0.146803\n",
      "Training Epoch: 2 [23680/50000]\tLoss: 3.4614\tLR: 0.147059\n",
      "Training Epoch: 2 [23808/50000]\tLoss: 3.4253\tLR: 0.147315\n",
      "Training Epoch: 2 [23936/50000]\tLoss: 3.3121\tLR: 0.147570\n",
      "Training Epoch: 2 [24064/50000]\tLoss: 3.3653\tLR: 0.147826\n",
      "Training Epoch: 2 [24192/50000]\tLoss: 3.3378\tLR: 0.148082\n",
      "Training Epoch: 2 [24320/50000]\tLoss: 3.3262\tLR: 0.148338\n",
      "Training Epoch: 2 [24448/50000]\tLoss: 3.4688\tLR: 0.148593\n",
      "Training Epoch: 2 [24576/50000]\tLoss: 3.4211\tLR: 0.148849\n",
      "Training Epoch: 2 [24704/50000]\tLoss: 3.2808\tLR: 0.149105\n",
      "Training Epoch: 2 [24832/50000]\tLoss: 3.2458\tLR: 0.149361\n",
      "Training Epoch: 2 [24960/50000]\tLoss: 3.4074\tLR: 0.149616\n",
      "Training Epoch: 2 [25088/50000]\tLoss: 3.3594\tLR: 0.149872\n",
      "Training Epoch: 2 [25216/50000]\tLoss: 3.3990\tLR: 0.150128\n",
      "Training Epoch: 2 [25344/50000]\tLoss: 3.3562\tLR: 0.150384\n",
      "Training Epoch: 2 [25472/50000]\tLoss: 3.3350\tLR: 0.150639\n",
      "Training Epoch: 2 [25600/50000]\tLoss: 3.2034\tLR: 0.150895\n",
      "Training Epoch: 2 [25728/50000]\tLoss: 3.3184\tLR: 0.151151\n",
      "Training Epoch: 2 [25856/50000]\tLoss: 3.3869\tLR: 0.151407\n",
      "Training Epoch: 2 [25984/50000]\tLoss: 3.3325\tLR: 0.151662\n",
      "Training Epoch: 2 [26112/50000]\tLoss: 3.3476\tLR: 0.151918\n",
      "Training Epoch: 2 [26240/50000]\tLoss: 3.1985\tLR: 0.152174\n",
      "Training Epoch: 2 [26368/50000]\tLoss: 3.1345\tLR: 0.152430\n",
      "Training Epoch: 2 [26496/50000]\tLoss: 3.3281\tLR: 0.152685\n",
      "Training Epoch: 2 [26624/50000]\tLoss: 3.3222\tLR: 0.152941\n",
      "Training Epoch: 2 [26752/50000]\tLoss: 3.5878\tLR: 0.153197\n",
      "Training Epoch: 2 [26880/50000]\tLoss: 3.1757\tLR: 0.153453\n",
      "Training Epoch: 2 [27008/50000]\tLoss: 3.2509\tLR: 0.153708\n",
      "Training Epoch: 2 [27136/50000]\tLoss: 3.2230\tLR: 0.153964\n",
      "Training Epoch: 2 [27264/50000]\tLoss: 3.4066\tLR: 0.154220\n",
      "Training Epoch: 2 [27392/50000]\tLoss: 3.4945\tLR: 0.154476\n",
      "Training Epoch: 2 [27520/50000]\tLoss: 3.2258\tLR: 0.154731\n",
      "Training Epoch: 2 [27648/50000]\tLoss: 3.3527\tLR: 0.154987\n",
      "Training Epoch: 2 [27776/50000]\tLoss: 3.3706\tLR: 0.155243\n",
      "Training Epoch: 2 [27904/50000]\tLoss: 3.3773\tLR: 0.155499\n",
      "Training Epoch: 2 [28032/50000]\tLoss: 3.3263\tLR: 0.155754\n",
      "Training Epoch: 2 [28160/50000]\tLoss: 3.4117\tLR: 0.156010\n",
      "Training Epoch: 2 [28288/50000]\tLoss: 3.5047\tLR: 0.156266\n",
      "Training Epoch: 2 [28416/50000]\tLoss: 3.2083\tLR: 0.156522\n",
      "Training Epoch: 2 [28544/50000]\tLoss: 3.3690\tLR: 0.156777\n",
      "Training Epoch: 2 [28672/50000]\tLoss: 3.1635\tLR: 0.157033\n",
      "Training Epoch: 2 [28800/50000]\tLoss: 3.2409\tLR: 0.157289\n",
      "Training Epoch: 2 [28928/50000]\tLoss: 3.4564\tLR: 0.157545\n",
      "Training Epoch: 2 [29056/50000]\tLoss: 3.3881\tLR: 0.157801\n",
      "Training Epoch: 2 [29184/50000]\tLoss: 3.3879\tLR: 0.158056\n",
      "Training Epoch: 2 [29312/50000]\tLoss: 3.1499\tLR: 0.158312\n",
      "Training Epoch: 2 [29440/50000]\tLoss: 3.3712\tLR: 0.158568\n",
      "Training Epoch: 2 [29568/50000]\tLoss: 3.1526\tLR: 0.158824\n",
      "Training Epoch: 2 [29696/50000]\tLoss: 3.3399\tLR: 0.159079\n",
      "Training Epoch: 2 [29824/50000]\tLoss: 3.2167\tLR: 0.159335\n",
      "Training Epoch: 2 [29952/50000]\tLoss: 3.3164\tLR: 0.159591\n",
      "Training Epoch: 2 [30080/50000]\tLoss: 3.1485\tLR: 0.159847\n",
      "Training Epoch: 2 [30208/50000]\tLoss: 3.3101\tLR: 0.160102\n",
      "Training Epoch: 2 [30336/50000]\tLoss: 3.3554\tLR: 0.160358\n",
      "Training Epoch: 2 [30464/50000]\tLoss: 3.1654\tLR: 0.160614\n",
      "Training Epoch: 2 [30592/50000]\tLoss: 3.4526\tLR: 0.160870\n",
      "Training Epoch: 2 [30720/50000]\tLoss: 3.3222\tLR: 0.161125\n",
      "Training Epoch: 2 [30848/50000]\tLoss: 3.3349\tLR: 0.161381\n",
      "Training Epoch: 2 [30976/50000]\tLoss: 3.3240\tLR: 0.161637\n",
      "Training Epoch: 2 [31104/50000]\tLoss: 3.2662\tLR: 0.161893\n",
      "Training Epoch: 2 [31232/50000]\tLoss: 3.5138\tLR: 0.162148\n",
      "Training Epoch: 2 [31360/50000]\tLoss: 3.3970\tLR: 0.162404\n",
      "Training Epoch: 2 [31488/50000]\tLoss: 3.4104\tLR: 0.162660\n",
      "Training Epoch: 2 [31616/50000]\tLoss: 3.2310\tLR: 0.162916\n",
      "Training Epoch: 2 [31744/50000]\tLoss: 3.3904\tLR: 0.163171\n",
      "Training Epoch: 2 [31872/50000]\tLoss: 3.4528\tLR: 0.163427\n",
      "Training Epoch: 2 [32000/50000]\tLoss: 3.2542\tLR: 0.163683\n",
      "Training Epoch: 2 [32128/50000]\tLoss: 3.1381\tLR: 0.163939\n",
      "Training Epoch: 2 [32256/50000]\tLoss: 3.5569\tLR: 0.164194\n",
      "Training Epoch: 2 [32384/50000]\tLoss: 3.2410\tLR: 0.164450\n",
      "Training Epoch: 2 [32512/50000]\tLoss: 3.2578\tLR: 0.164706\n",
      "Training Epoch: 2 [32640/50000]\tLoss: 3.4890\tLR: 0.164962\n",
      "Training Epoch: 2 [32768/50000]\tLoss: 3.0526\tLR: 0.165217\n",
      "Training Epoch: 2 [32896/50000]\tLoss: 3.5109\tLR: 0.165473\n",
      "Training Epoch: 2 [33024/50000]\tLoss: 3.4048\tLR: 0.165729\n",
      "Training Epoch: 2 [33152/50000]\tLoss: 3.1418\tLR: 0.165985\n",
      "Training Epoch: 2 [33280/50000]\tLoss: 3.2894\tLR: 0.166240\n",
      "Training Epoch: 2 [33408/50000]\tLoss: 3.3810\tLR: 0.166496\n",
      "Training Epoch: 2 [33536/50000]\tLoss: 3.3954\tLR: 0.166752\n",
      "Training Epoch: 2 [33664/50000]\tLoss: 3.3988\tLR: 0.167008\n",
      "Training Epoch: 2 [33792/50000]\tLoss: 3.1359\tLR: 0.167263\n",
      "Training Epoch: 2 [33920/50000]\tLoss: 3.2653\tLR: 0.167519\n",
      "Training Epoch: 2 [34048/50000]\tLoss: 3.2173\tLR: 0.167775\n",
      "Training Epoch: 2 [34176/50000]\tLoss: 3.2412\tLR: 0.168031\n",
      "Training Epoch: 2 [34304/50000]\tLoss: 3.4196\tLR: 0.168286\n",
      "Training Epoch: 2 [34432/50000]\tLoss: 3.4093\tLR: 0.168542\n",
      "Training Epoch: 2 [34560/50000]\tLoss: 3.2912\tLR: 0.168798\n",
      "Training Epoch: 2 [34688/50000]\tLoss: 3.1539\tLR: 0.169054\n",
      "Training Epoch: 2 [34816/50000]\tLoss: 3.0013\tLR: 0.169309\n",
      "Training Epoch: 2 [34944/50000]\tLoss: 3.1084\tLR: 0.169565\n",
      "Training Epoch: 2 [35072/50000]\tLoss: 3.1727\tLR: 0.169821\n",
      "Training Epoch: 2 [35200/50000]\tLoss: 3.4364\tLR: 0.170077\n",
      "Training Epoch: 2 [35328/50000]\tLoss: 3.3367\tLR: 0.170332\n",
      "Training Epoch: 2 [35456/50000]\tLoss: 3.1571\tLR: 0.170588\n",
      "Training Epoch: 2 [35584/50000]\tLoss: 3.2413\tLR: 0.170844\n",
      "Training Epoch: 2 [35712/50000]\tLoss: 3.1022\tLR: 0.171100\n",
      "Training Epoch: 2 [35840/50000]\tLoss: 3.2290\tLR: 0.171355\n",
      "Training Epoch: 2 [35968/50000]\tLoss: 3.3099\tLR: 0.171611\n",
      "Training Epoch: 2 [36096/50000]\tLoss: 3.2372\tLR: 0.171867\n",
      "Training Epoch: 2 [36224/50000]\tLoss: 3.0768\tLR: 0.172123\n",
      "Training Epoch: 2 [36352/50000]\tLoss: 3.3379\tLR: 0.172379\n",
      "Training Epoch: 2 [36480/50000]\tLoss: 3.3923\tLR: 0.172634\n",
      "Training Epoch: 2 [36608/50000]\tLoss: 3.2144\tLR: 0.172890\n",
      "Training Epoch: 2 [36736/50000]\tLoss: 3.5108\tLR: 0.173146\n",
      "Training Epoch: 2 [36864/50000]\tLoss: 3.2262\tLR: 0.173402\n",
      "Training Epoch: 2 [36992/50000]\tLoss: 3.3998\tLR: 0.173657\n",
      "Training Epoch: 2 [37120/50000]\tLoss: 3.1451\tLR: 0.173913\n",
      "Training Epoch: 2 [37248/50000]\tLoss: 3.5939\tLR: 0.174169\n",
      "Training Epoch: 2 [37376/50000]\tLoss: 3.0274\tLR: 0.174425\n",
      "Training Epoch: 2 [37504/50000]\tLoss: 3.4478\tLR: 0.174680\n",
      "Training Epoch: 2 [37632/50000]\tLoss: 3.2961\tLR: 0.174936\n",
      "Training Epoch: 2 [37760/50000]\tLoss: 3.4525\tLR: 0.175192\n",
      "Training Epoch: 2 [37888/50000]\tLoss: 3.5270\tLR: 0.175448\n",
      "Training Epoch: 2 [38016/50000]\tLoss: 3.3909\tLR: 0.175703\n",
      "Training Epoch: 2 [38144/50000]\tLoss: 3.2158\tLR: 0.175959\n",
      "Training Epoch: 2 [38272/50000]\tLoss: 3.2558\tLR: 0.176215\n",
      "Training Epoch: 2 [38400/50000]\tLoss: 3.0256\tLR: 0.176471\n",
      "Training Epoch: 2 [38528/50000]\tLoss: 3.2247\tLR: 0.176726\n",
      "Training Epoch: 2 [38656/50000]\tLoss: 3.0375\tLR: 0.176982\n",
      "Training Epoch: 2 [38784/50000]\tLoss: 3.4338\tLR: 0.177238\n",
      "Training Epoch: 2 [38912/50000]\tLoss: 3.4077\tLR: 0.177494\n",
      "Training Epoch: 2 [39040/50000]\tLoss: 3.2801\tLR: 0.177749\n",
      "Training Epoch: 2 [39168/50000]\tLoss: 2.8874\tLR: 0.178005\n",
      "Training Epoch: 2 [39296/50000]\tLoss: 3.1458\tLR: 0.178261\n",
      "Training Epoch: 2 [39424/50000]\tLoss: 2.9575\tLR: 0.178517\n",
      "Training Epoch: 2 [39552/50000]\tLoss: 3.3181\tLR: 0.178772\n",
      "Training Epoch: 2 [39680/50000]\tLoss: 3.1769\tLR: 0.179028\n",
      "Training Epoch: 2 [39808/50000]\tLoss: 3.2802\tLR: 0.179284\n",
      "Training Epoch: 2 [39936/50000]\tLoss: 3.2494\tLR: 0.179540\n",
      "Training Epoch: 2 [40064/50000]\tLoss: 3.1807\tLR: 0.179795\n",
      "Training Epoch: 2 [40192/50000]\tLoss: 3.2471\tLR: 0.180051\n",
      "Training Epoch: 2 [40320/50000]\tLoss: 3.1359\tLR: 0.180307\n",
      "Training Epoch: 2 [40448/50000]\tLoss: 3.0677\tLR: 0.180563\n",
      "Training Epoch: 2 [40576/50000]\tLoss: 3.1059\tLR: 0.180818\n",
      "Training Epoch: 2 [40704/50000]\tLoss: 3.0310\tLR: 0.181074\n",
      "Training Epoch: 2 [40832/50000]\tLoss: 3.3205\tLR: 0.181330\n",
      "Training Epoch: 2 [40960/50000]\tLoss: 3.1021\tLR: 0.181586\n",
      "Training Epoch: 2 [41088/50000]\tLoss: 3.1450\tLR: 0.181841\n",
      "Training Epoch: 2 [41216/50000]\tLoss: 2.8665\tLR: 0.182097\n",
      "Training Epoch: 2 [41344/50000]\tLoss: 3.3841\tLR: 0.182353\n",
      "Training Epoch: 2 [41472/50000]\tLoss: 2.9390\tLR: 0.182609\n",
      "Training Epoch: 2 [41600/50000]\tLoss: 3.1760\tLR: 0.182864\n",
      "Training Epoch: 2 [41728/50000]\tLoss: 3.0374\tLR: 0.183120\n",
      "Training Epoch: 2 [41856/50000]\tLoss: 3.0730\tLR: 0.183376\n",
      "Training Epoch: 2 [41984/50000]\tLoss: 3.0977\tLR: 0.183632\n",
      "Training Epoch: 2 [42112/50000]\tLoss: 3.0016\tLR: 0.183887\n",
      "Training Epoch: 2 [42240/50000]\tLoss: 3.0981\tLR: 0.184143\n",
      "Training Epoch: 2 [42368/50000]\tLoss: 3.0845\tLR: 0.184399\n",
      "Training Epoch: 2 [42496/50000]\tLoss: 3.2411\tLR: 0.184655\n",
      "Training Epoch: 2 [42624/50000]\tLoss: 3.0624\tLR: 0.184910\n",
      "Training Epoch: 2 [42752/50000]\tLoss: 3.4004\tLR: 0.185166\n",
      "Training Epoch: 2 [42880/50000]\tLoss: 2.9783\tLR: 0.185422\n",
      "Training Epoch: 2 [43008/50000]\tLoss: 3.0482\tLR: 0.185678\n",
      "Training Epoch: 2 [43136/50000]\tLoss: 2.9618\tLR: 0.185934\n",
      "Training Epoch: 2 [43264/50000]\tLoss: 3.1447\tLR: 0.186189\n",
      "Training Epoch: 2 [43392/50000]\tLoss: 3.3591\tLR: 0.186445\n",
      "Training Epoch: 2 [43520/50000]\tLoss: 3.0759\tLR: 0.186701\n",
      "Training Epoch: 2 [43648/50000]\tLoss: 3.1434\tLR: 0.186957\n",
      "Training Epoch: 2 [43776/50000]\tLoss: 3.3108\tLR: 0.187212\n",
      "Training Epoch: 2 [43904/50000]\tLoss: 2.8718\tLR: 0.187468\n",
      "Training Epoch: 2 [44032/50000]\tLoss: 2.9311\tLR: 0.187724\n",
      "Training Epoch: 2 [44160/50000]\tLoss: 3.1472\tLR: 0.187980\n",
      "Training Epoch: 2 [44288/50000]\tLoss: 3.1164\tLR: 0.188235\n",
      "Training Epoch: 2 [44416/50000]\tLoss: 3.1702\tLR: 0.188491\n",
      "Training Epoch: 2 [44544/50000]\tLoss: 3.1529\tLR: 0.188747\n",
      "Training Epoch: 2 [44672/50000]\tLoss: 3.0617\tLR: 0.189003\n",
      "Training Epoch: 2 [44800/50000]\tLoss: 3.1576\tLR: 0.189258\n",
      "Training Epoch: 2 [44928/50000]\tLoss: 3.2768\tLR: 0.189514\n",
      "Training Epoch: 2 [45056/50000]\tLoss: 3.1356\tLR: 0.189770\n",
      "Training Epoch: 2 [45184/50000]\tLoss: 3.3128\tLR: 0.190026\n",
      "Training Epoch: 2 [45312/50000]\tLoss: 3.1544\tLR: 0.190281\n",
      "Training Epoch: 2 [45440/50000]\tLoss: 3.0066\tLR: 0.190537\n",
      "Training Epoch: 2 [45568/50000]\tLoss: 2.8933\tLR: 0.190793\n",
      "Training Epoch: 2 [45696/50000]\tLoss: 3.0279\tLR: 0.191049\n",
      "Training Epoch: 2 [45824/50000]\tLoss: 3.2463\tLR: 0.191304\n",
      "Training Epoch: 2 [45952/50000]\tLoss: 3.0887\tLR: 0.191560\n",
      "Training Epoch: 2 [46080/50000]\tLoss: 3.2549\tLR: 0.191816\n",
      "Training Epoch: 2 [46208/50000]\tLoss: 3.2633\tLR: 0.192072\n",
      "Training Epoch: 2 [46336/50000]\tLoss: 3.0774\tLR: 0.192327\n",
      "Training Epoch: 2 [46464/50000]\tLoss: 2.8935\tLR: 0.192583\n",
      "Training Epoch: 2 [46592/50000]\tLoss: 3.2028\tLR: 0.192839\n",
      "Training Epoch: 2 [46720/50000]\tLoss: 3.2062\tLR: 0.193095\n",
      "Training Epoch: 2 [46848/50000]\tLoss: 3.0521\tLR: 0.193350\n",
      "Training Epoch: 2 [46976/50000]\tLoss: 3.3133\tLR: 0.193606\n",
      "Training Epoch: 2 [47104/50000]\tLoss: 2.9692\tLR: 0.193862\n",
      "Training Epoch: 2 [47232/50000]\tLoss: 3.1647\tLR: 0.194118\n",
      "Training Epoch: 2 [47360/50000]\tLoss: 3.0618\tLR: 0.194373\n",
      "Training Epoch: 2 [47488/50000]\tLoss: 3.1761\tLR: 0.194629\n",
      "Training Epoch: 2 [47616/50000]\tLoss: 3.1992\tLR: 0.194885\n",
      "Training Epoch: 2 [47744/50000]\tLoss: 3.0855\tLR: 0.195141\n",
      "Training Epoch: 2 [47872/50000]\tLoss: 3.2555\tLR: 0.195396\n",
      "Training Epoch: 2 [48000/50000]\tLoss: 3.0874\tLR: 0.195652\n",
      "Training Epoch: 2 [48128/50000]\tLoss: 3.2249\tLR: 0.195908\n",
      "Training Epoch: 2 [48256/50000]\tLoss: 3.1498\tLR: 0.196164\n",
      "Training Epoch: 2 [48384/50000]\tLoss: 3.2511\tLR: 0.196419\n",
      "Training Epoch: 2 [48512/50000]\tLoss: 2.9158\tLR: 0.196675\n",
      "Training Epoch: 2 [48640/50000]\tLoss: 3.1830\tLR: 0.196931\n",
      "Training Epoch: 2 [48768/50000]\tLoss: 2.9855\tLR: 0.197187\n",
      "Training Epoch: 2 [48896/50000]\tLoss: 3.2938\tLR: 0.197442\n",
      "Training Epoch: 2 [49024/50000]\tLoss: 3.0247\tLR: 0.197698\n",
      "Training Epoch: 2 [49152/50000]\tLoss: 3.1577\tLR: 0.197954\n",
      "Training Epoch: 2 [49280/50000]\tLoss: 3.2464\tLR: 0.198210\n",
      "Training Epoch: 2 [49408/50000]\tLoss: 2.9551\tLR: 0.198465\n",
      "Training Epoch: 2 [49536/50000]\tLoss: 3.3622\tLR: 0.198721\n",
      "Training Epoch: 2 [49664/50000]\tLoss: 3.1110\tLR: 0.198977\n",
      "Training Epoch: 2 [49792/50000]\tLoss: 3.0379\tLR: 0.199233\n",
      "Training Epoch: 2 [49920/50000]\tLoss: 3.2448\tLR: 0.199488\n",
      "Training Epoch: 2 [50000/50000]\tLoss: 2.9531\tLR: 0.199744\n",
      "epoch 2 training time consumed: 489.73s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |    2805 GB |    2805 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |    2796 GB |    2796 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |       8 GB |       8 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |    2805 GB |    2805 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |    2796 GB |    2796 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |       8 GB |       8 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |    2762 GB |    2762 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |    2753 GB |    2753 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |       8 GB |       8 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |  297864    |  297609    |\n",
      "|       from large pool |      24    |      65    |  126802    |  126778    |\n",
      "|       from small pool |     231    |     274    |  171062    |  170831    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |  297864    |  297609    |\n",
      "|       from large pool |      24    |      65    |  126802    |  126778    |\n",
      "|       from small pool |     231    |     274    |  171062    |  170831    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      45    |  172393    |  172357    |\n",
      "|       from large pool |      10    |      23    |   60946    |   60936    |\n",
      "|       from small pool |      26    |      35    |  111447    |  111421    |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 2, Average loss: 0.0258, Accuracy: 0.2014, Time consumed:32.75s\n",
      "\n",
      "Training Epoch: 3 [128/50000]\tLoss: 3.2707\tLR: 0.100000\n",
      "Training Epoch: 3 [256/50000]\tLoss: 3.3503\tLR: 0.200256\n",
      "Training Epoch: 3 [384/50000]\tLoss: 3.1561\tLR: 0.200512\n",
      "Training Epoch: 3 [512/50000]\tLoss: 3.1014\tLR: 0.200767\n",
      "Training Epoch: 3 [640/50000]\tLoss: 3.1309\tLR: 0.201023\n",
      "Training Epoch: 3 [768/50000]\tLoss: 3.2286\tLR: 0.201279\n",
      "Training Epoch: 3 [896/50000]\tLoss: 2.9605\tLR: 0.201535\n",
      "Training Epoch: 3 [1024/50000]\tLoss: 2.8760\tLR: 0.201790\n",
      "Training Epoch: 3 [1152/50000]\tLoss: 3.0915\tLR: 0.202046\n",
      "Training Epoch: 3 [1280/50000]\tLoss: 3.2187\tLR: 0.202302\n",
      "Training Epoch: 3 [1408/50000]\tLoss: 2.8902\tLR: 0.202558\n",
      "Training Epoch: 3 [1536/50000]\tLoss: 3.1255\tLR: 0.202813\n",
      "Training Epoch: 3 [1664/50000]\tLoss: 2.9441\tLR: 0.203069\n",
      "Training Epoch: 3 [1792/50000]\tLoss: 3.0955\tLR: 0.203325\n",
      "Training Epoch: 3 [1920/50000]\tLoss: 3.0996\tLR: 0.203581\n",
      "Training Epoch: 3 [2048/50000]\tLoss: 3.1483\tLR: 0.203836\n",
      "Training Epoch: 3 [2176/50000]\tLoss: 3.2215\tLR: 0.204092\n",
      "Training Epoch: 3 [2304/50000]\tLoss: 3.1326\tLR: 0.204348\n",
      "Training Epoch: 3 [2432/50000]\tLoss: 3.0233\tLR: 0.204604\n",
      "Training Epoch: 3 [2560/50000]\tLoss: 3.1770\tLR: 0.204859\n",
      "Training Epoch: 3 [2688/50000]\tLoss: 3.0721\tLR: 0.205115\n",
      "Training Epoch: 3 [2816/50000]\tLoss: 3.3303\tLR: 0.205371\n",
      "Training Epoch: 3 [2944/50000]\tLoss: 3.1927\tLR: 0.205627\n",
      "Training Epoch: 3 [3072/50000]\tLoss: 3.2417\tLR: 0.205882\n",
      "Training Epoch: 3 [3200/50000]\tLoss: 3.2226\tLR: 0.206138\n",
      "Training Epoch: 3 [3328/50000]\tLoss: 2.9766\tLR: 0.206394\n",
      "Training Epoch: 3 [3456/50000]\tLoss: 2.9775\tLR: 0.206650\n",
      "Training Epoch: 3 [3584/50000]\tLoss: 3.0209\tLR: 0.206905\n",
      "Training Epoch: 3 [3712/50000]\tLoss: 3.0929\tLR: 0.207161\n",
      "Training Epoch: 3 [3840/50000]\tLoss: 3.1638\tLR: 0.207417\n",
      "Training Epoch: 3 [3968/50000]\tLoss: 3.1297\tLR: 0.207673\n",
      "Training Epoch: 3 [4096/50000]\tLoss: 3.1966\tLR: 0.207928\n",
      "Training Epoch: 3 [4224/50000]\tLoss: 3.2378\tLR: 0.208184\n",
      "Training Epoch: 3 [4352/50000]\tLoss: 3.0742\tLR: 0.208440\n",
      "Training Epoch: 3 [4480/50000]\tLoss: 3.1083\tLR: 0.208696\n",
      "Training Epoch: 3 [4608/50000]\tLoss: 2.8744\tLR: 0.208951\n",
      "Training Epoch: 3 [4736/50000]\tLoss: 2.9805\tLR: 0.209207\n",
      "Training Epoch: 3 [4864/50000]\tLoss: 3.1506\tLR: 0.209463\n",
      "Training Epoch: 3 [4992/50000]\tLoss: 3.1454\tLR: 0.209719\n",
      "Training Epoch: 3 [5120/50000]\tLoss: 2.9735\tLR: 0.209974\n",
      "Training Epoch: 3 [5248/50000]\tLoss: 3.0410\tLR: 0.210230\n",
      "Training Epoch: 3 [5376/50000]\tLoss: 3.5319\tLR: 0.210486\n",
      "Training Epoch: 3 [5504/50000]\tLoss: 2.8742\tLR: 0.210742\n",
      "Training Epoch: 3 [5632/50000]\tLoss: 3.1528\tLR: 0.210997\n",
      "Training Epoch: 3 [5760/50000]\tLoss: 3.2897\tLR: 0.211253\n",
      "Training Epoch: 3 [5888/50000]\tLoss: 2.9855\tLR: 0.211509\n",
      "Training Epoch: 3 [6016/50000]\tLoss: 3.2400\tLR: 0.211765\n",
      "Training Epoch: 3 [6144/50000]\tLoss: 3.0782\tLR: 0.212020\n",
      "Training Epoch: 3 [6272/50000]\tLoss: 3.1397\tLR: 0.212276\n",
      "Training Epoch: 3 [6400/50000]\tLoss: 3.1747\tLR: 0.212532\n",
      "Training Epoch: 3 [6528/50000]\tLoss: 3.1110\tLR: 0.212788\n",
      "Training Epoch: 3 [6656/50000]\tLoss: 3.3539\tLR: 0.213043\n",
      "Training Epoch: 3 [6784/50000]\tLoss: 3.0882\tLR: 0.213299\n",
      "Training Epoch: 3 [6912/50000]\tLoss: 2.7863\tLR: 0.213555\n",
      "Training Epoch: 3 [7040/50000]\tLoss: 2.9416\tLR: 0.213811\n",
      "Training Epoch: 3 [7168/50000]\tLoss: 2.9256\tLR: 0.214066\n",
      "Training Epoch: 3 [7296/50000]\tLoss: 3.1447\tLR: 0.214322\n",
      "Training Epoch: 3 [7424/50000]\tLoss: 3.0107\tLR: 0.214578\n",
      "Training Epoch: 3 [7552/50000]\tLoss: 3.2150\tLR: 0.214834\n",
      "Training Epoch: 3 [7680/50000]\tLoss: 2.8436\tLR: 0.215090\n",
      "Training Epoch: 3 [7808/50000]\tLoss: 3.1296\tLR: 0.215345\n",
      "Training Epoch: 3 [7936/50000]\tLoss: 3.0407\tLR: 0.215601\n",
      "Training Epoch: 3 [8064/50000]\tLoss: 3.1745\tLR: 0.215857\n",
      "Training Epoch: 3 [8192/50000]\tLoss: 3.0362\tLR: 0.216113\n",
      "Training Epoch: 3 [8320/50000]\tLoss: 3.2077\tLR: 0.216368\n",
      "Training Epoch: 3 [8448/50000]\tLoss: 3.0807\tLR: 0.216624\n",
      "Training Epoch: 3 [8576/50000]\tLoss: 2.9213\tLR: 0.216880\n",
      "Training Epoch: 3 [8704/50000]\tLoss: 2.9794\tLR: 0.217136\n",
      "Training Epoch: 3 [8832/50000]\tLoss: 3.0183\tLR: 0.217391\n",
      "Training Epoch: 3 [8960/50000]\tLoss: 2.8136\tLR: 0.217647\n",
      "Training Epoch: 3 [9088/50000]\tLoss: 3.0764\tLR: 0.217903\n",
      "Training Epoch: 3 [9216/50000]\tLoss: 3.4447\tLR: 0.218159\n",
      "Training Epoch: 3 [9344/50000]\tLoss: 3.1583\tLR: 0.218414\n",
      "Training Epoch: 3 [9472/50000]\tLoss: 2.8453\tLR: 0.218670\n",
      "Training Epoch: 3 [9600/50000]\tLoss: 2.9802\tLR: 0.218926\n",
      "Training Epoch: 3 [9728/50000]\tLoss: 2.8712\tLR: 0.219182\n",
      "Training Epoch: 3 [9856/50000]\tLoss: 2.7585\tLR: 0.219437\n",
      "Training Epoch: 3 [9984/50000]\tLoss: 3.0755\tLR: 0.219693\n",
      "Training Epoch: 3 [10112/50000]\tLoss: 3.1993\tLR: 0.219949\n",
      "Training Epoch: 3 [10240/50000]\tLoss: 3.1460\tLR: 0.220205\n",
      "Training Epoch: 3 [10368/50000]\tLoss: 2.8907\tLR: 0.220460\n",
      "Training Epoch: 3 [10496/50000]\tLoss: 3.1907\tLR: 0.220716\n",
      "Training Epoch: 3 [10624/50000]\tLoss: 2.9882\tLR: 0.220972\n",
      "Training Epoch: 3 [10752/50000]\tLoss: 3.2555\tLR: 0.221228\n",
      "Training Epoch: 3 [10880/50000]\tLoss: 3.0566\tLR: 0.221483\n",
      "Training Epoch: 3 [11008/50000]\tLoss: 3.3685\tLR: 0.221739\n",
      "Training Epoch: 3 [11136/50000]\tLoss: 3.4267\tLR: 0.221995\n",
      "Training Epoch: 3 [11264/50000]\tLoss: 2.9974\tLR: 0.222251\n",
      "Training Epoch: 3 [11392/50000]\tLoss: 3.1103\tLR: 0.222506\n",
      "Training Epoch: 3 [11520/50000]\tLoss: 3.1645\tLR: 0.222762\n",
      "Training Epoch: 3 [11648/50000]\tLoss: 2.8790\tLR: 0.223018\n",
      "Training Epoch: 3 [11776/50000]\tLoss: 3.0746\tLR: 0.223274\n",
      "Training Epoch: 3 [11904/50000]\tLoss: 3.1497\tLR: 0.223529\n",
      "Training Epoch: 3 [12032/50000]\tLoss: 2.8810\tLR: 0.223785\n",
      "Training Epoch: 3 [12160/50000]\tLoss: 3.1583\tLR: 0.224041\n",
      "Training Epoch: 3 [12288/50000]\tLoss: 2.8716\tLR: 0.224297\n",
      "Training Epoch: 3 [12416/50000]\tLoss: 3.3866\tLR: 0.224552\n",
      "Training Epoch: 3 [12544/50000]\tLoss: 2.7520\tLR: 0.224808\n",
      "Training Epoch: 3 [12672/50000]\tLoss: 2.8344\tLR: 0.225064\n",
      "Training Epoch: 3 [12800/50000]\tLoss: 2.8845\tLR: 0.225320\n",
      "Training Epoch: 3 [12928/50000]\tLoss: 2.7926\tLR: 0.225575\n",
      "Training Epoch: 3 [13056/50000]\tLoss: 3.0221\tLR: 0.225831\n",
      "Training Epoch: 3 [13184/50000]\tLoss: 2.9399\tLR: 0.226087\n",
      "Training Epoch: 3 [13312/50000]\tLoss: 2.9799\tLR: 0.226343\n",
      "Training Epoch: 3 [13440/50000]\tLoss: 2.8786\tLR: 0.226598\n",
      "Training Epoch: 3 [13568/50000]\tLoss: 2.9863\tLR: 0.226854\n",
      "Training Epoch: 3 [13696/50000]\tLoss: 3.0157\tLR: 0.227110\n",
      "Training Epoch: 3 [13824/50000]\tLoss: 3.2178\tLR: 0.227366\n",
      "Training Epoch: 3 [13952/50000]\tLoss: 2.9908\tLR: 0.227621\n",
      "Training Epoch: 3 [14080/50000]\tLoss: 3.2881\tLR: 0.227877\n",
      "Training Epoch: 3 [14208/50000]\tLoss: 3.0974\tLR: 0.228133\n",
      "Training Epoch: 3 [14336/50000]\tLoss: 3.3526\tLR: 0.228389\n",
      "Training Epoch: 3 [14464/50000]\tLoss: 3.3854\tLR: 0.228645\n",
      "Training Epoch: 3 [14592/50000]\tLoss: 3.1430\tLR: 0.228900\n",
      "Training Epoch: 3 [14720/50000]\tLoss: 2.9079\tLR: 0.229156\n",
      "Training Epoch: 3 [14848/50000]\tLoss: 2.8299\tLR: 0.229412\n",
      "Training Epoch: 3 [14976/50000]\tLoss: 3.0353\tLR: 0.229668\n",
      "Training Epoch: 3 [15104/50000]\tLoss: 3.2162\tLR: 0.229923\n",
      "Training Epoch: 3 [15232/50000]\tLoss: 3.0570\tLR: 0.230179\n",
      "Training Epoch: 3 [15360/50000]\tLoss: 3.0581\tLR: 0.230435\n",
      "Training Epoch: 3 [15488/50000]\tLoss: 2.8628\tLR: 0.230691\n",
      "Training Epoch: 3 [15616/50000]\tLoss: 2.8549\tLR: 0.230946\n",
      "Training Epoch: 3 [15744/50000]\tLoss: 3.0506\tLR: 0.231202\n",
      "Training Epoch: 3 [15872/50000]\tLoss: 2.9323\tLR: 0.231458\n",
      "Training Epoch: 3 [16000/50000]\tLoss: 2.9041\tLR: 0.231714\n",
      "Training Epoch: 3 [16128/50000]\tLoss: 3.3600\tLR: 0.231969\n",
      "Training Epoch: 3 [16256/50000]\tLoss: 2.9665\tLR: 0.232225\n",
      "Training Epoch: 3 [16384/50000]\tLoss: 2.9410\tLR: 0.232481\n",
      "Training Epoch: 3 [16512/50000]\tLoss: 2.8587\tLR: 0.232737\n",
      "Training Epoch: 3 [16640/50000]\tLoss: 2.9369\tLR: 0.232992\n",
      "Training Epoch: 3 [16768/50000]\tLoss: 2.9268\tLR: 0.233248\n",
      "Training Epoch: 3 [16896/50000]\tLoss: 2.8597\tLR: 0.233504\n",
      "Training Epoch: 3 [17024/50000]\tLoss: 2.9019\tLR: 0.233760\n",
      "Training Epoch: 3 [17152/50000]\tLoss: 2.9135\tLR: 0.234015\n",
      "Training Epoch: 3 [17280/50000]\tLoss: 2.9917\tLR: 0.234271\n",
      "Training Epoch: 3 [17408/50000]\tLoss: 2.6818\tLR: 0.234527\n",
      "Training Epoch: 3 [17536/50000]\tLoss: 3.0320\tLR: 0.234783\n",
      "Training Epoch: 3 [17664/50000]\tLoss: 3.1523\tLR: 0.235038\n",
      "Training Epoch: 3 [17792/50000]\tLoss: 2.8889\tLR: 0.235294\n",
      "Training Epoch: 3 [17920/50000]\tLoss: 2.9377\tLR: 0.235550\n",
      "Training Epoch: 3 [18048/50000]\tLoss: 2.8963\tLR: 0.235806\n",
      "Training Epoch: 3 [18176/50000]\tLoss: 2.9100\tLR: 0.236061\n",
      "Training Epoch: 3 [18304/50000]\tLoss: 3.0055\tLR: 0.236317\n",
      "Training Epoch: 3 [18432/50000]\tLoss: 2.7587\tLR: 0.236573\n",
      "Training Epoch: 3 [18560/50000]\tLoss: 3.0439\tLR: 0.236829\n",
      "Training Epoch: 3 [18688/50000]\tLoss: 2.8015\tLR: 0.237084\n",
      "Training Epoch: 3 [18816/50000]\tLoss: 3.1901\tLR: 0.237340\n",
      "Training Epoch: 3 [18944/50000]\tLoss: 3.0635\tLR: 0.237596\n",
      "Training Epoch: 3 [19072/50000]\tLoss: 2.8670\tLR: 0.237852\n",
      "Training Epoch: 3 [19200/50000]\tLoss: 2.8615\tLR: 0.238107\n",
      "Training Epoch: 3 [19328/50000]\tLoss: 3.0589\tLR: 0.238363\n",
      "Training Epoch: 3 [19456/50000]\tLoss: 3.0004\tLR: 0.238619\n",
      "Training Epoch: 3 [19584/50000]\tLoss: 3.1974\tLR: 0.238875\n",
      "Training Epoch: 3 [19712/50000]\tLoss: 3.0597\tLR: 0.239130\n",
      "Training Epoch: 3 [19840/50000]\tLoss: 2.9309\tLR: 0.239386\n",
      "Training Epoch: 3 [19968/50000]\tLoss: 3.0668\tLR: 0.239642\n",
      "Training Epoch: 3 [20096/50000]\tLoss: 3.1846\tLR: 0.239898\n",
      "Training Epoch: 3 [20224/50000]\tLoss: 2.8754\tLR: 0.240153\n",
      "Training Epoch: 3 [20352/50000]\tLoss: 2.7522\tLR: 0.240409\n",
      "Training Epoch: 3 [20480/50000]\tLoss: 2.9525\tLR: 0.240665\n",
      "Training Epoch: 3 [20608/50000]\tLoss: 3.3153\tLR: 0.240921\n",
      "Training Epoch: 3 [20736/50000]\tLoss: 2.8503\tLR: 0.241176\n",
      "Training Epoch: 3 [20864/50000]\tLoss: 3.0243\tLR: 0.241432\n",
      "Training Epoch: 3 [20992/50000]\tLoss: 3.2173\tLR: 0.241688\n",
      "Training Epoch: 3 [21120/50000]\tLoss: 2.8023\tLR: 0.241944\n",
      "Training Epoch: 3 [21248/50000]\tLoss: 2.9373\tLR: 0.242199\n",
      "Training Epoch: 3 [21376/50000]\tLoss: 3.0236\tLR: 0.242455\n",
      "Training Epoch: 3 [21504/50000]\tLoss: 2.9534\tLR: 0.242711\n",
      "Training Epoch: 3 [21632/50000]\tLoss: 3.3128\tLR: 0.242967\n",
      "Training Epoch: 3 [21760/50000]\tLoss: 2.9334\tLR: 0.243223\n",
      "Training Epoch: 3 [21888/50000]\tLoss: 2.9460\tLR: 0.243478\n",
      "Training Epoch: 3 [22016/50000]\tLoss: 2.7864\tLR: 0.243734\n",
      "Training Epoch: 3 [22144/50000]\tLoss: 2.7557\tLR: 0.243990\n",
      "Training Epoch: 3 [22272/50000]\tLoss: 2.9558\tLR: 0.244246\n",
      "Training Epoch: 3 [22400/50000]\tLoss: 2.9133\tLR: 0.244501\n",
      "Training Epoch: 3 [22528/50000]\tLoss: 3.1528\tLR: 0.244757\n",
      "Training Epoch: 3 [22656/50000]\tLoss: 2.8992\tLR: 0.245013\n",
      "Training Epoch: 3 [22784/50000]\tLoss: 3.0971\tLR: 0.245269\n",
      "Training Epoch: 3 [22912/50000]\tLoss: 3.0420\tLR: 0.245524\n",
      "Training Epoch: 3 [23040/50000]\tLoss: 2.9858\tLR: 0.245780\n",
      "Training Epoch: 3 [23168/50000]\tLoss: 3.2444\tLR: 0.246036\n",
      "Training Epoch: 3 [23296/50000]\tLoss: 3.0200\tLR: 0.246292\n",
      "Training Epoch: 3 [23424/50000]\tLoss: 2.9503\tLR: 0.246547\n",
      "Training Epoch: 3 [23552/50000]\tLoss: 3.0289\tLR: 0.246803\n",
      "Training Epoch: 3 [23680/50000]\tLoss: 2.9590\tLR: 0.247059\n",
      "Training Epoch: 3 [23808/50000]\tLoss: 3.1821\tLR: 0.247315\n",
      "Training Epoch: 3 [23936/50000]\tLoss: 3.0198\tLR: 0.247570\n",
      "Training Epoch: 3 [24064/50000]\tLoss: 3.0406\tLR: 0.247826\n",
      "Training Epoch: 3 [24192/50000]\tLoss: 3.0552\tLR: 0.248082\n",
      "Training Epoch: 3 [24320/50000]\tLoss: 3.0702\tLR: 0.248338\n",
      "Training Epoch: 3 [24448/50000]\tLoss: 2.7715\tLR: 0.248593\n",
      "Training Epoch: 3 [24576/50000]\tLoss: 2.9924\tLR: 0.248849\n",
      "Training Epoch: 3 [24704/50000]\tLoss: 2.9419\tLR: 0.249105\n",
      "Training Epoch: 3 [24832/50000]\tLoss: 2.8617\tLR: 0.249361\n",
      "Training Epoch: 3 [24960/50000]\tLoss: 2.7372\tLR: 0.249616\n",
      "Training Epoch: 3 [25088/50000]\tLoss: 2.6695\tLR: 0.249872\n",
      "Training Epoch: 3 [25216/50000]\tLoss: 3.2962\tLR: 0.250128\n",
      "Training Epoch: 3 [25344/50000]\tLoss: 3.0798\tLR: 0.250384\n",
      "Training Epoch: 3 [25472/50000]\tLoss: 2.8571\tLR: 0.250639\n",
      "Training Epoch: 3 [25600/50000]\tLoss: 2.9236\tLR: 0.250895\n",
      "Training Epoch: 3 [25728/50000]\tLoss: 2.9727\tLR: 0.251151\n",
      "Training Epoch: 3 [25856/50000]\tLoss: 3.0357\tLR: 0.251407\n",
      "Training Epoch: 3 [25984/50000]\tLoss: 3.0641\tLR: 0.251662\n",
      "Training Epoch: 3 [26112/50000]\tLoss: 3.0890\tLR: 0.251918\n",
      "Training Epoch: 3 [26240/50000]\tLoss: 2.8900\tLR: 0.252174\n",
      "Training Epoch: 3 [26368/50000]\tLoss: 2.9562\tLR: 0.252430\n",
      "Training Epoch: 3 [26496/50000]\tLoss: 3.3380\tLR: 0.252685\n",
      "Training Epoch: 3 [26624/50000]\tLoss: 2.9409\tLR: 0.252941\n",
      "Training Epoch: 3 [26752/50000]\tLoss: 2.9163\tLR: 0.253197\n",
      "Training Epoch: 3 [26880/50000]\tLoss: 2.8570\tLR: 0.253453\n",
      "Training Epoch: 3 [27008/50000]\tLoss: 2.8165\tLR: 0.253708\n",
      "Training Epoch: 3 [27136/50000]\tLoss: 2.9296\tLR: 0.253964\n",
      "Training Epoch: 3 [27264/50000]\tLoss: 2.9287\tLR: 0.254220\n",
      "Training Epoch: 3 [27392/50000]\tLoss: 2.9008\tLR: 0.254476\n",
      "Training Epoch: 3 [27520/50000]\tLoss: 2.7739\tLR: 0.254731\n",
      "Training Epoch: 3 [27648/50000]\tLoss: 3.0158\tLR: 0.254987\n",
      "Training Epoch: 3 [27776/50000]\tLoss: 2.8625\tLR: 0.255243\n",
      "Training Epoch: 3 [27904/50000]\tLoss: 2.9583\tLR: 0.255499\n",
      "Training Epoch: 3 [28032/50000]\tLoss: 2.7253\tLR: 0.255754\n",
      "Training Epoch: 3 [28160/50000]\tLoss: 3.0096\tLR: 0.256010\n",
      "Training Epoch: 3 [28288/50000]\tLoss: 2.9996\tLR: 0.256266\n",
      "Training Epoch: 3 [28416/50000]\tLoss: 2.6086\tLR: 0.256522\n",
      "Training Epoch: 3 [28544/50000]\tLoss: 3.0111\tLR: 0.256777\n",
      "Training Epoch: 3 [28672/50000]\tLoss: 2.8513\tLR: 0.257033\n",
      "Training Epoch: 3 [28800/50000]\tLoss: 3.1453\tLR: 0.257289\n",
      "Training Epoch: 3 [28928/50000]\tLoss: 2.9303\tLR: 0.257545\n",
      "Training Epoch: 3 [29056/50000]\tLoss: 2.7419\tLR: 0.257801\n",
      "Training Epoch: 3 [29184/50000]\tLoss: 2.8813\tLR: 0.258056\n",
      "Training Epoch: 3 [29312/50000]\tLoss: 3.0497\tLR: 0.258312\n",
      "Training Epoch: 3 [29440/50000]\tLoss: 3.0966\tLR: 0.258568\n",
      "Training Epoch: 3 [29568/50000]\tLoss: 2.6347\tLR: 0.258824\n",
      "Training Epoch: 3 [29696/50000]\tLoss: 3.1957\tLR: 0.259079\n",
      "Training Epoch: 3 [29824/50000]\tLoss: 2.5974\tLR: 0.259335\n",
      "Training Epoch: 3 [29952/50000]\tLoss: 3.0076\tLR: 0.259591\n",
      "Training Epoch: 3 [30080/50000]\tLoss: 2.7784\tLR: 0.259847\n",
      "Training Epoch: 3 [30208/50000]\tLoss: 2.9780\tLR: 0.260102\n",
      "Training Epoch: 3 [30336/50000]\tLoss: 2.8091\tLR: 0.260358\n",
      "Training Epoch: 3 [30464/50000]\tLoss: 2.7372\tLR: 0.260614\n",
      "Training Epoch: 3 [30592/50000]\tLoss: 2.9222\tLR: 0.260870\n",
      "Training Epoch: 3 [30720/50000]\tLoss: 2.9474\tLR: 0.261125\n",
      "Training Epoch: 3 [30848/50000]\tLoss: 2.7846\tLR: 0.261381\n",
      "Training Epoch: 3 [30976/50000]\tLoss: 2.9302\tLR: 0.261637\n",
      "Training Epoch: 3 [31104/50000]\tLoss: 2.9206\tLR: 0.261893\n",
      "Training Epoch: 3 [31232/50000]\tLoss: 2.9554\tLR: 0.262148\n",
      "Training Epoch: 3 [31360/50000]\tLoss: 2.9126\tLR: 0.262404\n",
      "Training Epoch: 3 [31488/50000]\tLoss: 2.9366\tLR: 0.262660\n",
      "Training Epoch: 3 [31616/50000]\tLoss: 2.8196\tLR: 0.262916\n",
      "Training Epoch: 3 [31744/50000]\tLoss: 2.8547\tLR: 0.263171\n",
      "Training Epoch: 3 [31872/50000]\tLoss: 2.9111\tLR: 0.263427\n",
      "Training Epoch: 3 [32000/50000]\tLoss: 2.9850\tLR: 0.263683\n",
      "Training Epoch: 3 [32128/50000]\tLoss: 2.8673\tLR: 0.263939\n",
      "Training Epoch: 3 [32256/50000]\tLoss: 2.7171\tLR: 0.264194\n",
      "Training Epoch: 3 [32384/50000]\tLoss: 2.7543\tLR: 0.264450\n",
      "Training Epoch: 3 [32512/50000]\tLoss: 3.0145\tLR: 0.264706\n",
      "Training Epoch: 3 [32640/50000]\tLoss: 2.9115\tLR: 0.264962\n",
      "Training Epoch: 3 [32768/50000]\tLoss: 2.8835\tLR: 0.265217\n",
      "Training Epoch: 3 [32896/50000]\tLoss: 2.7539\tLR: 0.265473\n",
      "Training Epoch: 3 [33024/50000]\tLoss: 3.0008\tLR: 0.265729\n",
      "Training Epoch: 3 [33152/50000]\tLoss: 2.7953\tLR: 0.265985\n",
      "Training Epoch: 3 [33280/50000]\tLoss: 2.8553\tLR: 0.266240\n",
      "Training Epoch: 3 [33408/50000]\tLoss: 2.6950\tLR: 0.266496\n",
      "Training Epoch: 3 [33536/50000]\tLoss: 3.0356\tLR: 0.266752\n",
      "Training Epoch: 3 [33664/50000]\tLoss: 2.9507\tLR: 0.267008\n",
      "Training Epoch: 3 [33792/50000]\tLoss: 2.8274\tLR: 0.267263\n",
      "Training Epoch: 3 [33920/50000]\tLoss: 2.9239\tLR: 0.267519\n",
      "Training Epoch: 3 [34048/50000]\tLoss: 3.1981\tLR: 0.267775\n",
      "Training Epoch: 3 [34176/50000]\tLoss: 3.0306\tLR: 0.268031\n",
      "Training Epoch: 3 [34304/50000]\tLoss: 2.7850\tLR: 0.268286\n",
      "Training Epoch: 3 [34432/50000]\tLoss: 2.8704\tLR: 0.268542\n",
      "Training Epoch: 3 [34560/50000]\tLoss: 2.6296\tLR: 0.268798\n",
      "Training Epoch: 3 [34688/50000]\tLoss: 2.8438\tLR: 0.269054\n",
      "Training Epoch: 3 [34816/50000]\tLoss: 2.8994\tLR: 0.269309\n",
      "Training Epoch: 3 [34944/50000]\tLoss: 2.9829\tLR: 0.269565\n",
      "Training Epoch: 3 [35072/50000]\tLoss: 2.9104\tLR: 0.269821\n",
      "Training Epoch: 3 [35200/50000]\tLoss: 2.8284\tLR: 0.270077\n",
      "Training Epoch: 3 [35328/50000]\tLoss: 2.9813\tLR: 0.270332\n",
      "Training Epoch: 3 [35456/50000]\tLoss: 2.8797\tLR: 0.270588\n",
      "Training Epoch: 3 [35584/50000]\tLoss: 2.9638\tLR: 0.270844\n",
      "Training Epoch: 3 [35712/50000]\tLoss: 2.7058\tLR: 0.271100\n",
      "Training Epoch: 3 [35840/50000]\tLoss: 2.8525\tLR: 0.271355\n",
      "Training Epoch: 3 [35968/50000]\tLoss: 2.9870\tLR: 0.271611\n",
      "Training Epoch: 3 [36096/50000]\tLoss: 3.0189\tLR: 0.271867\n",
      "Training Epoch: 3 [36224/50000]\tLoss: 2.6885\tLR: 0.272123\n",
      "Training Epoch: 3 [36352/50000]\tLoss: 2.7505\tLR: 0.272379\n",
      "Training Epoch: 3 [36480/50000]\tLoss: 2.7764\tLR: 0.272634\n",
      "Training Epoch: 3 [36608/50000]\tLoss: 3.0775\tLR: 0.272890\n",
      "Training Epoch: 3 [36736/50000]\tLoss: 2.9672\tLR: 0.273146\n",
      "Training Epoch: 3 [36864/50000]\tLoss: 2.7349\tLR: 0.273402\n",
      "Training Epoch: 3 [36992/50000]\tLoss: 2.7536\tLR: 0.273657\n",
      "Training Epoch: 3 [37120/50000]\tLoss: 2.7629\tLR: 0.273913\n",
      "Training Epoch: 3 [37248/50000]\tLoss: 2.8570\tLR: 0.274169\n",
      "Training Epoch: 3 [37376/50000]\tLoss: 2.8917\tLR: 0.274425\n",
      "Training Epoch: 3 [37504/50000]\tLoss: 2.6904\tLR: 0.274680\n",
      "Training Epoch: 3 [37632/50000]\tLoss: 2.8113\tLR: 0.274936\n",
      "Training Epoch: 3 [37760/50000]\tLoss: 2.8783\tLR: 0.275192\n",
      "Training Epoch: 3 [37888/50000]\tLoss: 3.1233\tLR: 0.275448\n",
      "Training Epoch: 3 [38016/50000]\tLoss: 2.8832\tLR: 0.275703\n",
      "Training Epoch: 3 [38144/50000]\tLoss: 2.7782\tLR: 0.275959\n",
      "Training Epoch: 3 [38272/50000]\tLoss: 3.0216\tLR: 0.276215\n",
      "Training Epoch: 3 [38400/50000]\tLoss: 2.6394\tLR: 0.276471\n",
      "Training Epoch: 3 [38528/50000]\tLoss: 2.7362\tLR: 0.276726\n",
      "Training Epoch: 3 [38656/50000]\tLoss: 3.0739\tLR: 0.276982\n",
      "Training Epoch: 3 [38784/50000]\tLoss: 2.8364\tLR: 0.277238\n",
      "Training Epoch: 3 [38912/50000]\tLoss: 2.9137\tLR: 0.277494\n",
      "Training Epoch: 3 [39040/50000]\tLoss: 2.6571\tLR: 0.277749\n",
      "Training Epoch: 3 [39168/50000]\tLoss: 3.0602\tLR: 0.278005\n",
      "Training Epoch: 3 [39296/50000]\tLoss: 2.8917\tLR: 0.278261\n",
      "Training Epoch: 3 [39424/50000]\tLoss: 2.7417\tLR: 0.278517\n",
      "Training Epoch: 3 [39552/50000]\tLoss: 3.0365\tLR: 0.278772\n",
      "Training Epoch: 3 [39680/50000]\tLoss: 3.1414\tLR: 0.279028\n",
      "Training Epoch: 3 [39808/50000]\tLoss: 2.8086\tLR: 0.279284\n",
      "Training Epoch: 3 [39936/50000]\tLoss: 2.9174\tLR: 0.279540\n",
      "Training Epoch: 3 [40064/50000]\tLoss: 2.8096\tLR: 0.279795\n",
      "Training Epoch: 3 [40192/50000]\tLoss: 2.6537\tLR: 0.280051\n",
      "Training Epoch: 3 [40320/50000]\tLoss: 2.8035\tLR: 0.280307\n",
      "Training Epoch: 3 [40448/50000]\tLoss: 2.9802\tLR: 0.280563\n",
      "Training Epoch: 3 [40576/50000]\tLoss: 2.8414\tLR: 0.280818\n",
      "Training Epoch: 3 [40704/50000]\tLoss: 3.1771\tLR: 0.281074\n",
      "Training Epoch: 3 [40832/50000]\tLoss: 2.8478\tLR: 0.281330\n",
      "Training Epoch: 3 [40960/50000]\tLoss: 2.9072\tLR: 0.281586\n",
      "Training Epoch: 3 [41088/50000]\tLoss: 2.9214\tLR: 0.281841\n",
      "Training Epoch: 3 [41216/50000]\tLoss: 2.7482\tLR: 0.282097\n",
      "Training Epoch: 3 [41344/50000]\tLoss: 3.0477\tLR: 0.282353\n",
      "Training Epoch: 3 [41472/50000]\tLoss: 2.8010\tLR: 0.282609\n",
      "Training Epoch: 3 [41600/50000]\tLoss: 2.8945\tLR: 0.282864\n",
      "Training Epoch: 3 [41728/50000]\tLoss: 2.9120\tLR: 0.283120\n",
      "Training Epoch: 3 [41856/50000]\tLoss: 2.8008\tLR: 0.283376\n",
      "Training Epoch: 3 [41984/50000]\tLoss: 2.6397\tLR: 0.283632\n",
      "Training Epoch: 3 [42112/50000]\tLoss: 2.6977\tLR: 0.283887\n",
      "Training Epoch: 3 [42240/50000]\tLoss: 2.8345\tLR: 0.284143\n",
      "Training Epoch: 3 [42368/50000]\tLoss: 2.6867\tLR: 0.284399\n",
      "Training Epoch: 3 [42496/50000]\tLoss: 3.0918\tLR: 0.284655\n",
      "Training Epoch: 3 [42624/50000]\tLoss: 2.9769\tLR: 0.284910\n",
      "Training Epoch: 3 [42752/50000]\tLoss: 2.7578\tLR: 0.285166\n",
      "Training Epoch: 3 [42880/50000]\tLoss: 2.6980\tLR: 0.285422\n",
      "Training Epoch: 3 [43008/50000]\tLoss: 2.8702\tLR: 0.285678\n",
      "Training Epoch: 3 [43136/50000]\tLoss: 2.8740\tLR: 0.285934\n",
      "Training Epoch: 3 [43264/50000]\tLoss: 2.5206\tLR: 0.286189\n",
      "Training Epoch: 3 [43392/50000]\tLoss: 2.8040\tLR: 0.286445\n",
      "Training Epoch: 3 [43520/50000]\tLoss: 2.6080\tLR: 0.286701\n",
      "Training Epoch: 3 [43648/50000]\tLoss: 3.1833\tLR: 0.286957\n",
      "Training Epoch: 3 [43776/50000]\tLoss: 2.7613\tLR: 0.287212\n",
      "Training Epoch: 3 [43904/50000]\tLoss: 2.9241\tLR: 0.287468\n",
      "Training Epoch: 3 [44032/50000]\tLoss: 2.8891\tLR: 0.287724\n",
      "Training Epoch: 3 [44160/50000]\tLoss: 2.7828\tLR: 0.287980\n",
      "Training Epoch: 3 [44288/50000]\tLoss: 2.9255\tLR: 0.288235\n",
      "Training Epoch: 3 [44416/50000]\tLoss: 3.0677\tLR: 0.288491\n",
      "Training Epoch: 3 [44544/50000]\tLoss: 2.9354\tLR: 0.288747\n",
      "Training Epoch: 3 [44672/50000]\tLoss: 2.7618\tLR: 0.289003\n",
      "Training Epoch: 3 [44800/50000]\tLoss: 2.8218\tLR: 0.289258\n",
      "Training Epoch: 3 [44928/50000]\tLoss: 2.7718\tLR: 0.289514\n",
      "Training Epoch: 3 [45056/50000]\tLoss: 2.8701\tLR: 0.289770\n",
      "Training Epoch: 3 [45184/50000]\tLoss: 2.7581\tLR: 0.290026\n",
      "Training Epoch: 3 [45312/50000]\tLoss: 2.8640\tLR: 0.290281\n",
      "Training Epoch: 3 [45440/50000]\tLoss: 2.8330\tLR: 0.290537\n",
      "Training Epoch: 3 [45568/50000]\tLoss: 2.7009\tLR: 0.290793\n",
      "Training Epoch: 3 [45696/50000]\tLoss: 2.7068\tLR: 0.291049\n",
      "Training Epoch: 3 [45824/50000]\tLoss: 2.8018\tLR: 0.291304\n",
      "Training Epoch: 3 [45952/50000]\tLoss: 2.8409\tLR: 0.291560\n",
      "Training Epoch: 3 [46080/50000]\tLoss: 2.9913\tLR: 0.291816\n",
      "Training Epoch: 3 [46208/50000]\tLoss: 2.7423\tLR: 0.292072\n",
      "Training Epoch: 3 [46336/50000]\tLoss: 2.7549\tLR: 0.292327\n",
      "Training Epoch: 3 [46464/50000]\tLoss: 2.7002\tLR: 0.292583\n",
      "Training Epoch: 3 [46592/50000]\tLoss: 2.6714\tLR: 0.292839\n",
      "Training Epoch: 3 [46720/50000]\tLoss: 2.7830\tLR: 0.293095\n",
      "Training Epoch: 3 [46848/50000]\tLoss: 2.8003\tLR: 0.293350\n",
      "Training Epoch: 3 [46976/50000]\tLoss: 3.0493\tLR: 0.293606\n",
      "Training Epoch: 3 [47104/50000]\tLoss: 2.7729\tLR: 0.293862\n",
      "Training Epoch: 3 [47232/50000]\tLoss: 3.0465\tLR: 0.294118\n",
      "Training Epoch: 3 [47360/50000]\tLoss: 2.7028\tLR: 0.294373\n",
      "Training Epoch: 3 [47488/50000]\tLoss: 3.0915\tLR: 0.294629\n",
      "Training Epoch: 3 [47616/50000]\tLoss: 2.8161\tLR: 0.294885\n",
      "Training Epoch: 3 [47744/50000]\tLoss: 2.9567\tLR: 0.295141\n",
      "Training Epoch: 3 [47872/50000]\tLoss: 2.8822\tLR: 0.295396\n",
      "Training Epoch: 3 [48000/50000]\tLoss: 2.9772\tLR: 0.295652\n",
      "Training Epoch: 3 [48128/50000]\tLoss: 3.0033\tLR: 0.295908\n",
      "Training Epoch: 3 [48256/50000]\tLoss: 2.8723\tLR: 0.296164\n",
      "Training Epoch: 3 [48384/50000]\tLoss: 2.8928\tLR: 0.296419\n",
      "Training Epoch: 3 [48512/50000]\tLoss: 2.9250\tLR: 0.296675\n",
      "Training Epoch: 3 [48640/50000]\tLoss: 2.8515\tLR: 0.296931\n",
      "Training Epoch: 3 [48768/50000]\tLoss: 3.1401\tLR: 0.297187\n",
      "Training Epoch: 3 [48896/50000]\tLoss: 3.0560\tLR: 0.297442\n",
      "Training Epoch: 3 [49024/50000]\tLoss: 2.9196\tLR: 0.297698\n",
      "Training Epoch: 3 [49152/50000]\tLoss: 2.9279\tLR: 0.297954\n",
      "Training Epoch: 3 [49280/50000]\tLoss: 2.7586\tLR: 0.298210\n",
      "Training Epoch: 3 [49408/50000]\tLoss: 2.9330\tLR: 0.298465\n",
      "Training Epoch: 3 [49536/50000]\tLoss: 2.8473\tLR: 0.298721\n",
      "Training Epoch: 3 [49664/50000]\tLoss: 2.7123\tLR: 0.298977\n",
      "Training Epoch: 3 [49792/50000]\tLoss: 3.2907\tLR: 0.299233\n",
      "Training Epoch: 3 [49920/50000]\tLoss: 2.8743\tLR: 0.299488\n",
      "Training Epoch: 3 [50000/50000]\tLoss: 2.7101\tLR: 0.299744\n",
      "epoch 3 training time consumed: 488.46s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |    4207 GB |    4207 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |    4194 GB |    4194 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      12 GB |      12 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |    4207 GB |    4207 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |    4194 GB |    4194 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      12 GB |      12 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |    4144 GB |    4144 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |    4131 GB |    4131 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |      12 GB |      12 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |  446511    |  446256    |\n",
      "|       from large pool |      24    |      65    |  190171    |  190147    |\n",
      "|       from small pool |     231    |     274    |  256340    |  256109    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |  446511    |  446256    |\n",
      "|       from large pool |      24    |      65    |  190171    |  190147    |\n",
      "|       from small pool |     231    |     274    |  256340    |  256109    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      45    |  258401    |  258365    |\n",
      "|       from large pool |      10    |      23    |   91406    |   91396    |\n",
      "|       from small pool |      26    |      35    |  166995    |  166969    |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 3, Average loss: 0.0267, Accuracy: 0.2227, Time consumed:33.02s\n",
      "\n",
      "Training Epoch: 4 [128/50000]\tLoss: 2.7194\tLR: 0.100000\n",
      "Training Epoch: 4 [256/50000]\tLoss: 2.7406\tLR: 0.300256\n",
      "Training Epoch: 4 [384/50000]\tLoss: 2.6253\tLR: 0.300512\n",
      "Training Epoch: 4 [512/50000]\tLoss: 2.7467\tLR: 0.300767\n",
      "Training Epoch: 4 [640/50000]\tLoss: 2.6555\tLR: 0.301023\n",
      "Training Epoch: 4 [768/50000]\tLoss: 2.5826\tLR: 0.301279\n",
      "Training Epoch: 4 [896/50000]\tLoss: 2.8365\tLR: 0.301535\n",
      "Training Epoch: 4 [1024/50000]\tLoss: 2.7502\tLR: 0.301790\n",
      "Training Epoch: 4 [1152/50000]\tLoss: 2.8392\tLR: 0.302046\n",
      "Training Epoch: 4 [1280/50000]\tLoss: 2.9684\tLR: 0.302302\n",
      "Training Epoch: 4 [1408/50000]\tLoss: 2.7149\tLR: 0.302558\n",
      "Training Epoch: 4 [1536/50000]\tLoss: 2.7556\tLR: 0.302813\n",
      "Training Epoch: 4 [1664/50000]\tLoss: 2.8165\tLR: 0.303069\n",
      "Training Epoch: 4 [1792/50000]\tLoss: 2.5562\tLR: 0.303325\n",
      "Training Epoch: 4 [1920/50000]\tLoss: 2.6075\tLR: 0.303581\n",
      "Training Epoch: 4 [2048/50000]\tLoss: 2.5871\tLR: 0.303836\n",
      "Training Epoch: 4 [2176/50000]\tLoss: 2.6845\tLR: 0.304092\n",
      "Training Epoch: 4 [2304/50000]\tLoss: 2.8264\tLR: 0.304348\n",
      "Training Epoch: 4 [2432/50000]\tLoss: 3.0087\tLR: 0.304604\n",
      "Training Epoch: 4 [2560/50000]\tLoss: 2.8224\tLR: 0.304859\n",
      "Training Epoch: 4 [2688/50000]\tLoss: 2.6220\tLR: 0.305115\n",
      "Training Epoch: 4 [2816/50000]\tLoss: 3.0370\tLR: 0.305371\n",
      "Training Epoch: 4 [2944/50000]\tLoss: 2.8098\tLR: 0.305627\n",
      "Training Epoch: 4 [3072/50000]\tLoss: 2.6298\tLR: 0.305882\n",
      "Training Epoch: 4 [3200/50000]\tLoss: 2.8205\tLR: 0.306138\n",
      "Training Epoch: 4 [3328/50000]\tLoss: 2.8598\tLR: 0.306394\n",
      "Training Epoch: 4 [3456/50000]\tLoss: 2.9469\tLR: 0.306650\n",
      "Training Epoch: 4 [3584/50000]\tLoss: 2.7539\tLR: 0.306905\n",
      "Training Epoch: 4 [3712/50000]\tLoss: 2.8368\tLR: 0.307161\n",
      "Training Epoch: 4 [3840/50000]\tLoss: 2.7724\tLR: 0.307417\n",
      "Training Epoch: 4 [3968/50000]\tLoss: 2.8426\tLR: 0.307673\n",
      "Training Epoch: 4 [4096/50000]\tLoss: 2.8286\tLR: 0.307928\n",
      "Training Epoch: 4 [4224/50000]\tLoss: 2.8578\tLR: 0.308184\n",
      "Training Epoch: 4 [4352/50000]\tLoss: 2.8299\tLR: 0.308440\n",
      "Training Epoch: 4 [4480/50000]\tLoss: 2.5544\tLR: 0.308696\n",
      "Training Epoch: 4 [4608/50000]\tLoss: 2.8033\tLR: 0.308951\n",
      "Training Epoch: 4 [4736/50000]\tLoss: 2.8003\tLR: 0.309207\n",
      "Training Epoch: 4 [4864/50000]\tLoss: 2.6593\tLR: 0.309463\n",
      "Training Epoch: 4 [4992/50000]\tLoss: 2.5009\tLR: 0.309719\n",
      "Training Epoch: 4 [5120/50000]\tLoss: 2.7789\tLR: 0.309974\n",
      "Training Epoch: 4 [5248/50000]\tLoss: 2.7483\tLR: 0.310230\n",
      "Training Epoch: 4 [5376/50000]\tLoss: 2.6696\tLR: 0.310486\n",
      "Training Epoch: 4 [5504/50000]\tLoss: 2.4354\tLR: 0.310742\n",
      "Training Epoch: 4 [5632/50000]\tLoss: 2.8478\tLR: 0.310997\n",
      "Training Epoch: 4 [5760/50000]\tLoss: 2.8707\tLR: 0.311253\n",
      "Training Epoch: 4 [5888/50000]\tLoss: 3.0003\tLR: 0.311509\n",
      "Training Epoch: 4 [6016/50000]\tLoss: 3.1858\tLR: 0.311765\n",
      "Training Epoch: 4 [6144/50000]\tLoss: 2.8732\tLR: 0.312020\n",
      "Training Epoch: 4 [6272/50000]\tLoss: 2.9048\tLR: 0.312276\n",
      "Training Epoch: 4 [6400/50000]\tLoss: 2.8256\tLR: 0.312532\n",
      "Training Epoch: 4 [6528/50000]\tLoss: 2.7852\tLR: 0.312788\n",
      "Training Epoch: 4 [6656/50000]\tLoss: 2.7623\tLR: 0.313043\n",
      "Training Epoch: 4 [6784/50000]\tLoss: 2.6990\tLR: 0.313299\n",
      "Training Epoch: 4 [6912/50000]\tLoss: 2.8296\tLR: 0.313555\n",
      "Training Epoch: 4 [7040/50000]\tLoss: 2.7264\tLR: 0.313811\n",
      "Training Epoch: 4 [7168/50000]\tLoss: 2.8588\tLR: 0.314066\n",
      "Training Epoch: 4 [7296/50000]\tLoss: 2.8333\tLR: 0.314322\n",
      "Training Epoch: 4 [7424/50000]\tLoss: 2.8157\tLR: 0.314578\n",
      "Training Epoch: 4 [7552/50000]\tLoss: 2.8107\tLR: 0.314834\n",
      "Training Epoch: 4 [7680/50000]\tLoss: 2.6459\tLR: 0.315090\n",
      "Training Epoch: 4 [7808/50000]\tLoss: 2.8919\tLR: 0.315345\n",
      "Training Epoch: 4 [7936/50000]\tLoss: 2.9695\tLR: 0.315601\n",
      "Training Epoch: 4 [8064/50000]\tLoss: 3.1637\tLR: 0.315857\n",
      "Training Epoch: 4 [8192/50000]\tLoss: 2.8724\tLR: 0.316113\n",
      "Training Epoch: 4 [8320/50000]\tLoss: 3.2091\tLR: 0.316368\n",
      "Training Epoch: 4 [8448/50000]\tLoss: 2.9484\tLR: 0.316624\n",
      "Training Epoch: 4 [8576/50000]\tLoss: 2.7420\tLR: 0.316880\n",
      "Training Epoch: 4 [8704/50000]\tLoss: 2.7714\tLR: 0.317136\n",
      "Training Epoch: 4 [8832/50000]\tLoss: 2.7124\tLR: 0.317391\n",
      "Training Epoch: 4 [8960/50000]\tLoss: 2.8220\tLR: 0.317647\n",
      "Training Epoch: 4 [9088/50000]\tLoss: 3.0405\tLR: 0.317903\n",
      "Training Epoch: 4 [9216/50000]\tLoss: 2.8254\tLR: 0.318159\n",
      "Training Epoch: 4 [9344/50000]\tLoss: 3.0254\tLR: 0.318414\n",
      "Training Epoch: 4 [9472/50000]\tLoss: 2.7142\tLR: 0.318670\n",
      "Training Epoch: 4 [9600/50000]\tLoss: 2.8470\tLR: 0.318926\n",
      "Training Epoch: 4 [9728/50000]\tLoss: 2.6734\tLR: 0.319182\n",
      "Training Epoch: 4 [9856/50000]\tLoss: 2.5850\tLR: 0.319437\n",
      "Training Epoch: 4 [9984/50000]\tLoss: 2.4619\tLR: 0.319693\n",
      "Training Epoch: 4 [10112/50000]\tLoss: 2.7037\tLR: 0.319949\n",
      "Training Epoch: 4 [10240/50000]\tLoss: 2.7482\tLR: 0.320205\n",
      "Training Epoch: 4 [10368/50000]\tLoss: 2.6848\tLR: 0.320460\n",
      "Training Epoch: 4 [10496/50000]\tLoss: 2.7970\tLR: 0.320716\n",
      "Training Epoch: 4 [10624/50000]\tLoss: 2.8804\tLR: 0.320972\n",
      "Training Epoch: 4 [10752/50000]\tLoss: 2.6954\tLR: 0.321228\n",
      "Training Epoch: 4 [10880/50000]\tLoss: 2.5667\tLR: 0.321483\n",
      "Training Epoch: 4 [11008/50000]\tLoss: 2.6277\tLR: 0.321739\n",
      "Training Epoch: 4 [11136/50000]\tLoss: 2.7297\tLR: 0.321995\n",
      "Training Epoch: 4 [11264/50000]\tLoss: 2.7462\tLR: 0.322251\n",
      "Training Epoch: 4 [11392/50000]\tLoss: 2.7451\tLR: 0.322506\n",
      "Training Epoch: 4 [11520/50000]\tLoss: 2.5432\tLR: 0.322762\n",
      "Training Epoch: 4 [11648/50000]\tLoss: 2.7778\tLR: 0.323018\n",
      "Training Epoch: 4 [11776/50000]\tLoss: 2.6507\tLR: 0.323274\n",
      "Training Epoch: 4 [11904/50000]\tLoss: 2.9891\tLR: 0.323529\n",
      "Training Epoch: 4 [12032/50000]\tLoss: 2.7798\tLR: 0.323785\n",
      "Training Epoch: 4 [12160/50000]\tLoss: 2.5204\tLR: 0.324041\n",
      "Training Epoch: 4 [12288/50000]\tLoss: 2.4927\tLR: 0.324297\n",
      "Training Epoch: 4 [12416/50000]\tLoss: 2.7441\tLR: 0.324552\n",
      "Training Epoch: 4 [12544/50000]\tLoss: 2.8465\tLR: 0.324808\n",
      "Training Epoch: 4 [12672/50000]\tLoss: 2.5582\tLR: 0.325064\n",
      "Training Epoch: 4 [12800/50000]\tLoss: 2.6577\tLR: 0.325320\n",
      "Training Epoch: 4 [12928/50000]\tLoss: 2.7825\tLR: 0.325575\n",
      "Training Epoch: 4 [13056/50000]\tLoss: 2.9315\tLR: 0.325831\n",
      "Training Epoch: 4 [13184/50000]\tLoss: 2.8710\tLR: 0.326087\n",
      "Training Epoch: 4 [13312/50000]\tLoss: 2.5584\tLR: 0.326343\n",
      "Training Epoch: 4 [13440/50000]\tLoss: 2.9106\tLR: 0.326598\n",
      "Training Epoch: 4 [13568/50000]\tLoss: 2.7602\tLR: 0.326854\n",
      "Training Epoch: 4 [13696/50000]\tLoss: 2.7196\tLR: 0.327110\n",
      "Training Epoch: 4 [13824/50000]\tLoss: 2.6293\tLR: 0.327366\n",
      "Training Epoch: 4 [13952/50000]\tLoss: 2.6561\tLR: 0.327621\n",
      "Training Epoch: 4 [14080/50000]\tLoss: 2.8556\tLR: 0.327877\n",
      "Training Epoch: 4 [14208/50000]\tLoss: 3.0377\tLR: 0.328133\n",
      "Training Epoch: 4 [14336/50000]\tLoss: 2.8071\tLR: 0.328389\n",
      "Training Epoch: 4 [14464/50000]\tLoss: 2.9636\tLR: 0.328645\n",
      "Training Epoch: 4 [14592/50000]\tLoss: 2.9983\tLR: 0.328900\n",
      "Training Epoch: 4 [14720/50000]\tLoss: 2.6345\tLR: 0.329156\n",
      "Training Epoch: 4 [14848/50000]\tLoss: 2.8910\tLR: 0.329412\n",
      "Training Epoch: 4 [14976/50000]\tLoss: 2.8536\tLR: 0.329668\n",
      "Training Epoch: 4 [15104/50000]\tLoss: 2.6350\tLR: 0.329923\n",
      "Training Epoch: 4 [15232/50000]\tLoss: 2.7294\tLR: 0.330179\n",
      "Training Epoch: 4 [15360/50000]\tLoss: 2.9542\tLR: 0.330435\n",
      "Training Epoch: 4 [15488/50000]\tLoss: 2.8507\tLR: 0.330691\n",
      "Training Epoch: 4 [15616/50000]\tLoss: 2.7408\tLR: 0.330946\n",
      "Training Epoch: 4 [15744/50000]\tLoss: 2.5561\tLR: 0.331202\n",
      "Training Epoch: 4 [15872/50000]\tLoss: 2.7388\tLR: 0.331458\n",
      "Training Epoch: 4 [16000/50000]\tLoss: 2.9127\tLR: 0.331714\n",
      "Training Epoch: 4 [16128/50000]\tLoss: 2.7074\tLR: 0.331969\n",
      "Training Epoch: 4 [16256/50000]\tLoss: 2.8495\tLR: 0.332225\n",
      "Training Epoch: 4 [16384/50000]\tLoss: 2.7123\tLR: 0.332481\n",
      "Training Epoch: 4 [16512/50000]\tLoss: 2.5580\tLR: 0.332737\n",
      "Training Epoch: 4 [16640/50000]\tLoss: 2.5875\tLR: 0.332992\n",
      "Training Epoch: 4 [16768/50000]\tLoss: 2.9025\tLR: 0.333248\n",
      "Training Epoch: 4 [16896/50000]\tLoss: 2.9213\tLR: 0.333504\n",
      "Training Epoch: 4 [17024/50000]\tLoss: 3.0479\tLR: 0.333760\n",
      "Training Epoch: 4 [17152/50000]\tLoss: 2.8473\tLR: 0.334015\n",
      "Training Epoch: 4 [17280/50000]\tLoss: 2.6148\tLR: 0.334271\n",
      "Training Epoch: 4 [17408/50000]\tLoss: 2.9708\tLR: 0.334527\n",
      "Training Epoch: 4 [17536/50000]\tLoss: 2.8710\tLR: 0.334783\n",
      "Training Epoch: 4 [17664/50000]\tLoss: 2.8659\tLR: 0.335038\n",
      "Training Epoch: 4 [17792/50000]\tLoss: 2.8545\tLR: 0.335294\n",
      "Training Epoch: 4 [17920/50000]\tLoss: 2.7678\tLR: 0.335550\n",
      "Training Epoch: 4 [18048/50000]\tLoss: 3.1715\tLR: 0.335806\n",
      "Training Epoch: 4 [18176/50000]\tLoss: 2.5412\tLR: 0.336061\n",
      "Training Epoch: 4 [18304/50000]\tLoss: 2.6979\tLR: 0.336317\n",
      "Training Epoch: 4 [18432/50000]\tLoss: 2.5440\tLR: 0.336573\n",
      "Training Epoch: 4 [18560/50000]\tLoss: 2.8961\tLR: 0.336829\n",
      "Training Epoch: 4 [18688/50000]\tLoss: 2.7804\tLR: 0.337084\n",
      "Training Epoch: 4 [18816/50000]\tLoss: 2.8336\tLR: 0.337340\n",
      "Training Epoch: 4 [18944/50000]\tLoss: 2.8957\tLR: 0.337596\n",
      "Training Epoch: 4 [19072/50000]\tLoss: 2.7679\tLR: 0.337852\n",
      "Training Epoch: 4 [19200/50000]\tLoss: 2.5312\tLR: 0.338107\n",
      "Training Epoch: 4 [19328/50000]\tLoss: 2.7313\tLR: 0.338363\n",
      "Training Epoch: 4 [19456/50000]\tLoss: 2.7864\tLR: 0.338619\n",
      "Training Epoch: 4 [19584/50000]\tLoss: 2.7860\tLR: 0.338875\n",
      "Training Epoch: 4 [19712/50000]\tLoss: 2.8407\tLR: 0.339130\n",
      "Training Epoch: 4 [19840/50000]\tLoss: 2.6364\tLR: 0.339386\n",
      "Training Epoch: 4 [19968/50000]\tLoss: 2.8953\tLR: 0.339642\n",
      "Training Epoch: 4 [20096/50000]\tLoss: 2.6382\tLR: 0.339898\n",
      "Training Epoch: 4 [20224/50000]\tLoss: 2.5651\tLR: 0.340153\n",
      "Training Epoch: 4 [20352/50000]\tLoss: 2.4042\tLR: 0.340409\n",
      "Training Epoch: 4 [20480/50000]\tLoss: 2.8431\tLR: 0.340665\n",
      "Training Epoch: 4 [20608/50000]\tLoss: 3.0283\tLR: 0.340921\n",
      "Training Epoch: 4 [20736/50000]\tLoss: 2.8787\tLR: 0.341176\n",
      "Training Epoch: 4 [20864/50000]\tLoss: 2.8246\tLR: 0.341432\n",
      "Training Epoch: 4 [20992/50000]\tLoss: 2.7477\tLR: 0.341688\n",
      "Training Epoch: 4 [21120/50000]\tLoss: 2.8952\tLR: 0.341944\n",
      "Training Epoch: 4 [21248/50000]\tLoss: 2.6425\tLR: 0.342199\n",
      "Training Epoch: 4 [21376/50000]\tLoss: 2.4991\tLR: 0.342455\n",
      "Training Epoch: 4 [21504/50000]\tLoss: 2.9798\tLR: 0.342711\n",
      "Training Epoch: 4 [21632/50000]\tLoss: 2.7342\tLR: 0.342967\n",
      "Training Epoch: 4 [21760/50000]\tLoss: 2.5901\tLR: 0.343223\n",
      "Training Epoch: 4 [21888/50000]\tLoss: 2.4429\tLR: 0.343478\n",
      "Training Epoch: 4 [22016/50000]\tLoss: 2.6726\tLR: 0.343734\n",
      "Training Epoch: 4 [22144/50000]\tLoss: 2.8790\tLR: 0.343990\n",
      "Training Epoch: 4 [22272/50000]\tLoss: 2.7557\tLR: 0.344246\n",
      "Training Epoch: 4 [22400/50000]\tLoss: 2.9417\tLR: 0.344501\n",
      "Training Epoch: 4 [22528/50000]\tLoss: 2.9736\tLR: 0.344757\n",
      "Training Epoch: 4 [22656/50000]\tLoss: 2.7067\tLR: 0.345013\n",
      "Training Epoch: 4 [22784/50000]\tLoss: 2.8713\tLR: 0.345269\n",
      "Training Epoch: 4 [22912/50000]\tLoss: 2.7600\tLR: 0.345524\n",
      "Training Epoch: 4 [23040/50000]\tLoss: 2.4274\tLR: 0.345780\n",
      "Training Epoch: 4 [23168/50000]\tLoss: 2.5365\tLR: 0.346036\n",
      "Training Epoch: 4 [23296/50000]\tLoss: 2.9289\tLR: 0.346292\n",
      "Training Epoch: 4 [23424/50000]\tLoss: 2.6314\tLR: 0.346547\n",
      "Training Epoch: 4 [23552/50000]\tLoss: 2.9350\tLR: 0.346803\n",
      "Training Epoch: 4 [23680/50000]\tLoss: 2.6805\tLR: 0.347059\n",
      "Training Epoch: 4 [23808/50000]\tLoss: 2.9454\tLR: 0.347315\n",
      "Training Epoch: 4 [23936/50000]\tLoss: 2.7507\tLR: 0.347570\n",
      "Training Epoch: 4 [24064/50000]\tLoss: 2.6044\tLR: 0.347826\n",
      "Training Epoch: 4 [24192/50000]\tLoss: 2.5537\tLR: 0.348082\n",
      "Training Epoch: 4 [24320/50000]\tLoss: 2.9524\tLR: 0.348338\n",
      "Training Epoch: 4 [24448/50000]\tLoss: 2.6535\tLR: 0.348593\n",
      "Training Epoch: 4 [24576/50000]\tLoss: 2.5599\tLR: 0.348849\n",
      "Training Epoch: 4 [24704/50000]\tLoss: 2.8209\tLR: 0.349105\n",
      "Training Epoch: 4 [24832/50000]\tLoss: 2.6010\tLR: 0.349361\n",
      "Training Epoch: 4 [24960/50000]\tLoss: 3.0435\tLR: 0.349616\n",
      "Training Epoch: 4 [25088/50000]\tLoss: 2.7885\tLR: 0.349872\n",
      "Training Epoch: 4 [25216/50000]\tLoss: 2.9320\tLR: 0.350128\n",
      "Training Epoch: 4 [25344/50000]\tLoss: 2.7989\tLR: 0.350384\n",
      "Training Epoch: 4 [25472/50000]\tLoss: 3.0412\tLR: 0.350639\n",
      "Training Epoch: 4 [25600/50000]\tLoss: 2.8131\tLR: 0.350895\n",
      "Training Epoch: 4 [25728/50000]\tLoss: 2.9258\tLR: 0.351151\n",
      "Training Epoch: 4 [25856/50000]\tLoss: 2.7667\tLR: 0.351407\n",
      "Training Epoch: 4 [25984/50000]\tLoss: 2.5367\tLR: 0.351662\n",
      "Training Epoch: 4 [26112/50000]\tLoss: 2.8041\tLR: 0.351918\n",
      "Training Epoch: 4 [26240/50000]\tLoss: 2.8007\tLR: 0.352174\n",
      "Training Epoch: 4 [26368/50000]\tLoss: 3.0103\tLR: 0.352430\n",
      "Training Epoch: 4 [26496/50000]\tLoss: 2.8826\tLR: 0.352685\n",
      "Training Epoch: 4 [26624/50000]\tLoss: 2.8066\tLR: 0.352941\n",
      "Training Epoch: 4 [26752/50000]\tLoss: 2.7134\tLR: 0.353197\n",
      "Training Epoch: 4 [26880/50000]\tLoss: 2.7396\tLR: 0.353453\n",
      "Training Epoch: 4 [27008/50000]\tLoss: 2.7262\tLR: 0.353708\n",
      "Training Epoch: 4 [27136/50000]\tLoss: 2.6883\tLR: 0.353964\n",
      "Training Epoch: 4 [27264/50000]\tLoss: 2.6966\tLR: 0.354220\n",
      "Training Epoch: 4 [27392/50000]\tLoss: 2.6613\tLR: 0.354476\n",
      "Training Epoch: 4 [27520/50000]\tLoss: 2.8613\tLR: 0.354731\n",
      "Training Epoch: 4 [27648/50000]\tLoss: 2.7833\tLR: 0.354987\n",
      "Training Epoch: 4 [27776/50000]\tLoss: 3.0089\tLR: 0.355243\n",
      "Training Epoch: 4 [27904/50000]\tLoss: 2.9753\tLR: 0.355499\n",
      "Training Epoch: 4 [28032/50000]\tLoss: 2.8190\tLR: 0.355754\n",
      "Training Epoch: 4 [28160/50000]\tLoss: 2.9579\tLR: 0.356010\n",
      "Training Epoch: 4 [28288/50000]\tLoss: 2.7596\tLR: 0.356266\n",
      "Training Epoch: 4 [28416/50000]\tLoss: 2.5963\tLR: 0.356522\n",
      "Training Epoch: 4 [28544/50000]\tLoss: 2.5144\tLR: 0.356777\n",
      "Training Epoch: 4 [28672/50000]\tLoss: 2.5558\tLR: 0.357033\n",
      "Training Epoch: 4 [28800/50000]\tLoss: 2.3615\tLR: 0.357289\n",
      "Training Epoch: 4 [28928/50000]\tLoss: 2.6255\tLR: 0.357545\n",
      "Training Epoch: 4 [29056/50000]\tLoss: 2.9239\tLR: 0.357801\n",
      "Training Epoch: 4 [29184/50000]\tLoss: 2.6073\tLR: 0.358056\n",
      "Training Epoch: 4 [29312/50000]\tLoss: 2.6349\tLR: 0.358312\n",
      "Training Epoch: 4 [29440/50000]\tLoss: 2.8429\tLR: 0.358568\n",
      "Training Epoch: 4 [29568/50000]\tLoss: 2.6717\tLR: 0.358824\n",
      "Training Epoch: 4 [29696/50000]\tLoss: 3.0141\tLR: 0.359079\n",
      "Training Epoch: 4 [29824/50000]\tLoss: 2.8437\tLR: 0.359335\n",
      "Training Epoch: 4 [29952/50000]\tLoss: 2.9982\tLR: 0.359591\n",
      "Training Epoch: 4 [30080/50000]\tLoss: 2.3310\tLR: 0.359847\n",
      "Training Epoch: 4 [30208/50000]\tLoss: 2.7949\tLR: 0.360102\n",
      "Training Epoch: 4 [30336/50000]\tLoss: 2.7989\tLR: 0.360358\n",
      "Training Epoch: 4 [30464/50000]\tLoss: 2.5256\tLR: 0.360614\n",
      "Training Epoch: 4 [30592/50000]\tLoss: 2.5238\tLR: 0.360870\n",
      "Training Epoch: 4 [30720/50000]\tLoss: 2.6520\tLR: 0.361125\n",
      "Training Epoch: 4 [30848/50000]\tLoss: 2.8707\tLR: 0.361381\n",
      "Training Epoch: 4 [30976/50000]\tLoss: 2.6153\tLR: 0.361637\n",
      "Training Epoch: 4 [31104/50000]\tLoss: 2.9192\tLR: 0.361893\n",
      "Training Epoch: 4 [31232/50000]\tLoss: 2.7840\tLR: 0.362148\n",
      "Training Epoch: 4 [31360/50000]\tLoss: 2.5985\tLR: 0.362404\n",
      "Training Epoch: 4 [31488/50000]\tLoss: 2.6639\tLR: 0.362660\n",
      "Training Epoch: 4 [31616/50000]\tLoss: 2.8419\tLR: 0.362916\n",
      "Training Epoch: 4 [31744/50000]\tLoss: 2.5222\tLR: 0.363171\n",
      "Training Epoch: 4 [31872/50000]\tLoss: 2.5343\tLR: 0.363427\n",
      "Training Epoch: 4 [32000/50000]\tLoss: 2.7362\tLR: 0.363683\n",
      "Training Epoch: 4 [32128/50000]\tLoss: 2.3306\tLR: 0.363939\n",
      "Training Epoch: 4 [32256/50000]\tLoss: 2.8147\tLR: 0.364194\n",
      "Training Epoch: 4 [32384/50000]\tLoss: 2.4331\tLR: 0.364450\n",
      "Training Epoch: 4 [32512/50000]\tLoss: 2.4888\tLR: 0.364706\n",
      "Training Epoch: 4 [32640/50000]\tLoss: 2.5947\tLR: 0.364962\n",
      "Training Epoch: 4 [32768/50000]\tLoss: 2.6846\tLR: 0.365217\n",
      "Training Epoch: 4 [32896/50000]\tLoss: 2.9175\tLR: 0.365473\n",
      "Training Epoch: 4 [33024/50000]\tLoss: 2.5872\tLR: 0.365729\n",
      "Training Epoch: 4 [33152/50000]\tLoss: 2.7808\tLR: 0.365985\n",
      "Training Epoch: 4 [33280/50000]\tLoss: 2.8338\tLR: 0.366240\n",
      "Training Epoch: 4 [33408/50000]\tLoss: 2.7812\tLR: 0.366496\n",
      "Training Epoch: 4 [33536/50000]\tLoss: 2.7259\tLR: 0.366752\n",
      "Training Epoch: 4 [33664/50000]\tLoss: 2.8591\tLR: 0.367008\n",
      "Training Epoch: 4 [33792/50000]\tLoss: 2.6020\tLR: 0.367263\n",
      "Training Epoch: 4 [33920/50000]\tLoss: 2.9413\tLR: 0.367519\n",
      "Training Epoch: 4 [34048/50000]\tLoss: 2.8177\tLR: 0.367775\n",
      "Training Epoch: 4 [34176/50000]\tLoss: 2.4915\tLR: 0.368031\n",
      "Training Epoch: 4 [34304/50000]\tLoss: 2.6112\tLR: 0.368286\n",
      "Training Epoch: 4 [34432/50000]\tLoss: 2.8374\tLR: 0.368542\n",
      "Training Epoch: 4 [34560/50000]\tLoss: 2.9094\tLR: 0.368798\n",
      "Training Epoch: 4 [34688/50000]\tLoss: 2.6173\tLR: 0.369054\n",
      "Training Epoch: 4 [34816/50000]\tLoss: 2.8212\tLR: 0.369309\n",
      "Training Epoch: 4 [34944/50000]\tLoss: 2.8734\tLR: 0.369565\n",
      "Training Epoch: 4 [35072/50000]\tLoss: 3.1755\tLR: 0.369821\n",
      "Training Epoch: 4 [35200/50000]\tLoss: 2.9359\tLR: 0.370077\n",
      "Training Epoch: 4 [35328/50000]\tLoss: 2.8733\tLR: 0.370332\n",
      "Training Epoch: 4 [35456/50000]\tLoss: 2.7588\tLR: 0.370588\n",
      "Training Epoch: 4 [35584/50000]\tLoss: 2.7183\tLR: 0.370844\n",
      "Training Epoch: 4 [35712/50000]\tLoss: 2.7628\tLR: 0.371100\n",
      "Training Epoch: 4 [35840/50000]\tLoss: 2.8248\tLR: 0.371355\n",
      "Training Epoch: 4 [35968/50000]\tLoss: 2.6842\tLR: 0.371611\n",
      "Training Epoch: 4 [36096/50000]\tLoss: 2.8614\tLR: 0.371867\n",
      "Training Epoch: 4 [36224/50000]\tLoss: 2.6033\tLR: 0.372123\n",
      "Training Epoch: 4 [36352/50000]\tLoss: 2.5956\tLR: 0.372379\n",
      "Training Epoch: 4 [36480/50000]\tLoss: 2.7269\tLR: 0.372634\n",
      "Training Epoch: 4 [36608/50000]\tLoss: 2.8931\tLR: 0.372890\n",
      "Training Epoch: 4 [36736/50000]\tLoss: 2.5235\tLR: 0.373146\n",
      "Training Epoch: 4 [36864/50000]\tLoss: 2.8307\tLR: 0.373402\n",
      "Training Epoch: 4 [36992/50000]\tLoss: 2.6265\tLR: 0.373657\n",
      "Training Epoch: 4 [37120/50000]\tLoss: 2.7800\tLR: 0.373913\n",
      "Training Epoch: 4 [37248/50000]\tLoss: 3.0630\tLR: 0.374169\n",
      "Training Epoch: 4 [37376/50000]\tLoss: 2.6163\tLR: 0.374425\n",
      "Training Epoch: 4 [37504/50000]\tLoss: 2.9675\tLR: 0.374680\n",
      "Training Epoch: 4 [37632/50000]\tLoss: 2.9128\tLR: 0.374936\n",
      "Training Epoch: 4 [37760/50000]\tLoss: 2.6624\tLR: 0.375192\n",
      "Training Epoch: 4 [37888/50000]\tLoss: 2.6082\tLR: 0.375448\n",
      "Training Epoch: 4 [38016/50000]\tLoss: 2.5687\tLR: 0.375703\n",
      "Training Epoch: 4 [38144/50000]\tLoss: 2.7366\tLR: 0.375959\n",
      "Training Epoch: 4 [38272/50000]\tLoss: 2.7367\tLR: 0.376215\n",
      "Training Epoch: 4 [38400/50000]\tLoss: 2.7146\tLR: 0.376471\n",
      "Training Epoch: 4 [38528/50000]\tLoss: 2.7219\tLR: 0.376726\n",
      "Training Epoch: 4 [38656/50000]\tLoss: 2.7191\tLR: 0.376982\n",
      "Training Epoch: 4 [38784/50000]\tLoss: 2.4177\tLR: 0.377238\n",
      "Training Epoch: 4 [38912/50000]\tLoss: 2.9034\tLR: 0.377494\n",
      "Training Epoch: 4 [39040/50000]\tLoss: 2.7415\tLR: 0.377749\n",
      "Training Epoch: 4 [39168/50000]\tLoss: 2.5071\tLR: 0.378005\n",
      "Training Epoch: 4 [39296/50000]\tLoss: 2.5096\tLR: 0.378261\n",
      "Training Epoch: 4 [39424/50000]\tLoss: 2.5510\tLR: 0.378517\n",
      "Training Epoch: 4 [39552/50000]\tLoss: 2.6139\tLR: 0.378772\n",
      "Training Epoch: 4 [39680/50000]\tLoss: 2.5441\tLR: 0.379028\n",
      "Training Epoch: 4 [39808/50000]\tLoss: 2.5986\tLR: 0.379284\n",
      "Training Epoch: 4 [39936/50000]\tLoss: 2.6610\tLR: 0.379540\n",
      "Training Epoch: 4 [40064/50000]\tLoss: 2.7675\tLR: 0.379795\n",
      "Training Epoch: 4 [40192/50000]\tLoss: 2.5964\tLR: 0.380051\n",
      "Training Epoch: 4 [40320/50000]\tLoss: 2.7748\tLR: 0.380307\n",
      "Training Epoch: 4 [40448/50000]\tLoss: 2.5402\tLR: 0.380563\n",
      "Training Epoch: 4 [40576/50000]\tLoss: 2.8137\tLR: 0.380818\n",
      "Training Epoch: 4 [40704/50000]\tLoss: 2.6634\tLR: 0.381074\n",
      "Training Epoch: 4 [40832/50000]\tLoss: 2.7641\tLR: 0.381330\n",
      "Training Epoch: 4 [40960/50000]\tLoss: 2.5806\tLR: 0.381586\n",
      "Training Epoch: 4 [41088/50000]\tLoss: 2.5298\tLR: 0.381841\n",
      "Training Epoch: 4 [41216/50000]\tLoss: 2.6138\tLR: 0.382097\n",
      "Training Epoch: 4 [41344/50000]\tLoss: 2.7394\tLR: 0.382353\n",
      "Training Epoch: 4 [41472/50000]\tLoss: 2.9427\tLR: 0.382609\n",
      "Training Epoch: 4 [41600/50000]\tLoss: 2.8021\tLR: 0.382864\n",
      "Training Epoch: 4 [41728/50000]\tLoss: 2.5774\tLR: 0.383120\n",
      "Training Epoch: 4 [41856/50000]\tLoss: 2.7042\tLR: 0.383376\n",
      "Training Epoch: 4 [41984/50000]\tLoss: 2.4286\tLR: 0.383632\n",
      "Training Epoch: 4 [42112/50000]\tLoss: 2.6325\tLR: 0.383887\n",
      "Training Epoch: 4 [42240/50000]\tLoss: 2.8545\tLR: 0.384143\n",
      "Training Epoch: 4 [42368/50000]\tLoss: 2.5489\tLR: 0.384399\n",
      "Training Epoch: 4 [42496/50000]\tLoss: 2.7417\tLR: 0.384655\n",
      "Training Epoch: 4 [42624/50000]\tLoss: 2.7488\tLR: 0.384910\n",
      "Training Epoch: 4 [42752/50000]\tLoss: 2.6580\tLR: 0.385166\n",
      "Training Epoch: 4 [42880/50000]\tLoss: 2.9049\tLR: 0.385422\n",
      "Training Epoch: 4 [43008/50000]\tLoss: 2.6671\tLR: 0.385678\n",
      "Training Epoch: 4 [43136/50000]\tLoss: 2.5525\tLR: 0.385934\n",
      "Training Epoch: 4 [43264/50000]\tLoss: 2.7711\tLR: 0.386189\n",
      "Training Epoch: 4 [43392/50000]\tLoss: 2.6136\tLR: 0.386445\n",
      "Training Epoch: 4 [43520/50000]\tLoss: 2.5498\tLR: 0.386701\n",
      "Training Epoch: 4 [43648/50000]\tLoss: 2.5192\tLR: 0.386957\n",
      "Training Epoch: 4 [43776/50000]\tLoss: 2.6497\tLR: 0.387212\n",
      "Training Epoch: 4 [43904/50000]\tLoss: 2.8162\tLR: 0.387468\n",
      "Training Epoch: 4 [44032/50000]\tLoss: 2.7302\tLR: 0.387724\n",
      "Training Epoch: 4 [44160/50000]\tLoss: 2.5798\tLR: 0.387980\n",
      "Training Epoch: 4 [44288/50000]\tLoss: 2.6387\tLR: 0.388235\n",
      "Training Epoch: 4 [44416/50000]\tLoss: 2.6735\tLR: 0.388491\n",
      "Training Epoch: 4 [44544/50000]\tLoss: 2.6054\tLR: 0.388747\n",
      "Training Epoch: 4 [44672/50000]\tLoss: 2.6646\tLR: 0.389003\n",
      "Training Epoch: 4 [44800/50000]\tLoss: 2.8523\tLR: 0.389258\n",
      "Training Epoch: 4 [44928/50000]\tLoss: 2.6160\tLR: 0.389514\n",
      "Training Epoch: 4 [45056/50000]\tLoss: 2.8208\tLR: 0.389770\n",
      "Training Epoch: 4 [45184/50000]\tLoss: 2.6228\tLR: 0.390026\n",
      "Training Epoch: 4 [45312/50000]\tLoss: 2.7443\tLR: 0.390281\n",
      "Training Epoch: 4 [45440/50000]\tLoss: 2.7290\tLR: 0.390537\n",
      "Training Epoch: 4 [45568/50000]\tLoss: 2.8814\tLR: 0.390793\n",
      "Training Epoch: 4 [45696/50000]\tLoss: 2.8073\tLR: 0.391049\n",
      "Training Epoch: 4 [45824/50000]\tLoss: 2.8257\tLR: 0.391304\n",
      "Training Epoch: 4 [45952/50000]\tLoss: 2.4591\tLR: 0.391560\n",
      "Training Epoch: 4 [46080/50000]\tLoss: 2.6418\tLR: 0.391816\n",
      "Training Epoch: 4 [46208/50000]\tLoss: 2.7972\tLR: 0.392072\n",
      "Training Epoch: 4 [46336/50000]\tLoss: 2.7510\tLR: 0.392327\n",
      "Training Epoch: 4 [46464/50000]\tLoss: 2.6940\tLR: 0.392583\n",
      "Training Epoch: 4 [46592/50000]\tLoss: 2.6648\tLR: 0.392839\n",
      "Training Epoch: 4 [46720/50000]\tLoss: 2.6828\tLR: 0.393095\n",
      "Training Epoch: 4 [46848/50000]\tLoss: 2.7137\tLR: 0.393350\n",
      "Training Epoch: 4 [46976/50000]\tLoss: 2.6196\tLR: 0.393606\n",
      "Training Epoch: 4 [47104/50000]\tLoss: 2.8843\tLR: 0.393862\n",
      "Training Epoch: 4 [47232/50000]\tLoss: 2.8912\tLR: 0.394118\n",
      "Training Epoch: 4 [47360/50000]\tLoss: 2.7790\tLR: 0.394373\n",
      "Training Epoch: 4 [47488/50000]\tLoss: 2.6867\tLR: 0.394629\n",
      "Training Epoch: 4 [47616/50000]\tLoss: 2.5748\tLR: 0.394885\n",
      "Training Epoch: 4 [47744/50000]\tLoss: 2.5234\tLR: 0.395141\n",
      "Training Epoch: 4 [47872/50000]\tLoss: 2.8798\tLR: 0.395396\n",
      "Training Epoch: 4 [48000/50000]\tLoss: 3.0249\tLR: 0.395652\n",
      "Training Epoch: 4 [48128/50000]\tLoss: 2.7918\tLR: 0.395908\n",
      "Training Epoch: 4 [48256/50000]\tLoss: 2.8649\tLR: 0.396164\n",
      "Training Epoch: 4 [48384/50000]\tLoss: 2.5260\tLR: 0.396419\n",
      "Training Epoch: 4 [48512/50000]\tLoss: 2.8541\tLR: 0.396675\n",
      "Training Epoch: 4 [48640/50000]\tLoss: 2.9883\tLR: 0.396931\n",
      "Training Epoch: 4 [48768/50000]\tLoss: 2.6945\tLR: 0.397187\n",
      "Training Epoch: 4 [48896/50000]\tLoss: 2.6651\tLR: 0.397442\n",
      "Training Epoch: 4 [49024/50000]\tLoss: 2.4980\tLR: 0.397698\n",
      "Training Epoch: 4 [49152/50000]\tLoss: 2.5832\tLR: 0.397954\n",
      "Training Epoch: 4 [49280/50000]\tLoss: 2.6208\tLR: 0.398210\n",
      "Training Epoch: 4 [49408/50000]\tLoss: 2.8169\tLR: 0.398465\n",
      "Training Epoch: 4 [49536/50000]\tLoss: 2.6975\tLR: 0.398721\n",
      "Training Epoch: 4 [49664/50000]\tLoss: 2.8497\tLR: 0.398977\n",
      "Training Epoch: 4 [49792/50000]\tLoss: 2.7200\tLR: 0.399233\n",
      "Training Epoch: 4 [49920/50000]\tLoss: 2.5611\tLR: 0.399488\n",
      "Training Epoch: 4 [50000/50000]\tLoss: 2.2060\tLR: 0.399744\n",
      "epoch 4 training time consumed: 491.34s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |    5609 GB |    5609 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |    5592 GB |    5591 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      17 GB |      17 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |    5609 GB |    5609 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |    5592 GB |    5591 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      17 GB |      17 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |    5526 GB |    5526 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |    5509 GB |    5509 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |      17 GB |      17 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |  595158    |  594903    |\n",
      "|       from large pool |      24    |      65    |  253540    |  253516    |\n",
      "|       from small pool |     231    |     274    |  341618    |  341387    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |  595158    |  594903    |\n",
      "|       from large pool |      24    |      65    |  253540    |  253516    |\n",
      "|       from small pool |     231    |     274    |  341618    |  341387    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      45    |  344451    |  344414    |\n",
      "|       from large pool |      10    |      23    |  121866    |  121856    |\n",
      "|       from small pool |      27    |      35    |  222585    |  222558    |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 4, Average loss: 0.0235, Accuracy: 0.2785, Time consumed:34.34s\n",
      "\n",
      "Training Epoch: 5 [128/50000]\tLoss: 2.6395\tLR: 0.100000\n",
      "Training Epoch: 5 [256/50000]\tLoss: 2.4809\tLR: 0.400256\n",
      "Training Epoch: 5 [384/50000]\tLoss: 2.5689\tLR: 0.400512\n",
      "Training Epoch: 5 [512/50000]\tLoss: 2.3239\tLR: 0.400767\n",
      "Training Epoch: 5 [640/50000]\tLoss: 2.2312\tLR: 0.401023\n",
      "Training Epoch: 5 [768/50000]\tLoss: 2.5935\tLR: 0.401279\n",
      "Training Epoch: 5 [896/50000]\tLoss: 2.4207\tLR: 0.401535\n",
      "Training Epoch: 5 [1024/50000]\tLoss: 2.3129\tLR: 0.401790\n",
      "Training Epoch: 5 [1152/50000]\tLoss: 2.5358\tLR: 0.402046\n",
      "Training Epoch: 5 [1280/50000]\tLoss: 2.4794\tLR: 0.402302\n",
      "Training Epoch: 5 [1408/50000]\tLoss: 2.4628\tLR: 0.402558\n",
      "Training Epoch: 5 [1536/50000]\tLoss: 2.7900\tLR: 0.402813\n",
      "Training Epoch: 5 [1664/50000]\tLoss: 2.5965\tLR: 0.403069\n",
      "Training Epoch: 5 [1792/50000]\tLoss: 2.6674\tLR: 0.403325\n",
      "Training Epoch: 5 [1920/50000]\tLoss: 2.7044\tLR: 0.403581\n",
      "Training Epoch: 5 [2048/50000]\tLoss: 2.5746\tLR: 0.403836\n",
      "Training Epoch: 5 [2176/50000]\tLoss: 2.8206\tLR: 0.404092\n",
      "Training Epoch: 5 [2304/50000]\tLoss: 2.5625\tLR: 0.404348\n",
      "Training Epoch: 5 [2432/50000]\tLoss: 2.5757\tLR: 0.404604\n",
      "Training Epoch: 5 [2560/50000]\tLoss: 2.5876\tLR: 0.404859\n",
      "Training Epoch: 5 [2688/50000]\tLoss: 2.6528\tLR: 0.405115\n",
      "Training Epoch: 5 [2816/50000]\tLoss: 2.6880\tLR: 0.405371\n",
      "Training Epoch: 5 [2944/50000]\tLoss: 2.6401\tLR: 0.405627\n",
      "Training Epoch: 5 [3072/50000]\tLoss: 2.6961\tLR: 0.405882\n",
      "Training Epoch: 5 [3200/50000]\tLoss: 3.0974\tLR: 0.406138\n",
      "Training Epoch: 5 [3328/50000]\tLoss: 2.6221\tLR: 0.406394\n",
      "Training Epoch: 5 [3456/50000]\tLoss: 2.9732\tLR: 0.406650\n",
      "Training Epoch: 5 [3584/50000]\tLoss: 2.3538\tLR: 0.406905\n",
      "Training Epoch: 5 [3712/50000]\tLoss: 2.6915\tLR: 0.407161\n",
      "Training Epoch: 5 [3840/50000]\tLoss: 2.8088\tLR: 0.407417\n",
      "Training Epoch: 5 [3968/50000]\tLoss: 2.5954\tLR: 0.407673\n",
      "Training Epoch: 5 [4096/50000]\tLoss: 2.6493\tLR: 0.407928\n",
      "Training Epoch: 5 [4224/50000]\tLoss: 2.5200\tLR: 0.408184\n",
      "Training Epoch: 5 [4352/50000]\tLoss: 3.0413\tLR: 0.408440\n",
      "Training Epoch: 5 [4480/50000]\tLoss: 2.5424\tLR: 0.408696\n",
      "Training Epoch: 5 [4608/50000]\tLoss: 2.9180\tLR: 0.408951\n",
      "Training Epoch: 5 [4736/50000]\tLoss: 2.6622\tLR: 0.409207\n",
      "Training Epoch: 5 [4864/50000]\tLoss: 2.7201\tLR: 0.409463\n",
      "Training Epoch: 5 [4992/50000]\tLoss: 2.4436\tLR: 0.409719\n",
      "Training Epoch: 5 [5120/50000]\tLoss: 2.6713\tLR: 0.409974\n",
      "Training Epoch: 5 [5248/50000]\tLoss: 2.8731\tLR: 0.410230\n",
      "Training Epoch: 5 [5376/50000]\tLoss: 2.5896\tLR: 0.410486\n",
      "Training Epoch: 5 [5504/50000]\tLoss: 2.7132\tLR: 0.410742\n",
      "Training Epoch: 5 [5632/50000]\tLoss: 2.4902\tLR: 0.410997\n",
      "Training Epoch: 5 [5760/50000]\tLoss: 2.4838\tLR: 0.411253\n",
      "Training Epoch: 5 [5888/50000]\tLoss: 2.9637\tLR: 0.411509\n",
      "Training Epoch: 5 [6016/50000]\tLoss: 2.7401\tLR: 0.411765\n",
      "Training Epoch: 5 [6144/50000]\tLoss: 2.4527\tLR: 0.412020\n",
      "Training Epoch: 5 [6272/50000]\tLoss: 2.5618\tLR: 0.412276\n",
      "Training Epoch: 5 [6400/50000]\tLoss: 2.4657\tLR: 0.412532\n",
      "Training Epoch: 5 [6528/50000]\tLoss: 2.9492\tLR: 0.412788\n",
      "Training Epoch: 5 [6656/50000]\tLoss: 2.5077\tLR: 0.413043\n",
      "Training Epoch: 5 [6784/50000]\tLoss: 2.5884\tLR: 0.413299\n",
      "Training Epoch: 5 [6912/50000]\tLoss: 2.5988\tLR: 0.413555\n",
      "Training Epoch: 5 [7040/50000]\tLoss: 2.4501\tLR: 0.413811\n",
      "Training Epoch: 5 [7168/50000]\tLoss: 2.7126\tLR: 0.414066\n",
      "Training Epoch: 5 [7296/50000]\tLoss: 2.7144\tLR: 0.414322\n",
      "Training Epoch: 5 [7424/50000]\tLoss: 2.6696\tLR: 0.414578\n",
      "Training Epoch: 5 [7552/50000]\tLoss: 2.7875\tLR: 0.414834\n",
      "Training Epoch: 5 [7680/50000]\tLoss: 2.4776\tLR: 0.415090\n",
      "Training Epoch: 5 [7808/50000]\tLoss: 2.8333\tLR: 0.415345\n",
      "Training Epoch: 5 [7936/50000]\tLoss: 2.3325\tLR: 0.415601\n",
      "Training Epoch: 5 [8064/50000]\tLoss: 2.6226\tLR: 0.415857\n",
      "Training Epoch: 5 [8192/50000]\tLoss: 2.7215\tLR: 0.416113\n",
      "Training Epoch: 5 [8320/50000]\tLoss: 2.8040\tLR: 0.416368\n",
      "Training Epoch: 5 [8448/50000]\tLoss: 2.7076\tLR: 0.416624\n",
      "Training Epoch: 5 [8576/50000]\tLoss: 2.8827\tLR: 0.416880\n",
      "Training Epoch: 5 [8704/50000]\tLoss: 2.5420\tLR: 0.417136\n",
      "Training Epoch: 5 [8832/50000]\tLoss: 2.4220\tLR: 0.417391\n",
      "Training Epoch: 5 [8960/50000]\tLoss: 2.5427\tLR: 0.417647\n",
      "Training Epoch: 5 [9088/50000]\tLoss: 2.7956\tLR: 0.417903\n",
      "Training Epoch: 5 [9216/50000]\tLoss: 2.5850\tLR: 0.418159\n",
      "Training Epoch: 5 [9344/50000]\tLoss: 2.8625\tLR: 0.418414\n",
      "Training Epoch: 5 [9472/50000]\tLoss: 2.5010\tLR: 0.418670\n",
      "Training Epoch: 5 [9600/50000]\tLoss: 2.6461\tLR: 0.418926\n",
      "Training Epoch: 5 [9728/50000]\tLoss: 2.7768\tLR: 0.419182\n",
      "Training Epoch: 5 [9856/50000]\tLoss: 2.5249\tLR: 0.419437\n",
      "Training Epoch: 5 [9984/50000]\tLoss: 3.1361\tLR: 0.419693\n",
      "Training Epoch: 5 [10112/50000]\tLoss: 2.6583\tLR: 0.419949\n",
      "Training Epoch: 5 [10240/50000]\tLoss: 2.7407\tLR: 0.420205\n",
      "Training Epoch: 5 [10368/50000]\tLoss: 2.5960\tLR: 0.420460\n",
      "Training Epoch: 5 [10496/50000]\tLoss: 2.8864\tLR: 0.420716\n",
      "Training Epoch: 5 [10624/50000]\tLoss: 2.8728\tLR: 0.420972\n",
      "Training Epoch: 5 [10752/50000]\tLoss: 2.7615\tLR: 0.421228\n",
      "Training Epoch: 5 [10880/50000]\tLoss: 2.7843\tLR: 0.421483\n",
      "Training Epoch: 5 [11008/50000]\tLoss: 2.6574\tLR: 0.421739\n",
      "Training Epoch: 5 [11136/50000]\tLoss: 2.6203\tLR: 0.421995\n",
      "Training Epoch: 5 [11264/50000]\tLoss: 2.6393\tLR: 0.422251\n",
      "Training Epoch: 5 [11392/50000]\tLoss: 2.5590\tLR: 0.422506\n",
      "Training Epoch: 5 [11520/50000]\tLoss: 2.8902\tLR: 0.422762\n",
      "Training Epoch: 5 [11648/50000]\tLoss: 2.7239\tLR: 0.423018\n",
      "Training Epoch: 5 [11776/50000]\tLoss: 2.6601\tLR: 0.423274\n",
      "Training Epoch: 5 [11904/50000]\tLoss: 2.6572\tLR: 0.423529\n",
      "Training Epoch: 5 [12032/50000]\tLoss: 2.5128\tLR: 0.423785\n",
      "Training Epoch: 5 [12160/50000]\tLoss: 2.3498\tLR: 0.424041\n",
      "Training Epoch: 5 [12288/50000]\tLoss: 2.7931\tLR: 0.424297\n",
      "Training Epoch: 5 [12416/50000]\tLoss: 2.7061\tLR: 0.424552\n",
      "Training Epoch: 5 [12544/50000]\tLoss: 2.7471\tLR: 0.424808\n",
      "Training Epoch: 5 [12672/50000]\tLoss: 2.6138\tLR: 0.425064\n",
      "Training Epoch: 5 [12800/50000]\tLoss: 2.7383\tLR: 0.425320\n",
      "Training Epoch: 5 [12928/50000]\tLoss: 2.4628\tLR: 0.425575\n",
      "Training Epoch: 5 [13056/50000]\tLoss: 2.5864\tLR: 0.425831\n",
      "Training Epoch: 5 [13184/50000]\tLoss: 2.3287\tLR: 0.426087\n",
      "Training Epoch: 5 [13312/50000]\tLoss: 2.6320\tLR: 0.426343\n",
      "Training Epoch: 5 [13440/50000]\tLoss: 2.6117\tLR: 0.426598\n",
      "Training Epoch: 5 [13568/50000]\tLoss: 2.3767\tLR: 0.426854\n",
      "Training Epoch: 5 [13696/50000]\tLoss: 2.8300\tLR: 0.427110\n",
      "Training Epoch: 5 [13824/50000]\tLoss: 2.4079\tLR: 0.427366\n",
      "Training Epoch: 5 [13952/50000]\tLoss: 2.6367\tLR: 0.427621\n",
      "Training Epoch: 5 [14080/50000]\tLoss: 2.7836\tLR: 0.427877\n",
      "Training Epoch: 5 [14208/50000]\tLoss: 2.8539\tLR: 0.428133\n",
      "Training Epoch: 5 [14336/50000]\tLoss: 2.7301\tLR: 0.428389\n",
      "Training Epoch: 5 [14464/50000]\tLoss: 2.6735\tLR: 0.428645\n",
      "Training Epoch: 5 [14592/50000]\tLoss: 2.6082\tLR: 0.428900\n",
      "Training Epoch: 5 [14720/50000]\tLoss: 2.6074\tLR: 0.429156\n",
      "Training Epoch: 5 [14848/50000]\tLoss: 2.6466\tLR: 0.429412\n",
      "Training Epoch: 5 [14976/50000]\tLoss: 2.5284\tLR: 0.429668\n",
      "Training Epoch: 5 [15104/50000]\tLoss: 2.6767\tLR: 0.429923\n",
      "Training Epoch: 5 [15232/50000]\tLoss: 2.6881\tLR: 0.430179\n",
      "Training Epoch: 5 [15360/50000]\tLoss: 2.7681\tLR: 0.430435\n",
      "Training Epoch: 5 [15488/50000]\tLoss: 2.8221\tLR: 0.430691\n",
      "Training Epoch: 5 [15616/50000]\tLoss: 2.7724\tLR: 0.430946\n",
      "Training Epoch: 5 [15744/50000]\tLoss: 2.3882\tLR: 0.431202\n",
      "Training Epoch: 5 [15872/50000]\tLoss: 2.5721\tLR: 0.431458\n",
      "Training Epoch: 5 [16000/50000]\tLoss: 2.7070\tLR: 0.431714\n",
      "Training Epoch: 5 [16128/50000]\tLoss: 2.6394\tLR: 0.431969\n",
      "Training Epoch: 5 [16256/50000]\tLoss: 2.4126\tLR: 0.432225\n",
      "Training Epoch: 5 [16384/50000]\tLoss: 2.5367\tLR: 0.432481\n",
      "Training Epoch: 5 [16512/50000]\tLoss: 2.7322\tLR: 0.432737\n",
      "Training Epoch: 5 [16640/50000]\tLoss: 2.6551\tLR: 0.432992\n",
      "Training Epoch: 5 [16768/50000]\tLoss: 2.8574\tLR: 0.433248\n",
      "Training Epoch: 5 [16896/50000]\tLoss: 2.3612\tLR: 0.433504\n",
      "Training Epoch: 5 [17024/50000]\tLoss: 2.7677\tLR: 0.433760\n",
      "Training Epoch: 5 [17152/50000]\tLoss: 2.8298\tLR: 0.434015\n",
      "Training Epoch: 5 [17280/50000]\tLoss: 2.7072\tLR: 0.434271\n",
      "Training Epoch: 5 [17408/50000]\tLoss: 2.7567\tLR: 0.434527\n",
      "Training Epoch: 5 [17536/50000]\tLoss: 2.8945\tLR: 0.434783\n",
      "Training Epoch: 5 [17664/50000]\tLoss: 2.7583\tLR: 0.435038\n",
      "Training Epoch: 5 [17792/50000]\tLoss: 2.7421\tLR: 0.435294\n",
      "Training Epoch: 5 [17920/50000]\tLoss: 2.6204\tLR: 0.435550\n",
      "Training Epoch: 5 [18048/50000]\tLoss: 2.6289\tLR: 0.435806\n",
      "Training Epoch: 5 [18176/50000]\tLoss: 2.8911\tLR: 0.436061\n",
      "Training Epoch: 5 [18304/50000]\tLoss: 2.9646\tLR: 0.436317\n",
      "Training Epoch: 5 [18432/50000]\tLoss: 2.7259\tLR: 0.436573\n",
      "Training Epoch: 5 [18560/50000]\tLoss: 2.7398\tLR: 0.436829\n",
      "Training Epoch: 5 [18688/50000]\tLoss: 2.6917\tLR: 0.437084\n",
      "Training Epoch: 5 [18816/50000]\tLoss: 2.7180\tLR: 0.437340\n",
      "Training Epoch: 5 [18944/50000]\tLoss: 2.4941\tLR: 0.437596\n",
      "Training Epoch: 5 [19072/50000]\tLoss: 2.6906\tLR: 0.437852\n",
      "Training Epoch: 5 [19200/50000]\tLoss: 2.7135\tLR: 0.438107\n",
      "Training Epoch: 5 [19328/50000]\tLoss: 2.6620\tLR: 0.438363\n",
      "Training Epoch: 5 [19456/50000]\tLoss: 2.5800\tLR: 0.438619\n",
      "Training Epoch: 5 [19584/50000]\tLoss: 2.6032\tLR: 0.438875\n",
      "Training Epoch: 5 [19712/50000]\tLoss: 2.7904\tLR: 0.439130\n",
      "Training Epoch: 5 [19840/50000]\tLoss: 2.6617\tLR: 0.439386\n",
      "Training Epoch: 5 [19968/50000]\tLoss: 2.6214\tLR: 0.439642\n",
      "Training Epoch: 5 [20096/50000]\tLoss: 2.4971\tLR: 0.439898\n",
      "Training Epoch: 5 [20224/50000]\tLoss: 2.5642\tLR: 0.440153\n",
      "Training Epoch: 5 [20352/50000]\tLoss: 2.5726\tLR: 0.440409\n",
      "Training Epoch: 5 [20480/50000]\tLoss: 2.6849\tLR: 0.440665\n",
      "Training Epoch: 5 [20608/50000]\tLoss: 2.7785\tLR: 0.440921\n",
      "Training Epoch: 5 [20736/50000]\tLoss: 2.4772\tLR: 0.441176\n",
      "Training Epoch: 5 [20864/50000]\tLoss: 2.4380\tLR: 0.441432\n",
      "Training Epoch: 5 [20992/50000]\tLoss: 2.6563\tLR: 0.441688\n",
      "Training Epoch: 5 [21120/50000]\tLoss: 2.7381\tLR: 0.441944\n",
      "Training Epoch: 5 [21248/50000]\tLoss: 2.6434\tLR: 0.442199\n",
      "Training Epoch: 5 [21376/50000]\tLoss: 2.6353\tLR: 0.442455\n",
      "Training Epoch: 5 [21504/50000]\tLoss: 2.3685\tLR: 0.442711\n",
      "Training Epoch: 5 [21632/50000]\tLoss: 2.5732\tLR: 0.442967\n",
      "Training Epoch: 5 [21760/50000]\tLoss: 2.5549\tLR: 0.443223\n",
      "Training Epoch: 5 [21888/50000]\tLoss: 2.4775\tLR: 0.443478\n",
      "Training Epoch: 5 [22016/50000]\tLoss: 2.5601\tLR: 0.443734\n",
      "Training Epoch: 5 [22144/50000]\tLoss: 2.8040\tLR: 0.443990\n",
      "Training Epoch: 5 [22272/50000]\tLoss: 2.7219\tLR: 0.444246\n",
      "Training Epoch: 5 [22400/50000]\tLoss: 2.8348\tLR: 0.444501\n",
      "Training Epoch: 5 [22528/50000]\tLoss: 2.6577\tLR: 0.444757\n",
      "Training Epoch: 5 [22656/50000]\tLoss: 2.9794\tLR: 0.445013\n",
      "Training Epoch: 5 [22784/50000]\tLoss: 2.7578\tLR: 0.445269\n",
      "Training Epoch: 5 [22912/50000]\tLoss: 2.9531\tLR: 0.445524\n",
      "Training Epoch: 5 [23040/50000]\tLoss: 2.7263\tLR: 0.445780\n",
      "Training Epoch: 5 [23168/50000]\tLoss: 2.4600\tLR: 0.446036\n",
      "Training Epoch: 5 [23296/50000]\tLoss: 2.3848\tLR: 0.446292\n",
      "Training Epoch: 5 [23424/50000]\tLoss: 2.7518\tLR: 0.446547\n",
      "Training Epoch: 5 [23552/50000]\tLoss: 2.5429\tLR: 0.446803\n",
      "Training Epoch: 5 [23680/50000]\tLoss: 2.2299\tLR: 0.447059\n",
      "Training Epoch: 5 [23808/50000]\tLoss: 2.4904\tLR: 0.447315\n",
      "Training Epoch: 5 [23936/50000]\tLoss: 2.7237\tLR: 0.447570\n",
      "Training Epoch: 5 [24064/50000]\tLoss: 2.8486\tLR: 0.447826\n",
      "Training Epoch: 5 [24192/50000]\tLoss: 2.6123\tLR: 0.448082\n",
      "Training Epoch: 5 [24320/50000]\tLoss: 2.8719\tLR: 0.448338\n",
      "Training Epoch: 5 [24448/50000]\tLoss: 2.8425\tLR: 0.448593\n",
      "Training Epoch: 5 [24576/50000]\tLoss: 2.8596\tLR: 0.448849\n",
      "Training Epoch: 5 [24704/50000]\tLoss: 2.9616\tLR: 0.449105\n",
      "Training Epoch: 5 [24832/50000]\tLoss: 2.2518\tLR: 0.449361\n",
      "Training Epoch: 5 [24960/50000]\tLoss: 2.5159\tLR: 0.449616\n",
      "Training Epoch: 5 [25088/50000]\tLoss: 2.6495\tLR: 0.449872\n",
      "Training Epoch: 5 [25216/50000]\tLoss: 2.7124\tLR: 0.450128\n",
      "Training Epoch: 5 [25344/50000]\tLoss: 2.8532\tLR: 0.450384\n",
      "Training Epoch: 5 [25472/50000]\tLoss: 2.7975\tLR: 0.450639\n",
      "Training Epoch: 5 [25600/50000]\tLoss: 2.6364\tLR: 0.450895\n",
      "Training Epoch: 5 [25728/50000]\tLoss: 2.9003\tLR: 0.451151\n",
      "Training Epoch: 5 [25856/50000]\tLoss: 2.6918\tLR: 0.451407\n",
      "Training Epoch: 5 [25984/50000]\tLoss: 2.7125\tLR: 0.451662\n",
      "Training Epoch: 5 [26112/50000]\tLoss: 2.7239\tLR: 0.451918\n",
      "Training Epoch: 5 [26240/50000]\tLoss: 2.7199\tLR: 0.452174\n",
      "Training Epoch: 5 [26368/50000]\tLoss: 2.6766\tLR: 0.452430\n",
      "Training Epoch: 5 [26496/50000]\tLoss: 2.5444\tLR: 0.452685\n",
      "Training Epoch: 5 [26624/50000]\tLoss: 2.8040\tLR: 0.452941\n",
      "Training Epoch: 5 [26752/50000]\tLoss: 2.5875\tLR: 0.453197\n",
      "Training Epoch: 5 [26880/50000]\tLoss: 2.5728\tLR: 0.453453\n",
      "Training Epoch: 5 [27008/50000]\tLoss: 2.8126\tLR: 0.453708\n",
      "Training Epoch: 5 [27136/50000]\tLoss: 2.5596\tLR: 0.453964\n",
      "Training Epoch: 5 [27264/50000]\tLoss: 2.5325\tLR: 0.454220\n",
      "Training Epoch: 5 [27392/50000]\tLoss: 2.5744\tLR: 0.454476\n",
      "Training Epoch: 5 [27520/50000]\tLoss: 2.6329\tLR: 0.454731\n",
      "Training Epoch: 5 [27648/50000]\tLoss: 2.8034\tLR: 0.454987\n",
      "Training Epoch: 5 [27776/50000]\tLoss: 2.5558\tLR: 0.455243\n",
      "Training Epoch: 5 [27904/50000]\tLoss: 2.8768\tLR: 0.455499\n",
      "Training Epoch: 5 [28032/50000]\tLoss: 2.5917\tLR: 0.455754\n",
      "Training Epoch: 5 [28160/50000]\tLoss: 2.5440\tLR: 0.456010\n",
      "Training Epoch: 5 [28288/50000]\tLoss: 2.4915\tLR: 0.456266\n",
      "Training Epoch: 5 [28416/50000]\tLoss: 2.6672\tLR: 0.456522\n",
      "Training Epoch: 5 [28544/50000]\tLoss: 3.0971\tLR: 0.456777\n",
      "Training Epoch: 5 [28672/50000]\tLoss: 2.8123\tLR: 0.457033\n",
      "Training Epoch: 5 [28800/50000]\tLoss: 2.5493\tLR: 0.457289\n",
      "Training Epoch: 5 [28928/50000]\tLoss: 2.9036\tLR: 0.457545\n",
      "Training Epoch: 5 [29056/50000]\tLoss: 2.7184\tLR: 0.457801\n",
      "Training Epoch: 5 [29184/50000]\tLoss: 2.7760\tLR: 0.458056\n",
      "Training Epoch: 5 [29312/50000]\tLoss: 2.6626\tLR: 0.458312\n",
      "Training Epoch: 5 [29440/50000]\tLoss: 2.3803\tLR: 0.458568\n",
      "Training Epoch: 5 [29568/50000]\tLoss: 2.6725\tLR: 0.458824\n",
      "Training Epoch: 5 [29696/50000]\tLoss: 2.5739\tLR: 0.459079\n",
      "Training Epoch: 5 [29824/50000]\tLoss: 2.6290\tLR: 0.459335\n",
      "Training Epoch: 5 [29952/50000]\tLoss: 2.6855\tLR: 0.459591\n",
      "Training Epoch: 5 [30080/50000]\tLoss: 2.5822\tLR: 0.459847\n",
      "Training Epoch: 5 [30208/50000]\tLoss: 2.6354\tLR: 0.460102\n",
      "Training Epoch: 5 [30336/50000]\tLoss: 2.7154\tLR: 0.460358\n",
      "Training Epoch: 5 [30464/50000]\tLoss: 2.7781\tLR: 0.460614\n",
      "Training Epoch: 5 [30592/50000]\tLoss: 2.9729\tLR: 0.460870\n",
      "Training Epoch: 5 [30720/50000]\tLoss: 2.5484\tLR: 0.461125\n",
      "Training Epoch: 5 [30848/50000]\tLoss: 2.7948\tLR: 0.461381\n",
      "Training Epoch: 5 [30976/50000]\tLoss: 2.5559\tLR: 0.461637\n",
      "Training Epoch: 5 [31104/50000]\tLoss: 2.5756\tLR: 0.461893\n",
      "Training Epoch: 5 [31232/50000]\tLoss: 2.9950\tLR: 0.462148\n",
      "Training Epoch: 5 [31360/50000]\tLoss: 2.7232\tLR: 0.462404\n",
      "Training Epoch: 5 [31488/50000]\tLoss: 2.5945\tLR: 0.462660\n",
      "Training Epoch: 5 [31616/50000]\tLoss: 2.3270\tLR: 0.462916\n",
      "Training Epoch: 5 [31744/50000]\tLoss: 2.8756\tLR: 0.463171\n",
      "Training Epoch: 5 [31872/50000]\tLoss: 2.7476\tLR: 0.463427\n",
      "Training Epoch: 5 [32000/50000]\tLoss: 2.9484\tLR: 0.463683\n",
      "Training Epoch: 5 [32128/50000]\tLoss: 2.7769\tLR: 0.463939\n",
      "Training Epoch: 5 [32256/50000]\tLoss: 2.6813\tLR: 0.464194\n",
      "Training Epoch: 5 [32384/50000]\tLoss: 2.8756\tLR: 0.464450\n",
      "Training Epoch: 5 [32512/50000]\tLoss: 2.6146\tLR: 0.464706\n",
      "Training Epoch: 5 [32640/50000]\tLoss: 2.6680\tLR: 0.464962\n",
      "Training Epoch: 5 [32768/50000]\tLoss: 2.6118\tLR: 0.465217\n",
      "Training Epoch: 5 [32896/50000]\tLoss: 2.7158\tLR: 0.465473\n",
      "Training Epoch: 5 [33024/50000]\tLoss: 2.6267\tLR: 0.465729\n",
      "Training Epoch: 5 [33152/50000]\tLoss: 2.7583\tLR: 0.465985\n",
      "Training Epoch: 5 [33280/50000]\tLoss: 3.0027\tLR: 0.466240\n",
      "Training Epoch: 5 [33408/50000]\tLoss: 2.7126\tLR: 0.466496\n",
      "Training Epoch: 5 [33536/50000]\tLoss: 2.7289\tLR: 0.466752\n",
      "Training Epoch: 5 [33664/50000]\tLoss: 2.7963\tLR: 0.467008\n",
      "Training Epoch: 5 [33792/50000]\tLoss: 2.6449\tLR: 0.467263\n",
      "Training Epoch: 5 [33920/50000]\tLoss: 2.8858\tLR: 0.467519\n",
      "Training Epoch: 5 [34048/50000]\tLoss: 2.7301\tLR: 0.467775\n",
      "Training Epoch: 5 [34176/50000]\tLoss: 2.8215\tLR: 0.468031\n",
      "Training Epoch: 5 [34304/50000]\tLoss: 2.5100\tLR: 0.468286\n",
      "Training Epoch: 5 [34432/50000]\tLoss: 2.4936\tLR: 0.468542\n",
      "Training Epoch: 5 [34560/50000]\tLoss: 2.7650\tLR: 0.468798\n",
      "Training Epoch: 5 [34688/50000]\tLoss: 2.8140\tLR: 0.469054\n",
      "Training Epoch: 5 [34816/50000]\tLoss: 2.4839\tLR: 0.469309\n",
      "Training Epoch: 5 [34944/50000]\tLoss: 2.7217\tLR: 0.469565\n",
      "Training Epoch: 5 [35072/50000]\tLoss: 2.9488\tLR: 0.469821\n",
      "Training Epoch: 5 [35200/50000]\tLoss: 2.6358\tLR: 0.470077\n",
      "Training Epoch: 5 [35328/50000]\tLoss: 2.6668\tLR: 0.470332\n",
      "Training Epoch: 5 [35456/50000]\tLoss: 2.4749\tLR: 0.470588\n",
      "Training Epoch: 5 [35584/50000]\tLoss: 2.7843\tLR: 0.470844\n",
      "Training Epoch: 5 [35712/50000]\tLoss: 2.7318\tLR: 0.471100\n",
      "Training Epoch: 5 [35840/50000]\tLoss: 2.8972\tLR: 0.471355\n",
      "Training Epoch: 5 [35968/50000]\tLoss: 2.7184\tLR: 0.471611\n",
      "Training Epoch: 5 [36096/50000]\tLoss: 2.6019\tLR: 0.471867\n",
      "Training Epoch: 5 [36224/50000]\tLoss: 2.7121\tLR: 0.472123\n",
      "Training Epoch: 5 [36352/50000]\tLoss: 2.4959\tLR: 0.472379\n",
      "Training Epoch: 5 [36480/50000]\tLoss: 2.5977\tLR: 0.472634\n",
      "Training Epoch: 5 [36608/50000]\tLoss: 2.6754\tLR: 0.472890\n",
      "Training Epoch: 5 [36736/50000]\tLoss: 2.5136\tLR: 0.473146\n",
      "Training Epoch: 5 [36864/50000]\tLoss: 2.6192\tLR: 0.473402\n",
      "Training Epoch: 5 [36992/50000]\tLoss: 2.4027\tLR: 0.473657\n",
      "Training Epoch: 5 [37120/50000]\tLoss: 2.4737\tLR: 0.473913\n",
      "Training Epoch: 5 [37248/50000]\tLoss: 2.4724\tLR: 0.474169\n",
      "Training Epoch: 5 [37376/50000]\tLoss: 2.7955\tLR: 0.474425\n",
      "Training Epoch: 5 [37504/50000]\tLoss: 2.3690\tLR: 0.474680\n",
      "Training Epoch: 5 [37632/50000]\tLoss: 2.7037\tLR: 0.474936\n",
      "Training Epoch: 5 [37760/50000]\tLoss: 2.7687\tLR: 0.475192\n",
      "Training Epoch: 5 [37888/50000]\tLoss: 2.5430\tLR: 0.475448\n",
      "Training Epoch: 5 [38016/50000]\tLoss: 2.4676\tLR: 0.475703\n",
      "Training Epoch: 5 [38144/50000]\tLoss: 2.7026\tLR: 0.475959\n",
      "Training Epoch: 5 [38272/50000]\tLoss: 2.5911\tLR: 0.476215\n",
      "Training Epoch: 5 [38400/50000]\tLoss: 3.0711\tLR: 0.476471\n",
      "Training Epoch: 5 [38528/50000]\tLoss: 2.6299\tLR: 0.476726\n",
      "Training Epoch: 5 [38656/50000]\tLoss: 2.6568\tLR: 0.476982\n",
      "Training Epoch: 5 [38784/50000]\tLoss: 2.5846\tLR: 0.477238\n",
      "Training Epoch: 5 [38912/50000]\tLoss: 2.7316\tLR: 0.477494\n",
      "Training Epoch: 5 [39040/50000]\tLoss: 2.8286\tLR: 0.477749\n",
      "Training Epoch: 5 [39168/50000]\tLoss: 2.5063\tLR: 0.478005\n",
      "Training Epoch: 5 [39296/50000]\tLoss: 2.7371\tLR: 0.478261\n",
      "Training Epoch: 5 [39424/50000]\tLoss: 2.5492\tLR: 0.478517\n",
      "Training Epoch: 5 [39552/50000]\tLoss: 2.5479\tLR: 0.478772\n",
      "Training Epoch: 5 [39680/50000]\tLoss: 2.4927\tLR: 0.479028\n",
      "Training Epoch: 5 [39808/50000]\tLoss: 2.3605\tLR: 0.479284\n",
      "Training Epoch: 5 [39936/50000]\tLoss: 2.8852\tLR: 0.479540\n",
      "Training Epoch: 5 [40064/50000]\tLoss: 2.9143\tLR: 0.479795\n",
      "Training Epoch: 5 [40192/50000]\tLoss: 2.5705\tLR: 0.480051\n",
      "Training Epoch: 5 [40320/50000]\tLoss: 2.7949\tLR: 0.480307\n",
      "Training Epoch: 5 [40448/50000]\tLoss: 2.7580\tLR: 0.480563\n",
      "Training Epoch: 5 [40576/50000]\tLoss: 2.7624\tLR: 0.480818\n",
      "Training Epoch: 5 [40704/50000]\tLoss: 2.8535\tLR: 0.481074\n",
      "Training Epoch: 5 [40832/50000]\tLoss: 2.6971\tLR: 0.481330\n",
      "Training Epoch: 5 [40960/50000]\tLoss: 2.6577\tLR: 0.481586\n",
      "Training Epoch: 5 [41088/50000]\tLoss: 2.8019\tLR: 0.481841\n",
      "Training Epoch: 5 [41216/50000]\tLoss: 2.9116\tLR: 0.482097\n",
      "Training Epoch: 5 [41344/50000]\tLoss: 2.5030\tLR: 0.482353\n",
      "Training Epoch: 5 [41472/50000]\tLoss: 2.7040\tLR: 0.482609\n",
      "Training Epoch: 5 [41600/50000]\tLoss: 2.6460\tLR: 0.482864\n",
      "Training Epoch: 5 [41728/50000]\tLoss: 2.6937\tLR: 0.483120\n",
      "Training Epoch: 5 [41856/50000]\tLoss: 2.9426\tLR: 0.483376\n",
      "Training Epoch: 5 [41984/50000]\tLoss: 2.6147\tLR: 0.483632\n",
      "Training Epoch: 5 [42112/50000]\tLoss: 2.6776\tLR: 0.483887\n",
      "Training Epoch: 5 [42240/50000]\tLoss: 2.7302\tLR: 0.484143\n",
      "Training Epoch: 5 [42368/50000]\tLoss: 2.5190\tLR: 0.484399\n",
      "Training Epoch: 5 [42496/50000]\tLoss: 2.4353\tLR: 0.484655\n",
      "Training Epoch: 5 [42624/50000]\tLoss: 2.8099\tLR: 0.484910\n",
      "Training Epoch: 5 [42752/50000]\tLoss: 2.7524\tLR: 0.485166\n",
      "Training Epoch: 5 [42880/50000]\tLoss: 2.4138\tLR: 0.485422\n",
      "Training Epoch: 5 [43008/50000]\tLoss: 2.8465\tLR: 0.485678\n",
      "Training Epoch: 5 [43136/50000]\tLoss: 2.7407\tLR: 0.485934\n",
      "Training Epoch: 5 [43264/50000]\tLoss: 2.4464\tLR: 0.486189\n",
      "Training Epoch: 5 [43392/50000]\tLoss: 2.4436\tLR: 0.486445\n",
      "Training Epoch: 5 [43520/50000]\tLoss: 2.6404\tLR: 0.486701\n",
      "Training Epoch: 5 [43648/50000]\tLoss: 2.6921\tLR: 0.486957\n",
      "Training Epoch: 5 [43776/50000]\tLoss: 2.8388\tLR: 0.487212\n",
      "Training Epoch: 5 [43904/50000]\tLoss: 2.7165\tLR: 0.487468\n",
      "Training Epoch: 5 [44032/50000]\tLoss: 2.9473\tLR: 0.487724\n",
      "Training Epoch: 5 [44160/50000]\tLoss: 2.6681\tLR: 0.487980\n",
      "Training Epoch: 5 [44288/50000]\tLoss: 2.5299\tLR: 0.488235\n",
      "Training Epoch: 5 [44416/50000]\tLoss: 2.7208\tLR: 0.488491\n",
      "Training Epoch: 5 [44544/50000]\tLoss: 2.3487\tLR: 0.488747\n",
      "Training Epoch: 5 [44672/50000]\tLoss: 2.5685\tLR: 0.489003\n",
      "Training Epoch: 5 [44800/50000]\tLoss: 2.8106\tLR: 0.489258\n",
      "Training Epoch: 5 [44928/50000]\tLoss: 2.4843\tLR: 0.489514\n",
      "Training Epoch: 5 [45056/50000]\tLoss: 2.5858\tLR: 0.489770\n",
      "Training Epoch: 5 [45184/50000]\tLoss: 3.0638\tLR: 0.490026\n",
      "Training Epoch: 5 [45312/50000]\tLoss: 2.4961\tLR: 0.490281\n",
      "Training Epoch: 5 [45440/50000]\tLoss: 2.8342\tLR: 0.490537\n",
      "Training Epoch: 5 [45568/50000]\tLoss: 2.9661\tLR: 0.490793\n",
      "Training Epoch: 5 [45696/50000]\tLoss: 2.8244\tLR: 0.491049\n",
      "Training Epoch: 5 [45824/50000]\tLoss: 2.7071\tLR: 0.491304\n",
      "Training Epoch: 5 [45952/50000]\tLoss: 2.3788\tLR: 0.491560\n",
      "Training Epoch: 5 [46080/50000]\tLoss: 2.5742\tLR: 0.491816\n",
      "Training Epoch: 5 [46208/50000]\tLoss: 2.8650\tLR: 0.492072\n",
      "Training Epoch: 5 [46336/50000]\tLoss: 2.7437\tLR: 0.492327\n",
      "Training Epoch: 5 [46464/50000]\tLoss: 2.8032\tLR: 0.492583\n",
      "Training Epoch: 5 [46592/50000]\tLoss: 2.7890\tLR: 0.492839\n",
      "Training Epoch: 5 [46720/50000]\tLoss: 2.7564\tLR: 0.493095\n",
      "Training Epoch: 5 [46848/50000]\tLoss: 2.7211\tLR: 0.493350\n",
      "Training Epoch: 5 [46976/50000]\tLoss: 2.5945\tLR: 0.493606\n",
      "Training Epoch: 5 [47104/50000]\tLoss: 2.7488\tLR: 0.493862\n",
      "Training Epoch: 5 [47232/50000]\tLoss: 2.7432\tLR: 0.494118\n",
      "Training Epoch: 5 [47360/50000]\tLoss: 2.6185\tLR: 0.494373\n",
      "Training Epoch: 5 [47488/50000]\tLoss: 2.6569\tLR: 0.494629\n",
      "Training Epoch: 5 [47616/50000]\tLoss: 2.9635\tLR: 0.494885\n",
      "Training Epoch: 5 [47744/50000]\tLoss: 2.5289\tLR: 0.495141\n",
      "Training Epoch: 5 [47872/50000]\tLoss: 2.6918\tLR: 0.495396\n",
      "Training Epoch: 5 [48000/50000]\tLoss: 2.5285\tLR: 0.495652\n",
      "Training Epoch: 5 [48128/50000]\tLoss: 2.4928\tLR: 0.495908\n",
      "Training Epoch: 5 [48256/50000]\tLoss: 2.7879\tLR: 0.496164\n",
      "Training Epoch: 5 [48384/50000]\tLoss: 2.4167\tLR: 0.496419\n",
      "Training Epoch: 5 [48512/50000]\tLoss: 2.5015\tLR: 0.496675\n",
      "Training Epoch: 5 [48640/50000]\tLoss: 2.8022\tLR: 0.496931\n",
      "Training Epoch: 5 [48768/50000]\tLoss: 2.5942\tLR: 0.497187\n",
      "Training Epoch: 5 [48896/50000]\tLoss: 2.6902\tLR: 0.497442\n",
      "Training Epoch: 5 [49024/50000]\tLoss: 2.7090\tLR: 0.497698\n",
      "Training Epoch: 5 [49152/50000]\tLoss: 2.8298\tLR: 0.497954\n",
      "Training Epoch: 5 [49280/50000]\tLoss: 2.7150\tLR: 0.498210\n",
      "Training Epoch: 5 [49408/50000]\tLoss: 2.5855\tLR: 0.498465\n",
      "Training Epoch: 5 [49536/50000]\tLoss: 2.5633\tLR: 0.498721\n",
      "Training Epoch: 5 [49664/50000]\tLoss: 2.5188\tLR: 0.498977\n",
      "Training Epoch: 5 [49792/50000]\tLoss: 2.4517\tLR: 0.499233\n",
      "Training Epoch: 5 [49920/50000]\tLoss: 2.6954\tLR: 0.499488\n",
      "Training Epoch: 5 [50000/50000]\tLoss: 2.8243\tLR: 0.499744\n",
      "epoch 5 training time consumed: 490.88s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |    7011 GB |    7011 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |    6989 GB |    6989 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      21 GB |      21 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |    7011 GB |    7011 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |    6989 GB |    6989 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      21 GB |      21 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |    6909 GB |    6909 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |    6887 GB |    6887 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |      21 GB |      21 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |  743805    |  743550    |\n",
      "|       from large pool |      24    |      65    |  316909    |  316885    |\n",
      "|       from small pool |     231    |     274    |  426896    |  426665    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |  743805    |  743550    |\n",
      "|       from large pool |      24    |      65    |  316909    |  316885    |\n",
      "|       from small pool |     231    |     274    |  426896    |  426665    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      45    |  430463    |  430427    |\n",
      "|       from large pool |      10    |      23    |  152326    |  152316    |\n",
      "|       from small pool |      26    |      35    |  278137    |  278111    |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 5, Average loss: 0.0335, Accuracy: 0.1940, Time consumed:37.11s\n",
      "\n",
      "Training Epoch: 6 [128/50000]\tLoss: 2.8363\tLR: 0.100000\n",
      "Training Epoch: 6 [256/50000]\tLoss: 2.5141\tLR: 0.500256\n",
      "Training Epoch: 6 [384/50000]\tLoss: 2.5374\tLR: 0.500512\n",
      "Training Epoch: 6 [512/50000]\tLoss: 2.4877\tLR: 0.500767\n",
      "Training Epoch: 6 [640/50000]\tLoss: 2.5908\tLR: 0.501023\n",
      "Training Epoch: 6 [768/50000]\tLoss: 2.4735\tLR: 0.501279\n",
      "Training Epoch: 6 [896/50000]\tLoss: 2.4907\tLR: 0.501535\n",
      "Training Epoch: 6 [1024/50000]\tLoss: 2.5182\tLR: 0.501790\n",
      "Training Epoch: 6 [1152/50000]\tLoss: 2.6486\tLR: 0.502046\n",
      "Training Epoch: 6 [1280/50000]\tLoss: 2.8254\tLR: 0.502302\n",
      "Training Epoch: 6 [1408/50000]\tLoss: 2.8778\tLR: 0.502558\n",
      "Training Epoch: 6 [1536/50000]\tLoss: 2.9389\tLR: 0.502813\n",
      "Training Epoch: 6 [1664/50000]\tLoss: 2.6862\tLR: 0.503069\n",
      "Training Epoch: 6 [1792/50000]\tLoss: 2.5607\tLR: 0.503325\n",
      "Training Epoch: 6 [1920/50000]\tLoss: 2.3527\tLR: 0.503581\n",
      "Training Epoch: 6 [2048/50000]\tLoss: 2.6057\tLR: 0.503836\n",
      "Training Epoch: 6 [2176/50000]\tLoss: 2.6352\tLR: 0.504092\n",
      "Training Epoch: 6 [2304/50000]\tLoss: 2.5739\tLR: 0.504348\n",
      "Training Epoch: 6 [2432/50000]\tLoss: 2.9107\tLR: 0.504604\n",
      "Training Epoch: 6 [2560/50000]\tLoss: 2.8793\tLR: 0.504859\n",
      "Training Epoch: 6 [2688/50000]\tLoss: 2.5312\tLR: 0.505115\n",
      "Training Epoch: 6 [2816/50000]\tLoss: 2.5621\tLR: 0.505371\n",
      "Training Epoch: 6 [2944/50000]\tLoss: 2.5419\tLR: 0.505627\n",
      "Training Epoch: 6 [3072/50000]\tLoss: 2.6694\tLR: 0.505882\n",
      "Training Epoch: 6 [3200/50000]\tLoss: 2.6975\tLR: 0.506138\n",
      "Training Epoch: 6 [3328/50000]\tLoss: 2.5817\tLR: 0.506394\n",
      "Training Epoch: 6 [3456/50000]\tLoss: 2.5682\tLR: 0.506650\n",
      "Training Epoch: 6 [3584/50000]\tLoss: 2.8222\tLR: 0.506905\n",
      "Training Epoch: 6 [3712/50000]\tLoss: 3.0101\tLR: 0.507161\n",
      "Training Epoch: 6 [3840/50000]\tLoss: 2.5958\tLR: 0.507417\n",
      "Training Epoch: 6 [3968/50000]\tLoss: 2.6025\tLR: 0.507673\n",
      "Training Epoch: 6 [4096/50000]\tLoss: 2.5228\tLR: 0.507928\n",
      "Training Epoch: 6 [4224/50000]\tLoss: 2.8345\tLR: 0.508184\n",
      "Training Epoch: 6 [4352/50000]\tLoss: 2.5865\tLR: 0.508440\n",
      "Training Epoch: 6 [4480/50000]\tLoss: 2.7341\tLR: 0.508696\n",
      "Training Epoch: 6 [4608/50000]\tLoss: 2.7159\tLR: 0.508951\n",
      "Training Epoch: 6 [4736/50000]\tLoss: 2.6391\tLR: 0.509207\n",
      "Training Epoch: 6 [4864/50000]\tLoss: 2.6435\tLR: 0.509463\n",
      "Training Epoch: 6 [4992/50000]\tLoss: 2.6444\tLR: 0.509719\n",
      "Training Epoch: 6 [5120/50000]\tLoss: 2.4889\tLR: 0.509974\n",
      "Training Epoch: 6 [5248/50000]\tLoss: 2.7177\tLR: 0.510230\n",
      "Training Epoch: 6 [5376/50000]\tLoss: 2.3915\tLR: 0.510486\n",
      "Training Epoch: 6 [5504/50000]\tLoss: 2.6584\tLR: 0.510742\n",
      "Training Epoch: 6 [5632/50000]\tLoss: 2.6683\tLR: 0.510997\n",
      "Training Epoch: 6 [5760/50000]\tLoss: 2.8009\tLR: 0.511253\n",
      "Training Epoch: 6 [5888/50000]\tLoss: 2.6649\tLR: 0.511509\n",
      "Training Epoch: 6 [6016/50000]\tLoss: 2.4137\tLR: 0.511765\n",
      "Training Epoch: 6 [6144/50000]\tLoss: 2.8656\tLR: 0.512020\n",
      "Training Epoch: 6 [6272/50000]\tLoss: 2.5560\tLR: 0.512276\n",
      "Training Epoch: 6 [6400/50000]\tLoss: 2.5054\tLR: 0.512532\n",
      "Training Epoch: 6 [6528/50000]\tLoss: 2.3166\tLR: 0.512788\n",
      "Training Epoch: 6 [6656/50000]\tLoss: 2.7744\tLR: 0.513043\n",
      "Training Epoch: 6 [6784/50000]\tLoss: 2.8886\tLR: 0.513299\n",
      "Training Epoch: 6 [6912/50000]\tLoss: 2.7882\tLR: 0.513555\n",
      "Training Epoch: 6 [7040/50000]\tLoss: 2.7860\tLR: 0.513811\n",
      "Training Epoch: 6 [7168/50000]\tLoss: 2.6711\tLR: 0.514066\n",
      "Training Epoch: 6 [7296/50000]\tLoss: 2.9715\tLR: 0.514322\n",
      "Training Epoch: 6 [7424/50000]\tLoss: 2.5754\tLR: 0.514578\n",
      "Training Epoch: 6 [7552/50000]\tLoss: 2.6571\tLR: 0.514834\n",
      "Training Epoch: 6 [7680/50000]\tLoss: 2.7952\tLR: 0.515090\n",
      "Training Epoch: 6 [7808/50000]\tLoss: 2.5155\tLR: 0.515345\n",
      "Training Epoch: 6 [7936/50000]\tLoss: 2.7144\tLR: 0.515601\n",
      "Training Epoch: 6 [8064/50000]\tLoss: 2.7019\tLR: 0.515857\n",
      "Training Epoch: 6 [8192/50000]\tLoss: 2.6197\tLR: 0.516113\n",
      "Training Epoch: 6 [8320/50000]\tLoss: 2.6182\tLR: 0.516368\n",
      "Training Epoch: 6 [8448/50000]\tLoss: 2.7721\tLR: 0.516624\n",
      "Training Epoch: 6 [8576/50000]\tLoss: 2.8405\tLR: 0.516880\n",
      "Training Epoch: 6 [8704/50000]\tLoss: 2.4847\tLR: 0.517136\n",
      "Training Epoch: 6 [8832/50000]\tLoss: 2.8788\tLR: 0.517391\n",
      "Training Epoch: 6 [8960/50000]\tLoss: 2.3908\tLR: 0.517647\n",
      "Training Epoch: 6 [9088/50000]\tLoss: 2.7213\tLR: 0.517903\n",
      "Training Epoch: 6 [9216/50000]\tLoss: 2.6596\tLR: 0.518159\n",
      "Training Epoch: 6 [9344/50000]\tLoss: 2.5142\tLR: 0.518414\n",
      "Training Epoch: 6 [9472/50000]\tLoss: 2.8380\tLR: 0.518670\n",
      "Training Epoch: 6 [9600/50000]\tLoss: 2.7117\tLR: 0.518926\n",
      "Training Epoch: 6 [9728/50000]\tLoss: 2.7795\tLR: 0.519182\n",
      "Training Epoch: 6 [9856/50000]\tLoss: 2.6366\tLR: 0.519437\n",
      "Training Epoch: 6 [9984/50000]\tLoss: 2.5837\tLR: 0.519693\n",
      "Training Epoch: 6 [10112/50000]\tLoss: 2.5966\tLR: 0.519949\n",
      "Training Epoch: 6 [10240/50000]\tLoss: 2.8171\tLR: 0.520205\n",
      "Training Epoch: 6 [10368/50000]\tLoss: 2.6240\tLR: 0.520460\n",
      "Training Epoch: 6 [10496/50000]\tLoss: 2.8219\tLR: 0.520716\n",
      "Training Epoch: 6 [10624/50000]\tLoss: 2.8282\tLR: 0.520972\n",
      "Training Epoch: 6 [10752/50000]\tLoss: 2.6806\tLR: 0.521228\n",
      "Training Epoch: 6 [10880/50000]\tLoss: 2.6406\tLR: 0.521483\n",
      "Training Epoch: 6 [11008/50000]\tLoss: 2.6319\tLR: 0.521739\n",
      "Training Epoch: 6 [11136/50000]\tLoss: 2.8353\tLR: 0.521995\n",
      "Training Epoch: 6 [11264/50000]\tLoss: 2.5308\tLR: 0.522251\n",
      "Training Epoch: 6 [11392/50000]\tLoss: 2.8085\tLR: 0.522506\n",
      "Training Epoch: 6 [11520/50000]\tLoss: 2.6206\tLR: 0.522762\n",
      "Training Epoch: 6 [11648/50000]\tLoss: 2.6847\tLR: 0.523018\n",
      "Training Epoch: 6 [11776/50000]\tLoss: 2.6748\tLR: 0.523274\n",
      "Training Epoch: 6 [11904/50000]\tLoss: 2.4541\tLR: 0.523529\n",
      "Training Epoch: 6 [12032/50000]\tLoss: 2.8123\tLR: 0.523785\n",
      "Training Epoch: 6 [12160/50000]\tLoss: 2.6996\tLR: 0.524041\n",
      "Training Epoch: 6 [12288/50000]\tLoss: 2.6654\tLR: 0.524297\n",
      "Training Epoch: 6 [12416/50000]\tLoss: 2.5548\tLR: 0.524552\n",
      "Training Epoch: 6 [12544/50000]\tLoss: 2.6341\tLR: 0.524808\n",
      "Training Epoch: 6 [12672/50000]\tLoss: 2.7707\tLR: 0.525064\n",
      "Training Epoch: 6 [12800/50000]\tLoss: 2.6431\tLR: 0.525320\n",
      "Training Epoch: 6 [12928/50000]\tLoss: 2.8615\tLR: 0.525575\n",
      "Training Epoch: 6 [13056/50000]\tLoss: 2.3669\tLR: 0.525831\n",
      "Training Epoch: 6 [13184/50000]\tLoss: 2.7201\tLR: 0.526087\n",
      "Training Epoch: 6 [13312/50000]\tLoss: 2.4507\tLR: 0.526343\n",
      "Training Epoch: 6 [13440/50000]\tLoss: 2.8313\tLR: 0.526598\n",
      "Training Epoch: 6 [13568/50000]\tLoss: 2.7011\tLR: 0.526854\n",
      "Training Epoch: 6 [13696/50000]\tLoss: 2.6397\tLR: 0.527110\n",
      "Training Epoch: 6 [13824/50000]\tLoss: 2.5509\tLR: 0.527366\n",
      "Training Epoch: 6 [13952/50000]\tLoss: 2.6205\tLR: 0.527621\n",
      "Training Epoch: 6 [14080/50000]\tLoss: 2.5471\tLR: 0.527877\n",
      "Training Epoch: 6 [14208/50000]\tLoss: 2.5063\tLR: 0.528133\n",
      "Training Epoch: 6 [14336/50000]\tLoss: 2.6876\tLR: 0.528389\n",
      "Training Epoch: 6 [14464/50000]\tLoss: 2.7204\tLR: 0.528645\n",
      "Training Epoch: 6 [14592/50000]\tLoss: 2.5458\tLR: 0.528900\n",
      "Training Epoch: 6 [14720/50000]\tLoss: 2.6940\tLR: 0.529156\n",
      "Training Epoch: 6 [14848/50000]\tLoss: 2.4969\tLR: 0.529412\n",
      "Training Epoch: 6 [14976/50000]\tLoss: 2.9222\tLR: 0.529668\n",
      "Training Epoch: 6 [15104/50000]\tLoss: 2.6326\tLR: 0.529923\n",
      "Training Epoch: 6 [15232/50000]\tLoss: 2.6639\tLR: 0.530179\n",
      "Training Epoch: 6 [15360/50000]\tLoss: 2.5713\tLR: 0.530435\n",
      "Training Epoch: 6 [15488/50000]\tLoss: 2.6594\tLR: 0.530691\n",
      "Training Epoch: 6 [15616/50000]\tLoss: 2.4761\tLR: 0.530946\n",
      "Training Epoch: 6 [15744/50000]\tLoss: 2.4284\tLR: 0.531202\n",
      "Training Epoch: 6 [15872/50000]\tLoss: 2.6634\tLR: 0.531458\n",
      "Training Epoch: 6 [16000/50000]\tLoss: 2.8006\tLR: 0.531714\n",
      "Training Epoch: 6 [16128/50000]\tLoss: 2.5371\tLR: 0.531969\n",
      "Training Epoch: 6 [16256/50000]\tLoss: 2.8121\tLR: 0.532225\n",
      "Training Epoch: 6 [16384/50000]\tLoss: 2.6662\tLR: 0.532481\n",
      "Training Epoch: 6 [16512/50000]\tLoss: 2.5196\tLR: 0.532737\n",
      "Training Epoch: 6 [16640/50000]\tLoss: 2.7108\tLR: 0.532992\n",
      "Training Epoch: 6 [16768/50000]\tLoss: 2.5587\tLR: 0.533248\n",
      "Training Epoch: 6 [16896/50000]\tLoss: 2.6303\tLR: 0.533504\n",
      "Training Epoch: 6 [17024/50000]\tLoss: 2.5640\tLR: 0.533760\n",
      "Training Epoch: 6 [17152/50000]\tLoss: 2.8200\tLR: 0.534015\n",
      "Training Epoch: 6 [17280/50000]\tLoss: 2.8219\tLR: 0.534271\n",
      "Training Epoch: 6 [17408/50000]\tLoss: 2.8119\tLR: 0.534527\n",
      "Training Epoch: 6 [17536/50000]\tLoss: 2.7189\tLR: 0.534783\n",
      "Training Epoch: 6 [17664/50000]\tLoss: 2.7982\tLR: 0.535038\n",
      "Training Epoch: 6 [17792/50000]\tLoss: 2.6917\tLR: 0.535294\n",
      "Training Epoch: 6 [17920/50000]\tLoss: 2.6009\tLR: 0.535550\n",
      "Training Epoch: 6 [18048/50000]\tLoss: 2.5722\tLR: 0.535806\n",
      "Training Epoch: 6 [18176/50000]\tLoss: 2.6464\tLR: 0.536061\n",
      "Training Epoch: 6 [18304/50000]\tLoss: 2.5809\tLR: 0.536317\n",
      "Training Epoch: 6 [18432/50000]\tLoss: 2.8000\tLR: 0.536573\n",
      "Training Epoch: 6 [18560/50000]\tLoss: 2.6203\tLR: 0.536829\n",
      "Training Epoch: 6 [18688/50000]\tLoss: 2.7733\tLR: 0.537084\n",
      "Training Epoch: 6 [18816/50000]\tLoss: 2.8055\tLR: 0.537340\n",
      "Training Epoch: 6 [18944/50000]\tLoss: 2.5866\tLR: 0.537596\n",
      "Training Epoch: 6 [19072/50000]\tLoss: 2.4294\tLR: 0.537852\n",
      "Training Epoch: 6 [19200/50000]\tLoss: 2.5566\tLR: 0.538107\n",
      "Training Epoch: 6 [19328/50000]\tLoss: 2.7162\tLR: 0.538363\n",
      "Training Epoch: 6 [19456/50000]\tLoss: 2.6408\tLR: 0.538619\n",
      "Training Epoch: 6 [19584/50000]\tLoss: 2.8995\tLR: 0.538875\n",
      "Training Epoch: 6 [19712/50000]\tLoss: 2.4371\tLR: 0.539130\n",
      "Training Epoch: 6 [19840/50000]\tLoss: 2.4560\tLR: 0.539386\n",
      "Training Epoch: 6 [19968/50000]\tLoss: 2.5777\tLR: 0.539642\n",
      "Training Epoch: 6 [20096/50000]\tLoss: 2.4475\tLR: 0.539898\n",
      "Training Epoch: 6 [20224/50000]\tLoss: 2.7242\tLR: 0.540153\n",
      "Training Epoch: 6 [20352/50000]\tLoss: 2.5094\tLR: 0.540409\n",
      "Training Epoch: 6 [20480/50000]\tLoss: 2.7438\tLR: 0.540665\n",
      "Training Epoch: 6 [20608/50000]\tLoss: 3.0231\tLR: 0.540921\n",
      "Training Epoch: 6 [20736/50000]\tLoss: 2.5062\tLR: 0.541176\n",
      "Training Epoch: 6 [20864/50000]\tLoss: 2.7358\tLR: 0.541432\n",
      "Training Epoch: 6 [20992/50000]\tLoss: 2.6128\tLR: 0.541688\n",
      "Training Epoch: 6 [21120/50000]\tLoss: 2.7152\tLR: 0.541944\n",
      "Training Epoch: 6 [21248/50000]\tLoss: 2.9020\tLR: 0.542199\n",
      "Training Epoch: 6 [21376/50000]\tLoss: 2.6968\tLR: 0.542455\n",
      "Training Epoch: 6 [21504/50000]\tLoss: 2.6592\tLR: 0.542711\n",
      "Training Epoch: 6 [21632/50000]\tLoss: 2.9856\tLR: 0.542967\n",
      "Training Epoch: 6 [21760/50000]\tLoss: 2.6137\tLR: 0.543223\n",
      "Training Epoch: 6 [21888/50000]\tLoss: 2.8014\tLR: 0.543478\n",
      "Training Epoch: 6 [22016/50000]\tLoss: 2.9004\tLR: 0.543734\n",
      "Training Epoch: 6 [22144/50000]\tLoss: 2.4517\tLR: 0.543990\n",
      "Training Epoch: 6 [22272/50000]\tLoss: 2.5585\tLR: 0.544246\n",
      "Training Epoch: 6 [22400/50000]\tLoss: 2.7197\tLR: 0.544501\n",
      "Training Epoch: 6 [22528/50000]\tLoss: 2.7809\tLR: 0.544757\n",
      "Training Epoch: 6 [22656/50000]\tLoss: 2.8551\tLR: 0.545013\n",
      "Training Epoch: 6 [22784/50000]\tLoss: 2.5253\tLR: 0.545269\n",
      "Training Epoch: 6 [22912/50000]\tLoss: 2.5759\tLR: 0.545524\n",
      "Training Epoch: 6 [23040/50000]\tLoss: 2.8398\tLR: 0.545780\n",
      "Training Epoch: 6 [23168/50000]\tLoss: 2.5836\tLR: 0.546036\n",
      "Training Epoch: 6 [23296/50000]\tLoss: 2.8064\tLR: 0.546292\n",
      "Training Epoch: 6 [23424/50000]\tLoss: 2.6996\tLR: 0.546547\n",
      "Training Epoch: 6 [23552/50000]\tLoss: 2.5142\tLR: 0.546803\n",
      "Training Epoch: 6 [23680/50000]\tLoss: 2.4325\tLR: 0.547059\n",
      "Training Epoch: 6 [23808/50000]\tLoss: 2.7061\tLR: 0.547315\n",
      "Training Epoch: 6 [23936/50000]\tLoss: 2.5308\tLR: 0.547570\n",
      "Training Epoch: 6 [24064/50000]\tLoss: 2.5162\tLR: 0.547826\n",
      "Training Epoch: 6 [24192/50000]\tLoss: 2.5033\tLR: 0.548082\n",
      "Training Epoch: 6 [24320/50000]\tLoss: 2.5361\tLR: 0.548338\n",
      "Training Epoch: 6 [24448/50000]\tLoss: 2.3388\tLR: 0.548593\n",
      "Training Epoch: 6 [24576/50000]\tLoss: 2.9431\tLR: 0.548849\n",
      "Training Epoch: 6 [24704/50000]\tLoss: 2.8212\tLR: 0.549105\n",
      "Training Epoch: 6 [24832/50000]\tLoss: 2.8405\tLR: 0.549361\n",
      "Training Epoch: 6 [24960/50000]\tLoss: 2.6578\tLR: 0.549616\n",
      "Training Epoch: 6 [25088/50000]\tLoss: 2.4809\tLR: 0.549872\n",
      "Training Epoch: 6 [25216/50000]\tLoss: 3.0639\tLR: 0.550128\n",
      "Training Epoch: 6 [25344/50000]\tLoss: 2.6162\tLR: 0.550384\n",
      "Training Epoch: 6 [25472/50000]\tLoss: 2.8131\tLR: 0.550639\n",
      "Training Epoch: 6 [25600/50000]\tLoss: 2.6475\tLR: 0.550895\n",
      "Training Epoch: 6 [25728/50000]\tLoss: 2.6291\tLR: 0.551151\n",
      "Training Epoch: 6 [25856/50000]\tLoss: 2.5420\tLR: 0.551407\n",
      "Training Epoch: 6 [25984/50000]\tLoss: 3.0131\tLR: 0.551662\n",
      "Training Epoch: 6 [26112/50000]\tLoss: 2.5901\tLR: 0.551918\n",
      "Training Epoch: 6 [26240/50000]\tLoss: 2.5876\tLR: 0.552174\n",
      "Training Epoch: 6 [26368/50000]\tLoss: 2.6784\tLR: 0.552430\n",
      "Training Epoch: 6 [26496/50000]\tLoss: 2.4078\tLR: 0.552685\n",
      "Training Epoch: 6 [26624/50000]\tLoss: 2.7311\tLR: 0.552941\n",
      "Training Epoch: 6 [26752/50000]\tLoss: 2.8312\tLR: 0.553197\n",
      "Training Epoch: 6 [26880/50000]\tLoss: 2.8576\tLR: 0.553453\n",
      "Training Epoch: 6 [27008/50000]\tLoss: 2.6458\tLR: 0.553708\n",
      "Training Epoch: 6 [27136/50000]\tLoss: 2.3928\tLR: 0.553964\n",
      "Training Epoch: 6 [27264/50000]\tLoss: 2.8400\tLR: 0.554220\n",
      "Training Epoch: 6 [27392/50000]\tLoss: 2.6645\tLR: 0.554476\n",
      "Training Epoch: 6 [27520/50000]\tLoss: 2.7654\tLR: 0.554731\n",
      "Training Epoch: 6 [27648/50000]\tLoss: 2.8534\tLR: 0.554987\n",
      "Training Epoch: 6 [27776/50000]\tLoss: 2.6183\tLR: 0.555243\n",
      "Training Epoch: 6 [27904/50000]\tLoss: 2.8578\tLR: 0.555499\n",
      "Training Epoch: 6 [28032/50000]\tLoss: 2.6916\tLR: 0.555754\n",
      "Training Epoch: 6 [28160/50000]\tLoss: 2.5984\tLR: 0.556010\n",
      "Training Epoch: 6 [28288/50000]\tLoss: 2.8036\tLR: 0.556266\n",
      "Training Epoch: 6 [28416/50000]\tLoss: 2.5474\tLR: 0.556522\n",
      "Training Epoch: 6 [28544/50000]\tLoss: 2.6056\tLR: 0.556777\n",
      "Training Epoch: 6 [28672/50000]\tLoss: 2.6901\tLR: 0.557033\n",
      "Training Epoch: 6 [28800/50000]\tLoss: 2.6330\tLR: 0.557289\n",
      "Training Epoch: 6 [28928/50000]\tLoss: 2.4964\tLR: 0.557545\n",
      "Training Epoch: 6 [29056/50000]\tLoss: 2.5641\tLR: 0.557801\n",
      "Training Epoch: 6 [29184/50000]\tLoss: 2.7287\tLR: 0.558056\n",
      "Training Epoch: 6 [29312/50000]\tLoss: 2.6237\tLR: 0.558312\n",
      "Training Epoch: 6 [29440/50000]\tLoss: 2.6507\tLR: 0.558568\n",
      "Training Epoch: 6 [29568/50000]\tLoss: 2.5363\tLR: 0.558824\n",
      "Training Epoch: 6 [29696/50000]\tLoss: 2.4351\tLR: 0.559079\n",
      "Training Epoch: 6 [29824/50000]\tLoss: 2.5951\tLR: 0.559335\n",
      "Training Epoch: 6 [29952/50000]\tLoss: 2.6165\tLR: 0.559591\n",
      "Training Epoch: 6 [30080/50000]\tLoss: 2.6911\tLR: 0.559847\n",
      "Training Epoch: 6 [30208/50000]\tLoss: 2.8045\tLR: 0.560102\n",
      "Training Epoch: 6 [30336/50000]\tLoss: 2.6581\tLR: 0.560358\n",
      "Training Epoch: 6 [30464/50000]\tLoss: 2.5702\tLR: 0.560614\n",
      "Training Epoch: 6 [30592/50000]\tLoss: 2.4513\tLR: 0.560870\n",
      "Training Epoch: 6 [30720/50000]\tLoss: 2.7204\tLR: 0.561125\n",
      "Training Epoch: 6 [30848/50000]\tLoss: 2.7944\tLR: 0.561381\n",
      "Training Epoch: 6 [30976/50000]\tLoss: 2.8813\tLR: 0.561637\n",
      "Training Epoch: 6 [31104/50000]\tLoss: 2.4568\tLR: 0.561893\n",
      "Training Epoch: 6 [31232/50000]\tLoss: 2.4664\tLR: 0.562148\n",
      "Training Epoch: 6 [31360/50000]\tLoss: 2.8785\tLR: 0.562404\n",
      "Training Epoch: 6 [31488/50000]\tLoss: 2.4734\tLR: 0.562660\n",
      "Training Epoch: 6 [31616/50000]\tLoss: 2.7993\tLR: 0.562916\n",
      "Training Epoch: 6 [31744/50000]\tLoss: 2.8785\tLR: 0.563171\n",
      "Training Epoch: 6 [31872/50000]\tLoss: 2.7178\tLR: 0.563427\n",
      "Training Epoch: 6 [32000/50000]\tLoss: 2.7843\tLR: 0.563683\n",
      "Training Epoch: 6 [32128/50000]\tLoss: 2.7388\tLR: 0.563939\n",
      "Training Epoch: 6 [32256/50000]\tLoss: 2.3560\tLR: 0.564194\n",
      "Training Epoch: 6 [32384/50000]\tLoss: 2.9132\tLR: 0.564450\n",
      "Training Epoch: 6 [32512/50000]\tLoss: 2.3669\tLR: 0.564706\n",
      "Training Epoch: 6 [32640/50000]\tLoss: 2.5110\tLR: 0.564962\n",
      "Training Epoch: 6 [32768/50000]\tLoss: 2.5945\tLR: 0.565217\n",
      "Training Epoch: 6 [32896/50000]\tLoss: 2.8549\tLR: 0.565473\n",
      "Training Epoch: 6 [33024/50000]\tLoss: 2.7352\tLR: 0.565729\n",
      "Training Epoch: 6 [33152/50000]\tLoss: 2.4977\tLR: 0.565985\n",
      "Training Epoch: 6 [33280/50000]\tLoss: 2.7460\tLR: 0.566240\n",
      "Training Epoch: 6 [33408/50000]\tLoss: 2.6094\tLR: 0.566496\n",
      "Training Epoch: 6 [33536/50000]\tLoss: 2.7177\tLR: 0.566752\n",
      "Training Epoch: 6 [33664/50000]\tLoss: 2.3316\tLR: 0.567008\n",
      "Training Epoch: 6 [33792/50000]\tLoss: 2.3515\tLR: 0.567263\n",
      "Training Epoch: 6 [33920/50000]\tLoss: 2.6256\tLR: 0.567519\n",
      "Training Epoch: 6 [34048/50000]\tLoss: 2.7257\tLR: 0.567775\n",
      "Training Epoch: 6 [34176/50000]\tLoss: 2.5856\tLR: 0.568031\n",
      "Training Epoch: 6 [34304/50000]\tLoss: 2.7302\tLR: 0.568286\n",
      "Training Epoch: 6 [34432/50000]\tLoss: 2.5732\tLR: 0.568542\n",
      "Training Epoch: 6 [34560/50000]\tLoss: 2.5688\tLR: 0.568798\n",
      "Training Epoch: 6 [34688/50000]\tLoss: 2.7018\tLR: 0.569054\n",
      "Training Epoch: 6 [34816/50000]\tLoss: 2.6630\tLR: 0.569309\n",
      "Training Epoch: 6 [34944/50000]\tLoss: 2.6562\tLR: 0.569565\n",
      "Training Epoch: 6 [35072/50000]\tLoss: 2.5899\tLR: 0.569821\n",
      "Training Epoch: 6 [35200/50000]\tLoss: 2.6295\tLR: 0.570077\n",
      "Training Epoch: 6 [35328/50000]\tLoss: 2.8138\tLR: 0.570332\n",
      "Training Epoch: 6 [35456/50000]\tLoss: 2.8268\tLR: 0.570588\n",
      "Training Epoch: 6 [35584/50000]\tLoss: 2.4856\tLR: 0.570844\n",
      "Training Epoch: 6 [35712/50000]\tLoss: 2.8361\tLR: 0.571100\n",
      "Training Epoch: 6 [35840/50000]\tLoss: 2.7389\tLR: 0.571355\n",
      "Training Epoch: 6 [35968/50000]\tLoss: 2.8410\tLR: 0.571611\n",
      "Training Epoch: 6 [36096/50000]\tLoss: 2.5917\tLR: 0.571867\n",
      "Training Epoch: 6 [36224/50000]\tLoss: 2.7685\tLR: 0.572123\n",
      "Training Epoch: 6 [36352/50000]\tLoss: 2.8294\tLR: 0.572379\n",
      "Training Epoch: 6 [36480/50000]\tLoss: 2.6677\tLR: 0.572634\n",
      "Training Epoch: 6 [36608/50000]\tLoss: 2.4847\tLR: 0.572890\n",
      "Training Epoch: 6 [36736/50000]\tLoss: 2.4670\tLR: 0.573146\n",
      "Training Epoch: 6 [36864/50000]\tLoss: 2.8758\tLR: 0.573402\n",
      "Training Epoch: 6 [36992/50000]\tLoss: 2.8176\tLR: 0.573657\n",
      "Training Epoch: 6 [37120/50000]\tLoss: 2.6216\tLR: 0.573913\n",
      "Training Epoch: 6 [37248/50000]\tLoss: 2.7835\tLR: 0.574169\n",
      "Training Epoch: 6 [37376/50000]\tLoss: 2.7910\tLR: 0.574425\n",
      "Training Epoch: 6 [37504/50000]\tLoss: 2.6005\tLR: 0.574680\n",
      "Training Epoch: 6 [37632/50000]\tLoss: 2.4684\tLR: 0.574936\n",
      "Training Epoch: 6 [37760/50000]\tLoss: 2.6208\tLR: 0.575192\n",
      "Training Epoch: 6 [37888/50000]\tLoss: 2.7165\tLR: 0.575448\n",
      "Training Epoch: 6 [38016/50000]\tLoss: 2.7864\tLR: 0.575703\n",
      "Training Epoch: 6 [38144/50000]\tLoss: 2.9229\tLR: 0.575959\n",
      "Training Epoch: 6 [38272/50000]\tLoss: 2.8223\tLR: 0.576215\n",
      "Training Epoch: 6 [38400/50000]\tLoss: 2.6917\tLR: 0.576471\n",
      "Training Epoch: 6 [38528/50000]\tLoss: 2.5270\tLR: 0.576726\n",
      "Training Epoch: 6 [38656/50000]\tLoss: 2.4536\tLR: 0.576982\n",
      "Training Epoch: 6 [38784/50000]\tLoss: 2.8162\tLR: 0.577238\n",
      "Training Epoch: 6 [38912/50000]\tLoss: 2.6259\tLR: 0.577494\n",
      "Training Epoch: 6 [39040/50000]\tLoss: 2.5229\tLR: 0.577749\n",
      "Training Epoch: 6 [39168/50000]\tLoss: 2.6030\tLR: 0.578005\n",
      "Training Epoch: 6 [39296/50000]\tLoss: 2.6160\tLR: 0.578261\n",
      "Training Epoch: 6 [39424/50000]\tLoss: 2.5745\tLR: 0.578517\n",
      "Training Epoch: 6 [39552/50000]\tLoss: 2.5818\tLR: 0.578772\n",
      "Training Epoch: 6 [39680/50000]\tLoss: 2.7096\tLR: 0.579028\n",
      "Training Epoch: 6 [39808/50000]\tLoss: 2.5243\tLR: 0.579284\n",
      "Training Epoch: 6 [39936/50000]\tLoss: 2.4097\tLR: 0.579540\n",
      "Training Epoch: 6 [40064/50000]\tLoss: 3.0532\tLR: 0.579795\n",
      "Training Epoch: 6 [40192/50000]\tLoss: 2.7681\tLR: 0.580051\n",
      "Training Epoch: 6 [40320/50000]\tLoss: 2.8304\tLR: 0.580307\n",
      "Training Epoch: 6 [40448/50000]\tLoss: 2.8607\tLR: 0.580563\n",
      "Training Epoch: 6 [40576/50000]\tLoss: 2.5001\tLR: 0.580818\n",
      "Training Epoch: 6 [40704/50000]\tLoss: 2.7389\tLR: 0.581074\n",
      "Training Epoch: 6 [40832/50000]\tLoss: 2.6702\tLR: 0.581330\n",
      "Training Epoch: 6 [40960/50000]\tLoss: 2.7097\tLR: 0.581586\n",
      "Training Epoch: 6 [41088/50000]\tLoss: 2.6753\tLR: 0.581841\n",
      "Training Epoch: 6 [41216/50000]\tLoss: 2.6997\tLR: 0.582097\n",
      "Training Epoch: 6 [41344/50000]\tLoss: 2.9691\tLR: 0.582353\n",
      "Training Epoch: 6 [41472/50000]\tLoss: 2.6269\tLR: 0.582609\n",
      "Training Epoch: 6 [41600/50000]\tLoss: 2.9521\tLR: 0.582864\n",
      "Training Epoch: 6 [41728/50000]\tLoss: 3.1330\tLR: 0.583120\n",
      "Training Epoch: 6 [41856/50000]\tLoss: 2.8750\tLR: 0.583376\n",
      "Training Epoch: 6 [41984/50000]\tLoss: 2.5290\tLR: 0.583632\n",
      "Training Epoch: 6 [42112/50000]\tLoss: 2.5251\tLR: 0.583887\n",
      "Training Epoch: 6 [42240/50000]\tLoss: 2.6651\tLR: 0.584143\n",
      "Training Epoch: 6 [42368/50000]\tLoss: 2.6681\tLR: 0.584399\n",
      "Training Epoch: 6 [42496/50000]\tLoss: 2.7949\tLR: 0.584655\n",
      "Training Epoch: 6 [42624/50000]\tLoss: 2.7813\tLR: 0.584910\n",
      "Training Epoch: 6 [42752/50000]\tLoss: 2.6809\tLR: 0.585166\n",
      "Training Epoch: 6 [42880/50000]\tLoss: 2.6012\tLR: 0.585422\n",
      "Training Epoch: 6 [43008/50000]\tLoss: 2.5156\tLR: 0.585678\n",
      "Training Epoch: 6 [43136/50000]\tLoss: 2.5714\tLR: 0.585934\n",
      "Training Epoch: 6 [43264/50000]\tLoss: 2.7783\tLR: 0.586189\n",
      "Training Epoch: 6 [43392/50000]\tLoss: 2.5248\tLR: 0.586445\n",
      "Training Epoch: 6 [43520/50000]\tLoss: 2.7579\tLR: 0.586701\n",
      "Training Epoch: 6 [43648/50000]\tLoss: 2.8054\tLR: 0.586957\n",
      "Training Epoch: 6 [43776/50000]\tLoss: 2.7856\tLR: 0.587212\n",
      "Training Epoch: 6 [43904/50000]\tLoss: 2.7332\tLR: 0.587468\n",
      "Training Epoch: 6 [44032/50000]\tLoss: 2.7126\tLR: 0.587724\n",
      "Training Epoch: 6 [44160/50000]\tLoss: 2.9129\tLR: 0.587980\n",
      "Training Epoch: 6 [44288/50000]\tLoss: 2.6607\tLR: 0.588235\n",
      "Training Epoch: 6 [44416/50000]\tLoss: 2.8044\tLR: 0.588491\n",
      "Training Epoch: 6 [44544/50000]\tLoss: 2.6106\tLR: 0.588747\n",
      "Training Epoch: 6 [44672/50000]\tLoss: 2.5865\tLR: 0.589003\n",
      "Training Epoch: 6 [44800/50000]\tLoss: 2.9656\tLR: 0.589258\n",
      "Training Epoch: 6 [44928/50000]\tLoss: 2.7073\tLR: 0.589514\n",
      "Training Epoch: 6 [45056/50000]\tLoss: 2.9049\tLR: 0.589770\n",
      "Training Epoch: 6 [45184/50000]\tLoss: 2.7523\tLR: 0.590026\n",
      "Training Epoch: 6 [45312/50000]\tLoss: 2.6475\tLR: 0.590281\n",
      "Training Epoch: 6 [45440/50000]\tLoss: 2.7496\tLR: 0.590537\n",
      "Training Epoch: 6 [45568/50000]\tLoss: 2.7766\tLR: 0.590793\n",
      "Training Epoch: 6 [45696/50000]\tLoss: 2.5125\tLR: 0.591049\n",
      "Training Epoch: 6 [45824/50000]\tLoss: 2.6242\tLR: 0.591304\n",
      "Training Epoch: 6 [45952/50000]\tLoss: 2.4946\tLR: 0.591560\n",
      "Training Epoch: 6 [46080/50000]\tLoss: 2.4433\tLR: 0.591816\n",
      "Training Epoch: 6 [46208/50000]\tLoss: 2.5587\tLR: 0.592072\n",
      "Training Epoch: 6 [46336/50000]\tLoss: 2.3815\tLR: 0.592327\n",
      "Training Epoch: 6 [46464/50000]\tLoss: 2.5091\tLR: 0.592583\n",
      "Training Epoch: 6 [46592/50000]\tLoss: 2.6289\tLR: 0.592839\n",
      "Training Epoch: 6 [46720/50000]\tLoss: 2.9969\tLR: 0.593095\n",
      "Training Epoch: 6 [46848/50000]\tLoss: 2.6819\tLR: 0.593350\n",
      "Training Epoch: 6 [46976/50000]\tLoss: 2.5755\tLR: 0.593606\n",
      "Training Epoch: 6 [47104/50000]\tLoss: 2.8084\tLR: 0.593862\n",
      "Training Epoch: 6 [47232/50000]\tLoss: 2.6917\tLR: 0.594118\n",
      "Training Epoch: 6 [47360/50000]\tLoss: 2.5617\tLR: 0.594373\n",
      "Training Epoch: 6 [47488/50000]\tLoss: 2.8362\tLR: 0.594629\n",
      "Training Epoch: 6 [47616/50000]\tLoss: 2.7268\tLR: 0.594885\n",
      "Training Epoch: 6 [47744/50000]\tLoss: 2.6584\tLR: 0.595141\n",
      "Training Epoch: 6 [47872/50000]\tLoss: 2.6182\tLR: 0.595396\n",
      "Training Epoch: 6 [48000/50000]\tLoss: 2.5682\tLR: 0.595652\n",
      "Training Epoch: 6 [48128/50000]\tLoss: 2.6722\tLR: 0.595908\n",
      "Training Epoch: 6 [48256/50000]\tLoss: 2.7392\tLR: 0.596164\n",
      "Training Epoch: 6 [48384/50000]\tLoss: 2.5481\tLR: 0.596419\n",
      "Training Epoch: 6 [48512/50000]\tLoss: 2.7827\tLR: 0.596675\n",
      "Training Epoch: 6 [48640/50000]\tLoss: 2.4222\tLR: 0.596931\n",
      "Training Epoch: 6 [48768/50000]\tLoss: 2.6542\tLR: 0.597187\n",
      "Training Epoch: 6 [48896/50000]\tLoss: 2.6681\tLR: 0.597442\n",
      "Training Epoch: 6 [49024/50000]\tLoss: 2.4991\tLR: 0.597698\n",
      "Training Epoch: 6 [49152/50000]\tLoss: 2.5524\tLR: 0.597954\n",
      "Training Epoch: 6 [49280/50000]\tLoss: 2.6120\tLR: 0.598210\n",
      "Training Epoch: 6 [49408/50000]\tLoss: 2.7292\tLR: 0.598465\n",
      "Training Epoch: 6 [49536/50000]\tLoss: 2.7171\tLR: 0.598721\n",
      "Training Epoch: 6 [49664/50000]\tLoss: 2.5427\tLR: 0.598977\n",
      "Training Epoch: 6 [49792/50000]\tLoss: 2.5526\tLR: 0.599233\n",
      "Training Epoch: 6 [49920/50000]\tLoss: 2.8111\tLR: 0.599488\n",
      "Training Epoch: 6 [50000/50000]\tLoss: 2.4868\tLR: 0.599744\n",
      "epoch 6 training time consumed: 493.26s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |    8413 GB |    8413 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |    8387 GB |    8387 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      25 GB |      25 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |    8413 GB |    8413 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |    8387 GB |    8387 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      25 GB |      25 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |    8291 GB |    8291 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |    8265 GB |    8265 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |      25 GB |      25 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |     892 K  |     892 K  |\n",
      "|       from large pool |      24    |      65    |     380 K  |     380 K  |\n",
      "|       from small pool |     231    |     274    |     512 K  |     511 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |     892 K  |     892 K  |\n",
      "|       from large pool |      24    |      65    |     380 K  |     380 K  |\n",
      "|       from small pool |     231    |     274    |     512 K  |     511 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      45    |  516914    |  516878    |\n",
      "|       from large pool |      10    |      23    |  182786    |  182776    |\n",
      "|       from small pool |      26    |      35    |  334128    |  334102    |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 6, Average loss: 0.0228, Accuracy: 0.2985, Time consumed:35.59s\n",
      "\n",
      "Training Epoch: 7 [128/50000]\tLoss: 2.8281\tLR: 0.100000\n",
      "Training Epoch: 7 [256/50000]\tLoss: 2.5127\tLR: 0.600256\n",
      "Training Epoch: 7 [384/50000]\tLoss: 2.5176\tLR: 0.600512\n",
      "Training Epoch: 7 [512/50000]\tLoss: 2.6631\tLR: 0.600767\n",
      "Training Epoch: 7 [640/50000]\tLoss: 2.6609\tLR: 0.601023\n",
      "Training Epoch: 7 [768/50000]\tLoss: 2.8258\tLR: 0.601279\n",
      "Training Epoch: 7 [896/50000]\tLoss: 2.3909\tLR: 0.601535\n",
      "Training Epoch: 7 [1024/50000]\tLoss: 2.4814\tLR: 0.601790\n",
      "Training Epoch: 7 [1152/50000]\tLoss: 2.5511\tLR: 0.602046\n",
      "Training Epoch: 7 [1280/50000]\tLoss: 2.5442\tLR: 0.602302\n",
      "Training Epoch: 7 [1408/50000]\tLoss: 2.6072\tLR: 0.602558\n",
      "Training Epoch: 7 [1536/50000]\tLoss: 2.4581\tLR: 0.602813\n",
      "Training Epoch: 7 [1664/50000]\tLoss: 2.7315\tLR: 0.603069\n",
      "Training Epoch: 7 [1792/50000]\tLoss: 2.7005\tLR: 0.603325\n",
      "Training Epoch: 7 [1920/50000]\tLoss: 2.6081\tLR: 0.603581\n",
      "Training Epoch: 7 [2048/50000]\tLoss: 2.5683\tLR: 0.603836\n",
      "Training Epoch: 7 [2176/50000]\tLoss: 2.6830\tLR: 0.604092\n",
      "Training Epoch: 7 [2304/50000]\tLoss: 2.5855\tLR: 0.604348\n",
      "Training Epoch: 7 [2432/50000]\tLoss: 2.8955\tLR: 0.604604\n",
      "Training Epoch: 7 [2560/50000]\tLoss: 2.7257\tLR: 0.604859\n",
      "Training Epoch: 7 [2688/50000]\tLoss: 2.6813\tLR: 0.605115\n",
      "Training Epoch: 7 [2816/50000]\tLoss: 2.8025\tLR: 0.605371\n",
      "Training Epoch: 7 [2944/50000]\tLoss: 2.8888\tLR: 0.605627\n",
      "Training Epoch: 7 [3072/50000]\tLoss: 2.6323\tLR: 0.605882\n",
      "Training Epoch: 7 [3200/50000]\tLoss: 2.6493\tLR: 0.606138\n",
      "Training Epoch: 7 [3328/50000]\tLoss: 2.6003\tLR: 0.606394\n",
      "Training Epoch: 7 [3456/50000]\tLoss: 2.5520\tLR: 0.606650\n",
      "Training Epoch: 7 [3584/50000]\tLoss: 2.6198\tLR: 0.606905\n",
      "Training Epoch: 7 [3712/50000]\tLoss: 2.6553\tLR: 0.607161\n",
      "Training Epoch: 7 [3840/50000]\tLoss: 2.7092\tLR: 0.607417\n",
      "Training Epoch: 7 [3968/50000]\tLoss: 2.6977\tLR: 0.607673\n",
      "Training Epoch: 7 [4096/50000]\tLoss: 2.9821\tLR: 0.607928\n",
      "Training Epoch: 7 [4224/50000]\tLoss: 2.7610\tLR: 0.608184\n",
      "Training Epoch: 7 [4352/50000]\tLoss: 2.7399\tLR: 0.608440\n",
      "Training Epoch: 7 [4480/50000]\tLoss: 2.4263\tLR: 0.608696\n",
      "Training Epoch: 7 [4608/50000]\tLoss: 2.6725\tLR: 0.608951\n",
      "Training Epoch: 7 [4736/50000]\tLoss: 2.9011\tLR: 0.609207\n",
      "Training Epoch: 7 [4864/50000]\tLoss: 2.4790\tLR: 0.609463\n",
      "Training Epoch: 7 [4992/50000]\tLoss: 2.8125\tLR: 0.609719\n",
      "Training Epoch: 7 [5120/50000]\tLoss: 2.9406\tLR: 0.609974\n",
      "Training Epoch: 7 [5248/50000]\tLoss: 2.9591\tLR: 0.610230\n",
      "Training Epoch: 7 [5376/50000]\tLoss: 2.7400\tLR: 0.610486\n",
      "Training Epoch: 7 [5504/50000]\tLoss: 2.6740\tLR: 0.610742\n",
      "Training Epoch: 7 [5632/50000]\tLoss: 2.9524\tLR: 0.610997\n",
      "Training Epoch: 7 [5760/50000]\tLoss: 2.5260\tLR: 0.611253\n",
      "Training Epoch: 7 [5888/50000]\tLoss: 2.7995\tLR: 0.611509\n",
      "Training Epoch: 7 [6016/50000]\tLoss: 2.5955\tLR: 0.611765\n",
      "Training Epoch: 7 [6144/50000]\tLoss: 2.7877\tLR: 0.612020\n",
      "Training Epoch: 7 [6272/50000]\tLoss: 2.7767\tLR: 0.612276\n",
      "Training Epoch: 7 [6400/50000]\tLoss: 2.6020\tLR: 0.612532\n",
      "Training Epoch: 7 [6528/50000]\tLoss: 2.6587\tLR: 0.612788\n",
      "Training Epoch: 7 [6656/50000]\tLoss: 2.6477\tLR: 0.613043\n",
      "Training Epoch: 7 [6784/50000]\tLoss: 2.6130\tLR: 0.613299\n",
      "Training Epoch: 7 [6912/50000]\tLoss: 2.6923\tLR: 0.613555\n",
      "Training Epoch: 7 [7040/50000]\tLoss: 3.0449\tLR: 0.613811\n",
      "Training Epoch: 7 [7168/50000]\tLoss: 2.5643\tLR: 0.614066\n",
      "Training Epoch: 7 [7296/50000]\tLoss: 2.3822\tLR: 0.614322\n",
      "Training Epoch: 7 [7424/50000]\tLoss: 2.5124\tLR: 0.614578\n",
      "Training Epoch: 7 [7552/50000]\tLoss: 2.5472\tLR: 0.614834\n",
      "Training Epoch: 7 [7680/50000]\tLoss: 2.7787\tLR: 0.615090\n",
      "Training Epoch: 7 [7808/50000]\tLoss: 2.6333\tLR: 0.615345\n",
      "Training Epoch: 7 [7936/50000]\tLoss: 2.6139\tLR: 0.615601\n",
      "Training Epoch: 7 [8064/50000]\tLoss: 2.7352\tLR: 0.615857\n",
      "Training Epoch: 7 [8192/50000]\tLoss: 2.7383\tLR: 0.616113\n",
      "Training Epoch: 7 [8320/50000]\tLoss: 2.6158\tLR: 0.616368\n",
      "Training Epoch: 7 [8448/50000]\tLoss: 2.7498\tLR: 0.616624\n",
      "Training Epoch: 7 [8576/50000]\tLoss: 2.8174\tLR: 0.616880\n",
      "Training Epoch: 7 [8704/50000]\tLoss: 2.4966\tLR: 0.617136\n",
      "Training Epoch: 7 [8832/50000]\tLoss: 2.8089\tLR: 0.617391\n",
      "Training Epoch: 7 [8960/50000]\tLoss: 2.7717\tLR: 0.617647\n",
      "Training Epoch: 7 [9088/50000]\tLoss: 2.5167\tLR: 0.617903\n",
      "Training Epoch: 7 [9216/50000]\tLoss: 2.5789\tLR: 0.618159\n",
      "Training Epoch: 7 [9344/50000]\tLoss: 2.6045\tLR: 0.618414\n",
      "Training Epoch: 7 [9472/50000]\tLoss: 2.7673\tLR: 0.618670\n",
      "Training Epoch: 7 [9600/50000]\tLoss: 2.5464\tLR: 0.618926\n",
      "Training Epoch: 7 [9728/50000]\tLoss: 2.8072\tLR: 0.619182\n",
      "Training Epoch: 7 [9856/50000]\tLoss: 2.6059\tLR: 0.619437\n",
      "Training Epoch: 7 [9984/50000]\tLoss: 2.3465\tLR: 0.619693\n",
      "Training Epoch: 7 [10112/50000]\tLoss: 2.8565\tLR: 0.619949\n",
      "Training Epoch: 7 [10240/50000]\tLoss: 2.8701\tLR: 0.620205\n",
      "Training Epoch: 7 [10368/50000]\tLoss: 2.2812\tLR: 0.620460\n",
      "Training Epoch: 7 [10496/50000]\tLoss: 2.5941\tLR: 0.620716\n",
      "Training Epoch: 7 [10624/50000]\tLoss: 2.5905\tLR: 0.620972\n",
      "Training Epoch: 7 [10752/50000]\tLoss: 2.6631\tLR: 0.621228\n",
      "Training Epoch: 7 [10880/50000]\tLoss: 2.5206\tLR: 0.621483\n",
      "Training Epoch: 7 [11008/50000]\tLoss: 2.4072\tLR: 0.621739\n",
      "Training Epoch: 7 [11136/50000]\tLoss: 2.4888\tLR: 0.621995\n",
      "Training Epoch: 7 [11264/50000]\tLoss: 2.5628\tLR: 0.622251\n",
      "Training Epoch: 7 [11392/50000]\tLoss: 2.6703\tLR: 0.622506\n",
      "Training Epoch: 7 [11520/50000]\tLoss: 2.1883\tLR: 0.622762\n",
      "Training Epoch: 7 [11648/50000]\tLoss: 2.8149\tLR: 0.623018\n",
      "Training Epoch: 7 [11776/50000]\tLoss: 2.4779\tLR: 0.623274\n",
      "Training Epoch: 7 [11904/50000]\tLoss: 2.7770\tLR: 0.623529\n",
      "Training Epoch: 7 [12032/50000]\tLoss: 2.8633\tLR: 0.623785\n",
      "Training Epoch: 7 [12160/50000]\tLoss: 3.0558\tLR: 0.624041\n",
      "Training Epoch: 7 [12288/50000]\tLoss: 2.6275\tLR: 0.624297\n",
      "Training Epoch: 7 [12416/50000]\tLoss: 2.5896\tLR: 0.624552\n",
      "Training Epoch: 7 [12544/50000]\tLoss: 2.6351\tLR: 0.624808\n",
      "Training Epoch: 7 [12672/50000]\tLoss: 2.6115\tLR: 0.625064\n",
      "Training Epoch: 7 [12800/50000]\tLoss: 2.9155\tLR: 0.625320\n",
      "Training Epoch: 7 [12928/50000]\tLoss: 2.6707\tLR: 0.625575\n",
      "Training Epoch: 7 [13056/50000]\tLoss: 2.8004\tLR: 0.625831\n",
      "Training Epoch: 7 [13184/50000]\tLoss: 3.0127\tLR: 0.626087\n",
      "Training Epoch: 7 [13312/50000]\tLoss: 2.6827\tLR: 0.626343\n",
      "Training Epoch: 7 [13440/50000]\tLoss: 2.7510\tLR: 0.626598\n",
      "Training Epoch: 7 [13568/50000]\tLoss: 2.5038\tLR: 0.626854\n",
      "Training Epoch: 7 [13696/50000]\tLoss: 2.9057\tLR: 0.627110\n",
      "Training Epoch: 7 [13824/50000]\tLoss: 2.5309\tLR: 0.627366\n",
      "Training Epoch: 7 [13952/50000]\tLoss: 2.5513\tLR: 0.627621\n",
      "Training Epoch: 7 [14080/50000]\tLoss: 2.6781\tLR: 0.627877\n",
      "Training Epoch: 7 [14208/50000]\tLoss: 2.5664\tLR: 0.628133\n",
      "Training Epoch: 7 [14336/50000]\tLoss: 2.5714\tLR: 0.628389\n",
      "Training Epoch: 7 [14464/50000]\tLoss: 2.5511\tLR: 0.628645\n",
      "Training Epoch: 7 [14592/50000]\tLoss: 2.7117\tLR: 0.628900\n",
      "Training Epoch: 7 [14720/50000]\tLoss: 2.5188\tLR: 0.629156\n",
      "Training Epoch: 7 [14848/50000]\tLoss: 2.4938\tLR: 0.629412\n",
      "Training Epoch: 7 [14976/50000]\tLoss: 2.6158\tLR: 0.629668\n",
      "Training Epoch: 7 [15104/50000]\tLoss: 2.6554\tLR: 0.629923\n",
      "Training Epoch: 7 [15232/50000]\tLoss: 2.4489\tLR: 0.630179\n",
      "Training Epoch: 7 [15360/50000]\tLoss: 2.7290\tLR: 0.630435\n",
      "Training Epoch: 7 [15488/50000]\tLoss: 2.3748\tLR: 0.630691\n",
      "Training Epoch: 7 [15616/50000]\tLoss: 2.7325\tLR: 0.630946\n",
      "Training Epoch: 7 [15744/50000]\tLoss: 2.5612\tLR: 0.631202\n",
      "Training Epoch: 7 [15872/50000]\tLoss: 2.5695\tLR: 0.631458\n",
      "Training Epoch: 7 [16000/50000]\tLoss: 2.6370\tLR: 0.631714\n",
      "Training Epoch: 7 [16128/50000]\tLoss: 3.0163\tLR: 0.631969\n",
      "Training Epoch: 7 [16256/50000]\tLoss: 2.8713\tLR: 0.632225\n",
      "Training Epoch: 7 [16384/50000]\tLoss: 2.6252\tLR: 0.632481\n",
      "Training Epoch: 7 [16512/50000]\tLoss: 2.4100\tLR: 0.632737\n",
      "Training Epoch: 7 [16640/50000]\tLoss: 2.8659\tLR: 0.632992\n",
      "Training Epoch: 7 [16768/50000]\tLoss: 2.7361\tLR: 0.633248\n",
      "Training Epoch: 7 [16896/50000]\tLoss: 2.6027\tLR: 0.633504\n",
      "Training Epoch: 7 [17024/50000]\tLoss: 2.4618\tLR: 0.633760\n",
      "Training Epoch: 7 [17152/50000]\tLoss: 2.6586\tLR: 0.634015\n",
      "Training Epoch: 7 [17280/50000]\tLoss: 2.5790\tLR: 0.634271\n",
      "Training Epoch: 7 [17408/50000]\tLoss: 2.8537\tLR: 0.634527\n",
      "Training Epoch: 7 [17536/50000]\tLoss: 2.5150\tLR: 0.634783\n",
      "Training Epoch: 7 [17664/50000]\tLoss: 2.5393\tLR: 0.635038\n",
      "Training Epoch: 7 [17792/50000]\tLoss: 2.8671\tLR: 0.635294\n",
      "Training Epoch: 7 [17920/50000]\tLoss: 2.8842\tLR: 0.635550\n",
      "Training Epoch: 7 [18048/50000]\tLoss: 2.6135\tLR: 0.635806\n",
      "Training Epoch: 7 [18176/50000]\tLoss: 2.5522\tLR: 0.636061\n",
      "Training Epoch: 7 [18304/50000]\tLoss: 2.6547\tLR: 0.636317\n",
      "Training Epoch: 7 [18432/50000]\tLoss: 2.5928\tLR: 0.636573\n",
      "Training Epoch: 7 [18560/50000]\tLoss: 2.8501\tLR: 0.636829\n",
      "Training Epoch: 7 [18688/50000]\tLoss: 2.6501\tLR: 0.637084\n",
      "Training Epoch: 7 [18816/50000]\tLoss: 2.7275\tLR: 0.637340\n",
      "Training Epoch: 7 [18944/50000]\tLoss: 2.7004\tLR: 0.637596\n",
      "Training Epoch: 7 [19072/50000]\tLoss: 2.3819\tLR: 0.637852\n",
      "Training Epoch: 7 [19200/50000]\tLoss: 2.7235\tLR: 0.638107\n",
      "Training Epoch: 7 [19328/50000]\tLoss: 2.4950\tLR: 0.638363\n",
      "Training Epoch: 7 [19456/50000]\tLoss: 2.3624\tLR: 0.638619\n",
      "Training Epoch: 7 [19584/50000]\tLoss: 2.6719\tLR: 0.638875\n",
      "Training Epoch: 7 [19712/50000]\tLoss: 2.8874\tLR: 0.639130\n",
      "Training Epoch: 7 [19840/50000]\tLoss: 2.6534\tLR: 0.639386\n",
      "Training Epoch: 7 [19968/50000]\tLoss: 2.7713\tLR: 0.639642\n",
      "Training Epoch: 7 [20096/50000]\tLoss: 2.6475\tLR: 0.639898\n",
      "Training Epoch: 7 [20224/50000]\tLoss: 2.5887\tLR: 0.640153\n",
      "Training Epoch: 7 [20352/50000]\tLoss: 2.6129\tLR: 0.640409\n",
      "Training Epoch: 7 [20480/50000]\tLoss: 2.5926\tLR: 0.640665\n",
      "Training Epoch: 7 [20608/50000]\tLoss: 2.4942\tLR: 0.640921\n",
      "Training Epoch: 7 [20736/50000]\tLoss: 2.6342\tLR: 0.641176\n",
      "Training Epoch: 7 [20864/50000]\tLoss: 2.8178\tLR: 0.641432\n",
      "Training Epoch: 7 [20992/50000]\tLoss: 2.6868\tLR: 0.641688\n",
      "Training Epoch: 7 [21120/50000]\tLoss: 2.7397\tLR: 0.641944\n",
      "Training Epoch: 7 [21248/50000]\tLoss: 2.7272\tLR: 0.642199\n",
      "Training Epoch: 7 [21376/50000]\tLoss: 2.5890\tLR: 0.642455\n",
      "Training Epoch: 7 [21504/50000]\tLoss: 2.7824\tLR: 0.642711\n",
      "Training Epoch: 7 [21632/50000]\tLoss: 2.5890\tLR: 0.642967\n",
      "Training Epoch: 7 [21760/50000]\tLoss: 2.6871\tLR: 0.643223\n",
      "Training Epoch: 7 [21888/50000]\tLoss: 2.4957\tLR: 0.643478\n",
      "Training Epoch: 7 [22016/50000]\tLoss: 2.9975\tLR: 0.643734\n",
      "Training Epoch: 7 [22144/50000]\tLoss: 2.6451\tLR: 0.643990\n",
      "Training Epoch: 7 [22272/50000]\tLoss: 2.6041\tLR: 0.644246\n",
      "Training Epoch: 7 [22400/50000]\tLoss: 2.6667\tLR: 0.644501\n",
      "Training Epoch: 7 [22528/50000]\tLoss: 2.6763\tLR: 0.644757\n",
      "Training Epoch: 7 [22656/50000]\tLoss: 2.7404\tLR: 0.645013\n",
      "Training Epoch: 7 [22784/50000]\tLoss: 2.8981\tLR: 0.645269\n",
      "Training Epoch: 7 [22912/50000]\tLoss: 2.6125\tLR: 0.645524\n",
      "Training Epoch: 7 [23040/50000]\tLoss: 2.4634\tLR: 0.645780\n",
      "Training Epoch: 7 [23168/50000]\tLoss: 2.7227\tLR: 0.646036\n",
      "Training Epoch: 7 [23296/50000]\tLoss: 2.7449\tLR: 0.646292\n",
      "Training Epoch: 7 [23424/50000]\tLoss: 2.8532\tLR: 0.646547\n",
      "Training Epoch: 7 [23552/50000]\tLoss: 2.4257\tLR: 0.646803\n",
      "Training Epoch: 7 [23680/50000]\tLoss: 2.7919\tLR: 0.647059\n",
      "Training Epoch: 7 [23808/50000]\tLoss: 2.7999\tLR: 0.647315\n",
      "Training Epoch: 7 [23936/50000]\tLoss: 2.7882\tLR: 0.647570\n",
      "Training Epoch: 7 [24064/50000]\tLoss: 2.6223\tLR: 0.647826\n",
      "Training Epoch: 7 [24192/50000]\tLoss: 2.6509\tLR: 0.648082\n",
      "Training Epoch: 7 [24320/50000]\tLoss: 2.6847\tLR: 0.648338\n",
      "Training Epoch: 7 [24448/50000]\tLoss: 2.7795\tLR: 0.648593\n",
      "Training Epoch: 7 [24576/50000]\tLoss: 2.8502\tLR: 0.648849\n",
      "Training Epoch: 7 [24704/50000]\tLoss: 2.9133\tLR: 0.649105\n",
      "Training Epoch: 7 [24832/50000]\tLoss: 2.7326\tLR: 0.649361\n",
      "Training Epoch: 7 [24960/50000]\tLoss: 2.6582\tLR: 0.649616\n",
      "Training Epoch: 7 [25088/50000]\tLoss: 2.8841\tLR: 0.649872\n",
      "Training Epoch: 7 [25216/50000]\tLoss: 2.8833\tLR: 0.650128\n",
      "Training Epoch: 7 [25344/50000]\tLoss: 2.8368\tLR: 0.650384\n",
      "Training Epoch: 7 [25472/50000]\tLoss: 2.5278\tLR: 0.650639\n",
      "Training Epoch: 7 [25600/50000]\tLoss: 2.6842\tLR: 0.650895\n",
      "Training Epoch: 7 [25728/50000]\tLoss: 2.4190\tLR: 0.651151\n",
      "Training Epoch: 7 [25856/50000]\tLoss: 2.6555\tLR: 0.651407\n",
      "Training Epoch: 7 [25984/50000]\tLoss: 2.4335\tLR: 0.651662\n",
      "Training Epoch: 7 [26112/50000]\tLoss: 2.6183\tLR: 0.651918\n",
      "Training Epoch: 7 [26240/50000]\tLoss: 3.0832\tLR: 0.652174\n",
      "Training Epoch: 7 [26368/50000]\tLoss: 2.5467\tLR: 0.652430\n",
      "Training Epoch: 7 [26496/50000]\tLoss: 2.7430\tLR: 0.652685\n",
      "Training Epoch: 7 [26624/50000]\tLoss: 2.7948\tLR: 0.652941\n",
      "Training Epoch: 7 [26752/50000]\tLoss: 2.7236\tLR: 0.653197\n",
      "Training Epoch: 7 [26880/50000]\tLoss: 2.6874\tLR: 0.653453\n",
      "Training Epoch: 7 [27008/50000]\tLoss: 2.4725\tLR: 0.653708\n",
      "Training Epoch: 7 [27136/50000]\tLoss: 2.7913\tLR: 0.653964\n",
      "Training Epoch: 7 [27264/50000]\tLoss: 2.5873\tLR: 0.654220\n",
      "Training Epoch: 7 [27392/50000]\tLoss: 2.5450\tLR: 0.654476\n",
      "Training Epoch: 7 [27520/50000]\tLoss: 2.9935\tLR: 0.654731\n",
      "Training Epoch: 7 [27648/50000]\tLoss: 2.7520\tLR: 0.654987\n",
      "Training Epoch: 7 [27776/50000]\tLoss: 2.8181\tLR: 0.655243\n",
      "Training Epoch: 7 [27904/50000]\tLoss: 2.9387\tLR: 0.655499\n",
      "Training Epoch: 7 [28032/50000]\tLoss: 2.9405\tLR: 0.655754\n",
      "Training Epoch: 7 [28160/50000]\tLoss: 2.7855\tLR: 0.656010\n",
      "Training Epoch: 7 [28288/50000]\tLoss: 2.7077\tLR: 0.656266\n",
      "Training Epoch: 7 [28416/50000]\tLoss: 2.8662\tLR: 0.656522\n",
      "Training Epoch: 7 [28544/50000]\tLoss: 2.5301\tLR: 0.656777\n",
      "Training Epoch: 7 [28672/50000]\tLoss: 2.8666\tLR: 0.657033\n",
      "Training Epoch: 7 [28800/50000]\tLoss: 2.5631\tLR: 0.657289\n",
      "Training Epoch: 7 [28928/50000]\tLoss: 2.6514\tLR: 0.657545\n",
      "Training Epoch: 7 [29056/50000]\tLoss: 2.6496\tLR: 0.657801\n",
      "Training Epoch: 7 [29184/50000]\tLoss: 2.8102\tLR: 0.658056\n",
      "Training Epoch: 7 [29312/50000]\tLoss: 2.5957\tLR: 0.658312\n",
      "Training Epoch: 7 [29440/50000]\tLoss: 2.8493\tLR: 0.658568\n",
      "Training Epoch: 7 [29568/50000]\tLoss: 2.7077\tLR: 0.658824\n",
      "Training Epoch: 7 [29696/50000]\tLoss: 2.4433\tLR: 0.659079\n",
      "Training Epoch: 7 [29824/50000]\tLoss: 2.5361\tLR: 0.659335\n",
      "Training Epoch: 7 [29952/50000]\tLoss: 2.4433\tLR: 0.659591\n",
      "Training Epoch: 7 [30080/50000]\tLoss: 2.7889\tLR: 0.659847\n",
      "Training Epoch: 7 [30208/50000]\tLoss: 2.9312\tLR: 0.660102\n",
      "Training Epoch: 7 [30336/50000]\tLoss: 2.9501\tLR: 0.660358\n",
      "Training Epoch: 7 [30464/50000]\tLoss: 2.9737\tLR: 0.660614\n",
      "Training Epoch: 7 [30592/50000]\tLoss: 2.8489\tLR: 0.660870\n",
      "Training Epoch: 7 [30720/50000]\tLoss: 2.5090\tLR: 0.661125\n",
      "Training Epoch: 7 [30848/50000]\tLoss: 2.5880\tLR: 0.661381\n",
      "Training Epoch: 7 [30976/50000]\tLoss: 2.9150\tLR: 0.661637\n",
      "Training Epoch: 7 [31104/50000]\tLoss: 2.7724\tLR: 0.661893\n",
      "Training Epoch: 7 [31232/50000]\tLoss: 2.9908\tLR: 0.662148\n",
      "Training Epoch: 7 [31360/50000]\tLoss: 2.9872\tLR: 0.662404\n",
      "Training Epoch: 7 [31488/50000]\tLoss: 2.9995\tLR: 0.662660\n",
      "Training Epoch: 7 [31616/50000]\tLoss: 2.7815\tLR: 0.662916\n",
      "Training Epoch: 7 [31744/50000]\tLoss: 2.4882\tLR: 0.663171\n",
      "Training Epoch: 7 [31872/50000]\tLoss: 2.6664\tLR: 0.663427\n",
      "Training Epoch: 7 [32000/50000]\tLoss: 2.8674\tLR: 0.663683\n",
      "Training Epoch: 7 [32128/50000]\tLoss: 2.7089\tLR: 0.663939\n",
      "Training Epoch: 7 [32256/50000]\tLoss: 2.7300\tLR: 0.664194\n",
      "Training Epoch: 7 [32384/50000]\tLoss: 2.5526\tLR: 0.664450\n",
      "Training Epoch: 7 [32512/50000]\tLoss: 2.7691\tLR: 0.664706\n",
      "Training Epoch: 7 [32640/50000]\tLoss: 2.9072\tLR: 0.664962\n",
      "Training Epoch: 7 [32768/50000]\tLoss: 2.6198\tLR: 0.665217\n",
      "Training Epoch: 7 [32896/50000]\tLoss: 2.6425\tLR: 0.665473\n",
      "Training Epoch: 7 [33024/50000]\tLoss: 2.8363\tLR: 0.665729\n",
      "Training Epoch: 7 [33152/50000]\tLoss: 2.9831\tLR: 0.665985\n",
      "Training Epoch: 7 [33280/50000]\tLoss: 2.5662\tLR: 0.666240\n",
      "Training Epoch: 7 [33408/50000]\tLoss: 3.1469\tLR: 0.666496\n",
      "Training Epoch: 7 [33536/50000]\tLoss: 2.4782\tLR: 0.666752\n",
      "Training Epoch: 7 [33664/50000]\tLoss: 2.4556\tLR: 0.667008\n",
      "Training Epoch: 7 [33792/50000]\tLoss: 2.5372\tLR: 0.667263\n",
      "Training Epoch: 7 [33920/50000]\tLoss: 2.8507\tLR: 0.667519\n",
      "Training Epoch: 7 [34048/50000]\tLoss: 2.7306\tLR: 0.667775\n",
      "Training Epoch: 7 [34176/50000]\tLoss: 2.8178\tLR: 0.668031\n",
      "Training Epoch: 7 [34304/50000]\tLoss: 2.6474\tLR: 0.668286\n",
      "Training Epoch: 7 [34432/50000]\tLoss: 2.9019\tLR: 0.668542\n",
      "Training Epoch: 7 [34560/50000]\tLoss: 2.4886\tLR: 0.668798\n",
      "Training Epoch: 7 [34688/50000]\tLoss: 2.3972\tLR: 0.669054\n",
      "Training Epoch: 7 [34816/50000]\tLoss: 2.6620\tLR: 0.669309\n",
      "Training Epoch: 7 [34944/50000]\tLoss: 2.7110\tLR: 0.669565\n",
      "Training Epoch: 7 [35072/50000]\tLoss: 2.8143\tLR: 0.669821\n",
      "Training Epoch: 7 [35200/50000]\tLoss: 2.7985\tLR: 0.670077\n",
      "Training Epoch: 7 [35328/50000]\tLoss: 2.5865\tLR: 0.670332\n",
      "Training Epoch: 7 [35456/50000]\tLoss: 2.3852\tLR: 0.670588\n",
      "Training Epoch: 7 [35584/50000]\tLoss: 2.5067\tLR: 0.670844\n",
      "Training Epoch: 7 [35712/50000]\tLoss: 2.5934\tLR: 0.671100\n",
      "Training Epoch: 7 [35840/50000]\tLoss: 2.5396\tLR: 0.671355\n",
      "Training Epoch: 7 [35968/50000]\tLoss: 2.5799\tLR: 0.671611\n",
      "Training Epoch: 7 [36096/50000]\tLoss: 2.5926\tLR: 0.671867\n",
      "Training Epoch: 7 [36224/50000]\tLoss: 2.7651\tLR: 0.672123\n",
      "Training Epoch: 7 [36352/50000]\tLoss: 2.7047\tLR: 0.672379\n",
      "Training Epoch: 7 [36480/50000]\tLoss: 2.6995\tLR: 0.672634\n",
      "Training Epoch: 7 [36608/50000]\tLoss: 2.5103\tLR: 0.672890\n",
      "Training Epoch: 7 [36736/50000]\tLoss: 2.6281\tLR: 0.673146\n",
      "Training Epoch: 7 [36864/50000]\tLoss: 2.8672\tLR: 0.673402\n",
      "Training Epoch: 7 [36992/50000]\tLoss: 2.5233\tLR: 0.673657\n",
      "Training Epoch: 7 [37120/50000]\tLoss: 2.8522\tLR: 0.673913\n",
      "Training Epoch: 7 [37248/50000]\tLoss: 2.9817\tLR: 0.674169\n",
      "Training Epoch: 7 [37376/50000]\tLoss: 2.6018\tLR: 0.674425\n",
      "Training Epoch: 7 [37504/50000]\tLoss: 2.8995\tLR: 0.674680\n",
      "Training Epoch: 7 [37632/50000]\tLoss: 2.8771\tLR: 0.674936\n",
      "Training Epoch: 7 [37760/50000]\tLoss: 2.6929\tLR: 0.675192\n",
      "Training Epoch: 7 [37888/50000]\tLoss: 2.5971\tLR: 0.675448\n",
      "Training Epoch: 7 [38016/50000]\tLoss: 2.6370\tLR: 0.675703\n",
      "Training Epoch: 7 [38144/50000]\tLoss: 2.5582\tLR: 0.675959\n",
      "Training Epoch: 7 [38272/50000]\tLoss: 2.4069\tLR: 0.676215\n",
      "Training Epoch: 7 [38400/50000]\tLoss: 2.7491\tLR: 0.676471\n",
      "Training Epoch: 7 [38528/50000]\tLoss: 2.7900\tLR: 0.676726\n",
      "Training Epoch: 7 [38656/50000]\tLoss: 2.8961\tLR: 0.676982\n",
      "Training Epoch: 7 [38784/50000]\tLoss: 2.7912\tLR: 0.677238\n",
      "Training Epoch: 7 [38912/50000]\tLoss: 2.7129\tLR: 0.677494\n",
      "Training Epoch: 7 [39040/50000]\tLoss: 2.6962\tLR: 0.677749\n",
      "Training Epoch: 7 [39168/50000]\tLoss: 2.7517\tLR: 0.678005\n",
      "Training Epoch: 7 [39296/50000]\tLoss: 2.6770\tLR: 0.678261\n",
      "Training Epoch: 7 [39424/50000]\tLoss: 2.6697\tLR: 0.678517\n",
      "Training Epoch: 7 [39552/50000]\tLoss: 2.6707\tLR: 0.678772\n",
      "Training Epoch: 7 [39680/50000]\tLoss: 2.3710\tLR: 0.679028\n",
      "Training Epoch: 7 [39808/50000]\tLoss: 2.6899\tLR: 0.679284\n",
      "Training Epoch: 7 [39936/50000]\tLoss: 2.7291\tLR: 0.679540\n",
      "Training Epoch: 7 [40064/50000]\tLoss: 2.6263\tLR: 0.679795\n",
      "Training Epoch: 7 [40192/50000]\tLoss: 2.7366\tLR: 0.680051\n",
      "Training Epoch: 7 [40320/50000]\tLoss: 2.5738\tLR: 0.680307\n",
      "Training Epoch: 7 [40448/50000]\tLoss: 2.5799\tLR: 0.680563\n",
      "Training Epoch: 7 [40576/50000]\tLoss: 2.6845\tLR: 0.680818\n",
      "Training Epoch: 7 [40704/50000]\tLoss: 2.5787\tLR: 0.681074\n",
      "Training Epoch: 7 [40832/50000]\tLoss: 2.7253\tLR: 0.681330\n",
      "Training Epoch: 7 [40960/50000]\tLoss: 3.0677\tLR: 0.681586\n",
      "Training Epoch: 7 [41088/50000]\tLoss: 3.0225\tLR: 0.681841\n",
      "Training Epoch: 7 [41216/50000]\tLoss: 2.8150\tLR: 0.682097\n",
      "Training Epoch: 7 [41344/50000]\tLoss: 3.1010\tLR: 0.682353\n",
      "Training Epoch: 7 [41472/50000]\tLoss: 2.7941\tLR: 0.682609\n",
      "Training Epoch: 7 [41600/50000]\tLoss: 2.5905\tLR: 0.682864\n",
      "Training Epoch: 7 [41728/50000]\tLoss: 2.5451\tLR: 0.683120\n",
      "Training Epoch: 7 [41856/50000]\tLoss: 2.6906\tLR: 0.683376\n",
      "Training Epoch: 7 [41984/50000]\tLoss: 2.6782\tLR: 0.683632\n",
      "Training Epoch: 7 [42112/50000]\tLoss: 2.6375\tLR: 0.683887\n",
      "Training Epoch: 7 [42240/50000]\tLoss: 2.5814\tLR: 0.684143\n",
      "Training Epoch: 7 [42368/50000]\tLoss: 2.7711\tLR: 0.684399\n",
      "Training Epoch: 7 [42496/50000]\tLoss: 2.5811\tLR: 0.684655\n",
      "Training Epoch: 7 [42624/50000]\tLoss: 2.4032\tLR: 0.684910\n",
      "Training Epoch: 7 [42752/50000]\tLoss: 2.8061\tLR: 0.685166\n",
      "Training Epoch: 7 [42880/50000]\tLoss: 2.5824\tLR: 0.685422\n",
      "Training Epoch: 7 [43008/50000]\tLoss: 2.6972\tLR: 0.685678\n",
      "Training Epoch: 7 [43136/50000]\tLoss: 2.6525\tLR: 0.685934\n",
      "Training Epoch: 7 [43264/50000]\tLoss: 2.6259\tLR: 0.686189\n",
      "Training Epoch: 7 [43392/50000]\tLoss: 2.5982\tLR: 0.686445\n",
      "Training Epoch: 7 [43520/50000]\tLoss: 2.5656\tLR: 0.686701\n",
      "Training Epoch: 7 [43648/50000]\tLoss: 2.5994\tLR: 0.686957\n",
      "Training Epoch: 7 [43776/50000]\tLoss: 2.7689\tLR: 0.687212\n",
      "Training Epoch: 7 [43904/50000]\tLoss: 2.5998\tLR: 0.687468\n",
      "Training Epoch: 7 [44032/50000]\tLoss: 2.7021\tLR: 0.687724\n",
      "Training Epoch: 7 [44160/50000]\tLoss: 2.5794\tLR: 0.687980\n",
      "Training Epoch: 7 [44288/50000]\tLoss: 2.7548\tLR: 0.688235\n",
      "Training Epoch: 7 [44416/50000]\tLoss: 2.7341\tLR: 0.688491\n",
      "Training Epoch: 7 [44544/50000]\tLoss: 2.6462\tLR: 0.688747\n",
      "Training Epoch: 7 [44672/50000]\tLoss: 2.8175\tLR: 0.689003\n",
      "Training Epoch: 7 [44800/50000]\tLoss: 2.5412\tLR: 0.689258\n",
      "Training Epoch: 7 [44928/50000]\tLoss: 2.8809\tLR: 0.689514\n",
      "Training Epoch: 7 [45056/50000]\tLoss: 2.8677\tLR: 0.689770\n",
      "Training Epoch: 7 [45184/50000]\tLoss: 2.6768\tLR: 0.690026\n",
      "Training Epoch: 7 [45312/50000]\tLoss: 2.7132\tLR: 0.690281\n",
      "Training Epoch: 7 [45440/50000]\tLoss: 2.6422\tLR: 0.690537\n",
      "Training Epoch: 7 [45568/50000]\tLoss: 2.6231\tLR: 0.690793\n",
      "Training Epoch: 7 [45696/50000]\tLoss: 2.7267\tLR: 0.691049\n",
      "Training Epoch: 7 [45824/50000]\tLoss: 2.8590\tLR: 0.691304\n",
      "Training Epoch: 7 [45952/50000]\tLoss: 2.4872\tLR: 0.691560\n",
      "Training Epoch: 7 [46080/50000]\tLoss: 2.5844\tLR: 0.691816\n",
      "Training Epoch: 7 [46208/50000]\tLoss: 2.7003\tLR: 0.692072\n",
      "Training Epoch: 7 [46336/50000]\tLoss: 2.8017\tLR: 0.692327\n",
      "Training Epoch: 7 [46464/50000]\tLoss: 2.6711\tLR: 0.692583\n",
      "Training Epoch: 7 [46592/50000]\tLoss: 2.7314\tLR: 0.692839\n",
      "Training Epoch: 7 [46720/50000]\tLoss: 2.7270\tLR: 0.693095\n",
      "Training Epoch: 7 [46848/50000]\tLoss: 2.7184\tLR: 0.693350\n",
      "Training Epoch: 7 [46976/50000]\tLoss: 2.6369\tLR: 0.693606\n",
      "Training Epoch: 7 [47104/50000]\tLoss: 2.6147\tLR: 0.693862\n",
      "Training Epoch: 7 [47232/50000]\tLoss: 2.5473\tLR: 0.694118\n",
      "Training Epoch: 7 [47360/50000]\tLoss: 2.5702\tLR: 0.694373\n",
      "Training Epoch: 7 [47488/50000]\tLoss: 2.7799\tLR: 0.694629\n",
      "Training Epoch: 7 [47616/50000]\tLoss: 2.5809\tLR: 0.694885\n",
      "Training Epoch: 7 [47744/50000]\tLoss: 2.4756\tLR: 0.695141\n",
      "Training Epoch: 7 [47872/50000]\tLoss: 2.8577\tLR: 0.695396\n",
      "Training Epoch: 7 [48000/50000]\tLoss: 2.6397\tLR: 0.695652\n",
      "Training Epoch: 7 [48128/50000]\tLoss: 3.0387\tLR: 0.695908\n",
      "Training Epoch: 7 [48256/50000]\tLoss: 2.7789\tLR: 0.696164\n",
      "Training Epoch: 7 [48384/50000]\tLoss: 2.5582\tLR: 0.696419\n",
      "Training Epoch: 7 [48512/50000]\tLoss: 2.6365\tLR: 0.696675\n",
      "Training Epoch: 7 [48640/50000]\tLoss: 2.6191\tLR: 0.696931\n",
      "Training Epoch: 7 [48768/50000]\tLoss: 2.6612\tLR: 0.697187\n",
      "Training Epoch: 7 [48896/50000]\tLoss: 2.5118\tLR: 0.697442\n",
      "Training Epoch: 7 [49024/50000]\tLoss: 2.8012\tLR: 0.697698\n",
      "Training Epoch: 7 [49152/50000]\tLoss: 2.8538\tLR: 0.697954\n",
      "Training Epoch: 7 [49280/50000]\tLoss: 2.8628\tLR: 0.698210\n",
      "Training Epoch: 7 [49408/50000]\tLoss: 2.7048\tLR: 0.698465\n",
      "Training Epoch: 7 [49536/50000]\tLoss: 2.8806\tLR: 0.698721\n",
      "Training Epoch: 7 [49664/50000]\tLoss: 2.6940\tLR: 0.698977\n",
      "Training Epoch: 7 [49792/50000]\tLoss: 2.8668\tLR: 0.699233\n",
      "Training Epoch: 7 [49920/50000]\tLoss: 2.6382\tLR: 0.699488\n",
      "Training Epoch: 7 [50000/50000]\tLoss: 2.3308\tLR: 0.699744\n",
      "epoch 7 training time consumed: 489.67s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |    9815 GB |    9814 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |    9784 GB |    9784 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      30 GB |      30 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |    9815 GB |    9814 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |    9784 GB |    9784 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      30 GB |      30 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |    9673 GB |    9673 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |    9643 GB |    9643 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |      30 GB |      30 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    1041 K  |    1040 K  |\n",
      "|       from large pool |      24    |      65    |     443 K  |     443 K  |\n",
      "|       from small pool |     231    |     274    |     597 K  |     597 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    1041 K  |    1040 K  |\n",
      "|       from large pool |      24    |      65    |     443 K  |     443 K  |\n",
      "|       from small pool |     231    |     274    |     597 K  |     597 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      45    |  603109    |  603072    |\n",
      "|       from large pool |      10    |      23    |  213246    |  213236    |\n",
      "|       from small pool |      27    |      35    |  389863    |  389836    |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 7, Average loss: 0.0297, Accuracy: 0.1848, Time consumed:31.80s\n",
      "\n",
      "Training Epoch: 8 [128/50000]\tLoss: 2.6961\tLR: 0.100000\n",
      "Training Epoch: 8 [256/50000]\tLoss: 2.8494\tLR: 0.700256\n",
      "Training Epoch: 8 [384/50000]\tLoss: 2.6143\tLR: 0.700512\n",
      "Training Epoch: 8 [512/50000]\tLoss: 2.5518\tLR: 0.700767\n",
      "Training Epoch: 8 [640/50000]\tLoss: 2.7190\tLR: 0.701023\n",
      "Training Epoch: 8 [768/50000]\tLoss: 2.5571\tLR: 0.701279\n",
      "Training Epoch: 8 [896/50000]\tLoss: 2.7191\tLR: 0.701535\n",
      "Training Epoch: 8 [1024/50000]\tLoss: 2.4287\tLR: 0.701790\n",
      "Training Epoch: 8 [1152/50000]\tLoss: 2.7942\tLR: 0.702046\n",
      "Training Epoch: 8 [1280/50000]\tLoss: 2.7196\tLR: 0.702302\n",
      "Training Epoch: 8 [1408/50000]\tLoss: 2.5755\tLR: 0.702558\n",
      "Training Epoch: 8 [1536/50000]\tLoss: 2.4997\tLR: 0.702813\n",
      "Training Epoch: 8 [1664/50000]\tLoss: 2.6633\tLR: 0.703069\n",
      "Training Epoch: 8 [1792/50000]\tLoss: 2.6565\tLR: 0.703325\n",
      "Training Epoch: 8 [1920/50000]\tLoss: 2.7608\tLR: 0.703581\n",
      "Training Epoch: 8 [2048/50000]\tLoss: 2.5494\tLR: 0.703836\n",
      "Training Epoch: 8 [2176/50000]\tLoss: 2.7200\tLR: 0.704092\n",
      "Training Epoch: 8 [2304/50000]\tLoss: 2.6891\tLR: 0.704348\n",
      "Training Epoch: 8 [2432/50000]\tLoss: 2.7011\tLR: 0.704604\n",
      "Training Epoch: 8 [2560/50000]\tLoss: 2.8769\tLR: 0.704859\n",
      "Training Epoch: 8 [2688/50000]\tLoss: 2.8704\tLR: 0.705115\n",
      "Training Epoch: 8 [2816/50000]\tLoss: 2.6408\tLR: 0.705371\n",
      "Training Epoch: 8 [2944/50000]\tLoss: 2.9340\tLR: 0.705627\n",
      "Training Epoch: 8 [3072/50000]\tLoss: 2.8736\tLR: 0.705882\n",
      "Training Epoch: 8 [3200/50000]\tLoss: 2.6613\tLR: 0.706138\n",
      "Training Epoch: 8 [3328/50000]\tLoss: 2.8264\tLR: 0.706394\n",
      "Training Epoch: 8 [3456/50000]\tLoss: 3.4568\tLR: 0.706650\n",
      "Training Epoch: 8 [3584/50000]\tLoss: 2.6681\tLR: 0.706905\n",
      "Training Epoch: 8 [3712/50000]\tLoss: 2.4586\tLR: 0.707161\n",
      "Training Epoch: 8 [3840/50000]\tLoss: 2.6643\tLR: 0.707417\n",
      "Training Epoch: 8 [3968/50000]\tLoss: 2.9051\tLR: 0.707673\n",
      "Training Epoch: 8 [4096/50000]\tLoss: 2.8742\tLR: 0.707928\n",
      "Training Epoch: 8 [4224/50000]\tLoss: 2.5252\tLR: 0.708184\n",
      "Training Epoch: 8 [4352/50000]\tLoss: 2.5900\tLR: 0.708440\n",
      "Training Epoch: 8 [4480/50000]\tLoss: 2.7637\tLR: 0.708696\n",
      "Training Epoch: 8 [4608/50000]\tLoss: 2.5524\tLR: 0.708951\n",
      "Training Epoch: 8 [4736/50000]\tLoss: 2.9111\tLR: 0.709207\n",
      "Training Epoch: 8 [4864/50000]\tLoss: 2.9582\tLR: 0.709463\n",
      "Training Epoch: 8 [4992/50000]\tLoss: 2.4847\tLR: 0.709719\n",
      "Training Epoch: 8 [5120/50000]\tLoss: 2.9979\tLR: 0.709974\n",
      "Training Epoch: 8 [5248/50000]\tLoss: 3.1456\tLR: 0.710230\n",
      "Training Epoch: 8 [5376/50000]\tLoss: 2.4884\tLR: 0.710486\n",
      "Training Epoch: 8 [5504/50000]\tLoss: 2.6938\tLR: 0.710742\n",
      "Training Epoch: 8 [5632/50000]\tLoss: 2.5538\tLR: 0.710997\n",
      "Training Epoch: 8 [5760/50000]\tLoss: 2.5270\tLR: 0.711253\n",
      "Training Epoch: 8 [5888/50000]\tLoss: 2.5997\tLR: 0.711509\n",
      "Training Epoch: 8 [6016/50000]\tLoss: 2.5032\tLR: 0.711765\n",
      "Training Epoch: 8 [6144/50000]\tLoss: 2.7558\tLR: 0.712020\n",
      "Training Epoch: 8 [6272/50000]\tLoss: 2.7549\tLR: 0.712276\n",
      "Training Epoch: 8 [6400/50000]\tLoss: 2.6033\tLR: 0.712532\n",
      "Training Epoch: 8 [6528/50000]\tLoss: 2.3593\tLR: 0.712788\n",
      "Training Epoch: 8 [6656/50000]\tLoss: 2.7255\tLR: 0.713043\n",
      "Training Epoch: 8 [6784/50000]\tLoss: 2.8223\tLR: 0.713299\n",
      "Training Epoch: 8 [6912/50000]\tLoss: 2.7397\tLR: 0.713555\n",
      "Training Epoch: 8 [7040/50000]\tLoss: 2.7965\tLR: 0.713811\n",
      "Training Epoch: 8 [7168/50000]\tLoss: 2.5726\tLR: 0.714066\n",
      "Training Epoch: 8 [7296/50000]\tLoss: 2.5412\tLR: 0.714322\n",
      "Training Epoch: 8 [7424/50000]\tLoss: 2.6588\tLR: 0.714578\n",
      "Training Epoch: 8 [7552/50000]\tLoss: 2.7844\tLR: 0.714834\n",
      "Training Epoch: 8 [7680/50000]\tLoss: 2.7717\tLR: 0.715090\n",
      "Training Epoch: 8 [7808/50000]\tLoss: 3.0240\tLR: 0.715345\n",
      "Training Epoch: 8 [7936/50000]\tLoss: 2.8188\tLR: 0.715601\n",
      "Training Epoch: 8 [8064/50000]\tLoss: 2.5934\tLR: 0.715857\n",
      "Training Epoch: 8 [8192/50000]\tLoss: 2.6773\tLR: 0.716113\n",
      "Training Epoch: 8 [8320/50000]\tLoss: 2.8094\tLR: 0.716368\n",
      "Training Epoch: 8 [8448/50000]\tLoss: 2.8854\tLR: 0.716624\n",
      "Training Epoch: 8 [8576/50000]\tLoss: 2.7211\tLR: 0.716880\n",
      "Training Epoch: 8 [8704/50000]\tLoss: 2.6891\tLR: 0.717136\n",
      "Training Epoch: 8 [8832/50000]\tLoss: 2.6976\tLR: 0.717391\n",
      "Training Epoch: 8 [8960/50000]\tLoss: 2.9525\tLR: 0.717647\n",
      "Training Epoch: 8 [9088/50000]\tLoss: 2.5448\tLR: 0.717903\n",
      "Training Epoch: 8 [9216/50000]\tLoss: 2.6423\tLR: 0.718159\n",
      "Training Epoch: 8 [9344/50000]\tLoss: 2.8198\tLR: 0.718414\n",
      "Training Epoch: 8 [9472/50000]\tLoss: 2.6647\tLR: 0.718670\n",
      "Training Epoch: 8 [9600/50000]\tLoss: 2.9520\tLR: 0.718926\n",
      "Training Epoch: 8 [9728/50000]\tLoss: 2.8787\tLR: 0.719182\n",
      "Training Epoch: 8 [9856/50000]\tLoss: 2.4334\tLR: 0.719437\n",
      "Training Epoch: 8 [9984/50000]\tLoss: 2.5608\tLR: 0.719693\n",
      "Training Epoch: 8 [10112/50000]\tLoss: 2.7248\tLR: 0.719949\n",
      "Training Epoch: 8 [10240/50000]\tLoss: 2.6799\tLR: 0.720205\n",
      "Training Epoch: 8 [10368/50000]\tLoss: 2.6946\tLR: 0.720460\n",
      "Training Epoch: 8 [10496/50000]\tLoss: 2.5879\tLR: 0.720716\n",
      "Training Epoch: 8 [10624/50000]\tLoss: 2.8297\tLR: 0.720972\n",
      "Training Epoch: 8 [10752/50000]\tLoss: 2.9273\tLR: 0.721228\n",
      "Training Epoch: 8 [10880/50000]\tLoss: 2.7921\tLR: 0.721483\n",
      "Training Epoch: 8 [11008/50000]\tLoss: 2.7997\tLR: 0.721739\n",
      "Training Epoch: 8 [11136/50000]\tLoss: 2.8595\tLR: 0.721995\n",
      "Training Epoch: 8 [11264/50000]\tLoss: 2.8012\tLR: 0.722251\n",
      "Training Epoch: 8 [11392/50000]\tLoss: 2.6128\tLR: 0.722506\n",
      "Training Epoch: 8 [11520/50000]\tLoss: 2.6533\tLR: 0.722762\n",
      "Training Epoch: 8 [11648/50000]\tLoss: 2.7900\tLR: 0.723018\n",
      "Training Epoch: 8 [11776/50000]\tLoss: 2.7573\tLR: 0.723274\n",
      "Training Epoch: 8 [11904/50000]\tLoss: 2.6491\tLR: 0.723529\n",
      "Training Epoch: 8 [12032/50000]\tLoss: 2.6029\tLR: 0.723785\n",
      "Training Epoch: 8 [12160/50000]\tLoss: 2.6026\tLR: 0.724041\n",
      "Training Epoch: 8 [12288/50000]\tLoss: 2.6131\tLR: 0.724297\n",
      "Training Epoch: 8 [12416/50000]\tLoss: 2.8452\tLR: 0.724552\n",
      "Training Epoch: 8 [12544/50000]\tLoss: 2.6181\tLR: 0.724808\n",
      "Training Epoch: 8 [12672/50000]\tLoss: 2.8871\tLR: 0.725064\n",
      "Training Epoch: 8 [12800/50000]\tLoss: 2.5121\tLR: 0.725320\n",
      "Training Epoch: 8 [12928/50000]\tLoss: 2.4753\tLR: 0.725575\n",
      "Training Epoch: 8 [13056/50000]\tLoss: 2.9502\tLR: 0.725831\n",
      "Training Epoch: 8 [13184/50000]\tLoss: 2.6007\tLR: 0.726087\n",
      "Training Epoch: 8 [13312/50000]\tLoss: 2.4414\tLR: 0.726343\n",
      "Training Epoch: 8 [13440/50000]\tLoss: 2.6205\tLR: 0.726598\n",
      "Training Epoch: 8 [13568/50000]\tLoss: 2.6658\tLR: 0.726854\n",
      "Training Epoch: 8 [13696/50000]\tLoss: 2.6133\tLR: 0.727110\n",
      "Training Epoch: 8 [13824/50000]\tLoss: 2.9019\tLR: 0.727366\n",
      "Training Epoch: 8 [13952/50000]\tLoss: 2.5071\tLR: 0.727621\n",
      "Training Epoch: 8 [14080/50000]\tLoss: 2.4393\tLR: 0.727877\n",
      "Training Epoch: 8 [14208/50000]\tLoss: 2.5180\tLR: 0.728133\n",
      "Training Epoch: 8 [14336/50000]\tLoss: 2.9420\tLR: 0.728389\n",
      "Training Epoch: 8 [14464/50000]\tLoss: 2.7463\tLR: 0.728645\n",
      "Training Epoch: 8 [14592/50000]\tLoss: 2.6703\tLR: 0.728900\n",
      "Training Epoch: 8 [14720/50000]\tLoss: 2.6126\tLR: 0.729156\n",
      "Training Epoch: 8 [14848/50000]\tLoss: 2.7864\tLR: 0.729412\n",
      "Training Epoch: 8 [14976/50000]\tLoss: 2.6698\tLR: 0.729668\n",
      "Training Epoch: 8 [15104/50000]\tLoss: 2.6395\tLR: 0.729923\n",
      "Training Epoch: 8 [15232/50000]\tLoss: 2.3968\tLR: 0.730179\n",
      "Training Epoch: 8 [15360/50000]\tLoss: 2.8036\tLR: 0.730435\n",
      "Training Epoch: 8 [15488/50000]\tLoss: 2.8147\tLR: 0.730691\n",
      "Training Epoch: 8 [15616/50000]\tLoss: 2.4855\tLR: 0.730946\n",
      "Training Epoch: 8 [15744/50000]\tLoss: 2.7289\tLR: 0.731202\n",
      "Training Epoch: 8 [15872/50000]\tLoss: 2.7675\tLR: 0.731458\n",
      "Training Epoch: 8 [16000/50000]\tLoss: 2.5866\tLR: 0.731714\n",
      "Training Epoch: 8 [16128/50000]\tLoss: 2.7917\tLR: 0.731969\n",
      "Training Epoch: 8 [16256/50000]\tLoss: 2.8281\tLR: 0.732225\n",
      "Training Epoch: 8 [16384/50000]\tLoss: 2.5225\tLR: 0.732481\n",
      "Training Epoch: 8 [16512/50000]\tLoss: 2.5969\tLR: 0.732737\n",
      "Training Epoch: 8 [16640/50000]\tLoss: 2.5451\tLR: 0.732992\n",
      "Training Epoch: 8 [16768/50000]\tLoss: 2.6509\tLR: 0.733248\n",
      "Training Epoch: 8 [16896/50000]\tLoss: 2.7284\tLR: 0.733504\n",
      "Training Epoch: 8 [17024/50000]\tLoss: 2.4412\tLR: 0.733760\n",
      "Training Epoch: 8 [17152/50000]\tLoss: 2.5823\tLR: 0.734015\n",
      "Training Epoch: 8 [17280/50000]\tLoss: 2.7496\tLR: 0.734271\n",
      "Training Epoch: 8 [17408/50000]\tLoss: 2.8441\tLR: 0.734527\n",
      "Training Epoch: 8 [17536/50000]\tLoss: 2.6247\tLR: 0.734783\n",
      "Training Epoch: 8 [17664/50000]\tLoss: 2.8069\tLR: 0.735038\n",
      "Training Epoch: 8 [17792/50000]\tLoss: 2.8124\tLR: 0.735294\n",
      "Training Epoch: 8 [17920/50000]\tLoss: 2.9088\tLR: 0.735550\n",
      "Training Epoch: 8 [18048/50000]\tLoss: 2.7196\tLR: 0.735806\n",
      "Training Epoch: 8 [18176/50000]\tLoss: 2.6841\tLR: 0.736061\n",
      "Training Epoch: 8 [18304/50000]\tLoss: 2.6488\tLR: 0.736317\n",
      "Training Epoch: 8 [18432/50000]\tLoss: 2.8553\tLR: 0.736573\n",
      "Training Epoch: 8 [18560/50000]\tLoss: 2.9112\tLR: 0.736829\n",
      "Training Epoch: 8 [18688/50000]\tLoss: 2.8757\tLR: 0.737084\n",
      "Training Epoch: 8 [18816/50000]\tLoss: 2.8777\tLR: 0.737340\n",
      "Training Epoch: 8 [18944/50000]\tLoss: 2.5606\tLR: 0.737596\n",
      "Training Epoch: 8 [19072/50000]\tLoss: 2.8224\tLR: 0.737852\n",
      "Training Epoch: 8 [19200/50000]\tLoss: 2.8592\tLR: 0.738107\n",
      "Training Epoch: 8 [19328/50000]\tLoss: 2.7338\tLR: 0.738363\n",
      "Training Epoch: 8 [19456/50000]\tLoss: 2.6205\tLR: 0.738619\n",
      "Training Epoch: 8 [19584/50000]\tLoss: 2.6668\tLR: 0.738875\n",
      "Training Epoch: 8 [19712/50000]\tLoss: 2.6570\tLR: 0.739130\n",
      "Training Epoch: 8 [19840/50000]\tLoss: 2.7706\tLR: 0.739386\n",
      "Training Epoch: 8 [19968/50000]\tLoss: 2.8365\tLR: 0.739642\n",
      "Training Epoch: 8 [20096/50000]\tLoss: 2.5877\tLR: 0.739898\n",
      "Training Epoch: 8 [20224/50000]\tLoss: 2.4809\tLR: 0.740153\n",
      "Training Epoch: 8 [20352/50000]\tLoss: 2.8095\tLR: 0.740409\n",
      "Training Epoch: 8 [20480/50000]\tLoss: 2.7655\tLR: 0.740665\n",
      "Training Epoch: 8 [20608/50000]\tLoss: 2.7225\tLR: 0.740921\n",
      "Training Epoch: 8 [20736/50000]\tLoss: 2.8833\tLR: 0.741176\n",
      "Training Epoch: 8 [20864/50000]\tLoss: 2.7426\tLR: 0.741432\n",
      "Training Epoch: 8 [20992/50000]\tLoss: 2.8104\tLR: 0.741688\n",
      "Training Epoch: 8 [21120/50000]\tLoss: 2.6688\tLR: 0.741944\n",
      "Training Epoch: 8 [21248/50000]\tLoss: 2.7602\tLR: 0.742199\n",
      "Training Epoch: 8 [21376/50000]\tLoss: 2.7158\tLR: 0.742455\n",
      "Training Epoch: 8 [21504/50000]\tLoss: 2.8128\tLR: 0.742711\n",
      "Training Epoch: 8 [21632/50000]\tLoss: 2.8646\tLR: 0.742967\n",
      "Training Epoch: 8 [21760/50000]\tLoss: 2.7698\tLR: 0.743223\n",
      "Training Epoch: 8 [21888/50000]\tLoss: 3.0458\tLR: 0.743478\n",
      "Training Epoch: 8 [22016/50000]\tLoss: 2.7867\tLR: 0.743734\n",
      "Training Epoch: 8 [22144/50000]\tLoss: 2.9837\tLR: 0.743990\n",
      "Training Epoch: 8 [22272/50000]\tLoss: 2.9798\tLR: 0.744246\n",
      "Training Epoch: 8 [22400/50000]\tLoss: 2.9353\tLR: 0.744501\n",
      "Training Epoch: 8 [22528/50000]\tLoss: 2.8045\tLR: 0.744757\n",
      "Training Epoch: 8 [22656/50000]\tLoss: 2.7325\tLR: 0.745013\n",
      "Training Epoch: 8 [22784/50000]\tLoss: 2.8366\tLR: 0.745269\n",
      "Training Epoch: 8 [22912/50000]\tLoss: 2.8133\tLR: 0.745524\n",
      "Training Epoch: 8 [23040/50000]\tLoss: 2.6980\tLR: 0.745780\n",
      "Training Epoch: 8 [23168/50000]\tLoss: 2.7441\tLR: 0.746036\n",
      "Training Epoch: 8 [23296/50000]\tLoss: 2.6714\tLR: 0.746292\n",
      "Training Epoch: 8 [23424/50000]\tLoss: 2.7188\tLR: 0.746547\n",
      "Training Epoch: 8 [23552/50000]\tLoss: 2.8837\tLR: 0.746803\n",
      "Training Epoch: 8 [23680/50000]\tLoss: 2.7390\tLR: 0.747059\n",
      "Training Epoch: 8 [23808/50000]\tLoss: 2.5869\tLR: 0.747315\n",
      "Training Epoch: 8 [23936/50000]\tLoss: 2.7672\tLR: 0.747570\n",
      "Training Epoch: 8 [24064/50000]\tLoss: 3.0218\tLR: 0.747826\n",
      "Training Epoch: 8 [24192/50000]\tLoss: 2.9986\tLR: 0.748082\n",
      "Training Epoch: 8 [24320/50000]\tLoss: 2.9421\tLR: 0.748338\n",
      "Training Epoch: 8 [24448/50000]\tLoss: 2.4566\tLR: 0.748593\n",
      "Training Epoch: 8 [24576/50000]\tLoss: 2.7543\tLR: 0.748849\n",
      "Training Epoch: 8 [24704/50000]\tLoss: 2.6975\tLR: 0.749105\n",
      "Training Epoch: 8 [24832/50000]\tLoss: 2.6213\tLR: 0.749361\n",
      "Training Epoch: 8 [24960/50000]\tLoss: 2.7830\tLR: 0.749616\n",
      "Training Epoch: 8 [25088/50000]\tLoss: 3.0457\tLR: 0.749872\n",
      "Training Epoch: 8 [25216/50000]\tLoss: 2.6756\tLR: 0.750128\n",
      "Training Epoch: 8 [25344/50000]\tLoss: 2.5854\tLR: 0.750384\n",
      "Training Epoch: 8 [25472/50000]\tLoss: 3.0076\tLR: 0.750639\n",
      "Training Epoch: 8 [25600/50000]\tLoss: 2.4154\tLR: 0.750895\n",
      "Training Epoch: 8 [25728/50000]\tLoss: 2.8399\tLR: 0.751151\n",
      "Training Epoch: 8 [25856/50000]\tLoss: 2.7584\tLR: 0.751407\n",
      "Training Epoch: 8 [25984/50000]\tLoss: 2.7864\tLR: 0.751662\n",
      "Training Epoch: 8 [26112/50000]\tLoss: 2.9474\tLR: 0.751918\n",
      "Training Epoch: 8 [26240/50000]\tLoss: 2.9511\tLR: 0.752174\n",
      "Training Epoch: 8 [26368/50000]\tLoss: 2.6257\tLR: 0.752430\n",
      "Training Epoch: 8 [26496/50000]\tLoss: 2.7370\tLR: 0.752685\n",
      "Training Epoch: 8 [26624/50000]\tLoss: 2.4312\tLR: 0.752941\n",
      "Training Epoch: 8 [26752/50000]\tLoss: 2.4737\tLR: 0.753197\n",
      "Training Epoch: 8 [26880/50000]\tLoss: 2.7742\tLR: 0.753453\n",
      "Training Epoch: 8 [27008/50000]\tLoss: 2.8271\tLR: 0.753708\n",
      "Training Epoch: 8 [27136/50000]\tLoss: 2.6841\tLR: 0.753964\n",
      "Training Epoch: 8 [27264/50000]\tLoss: 2.6963\tLR: 0.754220\n",
      "Training Epoch: 8 [27392/50000]\tLoss: 2.7008\tLR: 0.754476\n",
      "Training Epoch: 8 [27520/50000]\tLoss: 2.9990\tLR: 0.754731\n",
      "Training Epoch: 8 [27648/50000]\tLoss: 2.4796\tLR: 0.754987\n",
      "Training Epoch: 8 [27776/50000]\tLoss: 2.7725\tLR: 0.755243\n",
      "Training Epoch: 8 [27904/50000]\tLoss: 2.8736\tLR: 0.755499\n",
      "Training Epoch: 8 [28032/50000]\tLoss: 2.6361\tLR: 0.755754\n",
      "Training Epoch: 8 [28160/50000]\tLoss: 2.6952\tLR: 0.756010\n",
      "Training Epoch: 8 [28288/50000]\tLoss: 2.5789\tLR: 0.756266\n",
      "Training Epoch: 8 [28416/50000]\tLoss: 2.9186\tLR: 0.756522\n",
      "Training Epoch: 8 [28544/50000]\tLoss: 2.6245\tLR: 0.756777\n",
      "Training Epoch: 8 [28672/50000]\tLoss: 2.5863\tLR: 0.757033\n",
      "Training Epoch: 8 [28800/50000]\tLoss: 2.8552\tLR: 0.757289\n",
      "Training Epoch: 8 [28928/50000]\tLoss: 2.9777\tLR: 0.757545\n",
      "Training Epoch: 8 [29056/50000]\tLoss: 2.8291\tLR: 0.757801\n",
      "Training Epoch: 8 [29184/50000]\tLoss: 2.6898\tLR: 0.758056\n",
      "Training Epoch: 8 [29312/50000]\tLoss: 2.2786\tLR: 0.758312\n",
      "Training Epoch: 8 [29440/50000]\tLoss: 2.6185\tLR: 0.758568\n",
      "Training Epoch: 8 [29568/50000]\tLoss: 3.0219\tLR: 0.758824\n",
      "Training Epoch: 8 [29696/50000]\tLoss: 2.9314\tLR: 0.759079\n",
      "Training Epoch: 8 [29824/50000]\tLoss: 2.6516\tLR: 0.759335\n",
      "Training Epoch: 8 [29952/50000]\tLoss: 2.6021\tLR: 0.759591\n",
      "Training Epoch: 8 [30080/50000]\tLoss: 2.6569\tLR: 0.759847\n",
      "Training Epoch: 8 [30208/50000]\tLoss: 2.4214\tLR: 0.760102\n",
      "Training Epoch: 8 [30336/50000]\tLoss: 2.8544\tLR: 0.760358\n",
      "Training Epoch: 8 [30464/50000]\tLoss: 2.4219\tLR: 0.760614\n",
      "Training Epoch: 8 [30592/50000]\tLoss: 2.5884\tLR: 0.760870\n",
      "Training Epoch: 8 [30720/50000]\tLoss: 2.6391\tLR: 0.761125\n",
      "Training Epoch: 8 [30848/50000]\tLoss: 2.7188\tLR: 0.761381\n",
      "Training Epoch: 8 [30976/50000]\tLoss: 2.7768\tLR: 0.761637\n",
      "Training Epoch: 8 [31104/50000]\tLoss: 2.5710\tLR: 0.761893\n",
      "Training Epoch: 8 [31232/50000]\tLoss: 2.5744\tLR: 0.762148\n",
      "Training Epoch: 8 [31360/50000]\tLoss: 2.8420\tLR: 0.762404\n",
      "Training Epoch: 8 [31488/50000]\tLoss: 2.5782\tLR: 0.762660\n",
      "Training Epoch: 8 [31616/50000]\tLoss: 2.8194\tLR: 0.762916\n",
      "Training Epoch: 8 [31744/50000]\tLoss: 2.7807\tLR: 0.763171\n",
      "Training Epoch: 8 [31872/50000]\tLoss: 2.5628\tLR: 0.763427\n",
      "Training Epoch: 8 [32000/50000]\tLoss: 3.0853\tLR: 0.763683\n",
      "Training Epoch: 8 [32128/50000]\tLoss: 2.7547\tLR: 0.763939\n",
      "Training Epoch: 8 [32256/50000]\tLoss: 2.8804\tLR: 0.764194\n",
      "Training Epoch: 8 [32384/50000]\tLoss: 2.6971\tLR: 0.764450\n",
      "Training Epoch: 8 [32512/50000]\tLoss: 2.9912\tLR: 0.764706\n",
      "Training Epoch: 8 [32640/50000]\tLoss: 2.8708\tLR: 0.764962\n",
      "Training Epoch: 8 [32768/50000]\tLoss: 2.7205\tLR: 0.765217\n",
      "Training Epoch: 8 [32896/50000]\tLoss: 3.0639\tLR: 0.765473\n",
      "Training Epoch: 8 [33024/50000]\tLoss: 2.7660\tLR: 0.765729\n",
      "Training Epoch: 8 [33152/50000]\tLoss: 2.4792\tLR: 0.765985\n",
      "Training Epoch: 8 [33280/50000]\tLoss: 2.8003\tLR: 0.766240\n",
      "Training Epoch: 8 [33408/50000]\tLoss: 2.9102\tLR: 0.766496\n",
      "Training Epoch: 8 [33536/50000]\tLoss: 2.6906\tLR: 0.766752\n",
      "Training Epoch: 8 [33664/50000]\tLoss: 2.9626\tLR: 0.767008\n",
      "Training Epoch: 8 [33792/50000]\tLoss: 2.7157\tLR: 0.767263\n",
      "Training Epoch: 8 [33920/50000]\tLoss: 2.7578\tLR: 0.767519\n",
      "Training Epoch: 8 [34048/50000]\tLoss: 2.7909\tLR: 0.767775\n",
      "Training Epoch: 8 [34176/50000]\tLoss: 2.8712\tLR: 0.768031\n",
      "Training Epoch: 8 [34304/50000]\tLoss: 2.9571\tLR: 0.768286\n",
      "Training Epoch: 8 [34432/50000]\tLoss: 2.8456\tLR: 0.768542\n",
      "Training Epoch: 8 [34560/50000]\tLoss: 2.5795\tLR: 0.768798\n",
      "Training Epoch: 8 [34688/50000]\tLoss: 2.7655\tLR: 0.769054\n",
      "Training Epoch: 8 [34816/50000]\tLoss: 2.9220\tLR: 0.769309\n",
      "Training Epoch: 8 [34944/50000]\tLoss: 2.5469\tLR: 0.769565\n",
      "Training Epoch: 8 [35072/50000]\tLoss: 2.5696\tLR: 0.769821\n",
      "Training Epoch: 8 [35200/50000]\tLoss: 2.5833\tLR: 0.770077\n",
      "Training Epoch: 8 [35328/50000]\tLoss: 2.8203\tLR: 0.770332\n",
      "Training Epoch: 8 [35456/50000]\tLoss: 2.6593\tLR: 0.770588\n",
      "Training Epoch: 8 [35584/50000]\tLoss: 2.8785\tLR: 0.770844\n",
      "Training Epoch: 8 [35712/50000]\tLoss: 2.6335\tLR: 0.771100\n",
      "Training Epoch: 8 [35840/50000]\tLoss: 2.5983\tLR: 0.771355\n",
      "Training Epoch: 8 [35968/50000]\tLoss: 2.8877\tLR: 0.771611\n",
      "Training Epoch: 8 [36096/50000]\tLoss: 2.7269\tLR: 0.771867\n",
      "Training Epoch: 8 [36224/50000]\tLoss: 3.0147\tLR: 0.772123\n",
      "Training Epoch: 8 [36352/50000]\tLoss: 2.7802\tLR: 0.772379\n",
      "Training Epoch: 8 [36480/50000]\tLoss: 2.9334\tLR: 0.772634\n",
      "Training Epoch: 8 [36608/50000]\tLoss: 2.9237\tLR: 0.772890\n",
      "Training Epoch: 8 [36736/50000]\tLoss: 2.8987\tLR: 0.773146\n",
      "Training Epoch: 8 [36864/50000]\tLoss: 2.6766\tLR: 0.773402\n",
      "Training Epoch: 8 [36992/50000]\tLoss: 2.8875\tLR: 0.773657\n",
      "Training Epoch: 8 [37120/50000]\tLoss: 2.8661\tLR: 0.773913\n",
      "Training Epoch: 8 [37248/50000]\tLoss: 2.7757\tLR: 0.774169\n",
      "Training Epoch: 8 [37376/50000]\tLoss: 2.9210\tLR: 0.774425\n",
      "Training Epoch: 8 [37504/50000]\tLoss: 2.9112\tLR: 0.774680\n",
      "Training Epoch: 8 [37632/50000]\tLoss: 2.8133\tLR: 0.774936\n",
      "Training Epoch: 8 [37760/50000]\tLoss: 2.7018\tLR: 0.775192\n",
      "Training Epoch: 8 [37888/50000]\tLoss: 2.5773\tLR: 0.775448\n",
      "Training Epoch: 8 [38016/50000]\tLoss: 2.7783\tLR: 0.775703\n",
      "Training Epoch: 8 [38144/50000]\tLoss: 2.8390\tLR: 0.775959\n",
      "Training Epoch: 8 [38272/50000]\tLoss: 2.5609\tLR: 0.776215\n",
      "Training Epoch: 8 [38400/50000]\tLoss: 2.7173\tLR: 0.776471\n",
      "Training Epoch: 8 [38528/50000]\tLoss: 2.8662\tLR: 0.776726\n",
      "Training Epoch: 8 [38656/50000]\tLoss: 2.8438\tLR: 0.776982\n",
      "Training Epoch: 8 [38784/50000]\tLoss: 2.9047\tLR: 0.777238\n",
      "Training Epoch: 8 [38912/50000]\tLoss: 2.7028\tLR: 0.777494\n",
      "Training Epoch: 8 [39040/50000]\tLoss: 2.4906\tLR: 0.777749\n",
      "Training Epoch: 8 [39168/50000]\tLoss: 2.8106\tLR: 0.778005\n",
      "Training Epoch: 8 [39296/50000]\tLoss: 2.7388\tLR: 0.778261\n",
      "Training Epoch: 8 [39424/50000]\tLoss: 3.0357\tLR: 0.778517\n",
      "Training Epoch: 8 [39552/50000]\tLoss: 2.7697\tLR: 0.778772\n",
      "Training Epoch: 8 [39680/50000]\tLoss: 2.7610\tLR: 0.779028\n",
      "Training Epoch: 8 [39808/50000]\tLoss: 2.8261\tLR: 0.779284\n",
      "Training Epoch: 8 [39936/50000]\tLoss: 2.8455\tLR: 0.779540\n",
      "Training Epoch: 8 [40064/50000]\tLoss: 2.9782\tLR: 0.779795\n",
      "Training Epoch: 8 [40192/50000]\tLoss: 2.9322\tLR: 0.780051\n",
      "Training Epoch: 8 [40320/50000]\tLoss: 2.9024\tLR: 0.780307\n",
      "Training Epoch: 8 [40448/50000]\tLoss: 2.9036\tLR: 0.780563\n",
      "Training Epoch: 8 [40576/50000]\tLoss: 2.7523\tLR: 0.780818\n",
      "Training Epoch: 8 [40704/50000]\tLoss: 2.7010\tLR: 0.781074\n",
      "Training Epoch: 8 [40832/50000]\tLoss: 2.8562\tLR: 0.781330\n",
      "Training Epoch: 8 [40960/50000]\tLoss: 2.9166\tLR: 0.781586\n",
      "Training Epoch: 8 [41088/50000]\tLoss: 2.7980\tLR: 0.781841\n",
      "Training Epoch: 8 [41216/50000]\tLoss: 2.6816\tLR: 0.782097\n",
      "Training Epoch: 8 [41344/50000]\tLoss: 2.9433\tLR: 0.782353\n",
      "Training Epoch: 8 [41472/50000]\tLoss: 2.8089\tLR: 0.782609\n",
      "Training Epoch: 8 [41600/50000]\tLoss: 2.5481\tLR: 0.782864\n",
      "Training Epoch: 8 [41728/50000]\tLoss: 2.7362\tLR: 0.783120\n",
      "Training Epoch: 8 [41856/50000]\tLoss: 3.0400\tLR: 0.783376\n",
      "Training Epoch: 8 [41984/50000]\tLoss: 2.6407\tLR: 0.783632\n",
      "Training Epoch: 8 [42112/50000]\tLoss: 2.8636\tLR: 0.783887\n",
      "Training Epoch: 8 [42240/50000]\tLoss: 2.5423\tLR: 0.784143\n",
      "Training Epoch: 8 [42368/50000]\tLoss: 2.4576\tLR: 0.784399\n",
      "Training Epoch: 8 [42496/50000]\tLoss: 2.9025\tLR: 0.784655\n",
      "Training Epoch: 8 [42624/50000]\tLoss: 2.8698\tLR: 0.784910\n",
      "Training Epoch: 8 [42752/50000]\tLoss: 2.6314\tLR: 0.785166\n",
      "Training Epoch: 8 [42880/50000]\tLoss: 2.6017\tLR: 0.785422\n",
      "Training Epoch: 8 [43008/50000]\tLoss: 2.9788\tLR: 0.785678\n",
      "Training Epoch: 8 [43136/50000]\tLoss: 2.9225\tLR: 0.785934\n",
      "Training Epoch: 8 [43264/50000]\tLoss: 2.5001\tLR: 0.786189\n",
      "Training Epoch: 8 [43392/50000]\tLoss: 2.5316\tLR: 0.786445\n",
      "Training Epoch: 8 [43520/50000]\tLoss: 2.6616\tLR: 0.786701\n",
      "Training Epoch: 8 [43648/50000]\tLoss: 2.9037\tLR: 0.786957\n",
      "Training Epoch: 8 [43776/50000]\tLoss: 2.5461\tLR: 0.787212\n",
      "Training Epoch: 8 [43904/50000]\tLoss: 2.9107\tLR: 0.787468\n",
      "Training Epoch: 8 [44032/50000]\tLoss: 2.5505\tLR: 0.787724\n",
      "Training Epoch: 8 [44160/50000]\tLoss: 2.8856\tLR: 0.787980\n",
      "Training Epoch: 8 [44288/50000]\tLoss: 2.8707\tLR: 0.788235\n",
      "Training Epoch: 8 [44416/50000]\tLoss: 2.8776\tLR: 0.788491\n",
      "Training Epoch: 8 [44544/50000]\tLoss: 2.7570\tLR: 0.788747\n",
      "Training Epoch: 8 [44672/50000]\tLoss: 2.8073\tLR: 0.789003\n",
      "Training Epoch: 8 [44800/50000]\tLoss: 2.9259\tLR: 0.789258\n",
      "Training Epoch: 8 [44928/50000]\tLoss: 2.5639\tLR: 0.789514\n",
      "Training Epoch: 8 [45056/50000]\tLoss: 3.1714\tLR: 0.789770\n",
      "Training Epoch: 8 [45184/50000]\tLoss: 2.4849\tLR: 0.790026\n",
      "Training Epoch: 8 [45312/50000]\tLoss: 3.0083\tLR: 0.790281\n",
      "Training Epoch: 8 [45440/50000]\tLoss: 2.9803\tLR: 0.790537\n",
      "Training Epoch: 8 [45568/50000]\tLoss: 2.7486\tLR: 0.790793\n",
      "Training Epoch: 8 [45696/50000]\tLoss: 2.5636\tLR: 0.791049\n",
      "Training Epoch: 8 [45824/50000]\tLoss: 2.9711\tLR: 0.791304\n",
      "Training Epoch: 8 [45952/50000]\tLoss: 2.7399\tLR: 0.791560\n",
      "Training Epoch: 8 [46080/50000]\tLoss: 2.7991\tLR: 0.791816\n",
      "Training Epoch: 8 [46208/50000]\tLoss: 2.6512\tLR: 0.792072\n",
      "Training Epoch: 8 [46336/50000]\tLoss: 2.7292\tLR: 0.792327\n",
      "Training Epoch: 8 [46464/50000]\tLoss: 3.0440\tLR: 0.792583\n",
      "Training Epoch: 8 [46592/50000]\tLoss: 2.6912\tLR: 0.792839\n",
      "Training Epoch: 8 [46720/50000]\tLoss: 2.9331\tLR: 0.793095\n",
      "Training Epoch: 8 [46848/50000]\tLoss: 2.9679\tLR: 0.793350\n",
      "Training Epoch: 8 [46976/50000]\tLoss: 2.6464\tLR: 0.793606\n",
      "Training Epoch: 8 [47104/50000]\tLoss: 2.9836\tLR: 0.793862\n",
      "Training Epoch: 8 [47232/50000]\tLoss: 2.7278\tLR: 0.794118\n",
      "Training Epoch: 8 [47360/50000]\tLoss: 2.6759\tLR: 0.794373\n",
      "Training Epoch: 8 [47488/50000]\tLoss: 2.9835\tLR: 0.794629\n",
      "Training Epoch: 8 [47616/50000]\tLoss: 2.9489\tLR: 0.794885\n",
      "Training Epoch: 8 [47744/50000]\tLoss: 2.8437\tLR: 0.795141\n",
      "Training Epoch: 8 [47872/50000]\tLoss: 2.8052\tLR: 0.795396\n",
      "Training Epoch: 8 [48000/50000]\tLoss: 2.6387\tLR: 0.795652\n",
      "Training Epoch: 8 [48128/50000]\tLoss: 2.8908\tLR: 0.795908\n",
      "Training Epoch: 8 [48256/50000]\tLoss: 2.7318\tLR: 0.796164\n",
      "Training Epoch: 8 [48384/50000]\tLoss: 2.8348\tLR: 0.796419\n",
      "Training Epoch: 8 [48512/50000]\tLoss: 2.9134\tLR: 0.796675\n",
      "Training Epoch: 8 [48640/50000]\tLoss: 2.7900\tLR: 0.796931\n",
      "Training Epoch: 8 [48768/50000]\tLoss: 2.4846\tLR: 0.797187\n",
      "Training Epoch: 8 [48896/50000]\tLoss: 2.8358\tLR: 0.797442\n",
      "Training Epoch: 8 [49024/50000]\tLoss: 2.7275\tLR: 0.797698\n",
      "Training Epoch: 8 [49152/50000]\tLoss: 2.6689\tLR: 0.797954\n",
      "Training Epoch: 8 [49280/50000]\tLoss: 2.7669\tLR: 0.798210\n",
      "Training Epoch: 8 [49408/50000]\tLoss: 2.5753\tLR: 0.798465\n",
      "Training Epoch: 8 [49536/50000]\tLoss: 2.8213\tLR: 0.798721\n",
      "Training Epoch: 8 [49664/50000]\tLoss: 2.6822\tLR: 0.798977\n",
      "Training Epoch: 8 [49792/50000]\tLoss: 2.9247\tLR: 0.799233\n",
      "Training Epoch: 8 [49920/50000]\tLoss: 3.0105\tLR: 0.799488\n",
      "Training Epoch: 8 [50000/50000]\tLoss: 2.8713\tLR: 0.799744\n",
      "epoch 8 training time consumed: 490.61s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   11216 GB |   11216 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   11182 GB |   11182 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      34 GB |      34 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   11216 GB |   11216 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   11182 GB |   11182 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      34 GB |      34 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   11056 GB |   11055 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   11021 GB |   11021 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |      34 GB |      34 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    1189 K  |    1189 K  |\n",
      "|       from large pool |      24    |      65    |     507 K  |     506 K  |\n",
      "|       from small pool |     231    |     274    |     682 K  |     682 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    1189 K  |    1189 K  |\n",
      "|       from large pool |      24    |      65    |     507 K  |     506 K  |\n",
      "|       from small pool |     231    |     274    |     682 K  |     682 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      45    |  689228    |  689191    |\n",
      "|       from large pool |      10    |      23    |  243706    |  243696    |\n",
      "|       from small pool |      27    |      35    |  445522    |  445495    |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 8, Average loss: 0.0267, Accuracy: 0.2211, Time consumed:32.11s\n",
      "\n",
      "Training Epoch: 9 [128/50000]\tLoss: 2.5847\tLR: 0.100000\n",
      "Training Epoch: 9 [256/50000]\tLoss: 2.7827\tLR: 0.800256\n",
      "Training Epoch: 9 [384/50000]\tLoss: 2.7302\tLR: 0.800512\n",
      "Training Epoch: 9 [512/50000]\tLoss: 2.8259\tLR: 0.800767\n",
      "Training Epoch: 9 [640/50000]\tLoss: 2.6985\tLR: 0.801023\n",
      "Training Epoch: 9 [768/50000]\tLoss: 2.9156\tLR: 0.801279\n",
      "Training Epoch: 9 [896/50000]\tLoss: 3.0382\tLR: 0.801535\n",
      "Training Epoch: 9 [1024/50000]\tLoss: 2.7994\tLR: 0.801790\n",
      "Training Epoch: 9 [1152/50000]\tLoss: 2.6351\tLR: 0.802046\n",
      "Training Epoch: 9 [1280/50000]\tLoss: 2.8887\tLR: 0.802302\n",
      "Training Epoch: 9 [1408/50000]\tLoss: 2.7568\tLR: 0.802558\n",
      "Training Epoch: 9 [1536/50000]\tLoss: 2.8444\tLR: 0.802813\n",
      "Training Epoch: 9 [1664/50000]\tLoss: 2.9542\tLR: 0.803069\n",
      "Training Epoch: 9 [1792/50000]\tLoss: 2.8639\tLR: 0.803325\n",
      "Training Epoch: 9 [1920/50000]\tLoss: 2.9519\tLR: 0.803581\n",
      "Training Epoch: 9 [2048/50000]\tLoss: 2.7980\tLR: 0.803836\n",
      "Training Epoch: 9 [2176/50000]\tLoss: 2.9433\tLR: 0.804092\n",
      "Training Epoch: 9 [2304/50000]\tLoss: 2.8416\tLR: 0.804348\n",
      "Training Epoch: 9 [2432/50000]\tLoss: 2.5526\tLR: 0.804604\n",
      "Training Epoch: 9 [2560/50000]\tLoss: 3.1230\tLR: 0.804859\n",
      "Training Epoch: 9 [2688/50000]\tLoss: 3.0443\tLR: 0.805115\n",
      "Training Epoch: 9 [2816/50000]\tLoss: 2.7170\tLR: 0.805371\n",
      "Training Epoch: 9 [2944/50000]\tLoss: 2.5932\tLR: 0.805627\n",
      "Training Epoch: 9 [3072/50000]\tLoss: 2.7547\tLR: 0.805882\n",
      "Training Epoch: 9 [3200/50000]\tLoss: 2.7780\tLR: 0.806138\n",
      "Training Epoch: 9 [3328/50000]\tLoss: 2.5857\tLR: 0.806394\n",
      "Training Epoch: 9 [3456/50000]\tLoss: 2.8482\tLR: 0.806650\n",
      "Training Epoch: 9 [3584/50000]\tLoss: 2.8086\tLR: 0.806905\n",
      "Training Epoch: 9 [3712/50000]\tLoss: 2.9312\tLR: 0.807161\n",
      "Training Epoch: 9 [3840/50000]\tLoss: 3.0443\tLR: 0.807417\n",
      "Training Epoch: 9 [3968/50000]\tLoss: 2.9107\tLR: 0.807673\n",
      "Training Epoch: 9 [4096/50000]\tLoss: 2.7820\tLR: 0.807928\n",
      "Training Epoch: 9 [4224/50000]\tLoss: 2.5344\tLR: 0.808184\n",
      "Training Epoch: 9 [4352/50000]\tLoss: 2.7398\tLR: 0.808440\n",
      "Training Epoch: 9 [4480/50000]\tLoss: 2.8933\tLR: 0.808696\n",
      "Training Epoch: 9 [4608/50000]\tLoss: 2.7920\tLR: 0.808951\n",
      "Training Epoch: 9 [4736/50000]\tLoss: 2.9960\tLR: 0.809207\n",
      "Training Epoch: 9 [4864/50000]\tLoss: 2.7234\tLR: 0.809463\n",
      "Training Epoch: 9 [4992/50000]\tLoss: 2.9032\tLR: 0.809719\n",
      "Training Epoch: 9 [5120/50000]\tLoss: 2.9229\tLR: 0.809974\n",
      "Training Epoch: 9 [5248/50000]\tLoss: 2.3453\tLR: 0.810230\n",
      "Training Epoch: 9 [5376/50000]\tLoss: 2.8824\tLR: 0.810486\n",
      "Training Epoch: 9 [5504/50000]\tLoss: 3.0737\tLR: 0.810742\n",
      "Training Epoch: 9 [5632/50000]\tLoss: 2.7253\tLR: 0.810997\n",
      "Training Epoch: 9 [5760/50000]\tLoss: 2.9402\tLR: 0.811253\n",
      "Training Epoch: 9 [5888/50000]\tLoss: 2.7983\tLR: 0.811509\n",
      "Training Epoch: 9 [6016/50000]\tLoss: 2.8827\tLR: 0.811765\n",
      "Training Epoch: 9 [6144/50000]\tLoss: 2.7616\tLR: 0.812020\n",
      "Training Epoch: 9 [6272/50000]\tLoss: 2.6105\tLR: 0.812276\n",
      "Training Epoch: 9 [6400/50000]\tLoss: 2.6618\tLR: 0.812532\n",
      "Training Epoch: 9 [6528/50000]\tLoss: 2.8393\tLR: 0.812788\n",
      "Training Epoch: 9 [6656/50000]\tLoss: 2.8919\tLR: 0.813043\n",
      "Training Epoch: 9 [6784/50000]\tLoss: 3.0207\tLR: 0.813299\n",
      "Training Epoch: 9 [6912/50000]\tLoss: 2.6332\tLR: 0.813555\n",
      "Training Epoch: 9 [7040/50000]\tLoss: 2.4464\tLR: 0.813811\n",
      "Training Epoch: 9 [7168/50000]\tLoss: 2.7732\tLR: 0.814066\n",
      "Training Epoch: 9 [7296/50000]\tLoss: 2.7279\tLR: 0.814322\n",
      "Training Epoch: 9 [7424/50000]\tLoss: 2.6401\tLR: 0.814578\n",
      "Training Epoch: 9 [7552/50000]\tLoss: 2.7626\tLR: 0.814834\n",
      "Training Epoch: 9 [7680/50000]\tLoss: 2.7399\tLR: 0.815090\n",
      "Training Epoch: 9 [7808/50000]\tLoss: 2.9307\tLR: 0.815345\n",
      "Training Epoch: 9 [7936/50000]\tLoss: 2.9477\tLR: 0.815601\n",
      "Training Epoch: 9 [8064/50000]\tLoss: 2.6290\tLR: 0.815857\n",
      "Training Epoch: 9 [8192/50000]\tLoss: 3.0232\tLR: 0.816113\n",
      "Training Epoch: 9 [8320/50000]\tLoss: 2.7539\tLR: 0.816368\n",
      "Training Epoch: 9 [8448/50000]\tLoss: 2.9678\tLR: 0.816624\n",
      "Training Epoch: 9 [8576/50000]\tLoss: 3.1306\tLR: 0.816880\n",
      "Training Epoch: 9 [8704/50000]\tLoss: 2.8466\tLR: 0.817136\n",
      "Training Epoch: 9 [8832/50000]\tLoss: 2.7725\tLR: 0.817391\n",
      "Training Epoch: 9 [8960/50000]\tLoss: 2.8216\tLR: 0.817647\n",
      "Training Epoch: 9 [9088/50000]\tLoss: 3.0273\tLR: 0.817903\n",
      "Training Epoch: 9 [9216/50000]\tLoss: 2.7256\tLR: 0.818159\n",
      "Training Epoch: 9 [9344/50000]\tLoss: 2.8600\tLR: 0.818414\n",
      "Training Epoch: 9 [9472/50000]\tLoss: 2.9690\tLR: 0.818670\n",
      "Training Epoch: 9 [9600/50000]\tLoss: 2.7729\tLR: 0.818926\n",
      "Training Epoch: 9 [9728/50000]\tLoss: 2.8653\tLR: 0.819182\n",
      "Training Epoch: 9 [9856/50000]\tLoss: 2.6571\tLR: 0.819437\n",
      "Training Epoch: 9 [9984/50000]\tLoss: 2.8908\tLR: 0.819693\n",
      "Training Epoch: 9 [10112/50000]\tLoss: 2.9404\tLR: 0.819949\n",
      "Training Epoch: 9 [10240/50000]\tLoss: 2.8119\tLR: 0.820205\n",
      "Training Epoch: 9 [10368/50000]\tLoss: 2.8656\tLR: 0.820460\n",
      "Training Epoch: 9 [10496/50000]\tLoss: 2.7832\tLR: 0.820716\n",
      "Training Epoch: 9 [10624/50000]\tLoss: 2.8674\tLR: 0.820972\n",
      "Training Epoch: 9 [10752/50000]\tLoss: 2.4997\tLR: 0.821228\n",
      "Training Epoch: 9 [10880/50000]\tLoss: 2.6922\tLR: 0.821483\n",
      "Training Epoch: 9 [11008/50000]\tLoss: 2.7791\tLR: 0.821739\n",
      "Training Epoch: 9 [11136/50000]\tLoss: 2.8161\tLR: 0.821995\n",
      "Training Epoch: 9 [11264/50000]\tLoss: 2.5680\tLR: 0.822251\n",
      "Training Epoch: 9 [11392/50000]\tLoss: 2.9682\tLR: 0.822506\n",
      "Training Epoch: 9 [11520/50000]\tLoss: 2.8374\tLR: 0.822762\n",
      "Training Epoch: 9 [11648/50000]\tLoss: 2.6140\tLR: 0.823018\n",
      "Training Epoch: 9 [11776/50000]\tLoss: 2.7329\tLR: 0.823274\n",
      "Training Epoch: 9 [11904/50000]\tLoss: 2.7093\tLR: 0.823529\n",
      "Training Epoch: 9 [12032/50000]\tLoss: 2.9234\tLR: 0.823785\n",
      "Training Epoch: 9 [12160/50000]\tLoss: 2.9358\tLR: 0.824041\n",
      "Training Epoch: 9 [12288/50000]\tLoss: 2.8425\tLR: 0.824297\n",
      "Training Epoch: 9 [12416/50000]\tLoss: 2.5497\tLR: 0.824552\n",
      "Training Epoch: 9 [12544/50000]\tLoss: 2.6975\tLR: 0.824808\n",
      "Training Epoch: 9 [12672/50000]\tLoss: 3.0021\tLR: 0.825064\n",
      "Training Epoch: 9 [12800/50000]\tLoss: 2.6551\tLR: 0.825320\n",
      "Training Epoch: 9 [12928/50000]\tLoss: 2.8877\tLR: 0.825575\n",
      "Training Epoch: 9 [13056/50000]\tLoss: 2.8851\tLR: 0.825831\n",
      "Training Epoch: 9 [13184/50000]\tLoss: 2.6575\tLR: 0.826087\n",
      "Training Epoch: 9 [13312/50000]\tLoss: 2.6547\tLR: 0.826343\n",
      "Training Epoch: 9 [13440/50000]\tLoss: 2.7838\tLR: 0.826598\n",
      "Training Epoch: 9 [13568/50000]\tLoss: 2.6998\tLR: 0.826854\n",
      "Training Epoch: 9 [13696/50000]\tLoss: 2.9699\tLR: 0.827110\n",
      "Training Epoch: 9 [13824/50000]\tLoss: 2.9421\tLR: 0.827366\n",
      "Training Epoch: 9 [13952/50000]\tLoss: 2.6316\tLR: 0.827621\n",
      "Training Epoch: 9 [14080/50000]\tLoss: 2.8141\tLR: 0.827877\n",
      "Training Epoch: 9 [14208/50000]\tLoss: 2.6853\tLR: 0.828133\n",
      "Training Epoch: 9 [14336/50000]\tLoss: 3.0876\tLR: 0.828389\n",
      "Training Epoch: 9 [14464/50000]\tLoss: 3.0200\tLR: 0.828645\n",
      "Training Epoch: 9 [14592/50000]\tLoss: 2.7497\tLR: 0.828900\n",
      "Training Epoch: 9 [14720/50000]\tLoss: 2.8100\tLR: 0.829156\n",
      "Training Epoch: 9 [14848/50000]\tLoss: 2.6710\tLR: 0.829412\n",
      "Training Epoch: 9 [14976/50000]\tLoss: 2.6790\tLR: 0.829668\n",
      "Training Epoch: 9 [15104/50000]\tLoss: 2.4237\tLR: 0.829923\n",
      "Training Epoch: 9 [15232/50000]\tLoss: 2.4662\tLR: 0.830179\n",
      "Training Epoch: 9 [15360/50000]\tLoss: 2.9026\tLR: 0.830435\n",
      "Training Epoch: 9 [15488/50000]\tLoss: 2.4398\tLR: 0.830691\n",
      "Training Epoch: 9 [15616/50000]\tLoss: 2.8682\tLR: 0.830946\n",
      "Training Epoch: 9 [15744/50000]\tLoss: 3.0381\tLR: 0.831202\n",
      "Training Epoch: 9 [15872/50000]\tLoss: 2.9786\tLR: 0.831458\n",
      "Training Epoch: 9 [16000/50000]\tLoss: 2.5762\tLR: 0.831714\n",
      "Training Epoch: 9 [16128/50000]\tLoss: 2.7841\tLR: 0.831969\n",
      "Training Epoch: 9 [16256/50000]\tLoss: 2.6903\tLR: 0.832225\n",
      "Training Epoch: 9 [16384/50000]\tLoss: 3.0153\tLR: 0.832481\n",
      "Training Epoch: 9 [16512/50000]\tLoss: 2.7727\tLR: 0.832737\n",
      "Training Epoch: 9 [16640/50000]\tLoss: 2.7052\tLR: 0.832992\n",
      "Training Epoch: 9 [16768/50000]\tLoss: 2.7629\tLR: 0.833248\n",
      "Training Epoch: 9 [16896/50000]\tLoss: 2.9739\tLR: 0.833504\n",
      "Training Epoch: 9 [17024/50000]\tLoss: 2.7720\tLR: 0.833760\n",
      "Training Epoch: 9 [17152/50000]\tLoss: 2.7148\tLR: 0.834015\n",
      "Training Epoch: 9 [17280/50000]\tLoss: 2.7265\tLR: 0.834271\n",
      "Training Epoch: 9 [17408/50000]\tLoss: 2.7829\tLR: 0.834527\n",
      "Training Epoch: 9 [17536/50000]\tLoss: 2.8995\tLR: 0.834783\n",
      "Training Epoch: 9 [17664/50000]\tLoss: 2.9092\tLR: 0.835038\n",
      "Training Epoch: 9 [17792/50000]\tLoss: 2.9347\tLR: 0.835294\n",
      "Training Epoch: 9 [17920/50000]\tLoss: 2.5817\tLR: 0.835550\n",
      "Training Epoch: 9 [18048/50000]\tLoss: 2.9250\tLR: 0.835806\n",
      "Training Epoch: 9 [18176/50000]\tLoss: 2.9383\tLR: 0.836061\n",
      "Training Epoch: 9 [18304/50000]\tLoss: 2.9352\tLR: 0.836317\n",
      "Training Epoch: 9 [18432/50000]\tLoss: 3.0985\tLR: 0.836573\n",
      "Training Epoch: 9 [18560/50000]\tLoss: 2.9092\tLR: 0.836829\n",
      "Training Epoch: 9 [18688/50000]\tLoss: 2.7190\tLR: 0.837084\n",
      "Training Epoch: 9 [18816/50000]\tLoss: 2.9174\tLR: 0.837340\n",
      "Training Epoch: 9 [18944/50000]\tLoss: 2.9331\tLR: 0.837596\n",
      "Training Epoch: 9 [19072/50000]\tLoss: 3.0679\tLR: 0.837852\n",
      "Training Epoch: 9 [19200/50000]\tLoss: 2.9383\tLR: 0.838107\n",
      "Training Epoch: 9 [19328/50000]\tLoss: 3.0209\tLR: 0.838363\n",
      "Training Epoch: 9 [19456/50000]\tLoss: 2.8696\tLR: 0.838619\n",
      "Training Epoch: 9 [19584/50000]\tLoss: 2.4403\tLR: 0.838875\n",
      "Training Epoch: 9 [19712/50000]\tLoss: 2.8693\tLR: 0.839130\n",
      "Training Epoch: 9 [19840/50000]\tLoss: 2.8407\tLR: 0.839386\n",
      "Training Epoch: 9 [19968/50000]\tLoss: 2.7964\tLR: 0.839642\n",
      "Training Epoch: 9 [20096/50000]\tLoss: 2.6177\tLR: 0.839898\n",
      "Training Epoch: 9 [20224/50000]\tLoss: 2.6908\tLR: 0.840153\n",
      "Training Epoch: 9 [20352/50000]\tLoss: 2.7957\tLR: 0.840409\n",
      "Training Epoch: 9 [20480/50000]\tLoss: 2.6559\tLR: 0.840665\n",
      "Training Epoch: 9 [20608/50000]\tLoss: 3.0186\tLR: 0.840921\n",
      "Training Epoch: 9 [20736/50000]\tLoss: 2.9117\tLR: 0.841176\n",
      "Training Epoch: 9 [20864/50000]\tLoss: 2.7790\tLR: 0.841432\n",
      "Training Epoch: 9 [20992/50000]\tLoss: 2.3257\tLR: 0.841688\n",
      "Training Epoch: 9 [21120/50000]\tLoss: 2.8706\tLR: 0.841944\n",
      "Training Epoch: 9 [21248/50000]\tLoss: 2.5466\tLR: 0.842199\n",
      "Training Epoch: 9 [21376/50000]\tLoss: 2.9082\tLR: 0.842455\n",
      "Training Epoch: 9 [21504/50000]\tLoss: 2.9444\tLR: 0.842711\n",
      "Training Epoch: 9 [21632/50000]\tLoss: 2.8323\tLR: 0.842967\n",
      "Training Epoch: 9 [21760/50000]\tLoss: 3.0077\tLR: 0.843223\n",
      "Training Epoch: 9 [21888/50000]\tLoss: 3.1322\tLR: 0.843478\n",
      "Training Epoch: 9 [22016/50000]\tLoss: 3.0942\tLR: 0.843734\n",
      "Training Epoch: 9 [22144/50000]\tLoss: 2.8124\tLR: 0.843990\n",
      "Training Epoch: 9 [22272/50000]\tLoss: 2.7431\tLR: 0.844246\n",
      "Training Epoch: 9 [22400/50000]\tLoss: 2.7358\tLR: 0.844501\n",
      "Training Epoch: 9 [22528/50000]\tLoss: 2.8918\tLR: 0.844757\n",
      "Training Epoch: 9 [22656/50000]\tLoss: 2.8297\tLR: 0.845013\n",
      "Training Epoch: 9 [22784/50000]\tLoss: 2.9272\tLR: 0.845269\n",
      "Training Epoch: 9 [22912/50000]\tLoss: 2.9306\tLR: 0.845524\n",
      "Training Epoch: 9 [23040/50000]\tLoss: 3.0450\tLR: 0.845780\n",
      "Training Epoch: 9 [23168/50000]\tLoss: 2.9045\tLR: 0.846036\n",
      "Training Epoch: 9 [23296/50000]\tLoss: 2.7433\tLR: 0.846292\n",
      "Training Epoch: 9 [23424/50000]\tLoss: 3.0585\tLR: 0.846547\n",
      "Training Epoch: 9 [23552/50000]\tLoss: 2.9411\tLR: 0.846803\n",
      "Training Epoch: 9 [23680/50000]\tLoss: 2.7081\tLR: 0.847059\n",
      "Training Epoch: 9 [23808/50000]\tLoss: 2.9105\tLR: 0.847315\n",
      "Training Epoch: 9 [23936/50000]\tLoss: 2.8123\tLR: 0.847570\n",
      "Training Epoch: 9 [24064/50000]\tLoss: 2.7653\tLR: 0.847826\n",
      "Training Epoch: 9 [24192/50000]\tLoss: 2.8089\tLR: 0.848082\n",
      "Training Epoch: 9 [24320/50000]\tLoss: 2.8202\tLR: 0.848338\n",
      "Training Epoch: 9 [24448/50000]\tLoss: 2.9883\tLR: 0.848593\n",
      "Training Epoch: 9 [24576/50000]\tLoss: 2.6693\tLR: 0.848849\n",
      "Training Epoch: 9 [24704/50000]\tLoss: 2.5065\tLR: 0.849105\n",
      "Training Epoch: 9 [24832/50000]\tLoss: 2.8182\tLR: 0.849361\n",
      "Training Epoch: 9 [24960/50000]\tLoss: 2.4434\tLR: 0.849616\n",
      "Training Epoch: 9 [25088/50000]\tLoss: 2.4383\tLR: 0.849872\n",
      "Training Epoch: 9 [25216/50000]\tLoss: 2.9400\tLR: 0.850128\n",
      "Training Epoch: 9 [25344/50000]\tLoss: 2.9681\tLR: 0.850384\n",
      "Training Epoch: 9 [25472/50000]\tLoss: 2.8468\tLR: 0.850639\n",
      "Training Epoch: 9 [25600/50000]\tLoss: 2.8708\tLR: 0.850895\n",
      "Training Epoch: 9 [25728/50000]\tLoss: 2.6088\tLR: 0.851151\n",
      "Training Epoch: 9 [25856/50000]\tLoss: 2.4922\tLR: 0.851407\n",
      "Training Epoch: 9 [25984/50000]\tLoss: 2.9410\tLR: 0.851662\n",
      "Training Epoch: 9 [26112/50000]\tLoss: 2.9997\tLR: 0.851918\n",
      "Training Epoch: 9 [26240/50000]\tLoss: 2.7307\tLR: 0.852174\n",
      "Training Epoch: 9 [26368/50000]\tLoss: 2.6815\tLR: 0.852430\n",
      "Training Epoch: 9 [26496/50000]\tLoss: 2.8694\tLR: 0.852685\n",
      "Training Epoch: 9 [26624/50000]\tLoss: 2.8924\tLR: 0.852941\n",
      "Training Epoch: 9 [26752/50000]\tLoss: 2.7504\tLR: 0.853197\n",
      "Training Epoch: 9 [26880/50000]\tLoss: 2.8811\tLR: 0.853453\n",
      "Training Epoch: 9 [27008/50000]\tLoss: 2.9843\tLR: 0.853708\n",
      "Training Epoch: 9 [27136/50000]\tLoss: 2.6225\tLR: 0.853964\n",
      "Training Epoch: 9 [27264/50000]\tLoss: 2.6383\tLR: 0.854220\n",
      "Training Epoch: 9 [27392/50000]\tLoss: 2.7553\tLR: 0.854476\n",
      "Training Epoch: 9 [27520/50000]\tLoss: 2.7478\tLR: 0.854731\n",
      "Training Epoch: 9 [27648/50000]\tLoss: 2.9983\tLR: 0.854987\n",
      "Training Epoch: 9 [27776/50000]\tLoss: 2.8259\tLR: 0.855243\n",
      "Training Epoch: 9 [27904/50000]\tLoss: 2.5313\tLR: 0.855499\n",
      "Training Epoch: 9 [28032/50000]\tLoss: 2.6036\tLR: 0.855754\n",
      "Training Epoch: 9 [28160/50000]\tLoss: 2.7831\tLR: 0.856010\n",
      "Training Epoch: 9 [28288/50000]\tLoss: 2.6589\tLR: 0.856266\n",
      "Training Epoch: 9 [28416/50000]\tLoss: 2.9190\tLR: 0.856522\n",
      "Training Epoch: 9 [28544/50000]\tLoss: 2.6353\tLR: 0.856777\n",
      "Training Epoch: 9 [28672/50000]\tLoss: 2.8819\tLR: 0.857033\n",
      "Training Epoch: 9 [28800/50000]\tLoss: 2.9707\tLR: 0.857289\n",
      "Training Epoch: 9 [28928/50000]\tLoss: 2.8607\tLR: 0.857545\n",
      "Training Epoch: 9 [29056/50000]\tLoss: 2.8532\tLR: 0.857801\n",
      "Training Epoch: 9 [29184/50000]\tLoss: 2.9848\tLR: 0.858056\n",
      "Training Epoch: 9 [29312/50000]\tLoss: 2.8398\tLR: 0.858312\n",
      "Training Epoch: 9 [29440/50000]\tLoss: 2.9203\tLR: 0.858568\n",
      "Training Epoch: 9 [29568/50000]\tLoss: 2.6720\tLR: 0.858824\n",
      "Training Epoch: 9 [29696/50000]\tLoss: 2.6852\tLR: 0.859079\n",
      "Training Epoch: 9 [29824/50000]\tLoss: 2.7249\tLR: 0.859335\n",
      "Training Epoch: 9 [29952/50000]\tLoss: 2.7793\tLR: 0.859591\n",
      "Training Epoch: 9 [30080/50000]\tLoss: 2.8327\tLR: 0.859847\n",
      "Training Epoch: 9 [30208/50000]\tLoss: 2.6230\tLR: 0.860102\n",
      "Training Epoch: 9 [30336/50000]\tLoss: 2.7999\tLR: 0.860358\n",
      "Training Epoch: 9 [30464/50000]\tLoss: 2.9661\tLR: 0.860614\n",
      "Training Epoch: 9 [30592/50000]\tLoss: 2.7875\tLR: 0.860870\n",
      "Training Epoch: 9 [30720/50000]\tLoss: 2.5182\tLR: 0.861125\n",
      "Training Epoch: 9 [30848/50000]\tLoss: 2.4989\tLR: 0.861381\n",
      "Training Epoch: 9 [30976/50000]\tLoss: 2.8216\tLR: 0.861637\n",
      "Training Epoch: 9 [31104/50000]\tLoss: 2.7493\tLR: 0.861893\n",
      "Training Epoch: 9 [31232/50000]\tLoss: 2.6902\tLR: 0.862148\n",
      "Training Epoch: 9 [31360/50000]\tLoss: 2.5478\tLR: 0.862404\n",
      "Training Epoch: 9 [31488/50000]\tLoss: 3.0784\tLR: 0.862660\n",
      "Training Epoch: 9 [31616/50000]\tLoss: 2.5787\tLR: 0.862916\n",
      "Training Epoch: 9 [31744/50000]\tLoss: 2.6924\tLR: 0.863171\n",
      "Training Epoch: 9 [31872/50000]\tLoss: 2.9533\tLR: 0.863427\n",
      "Training Epoch: 9 [32000/50000]\tLoss: 3.0102\tLR: 0.863683\n",
      "Training Epoch: 9 [32128/50000]\tLoss: 2.8906\tLR: 0.863939\n",
      "Training Epoch: 9 [32256/50000]\tLoss: 2.8360\tLR: 0.864194\n",
      "Training Epoch: 9 [32384/50000]\tLoss: 2.8093\tLR: 0.864450\n",
      "Training Epoch: 9 [32512/50000]\tLoss: 2.6885\tLR: 0.864706\n",
      "Training Epoch: 9 [32640/50000]\tLoss: 2.6878\tLR: 0.864962\n",
      "Training Epoch: 9 [32768/50000]\tLoss: 2.8249\tLR: 0.865217\n",
      "Training Epoch: 9 [32896/50000]\tLoss: 2.8624\tLR: 0.865473\n",
      "Training Epoch: 9 [33024/50000]\tLoss: 2.9199\tLR: 0.865729\n",
      "Training Epoch: 9 [33152/50000]\tLoss: 2.7607\tLR: 0.865985\n",
      "Training Epoch: 9 [33280/50000]\tLoss: 2.9478\tLR: 0.866240\n",
      "Training Epoch: 9 [33408/50000]\tLoss: 2.8564\tLR: 0.866496\n",
      "Training Epoch: 9 [33536/50000]\tLoss: 2.9709\tLR: 0.866752\n",
      "Training Epoch: 9 [33664/50000]\tLoss: 2.8144\tLR: 0.867008\n",
      "Training Epoch: 9 [33792/50000]\tLoss: 2.7323\tLR: 0.867263\n",
      "Training Epoch: 9 [33920/50000]\tLoss: 2.9075\tLR: 0.867519\n",
      "Training Epoch: 9 [34048/50000]\tLoss: 2.9769\tLR: 0.867775\n",
      "Training Epoch: 9 [34176/50000]\tLoss: 2.6847\tLR: 0.868031\n",
      "Training Epoch: 9 [34304/50000]\tLoss: 2.8794\tLR: 0.868286\n",
      "Training Epoch: 9 [34432/50000]\tLoss: 2.8935\tLR: 0.868542\n",
      "Training Epoch: 9 [34560/50000]\tLoss: 2.7135\tLR: 0.868798\n",
      "Training Epoch: 9 [34688/50000]\tLoss: 2.9566\tLR: 0.869054\n",
      "Training Epoch: 9 [34816/50000]\tLoss: 2.5468\tLR: 0.869309\n",
      "Training Epoch: 9 [34944/50000]\tLoss: 2.5897\tLR: 0.869565\n",
      "Training Epoch: 9 [35072/50000]\tLoss: 2.8056\tLR: 0.869821\n",
      "Training Epoch: 9 [35200/50000]\tLoss: 2.8144\tLR: 0.870077\n",
      "Training Epoch: 9 [35328/50000]\tLoss: 2.4676\tLR: 0.870332\n",
      "Training Epoch: 9 [35456/50000]\tLoss: 2.6676\tLR: 0.870588\n",
      "Training Epoch: 9 [35584/50000]\tLoss: 2.8714\tLR: 0.870844\n",
      "Training Epoch: 9 [35712/50000]\tLoss: 2.7840\tLR: 0.871100\n",
      "Training Epoch: 9 [35840/50000]\tLoss: 2.7155\tLR: 0.871355\n",
      "Training Epoch: 9 [35968/50000]\tLoss: 2.9300\tLR: 0.871611\n",
      "Training Epoch: 9 [36096/50000]\tLoss: 2.8272\tLR: 0.871867\n",
      "Training Epoch: 9 [36224/50000]\tLoss: 2.6996\tLR: 0.872123\n",
      "Training Epoch: 9 [36352/50000]\tLoss: 3.1165\tLR: 0.872379\n",
      "Training Epoch: 9 [36480/50000]\tLoss: 2.7547\tLR: 0.872634\n",
      "Training Epoch: 9 [36608/50000]\tLoss: 2.6882\tLR: 0.872890\n",
      "Training Epoch: 9 [36736/50000]\tLoss: 2.5863\tLR: 0.873146\n",
      "Training Epoch: 9 [36864/50000]\tLoss: 3.1850\tLR: 0.873402\n",
      "Training Epoch: 9 [36992/50000]\tLoss: 2.8666\tLR: 0.873657\n",
      "Training Epoch: 9 [37120/50000]\tLoss: 2.8233\tLR: 0.873913\n",
      "Training Epoch: 9 [37248/50000]\tLoss: 2.7543\tLR: 0.874169\n",
      "Training Epoch: 9 [37376/50000]\tLoss: 2.8149\tLR: 0.874425\n",
      "Training Epoch: 9 [37504/50000]\tLoss: 2.8262\tLR: 0.874680\n",
      "Training Epoch: 9 [37632/50000]\tLoss: 2.8126\tLR: 0.874936\n",
      "Training Epoch: 9 [37760/50000]\tLoss: 2.7352\tLR: 0.875192\n",
      "Training Epoch: 9 [37888/50000]\tLoss: 2.9014\tLR: 0.875448\n",
      "Training Epoch: 9 [38016/50000]\tLoss: 2.7507\tLR: 0.875703\n",
      "Training Epoch: 9 [38144/50000]\tLoss: 2.8745\tLR: 0.875959\n",
      "Training Epoch: 9 [38272/50000]\tLoss: 3.1721\tLR: 0.876215\n",
      "Training Epoch: 9 [38400/50000]\tLoss: 2.8466\tLR: 0.876471\n",
      "Training Epoch: 9 [38528/50000]\tLoss: 2.6301\tLR: 0.876726\n",
      "Training Epoch: 9 [38656/50000]\tLoss: 2.7825\tLR: 0.876982\n",
      "Training Epoch: 9 [38784/50000]\tLoss: 2.7102\tLR: 0.877238\n",
      "Training Epoch: 9 [38912/50000]\tLoss: 2.6304\tLR: 0.877494\n",
      "Training Epoch: 9 [39040/50000]\tLoss: 2.7751\tLR: 0.877749\n",
      "Training Epoch: 9 [39168/50000]\tLoss: 2.9125\tLR: 0.878005\n",
      "Training Epoch: 9 [39296/50000]\tLoss: 2.7481\tLR: 0.878261\n",
      "Training Epoch: 9 [39424/50000]\tLoss: 2.4427\tLR: 0.878517\n",
      "Training Epoch: 9 [39552/50000]\tLoss: 2.7412\tLR: 0.878772\n",
      "Training Epoch: 9 [39680/50000]\tLoss: 2.9006\tLR: 0.879028\n",
      "Training Epoch: 9 [39808/50000]\tLoss: 3.0085\tLR: 0.879284\n",
      "Training Epoch: 9 [39936/50000]\tLoss: 2.8263\tLR: 0.879540\n",
      "Training Epoch: 9 [40064/50000]\tLoss: 2.7083\tLR: 0.879795\n",
      "Training Epoch: 9 [40192/50000]\tLoss: 2.9029\tLR: 0.880051\n",
      "Training Epoch: 9 [40320/50000]\tLoss: 2.9488\tLR: 0.880307\n",
      "Training Epoch: 9 [40448/50000]\tLoss: 2.7885\tLR: 0.880563\n",
      "Training Epoch: 9 [40576/50000]\tLoss: 2.6614\tLR: 0.880818\n",
      "Training Epoch: 9 [40704/50000]\tLoss: 3.0610\tLR: 0.881074\n",
      "Training Epoch: 9 [40832/50000]\tLoss: 2.8617\tLR: 0.881330\n",
      "Training Epoch: 9 [40960/50000]\tLoss: 2.7307\tLR: 0.881586\n",
      "Training Epoch: 9 [41088/50000]\tLoss: 3.0041\tLR: 0.881841\n",
      "Training Epoch: 9 [41216/50000]\tLoss: 3.0789\tLR: 0.882097\n",
      "Training Epoch: 9 [41344/50000]\tLoss: 2.7954\tLR: 0.882353\n",
      "Training Epoch: 9 [41472/50000]\tLoss: 2.9633\tLR: 0.882609\n",
      "Training Epoch: 9 [41600/50000]\tLoss: 2.7298\tLR: 0.882864\n",
      "Training Epoch: 9 [41728/50000]\tLoss: 2.7644\tLR: 0.883120\n",
      "Training Epoch: 9 [41856/50000]\tLoss: 2.9700\tLR: 0.883376\n",
      "Training Epoch: 9 [41984/50000]\tLoss: 2.7250\tLR: 0.883632\n",
      "Training Epoch: 9 [42112/50000]\tLoss: 2.9890\tLR: 0.883887\n",
      "Training Epoch: 9 [42240/50000]\tLoss: 3.0663\tLR: 0.884143\n",
      "Training Epoch: 9 [42368/50000]\tLoss: 2.9570\tLR: 0.884399\n",
      "Training Epoch: 9 [42496/50000]\tLoss: 2.6565\tLR: 0.884655\n",
      "Training Epoch: 9 [42624/50000]\tLoss: 2.7907\tLR: 0.884910\n",
      "Training Epoch: 9 [42752/50000]\tLoss: 2.7877\tLR: 0.885166\n",
      "Training Epoch: 9 [42880/50000]\tLoss: 2.6517\tLR: 0.885422\n",
      "Training Epoch: 9 [43008/50000]\tLoss: 2.4927\tLR: 0.885678\n",
      "Training Epoch: 9 [43136/50000]\tLoss: 2.7166\tLR: 0.885934\n",
      "Training Epoch: 9 [43264/50000]\tLoss: 2.7137\tLR: 0.886189\n",
      "Training Epoch: 9 [43392/50000]\tLoss: 2.9603\tLR: 0.886445\n",
      "Training Epoch: 9 [43520/50000]\tLoss: 2.7594\tLR: 0.886701\n",
      "Training Epoch: 9 [43648/50000]\tLoss: 2.8574\tLR: 0.886957\n",
      "Training Epoch: 9 [43776/50000]\tLoss: 2.9171\tLR: 0.887212\n",
      "Training Epoch: 9 [43904/50000]\tLoss: 2.8348\tLR: 0.887468\n",
      "Training Epoch: 9 [44032/50000]\tLoss: 2.9964\tLR: 0.887724\n",
      "Training Epoch: 9 [44160/50000]\tLoss: 2.8118\tLR: 0.887980\n",
      "Training Epoch: 9 [44288/50000]\tLoss: 2.8407\tLR: 0.888235\n",
      "Training Epoch: 9 [44416/50000]\tLoss: 2.8106\tLR: 0.888491\n",
      "Training Epoch: 9 [44544/50000]\tLoss: 2.8148\tLR: 0.888747\n",
      "Training Epoch: 9 [44672/50000]\tLoss: 2.7470\tLR: 0.889003\n",
      "Training Epoch: 9 [44800/50000]\tLoss: 2.7420\tLR: 0.889258\n",
      "Training Epoch: 9 [44928/50000]\tLoss: 3.0080\tLR: 0.889514\n",
      "Training Epoch: 9 [45056/50000]\tLoss: 2.6669\tLR: 0.889770\n",
      "Training Epoch: 9 [45184/50000]\tLoss: 2.6593\tLR: 0.890026\n",
      "Training Epoch: 9 [45312/50000]\tLoss: 2.8197\tLR: 0.890281\n",
      "Training Epoch: 9 [45440/50000]\tLoss: 3.1620\tLR: 0.890537\n",
      "Training Epoch: 9 [45568/50000]\tLoss: 2.9017\tLR: 0.890793\n",
      "Training Epoch: 9 [45696/50000]\tLoss: 2.8211\tLR: 0.891049\n",
      "Training Epoch: 9 [45824/50000]\tLoss: 2.8103\tLR: 0.891304\n",
      "Training Epoch: 9 [45952/50000]\tLoss: 2.8064\tLR: 0.891560\n",
      "Training Epoch: 9 [46080/50000]\tLoss: 2.5562\tLR: 0.891816\n",
      "Training Epoch: 9 [46208/50000]\tLoss: 2.7470\tLR: 0.892072\n",
      "Training Epoch: 9 [46336/50000]\tLoss: 2.4061\tLR: 0.892327\n",
      "Training Epoch: 9 [46464/50000]\tLoss: 2.7558\tLR: 0.892583\n",
      "Training Epoch: 9 [46592/50000]\tLoss: 3.0177\tLR: 0.892839\n",
      "Training Epoch: 9 [46720/50000]\tLoss: 2.7192\tLR: 0.893095\n",
      "Training Epoch: 9 [46848/50000]\tLoss: 2.8480\tLR: 0.893350\n",
      "Training Epoch: 9 [46976/50000]\tLoss: 2.8848\tLR: 0.893606\n",
      "Training Epoch: 9 [47104/50000]\tLoss: 3.0891\tLR: 0.893862\n",
      "Training Epoch: 9 [47232/50000]\tLoss: 2.7899\tLR: 0.894118\n",
      "Training Epoch: 9 [47360/50000]\tLoss: 2.7757\tLR: 0.894373\n",
      "Training Epoch: 9 [47488/50000]\tLoss: 2.7859\tLR: 0.894629\n",
      "Training Epoch: 9 [47616/50000]\tLoss: 2.7725\tLR: 0.894885\n",
      "Training Epoch: 9 [47744/50000]\tLoss: 2.9065\tLR: 0.895141\n",
      "Training Epoch: 9 [47872/50000]\tLoss: 2.9473\tLR: 0.895396\n",
      "Training Epoch: 9 [48000/50000]\tLoss: 2.7169\tLR: 0.895652\n",
      "Training Epoch: 9 [48128/50000]\tLoss: 2.9263\tLR: 0.895908\n",
      "Training Epoch: 9 [48256/50000]\tLoss: 2.9843\tLR: 0.896164\n",
      "Training Epoch: 9 [48384/50000]\tLoss: 2.6247\tLR: 0.896419\n",
      "Training Epoch: 9 [48512/50000]\tLoss: 2.9241\tLR: 0.896675\n",
      "Training Epoch: 9 [48640/50000]\tLoss: 3.0434\tLR: 0.896931\n",
      "Training Epoch: 9 [48768/50000]\tLoss: 2.6269\tLR: 0.897187\n",
      "Training Epoch: 9 [48896/50000]\tLoss: 2.9395\tLR: 0.897442\n",
      "Training Epoch: 9 [49024/50000]\tLoss: 2.8932\tLR: 0.897698\n",
      "Training Epoch: 9 [49152/50000]\tLoss: 2.9241\tLR: 0.897954\n",
      "Training Epoch: 9 [49280/50000]\tLoss: 2.9215\tLR: 0.898210\n",
      "Training Epoch: 9 [49408/50000]\tLoss: 2.8795\tLR: 0.898465\n",
      "Training Epoch: 9 [49536/50000]\tLoss: 3.0687\tLR: 0.898721\n",
      "Training Epoch: 9 [49664/50000]\tLoss: 2.6994\tLR: 0.898977\n",
      "Training Epoch: 9 [49792/50000]\tLoss: 3.0394\tLR: 0.899233\n",
      "Training Epoch: 9 [49920/50000]\tLoss: 2.9806\tLR: 0.899488\n",
      "Training Epoch: 9 [50000/50000]\tLoss: 2.5124\tLR: 0.899744\n",
      "epoch 9 training time consumed: 492.03s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   12618 GB |   12618 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   12580 GB |   12580 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      38 GB |      38 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   12618 GB |   12618 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   12580 GB |   12580 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      38 GB |      38 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   12438 GB |   12438 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   12399 GB |   12399 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |      38 GB |      38 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    1338 K  |    1338 K  |\n",
      "|       from large pool |      24    |      65    |     570 K  |     570 K  |\n",
      "|       from small pool |     231    |     274    |     768 K  |     767 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    1338 K  |    1338 K  |\n",
      "|       from large pool |      24    |      65    |     570 K  |     570 K  |\n",
      "|       from small pool |     231    |     274    |     768 K  |     767 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      45    |     775 K  |     775 K  |\n",
      "|       from large pool |      10    |      23    |     274 K  |     274 K  |\n",
      "|       from small pool |      25    |      35    |     501 K  |     500 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 9, Average loss: 0.0336, Accuracy: 0.1651, Time consumed:35.39s\n",
      "\n",
      "Training Epoch: 10 [128/50000]\tLoss: 2.4661\tLR: 0.100000\n",
      "Training Epoch: 10 [256/50000]\tLoss: 2.8653\tLR: 0.900256\n",
      "Training Epoch: 10 [384/50000]\tLoss: 3.0906\tLR: 0.900512\n",
      "Training Epoch: 10 [512/50000]\tLoss: 2.8154\tLR: 0.900767\n",
      "Training Epoch: 10 [640/50000]\tLoss: 2.8468\tLR: 0.901023\n",
      "Training Epoch: 10 [768/50000]\tLoss: 2.9973\tLR: 0.901279\n",
      "Training Epoch: 10 [896/50000]\tLoss: 2.9218\tLR: 0.901535\n",
      "Training Epoch: 10 [1024/50000]\tLoss: 2.9531\tLR: 0.901790\n",
      "Training Epoch: 10 [1152/50000]\tLoss: 2.8655\tLR: 0.902046\n",
      "Training Epoch: 10 [1280/50000]\tLoss: 2.8658\tLR: 0.902302\n",
      "Training Epoch: 10 [1408/50000]\tLoss: 2.7876\tLR: 0.902558\n",
      "Training Epoch: 10 [1536/50000]\tLoss: 2.8674\tLR: 0.902813\n",
      "Training Epoch: 10 [1664/50000]\tLoss: 2.6326\tLR: 0.903069\n",
      "Training Epoch: 10 [1792/50000]\tLoss: 2.9568\tLR: 0.903325\n",
      "Training Epoch: 10 [1920/50000]\tLoss: 3.0717\tLR: 0.903581\n",
      "Training Epoch: 10 [2048/50000]\tLoss: 2.9866\tLR: 0.903836\n",
      "Training Epoch: 10 [2176/50000]\tLoss: 3.0529\tLR: 0.904092\n",
      "Training Epoch: 10 [2304/50000]\tLoss: 2.7413\tLR: 0.904348\n",
      "Training Epoch: 10 [2432/50000]\tLoss: 2.5845\tLR: 0.904604\n",
      "Training Epoch: 10 [2560/50000]\tLoss: 3.0447\tLR: 0.904859\n",
      "Training Epoch: 10 [2688/50000]\tLoss: 2.7730\tLR: 0.905115\n",
      "Training Epoch: 10 [2816/50000]\tLoss: 2.5785\tLR: 0.905371\n",
      "Training Epoch: 10 [2944/50000]\tLoss: 2.7188\tLR: 0.905627\n",
      "Training Epoch: 10 [3072/50000]\tLoss: 2.8487\tLR: 0.905882\n",
      "Training Epoch: 10 [3200/50000]\tLoss: 2.6249\tLR: 0.906138\n",
      "Training Epoch: 10 [3328/50000]\tLoss: 2.5841\tLR: 0.906394\n",
      "Training Epoch: 10 [3456/50000]\tLoss: 2.4460\tLR: 0.906650\n",
      "Training Epoch: 10 [3584/50000]\tLoss: 2.6477\tLR: 0.906905\n",
      "Training Epoch: 10 [3712/50000]\tLoss: 3.0012\tLR: 0.907161\n",
      "Training Epoch: 10 [3840/50000]\tLoss: 2.4106\tLR: 0.907417\n",
      "Training Epoch: 10 [3968/50000]\tLoss: 2.9098\tLR: 0.907673\n",
      "Training Epoch: 10 [4096/50000]\tLoss: 2.7601\tLR: 0.907928\n",
      "Training Epoch: 10 [4224/50000]\tLoss: 2.6100\tLR: 0.908184\n",
      "Training Epoch: 10 [4352/50000]\tLoss: 2.8161\tLR: 0.908440\n",
      "Training Epoch: 10 [4480/50000]\tLoss: 2.8199\tLR: 0.908696\n",
      "Training Epoch: 10 [4608/50000]\tLoss: 2.9117\tLR: 0.908951\n",
      "Training Epoch: 10 [4736/50000]\tLoss: 2.9420\tLR: 0.909207\n",
      "Training Epoch: 10 [4864/50000]\tLoss: 3.0258\tLR: 0.909463\n",
      "Training Epoch: 10 [4992/50000]\tLoss: 3.0022\tLR: 0.909719\n",
      "Training Epoch: 10 [5120/50000]\tLoss: 2.9106\tLR: 0.909974\n",
      "Training Epoch: 10 [5248/50000]\tLoss: 2.8098\tLR: 0.910230\n",
      "Training Epoch: 10 [5376/50000]\tLoss: 2.7408\tLR: 0.910486\n",
      "Training Epoch: 10 [5504/50000]\tLoss: 3.1700\tLR: 0.910742\n",
      "Training Epoch: 10 [5632/50000]\tLoss: 3.0118\tLR: 0.910997\n",
      "Training Epoch: 10 [5760/50000]\tLoss: 2.7419\tLR: 0.911253\n",
      "Training Epoch: 10 [5888/50000]\tLoss: 2.9590\tLR: 0.911509\n",
      "Training Epoch: 10 [6016/50000]\tLoss: 2.9969\tLR: 0.911765\n",
      "Training Epoch: 10 [6144/50000]\tLoss: 2.8802\tLR: 0.912020\n",
      "Training Epoch: 10 [6272/50000]\tLoss: 2.8723\tLR: 0.912276\n",
      "Training Epoch: 10 [6400/50000]\tLoss: 2.7888\tLR: 0.912532\n",
      "Training Epoch: 10 [6528/50000]\tLoss: 2.7676\tLR: 0.912788\n",
      "Training Epoch: 10 [6656/50000]\tLoss: 3.0085\tLR: 0.913043\n",
      "Training Epoch: 10 [6784/50000]\tLoss: 2.7422\tLR: 0.913299\n",
      "Training Epoch: 10 [6912/50000]\tLoss: 2.9864\tLR: 0.913555\n",
      "Training Epoch: 10 [7040/50000]\tLoss: 2.6710\tLR: 0.913811\n",
      "Training Epoch: 10 [7168/50000]\tLoss: 2.7279\tLR: 0.914066\n",
      "Training Epoch: 10 [7296/50000]\tLoss: 3.0171\tLR: 0.914322\n",
      "Training Epoch: 10 [7424/50000]\tLoss: 3.0045\tLR: 0.914578\n",
      "Training Epoch: 10 [7552/50000]\tLoss: 2.6300\tLR: 0.914834\n",
      "Training Epoch: 10 [7680/50000]\tLoss: 2.6871\tLR: 0.915090\n",
      "Training Epoch: 10 [7808/50000]\tLoss: 2.9830\tLR: 0.915345\n",
      "Training Epoch: 10 [7936/50000]\tLoss: 2.8013\tLR: 0.915601\n",
      "Training Epoch: 10 [8064/50000]\tLoss: 2.7617\tLR: 0.915857\n",
      "Training Epoch: 10 [8192/50000]\tLoss: 3.0767\tLR: 0.916113\n",
      "Training Epoch: 10 [8320/50000]\tLoss: 2.8548\tLR: 0.916368\n",
      "Training Epoch: 10 [8448/50000]\tLoss: 2.5385\tLR: 0.916624\n",
      "Training Epoch: 10 [8576/50000]\tLoss: 2.6073\tLR: 0.916880\n",
      "Training Epoch: 10 [8704/50000]\tLoss: 2.9076\tLR: 0.917136\n",
      "Training Epoch: 10 [8832/50000]\tLoss: 2.6075\tLR: 0.917391\n",
      "Training Epoch: 10 [8960/50000]\tLoss: 2.9462\tLR: 0.917647\n",
      "Training Epoch: 10 [9088/50000]\tLoss: 2.8328\tLR: 0.917903\n",
      "Training Epoch: 10 [9216/50000]\tLoss: 2.8470\tLR: 0.918159\n",
      "Training Epoch: 10 [9344/50000]\tLoss: 2.7753\tLR: 0.918414\n",
      "Training Epoch: 10 [9472/50000]\tLoss: 2.8862\tLR: 0.918670\n",
      "Training Epoch: 10 [9600/50000]\tLoss: 2.9092\tLR: 0.918926\n",
      "Training Epoch: 10 [9728/50000]\tLoss: 2.5335\tLR: 0.919182\n",
      "Training Epoch: 10 [9856/50000]\tLoss: 2.5843\tLR: 0.919437\n",
      "Training Epoch: 10 [9984/50000]\tLoss: 2.8428\tLR: 0.919693\n",
      "Training Epoch: 10 [10112/50000]\tLoss: 2.9971\tLR: 0.919949\n",
      "Training Epoch: 10 [10240/50000]\tLoss: 2.7851\tLR: 0.920205\n",
      "Training Epoch: 10 [10368/50000]\tLoss: 2.5630\tLR: 0.920460\n",
      "Training Epoch: 10 [10496/50000]\tLoss: 2.6667\tLR: 0.920716\n",
      "Training Epoch: 10 [10624/50000]\tLoss: 2.6671\tLR: 0.920972\n",
      "Training Epoch: 10 [10752/50000]\tLoss: 2.7556\tLR: 0.921228\n",
      "Training Epoch: 10 [10880/50000]\tLoss: 2.7281\tLR: 0.921483\n",
      "Training Epoch: 10 [11008/50000]\tLoss: 3.0086\tLR: 0.921739\n",
      "Training Epoch: 10 [11136/50000]\tLoss: 2.7483\tLR: 0.921995\n",
      "Training Epoch: 10 [11264/50000]\tLoss: 2.5713\tLR: 0.922251\n",
      "Training Epoch: 10 [11392/50000]\tLoss: 2.9226\tLR: 0.922506\n",
      "Training Epoch: 10 [11520/50000]\tLoss: 3.0194\tLR: 0.922762\n",
      "Training Epoch: 10 [11648/50000]\tLoss: 2.6153\tLR: 0.923018\n",
      "Training Epoch: 10 [11776/50000]\tLoss: 2.8214\tLR: 0.923274\n",
      "Training Epoch: 10 [11904/50000]\tLoss: 2.7251\tLR: 0.923529\n",
      "Training Epoch: 10 [12032/50000]\tLoss: 3.1353\tLR: 0.923785\n",
      "Training Epoch: 10 [12160/50000]\tLoss: 2.5306\tLR: 0.924041\n",
      "Training Epoch: 10 [12288/50000]\tLoss: 2.8820\tLR: 0.924297\n",
      "Training Epoch: 10 [12416/50000]\tLoss: 2.8822\tLR: 0.924552\n",
      "Training Epoch: 10 [12544/50000]\tLoss: 3.0325\tLR: 0.924808\n",
      "Training Epoch: 10 [12672/50000]\tLoss: 3.0560\tLR: 0.925064\n",
      "Training Epoch: 10 [12800/50000]\tLoss: 2.7122\tLR: 0.925320\n",
      "Training Epoch: 10 [12928/50000]\tLoss: 2.8762\tLR: 0.925575\n",
      "Training Epoch: 10 [13056/50000]\tLoss: 3.0700\tLR: 0.925831\n",
      "Training Epoch: 10 [13184/50000]\tLoss: 2.8804\tLR: 0.926087\n",
      "Training Epoch: 10 [13312/50000]\tLoss: 2.7917\tLR: 0.926343\n",
      "Training Epoch: 10 [13440/50000]\tLoss: 2.7925\tLR: 0.926598\n",
      "Training Epoch: 10 [13568/50000]\tLoss: 3.0668\tLR: 0.926854\n",
      "Training Epoch: 10 [13696/50000]\tLoss: 3.0769\tLR: 0.927110\n",
      "Training Epoch: 10 [13824/50000]\tLoss: 3.0892\tLR: 0.927366\n",
      "Training Epoch: 10 [13952/50000]\tLoss: 2.8086\tLR: 0.927621\n",
      "Training Epoch: 10 [14080/50000]\tLoss: 2.9212\tLR: 0.927877\n",
      "Training Epoch: 10 [14208/50000]\tLoss: 3.0752\tLR: 0.928133\n",
      "Training Epoch: 10 [14336/50000]\tLoss: 2.8896\tLR: 0.928389\n",
      "Training Epoch: 10 [14464/50000]\tLoss: 2.5735\tLR: 0.928645\n",
      "Training Epoch: 10 [14592/50000]\tLoss: 3.0406\tLR: 0.928900\n",
      "Training Epoch: 10 [14720/50000]\tLoss: 2.8911\tLR: 0.929156\n",
      "Training Epoch: 10 [14848/50000]\tLoss: 2.8303\tLR: 0.929412\n",
      "Training Epoch: 10 [14976/50000]\tLoss: 2.7800\tLR: 0.929668\n",
      "Training Epoch: 10 [15104/50000]\tLoss: 2.7543\tLR: 0.929923\n",
      "Training Epoch: 10 [15232/50000]\tLoss: 2.5844\tLR: 0.930179\n",
      "Training Epoch: 10 [15360/50000]\tLoss: 2.8768\tLR: 0.930435\n",
      "Training Epoch: 10 [15488/50000]\tLoss: 2.7080\tLR: 0.930691\n",
      "Training Epoch: 10 [15616/50000]\tLoss: 3.2025\tLR: 0.930946\n",
      "Training Epoch: 10 [15744/50000]\tLoss: 2.7396\tLR: 0.931202\n",
      "Training Epoch: 10 [15872/50000]\tLoss: 2.8807\tLR: 0.931458\n",
      "Training Epoch: 10 [16000/50000]\tLoss: 2.7405\tLR: 0.931714\n",
      "Training Epoch: 10 [16128/50000]\tLoss: 2.8427\tLR: 0.931969\n",
      "Training Epoch: 10 [16256/50000]\tLoss: 2.8632\tLR: 0.932225\n",
      "Training Epoch: 10 [16384/50000]\tLoss: 2.6589\tLR: 0.932481\n",
      "Training Epoch: 10 [16512/50000]\tLoss: 2.7424\tLR: 0.932737\n",
      "Training Epoch: 10 [16640/50000]\tLoss: 2.8566\tLR: 0.932992\n",
      "Training Epoch: 10 [16768/50000]\tLoss: 2.8057\tLR: 0.933248\n",
      "Training Epoch: 10 [16896/50000]\tLoss: 2.8016\tLR: 0.933504\n",
      "Training Epoch: 10 [17024/50000]\tLoss: 2.7521\tLR: 0.933760\n",
      "Training Epoch: 10 [17152/50000]\tLoss: 2.8817\tLR: 0.934015\n",
      "Training Epoch: 10 [17280/50000]\tLoss: 2.9099\tLR: 0.934271\n",
      "Training Epoch: 10 [17408/50000]\tLoss: 2.9473\tLR: 0.934527\n",
      "Training Epoch: 10 [17536/50000]\tLoss: 2.7897\tLR: 0.934783\n",
      "Training Epoch: 10 [17664/50000]\tLoss: 2.7252\tLR: 0.935038\n",
      "Training Epoch: 10 [17792/50000]\tLoss: 3.1300\tLR: 0.935294\n",
      "Training Epoch: 10 [17920/50000]\tLoss: 3.0250\tLR: 0.935550\n",
      "Training Epoch: 10 [18048/50000]\tLoss: 2.7006\tLR: 0.935806\n",
      "Training Epoch: 10 [18176/50000]\tLoss: 2.7219\tLR: 0.936061\n",
      "Training Epoch: 10 [18304/50000]\tLoss: 2.9863\tLR: 0.936317\n",
      "Training Epoch: 10 [18432/50000]\tLoss: 2.8284\tLR: 0.936573\n",
      "Training Epoch: 10 [18560/50000]\tLoss: 2.8524\tLR: 0.936829\n",
      "Training Epoch: 10 [18688/50000]\tLoss: 2.8217\tLR: 0.937084\n",
      "Training Epoch: 10 [18816/50000]\tLoss: 3.0223\tLR: 0.937340\n",
      "Training Epoch: 10 [18944/50000]\tLoss: 2.4968\tLR: 0.937596\n",
      "Training Epoch: 10 [19072/50000]\tLoss: 2.7455\tLR: 0.937852\n",
      "Training Epoch: 10 [19200/50000]\tLoss: 2.7268\tLR: 0.938107\n",
      "Training Epoch: 10 [19328/50000]\tLoss: 2.8455\tLR: 0.938363\n",
      "Training Epoch: 10 [19456/50000]\tLoss: 3.0534\tLR: 0.938619\n",
      "Training Epoch: 10 [19584/50000]\tLoss: 2.9540\tLR: 0.938875\n",
      "Training Epoch: 10 [19712/50000]\tLoss: 2.9560\tLR: 0.939130\n",
      "Training Epoch: 10 [19840/50000]\tLoss: 2.6893\tLR: 0.939386\n",
      "Training Epoch: 10 [19968/50000]\tLoss: 2.8722\tLR: 0.939642\n",
      "Training Epoch: 10 [20096/50000]\tLoss: 2.7235\tLR: 0.939898\n",
      "Training Epoch: 10 [20224/50000]\tLoss: 2.9061\tLR: 0.940153\n",
      "Training Epoch: 10 [20352/50000]\tLoss: 2.8870\tLR: 0.940409\n",
      "Training Epoch: 10 [20480/50000]\tLoss: 3.1604\tLR: 0.940665\n",
      "Training Epoch: 10 [20608/50000]\tLoss: 2.8222\tLR: 0.940921\n",
      "Training Epoch: 10 [20736/50000]\tLoss: 2.8721\tLR: 0.941176\n",
      "Training Epoch: 10 [20864/50000]\tLoss: 2.9312\tLR: 0.941432\n",
      "Training Epoch: 10 [20992/50000]\tLoss: 2.9874\tLR: 0.941688\n",
      "Training Epoch: 10 [21120/50000]\tLoss: 2.9502\tLR: 0.941944\n",
      "Training Epoch: 10 [21248/50000]\tLoss: 2.9168\tLR: 0.942199\n",
      "Training Epoch: 10 [21376/50000]\tLoss: 2.8542\tLR: 0.942455\n",
      "Training Epoch: 10 [21504/50000]\tLoss: 2.7807\tLR: 0.942711\n",
      "Training Epoch: 10 [21632/50000]\tLoss: 2.8120\tLR: 0.942967\n",
      "Training Epoch: 10 [21760/50000]\tLoss: 2.6594\tLR: 0.943223\n",
      "Training Epoch: 10 [21888/50000]\tLoss: 2.9487\tLR: 0.943478\n",
      "Training Epoch: 10 [22016/50000]\tLoss: 3.1728\tLR: 0.943734\n",
      "Training Epoch: 10 [22144/50000]\tLoss: 2.6111\tLR: 0.943990\n",
      "Training Epoch: 10 [22272/50000]\tLoss: 2.9389\tLR: 0.944246\n",
      "Training Epoch: 10 [22400/50000]\tLoss: 2.8474\tLR: 0.944501\n",
      "Training Epoch: 10 [22528/50000]\tLoss: 2.9001\tLR: 0.944757\n",
      "Training Epoch: 10 [22656/50000]\tLoss: 2.8152\tLR: 0.945013\n",
      "Training Epoch: 10 [22784/50000]\tLoss: 3.0774\tLR: 0.945269\n",
      "Training Epoch: 10 [22912/50000]\tLoss: 2.6272\tLR: 0.945524\n",
      "Training Epoch: 10 [23040/50000]\tLoss: 3.0275\tLR: 0.945780\n",
      "Training Epoch: 10 [23168/50000]\tLoss: 2.6867\tLR: 0.946036\n",
      "Training Epoch: 10 [23296/50000]\tLoss: 2.8466\tLR: 0.946292\n",
      "Training Epoch: 10 [23424/50000]\tLoss: 2.7665\tLR: 0.946547\n",
      "Training Epoch: 10 [23552/50000]\tLoss: 3.3214\tLR: 0.946803\n",
      "Training Epoch: 10 [23680/50000]\tLoss: 2.9019\tLR: 0.947059\n",
      "Training Epoch: 10 [23808/50000]\tLoss: 2.8033\tLR: 0.947315\n",
      "Training Epoch: 10 [23936/50000]\tLoss: 3.0617\tLR: 0.947570\n",
      "Training Epoch: 10 [24064/50000]\tLoss: 2.7169\tLR: 0.947826\n",
      "Training Epoch: 10 [24192/50000]\tLoss: 2.7627\tLR: 0.948082\n",
      "Training Epoch: 10 [24320/50000]\tLoss: 2.9696\tLR: 0.948338\n",
      "Training Epoch: 10 [24448/50000]\tLoss: 2.7190\tLR: 0.948593\n",
      "Training Epoch: 10 [24576/50000]\tLoss: 3.1577\tLR: 0.948849\n",
      "Training Epoch: 10 [24704/50000]\tLoss: 2.7854\tLR: 0.949105\n",
      "Training Epoch: 10 [24832/50000]\tLoss: 2.6025\tLR: 0.949361\n",
      "Training Epoch: 10 [24960/50000]\tLoss: 2.9249\tLR: 0.949616\n",
      "Training Epoch: 10 [25088/50000]\tLoss: 2.7724\tLR: 0.949872\n",
      "Training Epoch: 10 [25216/50000]\tLoss: 3.0320\tLR: 0.950128\n",
      "Training Epoch: 10 [25344/50000]\tLoss: 2.8422\tLR: 0.950384\n",
      "Training Epoch: 10 [25472/50000]\tLoss: 2.8883\tLR: 0.950639\n",
      "Training Epoch: 10 [25600/50000]\tLoss: 2.8340\tLR: 0.950895\n",
      "Training Epoch: 10 [25728/50000]\tLoss: 2.7429\tLR: 0.951151\n",
      "Training Epoch: 10 [25856/50000]\tLoss: 2.7573\tLR: 0.951407\n",
      "Training Epoch: 10 [25984/50000]\tLoss: 2.8925\tLR: 0.951662\n",
      "Training Epoch: 10 [26112/50000]\tLoss: 3.0214\tLR: 0.951918\n",
      "Training Epoch: 10 [26240/50000]\tLoss: 2.6445\tLR: 0.952174\n",
      "Training Epoch: 10 [26368/50000]\tLoss: 2.6674\tLR: 0.952430\n",
      "Training Epoch: 10 [26496/50000]\tLoss: 2.8134\tLR: 0.952685\n",
      "Training Epoch: 10 [26624/50000]\tLoss: 3.1010\tLR: 0.952941\n",
      "Training Epoch: 10 [26752/50000]\tLoss: 3.1256\tLR: 0.953197\n",
      "Training Epoch: 10 [26880/50000]\tLoss: 2.9209\tLR: 0.953453\n",
      "Training Epoch: 10 [27008/50000]\tLoss: 2.6251\tLR: 0.953708\n",
      "Training Epoch: 10 [27136/50000]\tLoss: 2.6829\tLR: 0.953964\n",
      "Training Epoch: 10 [27264/50000]\tLoss: 2.7531\tLR: 0.954220\n",
      "Training Epoch: 10 [27392/50000]\tLoss: 2.9617\tLR: 0.954476\n",
      "Training Epoch: 10 [27520/50000]\tLoss: 2.9475\tLR: 0.954731\n",
      "Training Epoch: 10 [27648/50000]\tLoss: 2.7040\tLR: 0.954987\n",
      "Training Epoch: 10 [27776/50000]\tLoss: 2.8184\tLR: 0.955243\n",
      "Training Epoch: 10 [27904/50000]\tLoss: 3.1161\tLR: 0.955499\n",
      "Training Epoch: 10 [28032/50000]\tLoss: 2.8989\tLR: 0.955754\n",
      "Training Epoch: 10 [28160/50000]\tLoss: 3.0492\tLR: 0.956010\n",
      "Training Epoch: 10 [28288/50000]\tLoss: 2.7666\tLR: 0.956266\n",
      "Training Epoch: 10 [28416/50000]\tLoss: 2.9537\tLR: 0.956522\n",
      "Training Epoch: 10 [28544/50000]\tLoss: 3.1284\tLR: 0.956777\n",
      "Training Epoch: 10 [28672/50000]\tLoss: 2.7181\tLR: 0.957033\n",
      "Training Epoch: 10 [28800/50000]\tLoss: 3.2286\tLR: 0.957289\n",
      "Training Epoch: 10 [28928/50000]\tLoss: 2.9180\tLR: 0.957545\n",
      "Training Epoch: 10 [29056/50000]\tLoss: 3.0071\tLR: 0.957801\n",
      "Training Epoch: 10 [29184/50000]\tLoss: 2.9541\tLR: 0.958056\n",
      "Training Epoch: 10 [29312/50000]\tLoss: 2.9805\tLR: 0.958312\n",
      "Training Epoch: 10 [29440/50000]\tLoss: 2.9357\tLR: 0.958568\n",
      "Training Epoch: 10 [29568/50000]\tLoss: 3.1264\tLR: 0.958824\n",
      "Training Epoch: 10 [29696/50000]\tLoss: 2.9413\tLR: 0.959079\n",
      "Training Epoch: 10 [29824/50000]\tLoss: 3.1593\tLR: 0.959335\n",
      "Training Epoch: 10 [29952/50000]\tLoss: 2.9488\tLR: 0.959591\n",
      "Training Epoch: 10 [30080/50000]\tLoss: 2.7926\tLR: 0.959847\n",
      "Training Epoch: 10 [30208/50000]\tLoss: 2.9234\tLR: 0.960102\n",
      "Training Epoch: 10 [30336/50000]\tLoss: 2.7876\tLR: 0.960358\n",
      "Training Epoch: 10 [30464/50000]\tLoss: 2.8661\tLR: 0.960614\n",
      "Training Epoch: 10 [30592/50000]\tLoss: 2.6564\tLR: 0.960870\n",
      "Training Epoch: 10 [30720/50000]\tLoss: 2.9413\tLR: 0.961125\n",
      "Training Epoch: 10 [30848/50000]\tLoss: 3.1106\tLR: 0.961381\n",
      "Training Epoch: 10 [30976/50000]\tLoss: 2.9995\tLR: 0.961637\n",
      "Training Epoch: 10 [31104/50000]\tLoss: 3.0369\tLR: 0.961893\n",
      "Training Epoch: 10 [31232/50000]\tLoss: 3.0919\tLR: 0.962148\n",
      "Training Epoch: 10 [31360/50000]\tLoss: 2.9977\tLR: 0.962404\n",
      "Training Epoch: 10 [31488/50000]\tLoss: 2.9216\tLR: 0.962660\n",
      "Training Epoch: 10 [31616/50000]\tLoss: 2.7317\tLR: 0.962916\n",
      "Training Epoch: 10 [31744/50000]\tLoss: 2.8781\tLR: 0.963171\n",
      "Training Epoch: 10 [31872/50000]\tLoss: 3.0914\tLR: 0.963427\n",
      "Training Epoch: 10 [32000/50000]\tLoss: 3.1071\tLR: 0.963683\n",
      "Training Epoch: 10 [32128/50000]\tLoss: 3.1154\tLR: 0.963939\n",
      "Training Epoch: 10 [32256/50000]\tLoss: 3.2989\tLR: 0.964194\n",
      "Training Epoch: 10 [32384/50000]\tLoss: 3.0065\tLR: 0.964450\n",
      "Training Epoch: 10 [32512/50000]\tLoss: 2.9451\tLR: 0.964706\n",
      "Training Epoch: 10 [32640/50000]\tLoss: 3.2201\tLR: 0.964962\n",
      "Training Epoch: 10 [32768/50000]\tLoss: 2.9960\tLR: 0.965217\n",
      "Training Epoch: 10 [32896/50000]\tLoss: 2.8953\tLR: 0.965473\n",
      "Training Epoch: 10 [33024/50000]\tLoss: 2.9699\tLR: 0.965729\n",
      "Training Epoch: 10 [33152/50000]\tLoss: 3.0307\tLR: 0.965985\n",
      "Training Epoch: 10 [33280/50000]\tLoss: 2.9157\tLR: 0.966240\n",
      "Training Epoch: 10 [33408/50000]\tLoss: 3.1068\tLR: 0.966496\n",
      "Training Epoch: 10 [33536/50000]\tLoss: 3.0711\tLR: 0.966752\n",
      "Training Epoch: 10 [33664/50000]\tLoss: 2.8919\tLR: 0.967008\n",
      "Training Epoch: 10 [33792/50000]\tLoss: 3.1746\tLR: 0.967263\n",
      "Training Epoch: 10 [33920/50000]\tLoss: 2.9185\tLR: 0.967519\n",
      "Training Epoch: 10 [34048/50000]\tLoss: 3.1287\tLR: 0.967775\n",
      "Training Epoch: 10 [34176/50000]\tLoss: 2.9876\tLR: 0.968031\n",
      "Training Epoch: 10 [34304/50000]\tLoss: 2.9791\tLR: 0.968286\n",
      "Training Epoch: 10 [34432/50000]\tLoss: 2.6208\tLR: 0.968542\n",
      "Training Epoch: 10 [34560/50000]\tLoss: 2.9968\tLR: 0.968798\n",
      "Training Epoch: 10 [34688/50000]\tLoss: 3.2281\tLR: 0.969054\n",
      "Training Epoch: 10 [34816/50000]\tLoss: 2.6998\tLR: 0.969309\n",
      "Training Epoch: 10 [34944/50000]\tLoss: 3.2741\tLR: 0.969565\n",
      "Training Epoch: 10 [35072/50000]\tLoss: 2.5033\tLR: 0.969821\n",
      "Training Epoch: 10 [35200/50000]\tLoss: 2.7187\tLR: 0.970077\n",
      "Training Epoch: 10 [35328/50000]\tLoss: 2.8541\tLR: 0.970332\n",
      "Training Epoch: 10 [35456/50000]\tLoss: 2.9087\tLR: 0.970588\n",
      "Training Epoch: 10 [35584/50000]\tLoss: 2.8692\tLR: 0.970844\n",
      "Training Epoch: 10 [35712/50000]\tLoss: 2.7443\tLR: 0.971100\n",
      "Training Epoch: 10 [35840/50000]\tLoss: 3.1828\tLR: 0.971355\n",
      "Training Epoch: 10 [35968/50000]\tLoss: 3.0560\tLR: 0.971611\n",
      "Training Epoch: 10 [36096/50000]\tLoss: 2.8271\tLR: 0.971867\n",
      "Training Epoch: 10 [36224/50000]\tLoss: 2.9674\tLR: 0.972123\n",
      "Training Epoch: 10 [36352/50000]\tLoss: 2.9597\tLR: 0.972379\n",
      "Training Epoch: 10 [36480/50000]\tLoss: 2.7720\tLR: 0.972634\n",
      "Training Epoch: 10 [36608/50000]\tLoss: 3.0131\tLR: 0.972890\n",
      "Training Epoch: 10 [36736/50000]\tLoss: 2.8054\tLR: 0.973146\n",
      "Training Epoch: 10 [36864/50000]\tLoss: 2.8794\tLR: 0.973402\n",
      "Training Epoch: 10 [36992/50000]\tLoss: 3.1408\tLR: 0.973657\n",
      "Training Epoch: 10 [37120/50000]\tLoss: 3.0006\tLR: 0.973913\n",
      "Training Epoch: 10 [37248/50000]\tLoss: 2.5423\tLR: 0.974169\n",
      "Training Epoch: 10 [37376/50000]\tLoss: 2.9537\tLR: 0.974425\n",
      "Training Epoch: 10 [37504/50000]\tLoss: 2.7467\tLR: 0.974680\n",
      "Training Epoch: 10 [37632/50000]\tLoss: 2.9551\tLR: 0.974936\n",
      "Training Epoch: 10 [37760/50000]\tLoss: 2.9968\tLR: 0.975192\n",
      "Training Epoch: 10 [37888/50000]\tLoss: 2.7006\tLR: 0.975448\n",
      "Training Epoch: 10 [38016/50000]\tLoss: 2.9421\tLR: 0.975703\n",
      "Training Epoch: 10 [38144/50000]\tLoss: 3.0229\tLR: 0.975959\n",
      "Training Epoch: 10 [38272/50000]\tLoss: 3.1109\tLR: 0.976215\n",
      "Training Epoch: 10 [38400/50000]\tLoss: 2.7635\tLR: 0.976471\n",
      "Training Epoch: 10 [38528/50000]\tLoss: 2.9178\tLR: 0.976726\n",
      "Training Epoch: 10 [38656/50000]\tLoss: 2.8776\tLR: 0.976982\n",
      "Training Epoch: 10 [38784/50000]\tLoss: 2.7008\tLR: 0.977238\n",
      "Training Epoch: 10 [38912/50000]\tLoss: 2.9334\tLR: 0.977494\n",
      "Training Epoch: 10 [39040/50000]\tLoss: 2.7845\tLR: 0.977749\n",
      "Training Epoch: 10 [39168/50000]\tLoss: 2.6947\tLR: 0.978005\n",
      "Training Epoch: 10 [39296/50000]\tLoss: 2.8550\tLR: 0.978261\n",
      "Training Epoch: 10 [39424/50000]\tLoss: 2.7634\tLR: 0.978517\n",
      "Training Epoch: 10 [39552/50000]\tLoss: 2.9246\tLR: 0.978772\n",
      "Training Epoch: 10 [39680/50000]\tLoss: 2.8712\tLR: 0.979028\n",
      "Training Epoch: 10 [39808/50000]\tLoss: 2.7953\tLR: 0.979284\n",
      "Training Epoch: 10 [39936/50000]\tLoss: 2.8236\tLR: 0.979540\n",
      "Training Epoch: 10 [40064/50000]\tLoss: 3.0488\tLR: 0.979795\n",
      "Training Epoch: 10 [40192/50000]\tLoss: 2.9411\tLR: 0.980051\n",
      "Training Epoch: 10 [40320/50000]\tLoss: 2.9306\tLR: 0.980307\n",
      "Training Epoch: 10 [40448/50000]\tLoss: 2.4742\tLR: 0.980563\n",
      "Training Epoch: 10 [40576/50000]\tLoss: 2.6013\tLR: 0.980818\n",
      "Training Epoch: 10 [40704/50000]\tLoss: 3.1046\tLR: 0.981074\n",
      "Training Epoch: 10 [40832/50000]\tLoss: 3.0047\tLR: 0.981330\n",
      "Training Epoch: 10 [40960/50000]\tLoss: 2.8121\tLR: 0.981586\n",
      "Training Epoch: 10 [41088/50000]\tLoss: 2.8086\tLR: 0.981841\n",
      "Training Epoch: 10 [41216/50000]\tLoss: 2.9401\tLR: 0.982097\n",
      "Training Epoch: 10 [41344/50000]\tLoss: 2.7425\tLR: 0.982353\n",
      "Training Epoch: 10 [41472/50000]\tLoss: 2.9645\tLR: 0.982609\n",
      "Training Epoch: 10 [41600/50000]\tLoss: 3.1561\tLR: 0.982864\n",
      "Training Epoch: 10 [41728/50000]\tLoss: 2.8388\tLR: 0.983120\n",
      "Training Epoch: 10 [41856/50000]\tLoss: 3.0612\tLR: 0.983376\n",
      "Training Epoch: 10 [41984/50000]\tLoss: 3.1207\tLR: 0.983632\n",
      "Training Epoch: 10 [42112/50000]\tLoss: 3.1766\tLR: 0.983887\n",
      "Training Epoch: 10 [42240/50000]\tLoss: 3.1610\tLR: 0.984143\n",
      "Training Epoch: 10 [42368/50000]\tLoss: 3.1624\tLR: 0.984399\n",
      "Training Epoch: 10 [42496/50000]\tLoss: 2.7374\tLR: 0.984655\n",
      "Training Epoch: 10 [42624/50000]\tLoss: 2.9944\tLR: 0.984910\n",
      "Training Epoch: 10 [42752/50000]\tLoss: 2.6007\tLR: 0.985166\n",
      "Training Epoch: 10 [42880/50000]\tLoss: 3.0156\tLR: 0.985422\n",
      "Training Epoch: 10 [43008/50000]\tLoss: 2.9040\tLR: 0.985678\n",
      "Training Epoch: 10 [43136/50000]\tLoss: 2.7310\tLR: 0.985934\n",
      "Training Epoch: 10 [43264/50000]\tLoss: 2.6948\tLR: 0.986189\n",
      "Training Epoch: 10 [43392/50000]\tLoss: 3.0603\tLR: 0.986445\n",
      "Training Epoch: 10 [43520/50000]\tLoss: 3.0662\tLR: 0.986701\n",
      "Training Epoch: 10 [43648/50000]\tLoss: 2.9095\tLR: 0.986957\n",
      "Training Epoch: 10 [43776/50000]\tLoss: 2.6958\tLR: 0.987212\n",
      "Training Epoch: 10 [43904/50000]\tLoss: 2.8370\tLR: 0.987468\n",
      "Training Epoch: 10 [44032/50000]\tLoss: 2.6147\tLR: 0.987724\n",
      "Training Epoch: 10 [44160/50000]\tLoss: 3.1978\tLR: 0.987980\n",
      "Training Epoch: 10 [44288/50000]\tLoss: 2.8097\tLR: 0.988235\n",
      "Training Epoch: 10 [44416/50000]\tLoss: 2.7463\tLR: 0.988491\n",
      "Training Epoch: 10 [44544/50000]\tLoss: 3.0826\tLR: 0.988747\n",
      "Training Epoch: 10 [44672/50000]\tLoss: 3.0566\tLR: 0.989003\n",
      "Training Epoch: 10 [44800/50000]\tLoss: 2.9383\tLR: 0.989258\n",
      "Training Epoch: 10 [44928/50000]\tLoss: 3.2152\tLR: 0.989514\n",
      "Training Epoch: 10 [45056/50000]\tLoss: 2.9685\tLR: 0.989770\n",
      "Training Epoch: 10 [45184/50000]\tLoss: 3.0124\tLR: 0.990026\n",
      "Training Epoch: 10 [45312/50000]\tLoss: 2.8504\tLR: 0.990281\n",
      "Training Epoch: 10 [45440/50000]\tLoss: 2.8375\tLR: 0.990537\n",
      "Training Epoch: 10 [45568/50000]\tLoss: 2.9029\tLR: 0.990793\n",
      "Training Epoch: 10 [45696/50000]\tLoss: 2.8449\tLR: 0.991049\n",
      "Training Epoch: 10 [45824/50000]\tLoss: 2.9775\tLR: 0.991304\n",
      "Training Epoch: 10 [45952/50000]\tLoss: 2.6684\tLR: 0.991560\n",
      "Training Epoch: 10 [46080/50000]\tLoss: 3.1321\tLR: 0.991816\n",
      "Training Epoch: 10 [46208/50000]\tLoss: 2.9209\tLR: 0.992072\n",
      "Training Epoch: 10 [46336/50000]\tLoss: 3.1556\tLR: 0.992327\n",
      "Training Epoch: 10 [46464/50000]\tLoss: 3.1398\tLR: 0.992583\n",
      "Training Epoch: 10 [46592/50000]\tLoss: 2.5231\tLR: 0.992839\n",
      "Training Epoch: 10 [46720/50000]\tLoss: 2.8407\tLR: 0.993095\n",
      "Training Epoch: 10 [46848/50000]\tLoss: 2.7604\tLR: 0.993350\n",
      "Training Epoch: 10 [46976/50000]\tLoss: 2.7377\tLR: 0.993606\n",
      "Training Epoch: 10 [47104/50000]\tLoss: 2.6125\tLR: 0.993862\n",
      "Training Epoch: 10 [47232/50000]\tLoss: 3.0102\tLR: 0.994118\n",
      "Training Epoch: 10 [47360/50000]\tLoss: 2.6614\tLR: 0.994373\n",
      "Training Epoch: 10 [47488/50000]\tLoss: 3.1116\tLR: 0.994629\n",
      "Training Epoch: 10 [47616/50000]\tLoss: 2.7249\tLR: 0.994885\n",
      "Training Epoch: 10 [47744/50000]\tLoss: 2.8712\tLR: 0.995141\n",
      "Training Epoch: 10 [47872/50000]\tLoss: 2.9138\tLR: 0.995396\n",
      "Training Epoch: 10 [48000/50000]\tLoss: 2.8737\tLR: 0.995652\n",
      "Training Epoch: 10 [48128/50000]\tLoss: 2.5982\tLR: 0.995908\n",
      "Training Epoch: 10 [48256/50000]\tLoss: 2.9388\tLR: 0.996164\n",
      "Training Epoch: 10 [48384/50000]\tLoss: 2.7801\tLR: 0.996419\n",
      "Training Epoch: 10 [48512/50000]\tLoss: 2.8373\tLR: 0.996675\n",
      "Training Epoch: 10 [48640/50000]\tLoss: 2.7014\tLR: 0.996931\n",
      "Training Epoch: 10 [48768/50000]\tLoss: 2.6958\tLR: 0.997187\n",
      "Training Epoch: 10 [48896/50000]\tLoss: 2.5964\tLR: 0.997442\n",
      "Training Epoch: 10 [49024/50000]\tLoss: 3.0711\tLR: 0.997698\n",
      "Training Epoch: 10 [49152/50000]\tLoss: 2.8290\tLR: 0.997954\n",
      "Training Epoch: 10 [49280/50000]\tLoss: 2.8937\tLR: 0.998210\n",
      "Training Epoch: 10 [49408/50000]\tLoss: 2.8094\tLR: 0.998465\n",
      "Training Epoch: 10 [49536/50000]\tLoss: 3.0199\tLR: 0.998721\n",
      "Training Epoch: 10 [49664/50000]\tLoss: 2.9654\tLR: 0.998977\n",
      "Training Epoch: 10 [49792/50000]\tLoss: 3.0873\tLR: 0.999233\n",
      "Training Epoch: 10 [49920/50000]\tLoss: 2.9277\tLR: 0.999488\n",
      "Training Epoch: 10 [50000/50000]\tLoss: 2.9418\tLR: 0.999744\n",
      "epoch 10 training time consumed: 491.51s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   14020 GB |   14020 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   13977 GB |   13977 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      43 GB |      43 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   14020 GB |   14020 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   13977 GB |   13977 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      43 GB |      43 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   13820 GB |   13820 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   13777 GB |   13777 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |      43 GB |      43 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    1487 K  |    1486 K  |\n",
      "|       from large pool |      24    |      65    |     633 K  |     633 K  |\n",
      "|       from small pool |     231    |     274    |     853 K  |     853 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    1487 K  |    1486 K  |\n",
      "|       from large pool |      24    |      65    |     633 K  |     633 K  |\n",
      "|       from small pool |     231    |     274    |     853 K  |     853 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      45    |     861 K  |     861 K  |\n",
      "|       from large pool |      10    |      23    |     304 K  |     304 K  |\n",
      "|       from small pool |      26    |      35    |     556 K  |     556 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 10, Average loss: 0.0289, Accuracy: 0.1753, Time consumed:30.80s\n",
      "\n",
      "saving weights file to checkpoint\\resnet18\\Monday_25_July_2022_16h_56m_47s\\resnet18-10-regular.pth\n",
      "Training Epoch: 11 [128/50000]\tLoss: 2.9544\tLR: 0.100000\n",
      "Training Epoch: 11 [256/50000]\tLoss: 2.9682\tLR: 1.000256\n",
      "Training Epoch: 11 [384/50000]\tLoss: 2.6683\tLR: 1.000512\n",
      "Training Epoch: 11 [512/50000]\tLoss: 2.6414\tLR: 1.000767\n",
      "Training Epoch: 11 [640/50000]\tLoss: 3.0633\tLR: 1.001023\n",
      "Training Epoch: 11 [768/50000]\tLoss: 2.7615\tLR: 1.001279\n",
      "Training Epoch: 11 [896/50000]\tLoss: 3.1181\tLR: 1.001535\n",
      "Training Epoch: 11 [1024/50000]\tLoss: 2.9116\tLR: 1.001790\n",
      "Training Epoch: 11 [1152/50000]\tLoss: 3.1106\tLR: 1.002046\n",
      "Training Epoch: 11 [1280/50000]\tLoss: 2.7079\tLR: 1.002302\n",
      "Training Epoch: 11 [1408/50000]\tLoss: 3.0302\tLR: 1.002558\n",
      "Training Epoch: 11 [1536/50000]\tLoss: 2.8541\tLR: 1.002813\n",
      "Training Epoch: 11 [1664/50000]\tLoss: 2.7172\tLR: 1.003069\n",
      "Training Epoch: 11 [1792/50000]\tLoss: 3.2717\tLR: 1.003325\n",
      "Training Epoch: 11 [1920/50000]\tLoss: 3.0178\tLR: 1.003581\n",
      "Training Epoch: 11 [2048/50000]\tLoss: 2.9370\tLR: 1.003836\n",
      "Training Epoch: 11 [2176/50000]\tLoss: 2.9503\tLR: 1.004092\n",
      "Training Epoch: 11 [2304/50000]\tLoss: 3.0206\tLR: 1.004348\n",
      "Training Epoch: 11 [2432/50000]\tLoss: 2.7254\tLR: 1.004604\n",
      "Training Epoch: 11 [2560/50000]\tLoss: 2.9986\tLR: 1.004859\n",
      "Training Epoch: 11 [2688/50000]\tLoss: 2.9638\tLR: 1.005115\n",
      "Training Epoch: 11 [2816/50000]\tLoss: 2.9306\tLR: 1.005371\n",
      "Training Epoch: 11 [2944/50000]\tLoss: 2.6377\tLR: 1.005627\n",
      "Training Epoch: 11 [3072/50000]\tLoss: 2.8431\tLR: 1.005882\n",
      "Training Epoch: 11 [3200/50000]\tLoss: 2.8173\tLR: 1.006138\n",
      "Training Epoch: 11 [3328/50000]\tLoss: 3.1171\tLR: 1.006394\n",
      "Training Epoch: 11 [3456/50000]\tLoss: 2.8968\tLR: 1.006650\n",
      "Training Epoch: 11 [3584/50000]\tLoss: 3.1567\tLR: 1.006905\n",
      "Training Epoch: 11 [3712/50000]\tLoss: 3.1254\tLR: 1.007161\n",
      "Training Epoch: 11 [3840/50000]\tLoss: 2.8663\tLR: 1.007417\n",
      "Training Epoch: 11 [3968/50000]\tLoss: 2.5598\tLR: 1.007673\n",
      "Training Epoch: 11 [4096/50000]\tLoss: 2.8982\tLR: 1.007928\n",
      "Training Epoch: 11 [4224/50000]\tLoss: 2.8186\tLR: 1.008184\n",
      "Training Epoch: 11 [4352/50000]\tLoss: 3.1679\tLR: 1.008440\n",
      "Training Epoch: 11 [4480/50000]\tLoss: 2.8186\tLR: 1.008696\n",
      "Training Epoch: 11 [4608/50000]\tLoss: 2.8879\tLR: 1.008951\n",
      "Training Epoch: 11 [4736/50000]\tLoss: 2.7189\tLR: 1.009207\n",
      "Training Epoch: 11 [4864/50000]\tLoss: 2.8057\tLR: 1.009463\n",
      "Training Epoch: 11 [4992/50000]\tLoss: 3.0534\tLR: 1.009719\n",
      "Training Epoch: 11 [5120/50000]\tLoss: 3.1574\tLR: 1.009974\n",
      "Training Epoch: 11 [5248/50000]\tLoss: 2.6694\tLR: 1.010230\n",
      "Training Epoch: 11 [5376/50000]\tLoss: 2.9511\tLR: 1.010486\n",
      "Training Epoch: 11 [5504/50000]\tLoss: 2.8847\tLR: 1.010742\n",
      "Training Epoch: 11 [5632/50000]\tLoss: 2.7829\tLR: 1.010997\n",
      "Training Epoch: 11 [5760/50000]\tLoss: 2.8636\tLR: 1.011253\n",
      "Training Epoch: 11 [5888/50000]\tLoss: 2.8580\tLR: 1.011509\n",
      "Training Epoch: 11 [6016/50000]\tLoss: 2.6606\tLR: 1.011765\n",
      "Training Epoch: 11 [6144/50000]\tLoss: 2.8880\tLR: 1.012020\n",
      "Training Epoch: 11 [6272/50000]\tLoss: 2.7083\tLR: 1.012276\n",
      "Training Epoch: 11 [6400/50000]\tLoss: 3.0046\tLR: 1.012532\n",
      "Training Epoch: 11 [6528/50000]\tLoss: 3.2772\tLR: 1.012788\n",
      "Training Epoch: 11 [6656/50000]\tLoss: 2.8540\tLR: 1.013043\n",
      "Training Epoch: 11 [6784/50000]\tLoss: 3.0336\tLR: 1.013299\n",
      "Training Epoch: 11 [6912/50000]\tLoss: 3.0213\tLR: 1.013555\n",
      "Training Epoch: 11 [7040/50000]\tLoss: 2.9050\tLR: 1.013811\n",
      "Training Epoch: 11 [7168/50000]\tLoss: 2.8563\tLR: 1.014066\n",
      "Training Epoch: 11 [7296/50000]\tLoss: 2.7879\tLR: 1.014322\n",
      "Training Epoch: 11 [7424/50000]\tLoss: 2.9643\tLR: 1.014578\n",
      "Training Epoch: 11 [7552/50000]\tLoss: 3.0526\tLR: 1.014834\n",
      "Training Epoch: 11 [7680/50000]\tLoss: 3.1376\tLR: 1.015090\n",
      "Training Epoch: 11 [7808/50000]\tLoss: 2.8549\tLR: 1.015345\n",
      "Training Epoch: 11 [7936/50000]\tLoss: 2.9322\tLR: 1.015601\n",
      "Training Epoch: 11 [8064/50000]\tLoss: 2.9948\tLR: 1.015857\n",
      "Training Epoch: 11 [8192/50000]\tLoss: 3.2056\tLR: 1.016113\n",
      "Training Epoch: 11 [8320/50000]\tLoss: 2.8313\tLR: 1.016368\n",
      "Training Epoch: 11 [8448/50000]\tLoss: 3.0930\tLR: 1.016624\n",
      "Training Epoch: 11 [8576/50000]\tLoss: 2.6972\tLR: 1.016880\n",
      "Training Epoch: 11 [8704/50000]\tLoss: 2.8034\tLR: 1.017136\n",
      "Training Epoch: 11 [8832/50000]\tLoss: 2.9950\tLR: 1.017391\n",
      "Training Epoch: 11 [8960/50000]\tLoss: 2.7791\tLR: 1.017647\n",
      "Training Epoch: 11 [9088/50000]\tLoss: 2.9257\tLR: 1.017903\n",
      "Training Epoch: 11 [9216/50000]\tLoss: 2.9690\tLR: 1.018159\n",
      "Training Epoch: 11 [9344/50000]\tLoss: 2.8723\tLR: 1.018414\n",
      "Training Epoch: 11 [9472/50000]\tLoss: 2.9564\tLR: 1.018670\n",
      "Training Epoch: 11 [9600/50000]\tLoss: 2.9787\tLR: 1.018926\n",
      "Training Epoch: 11 [9728/50000]\tLoss: 2.6147\tLR: 1.019182\n",
      "Training Epoch: 11 [9856/50000]\tLoss: 2.6505\tLR: 1.019437\n",
      "Training Epoch: 11 [9984/50000]\tLoss: 3.1057\tLR: 1.019693\n",
      "Training Epoch: 11 [10112/50000]\tLoss: 2.6896\tLR: 1.019949\n",
      "Training Epoch: 11 [10240/50000]\tLoss: 2.8775\tLR: 1.020205\n",
      "Training Epoch: 11 [10368/50000]\tLoss: 2.6381\tLR: 1.020460\n",
      "Training Epoch: 11 [10496/50000]\tLoss: 2.9873\tLR: 1.020716\n",
      "Training Epoch: 11 [10624/50000]\tLoss: 2.9172\tLR: 1.020972\n",
      "Training Epoch: 11 [10752/50000]\tLoss: 2.9942\tLR: 1.021228\n",
      "Training Epoch: 11 [10880/50000]\tLoss: 3.1254\tLR: 1.021483\n",
      "Training Epoch: 11 [11008/50000]\tLoss: 2.8349\tLR: 1.021739\n",
      "Training Epoch: 11 [11136/50000]\tLoss: 2.7710\tLR: 1.021995\n",
      "Training Epoch: 11 [11264/50000]\tLoss: 2.9324\tLR: 1.022251\n",
      "Training Epoch: 11 [11392/50000]\tLoss: 3.0358\tLR: 1.022506\n",
      "Training Epoch: 11 [11520/50000]\tLoss: 2.7542\tLR: 1.022762\n",
      "Training Epoch: 11 [11648/50000]\tLoss: 3.0195\tLR: 1.023018\n",
      "Training Epoch: 11 [11776/50000]\tLoss: 2.9233\tLR: 1.023274\n",
      "Training Epoch: 11 [11904/50000]\tLoss: 2.8598\tLR: 1.023529\n",
      "Training Epoch: 11 [12032/50000]\tLoss: 2.7625\tLR: 1.023785\n",
      "Training Epoch: 11 [12160/50000]\tLoss: 2.7347\tLR: 1.024041\n",
      "Training Epoch: 11 [12288/50000]\tLoss: 2.9056\tLR: 1.024297\n",
      "Training Epoch: 11 [12416/50000]\tLoss: 3.2626\tLR: 1.024552\n",
      "Training Epoch: 11 [12544/50000]\tLoss: 2.8559\tLR: 1.024808\n",
      "Training Epoch: 11 [12672/50000]\tLoss: 2.9943\tLR: 1.025064\n",
      "Training Epoch: 11 [12800/50000]\tLoss: 2.9304\tLR: 1.025320\n",
      "Training Epoch: 11 [12928/50000]\tLoss: 2.7938\tLR: 1.025575\n",
      "Training Epoch: 11 [13056/50000]\tLoss: 2.8048\tLR: 1.025831\n",
      "Training Epoch: 11 [13184/50000]\tLoss: 2.7821\tLR: 1.026087\n",
      "Training Epoch: 11 [13312/50000]\tLoss: 3.0664\tLR: 1.026343\n",
      "Training Epoch: 11 [13440/50000]\tLoss: 3.0171\tLR: 1.026598\n",
      "Training Epoch: 11 [13568/50000]\tLoss: 3.3735\tLR: 1.026854\n",
      "Training Epoch: 11 [13696/50000]\tLoss: 2.7995\tLR: 1.027110\n",
      "Training Epoch: 11 [13824/50000]\tLoss: 3.0463\tLR: 1.027366\n",
      "Training Epoch: 11 [13952/50000]\tLoss: 2.6396\tLR: 1.027621\n",
      "Training Epoch: 11 [14080/50000]\tLoss: 3.0220\tLR: 1.027877\n",
      "Training Epoch: 11 [14208/50000]\tLoss: 2.8494\tLR: 1.028133\n",
      "Training Epoch: 11 [14336/50000]\tLoss: 2.9846\tLR: 1.028389\n",
      "Training Epoch: 11 [14464/50000]\tLoss: 3.0036\tLR: 1.028645\n",
      "Training Epoch: 11 [14592/50000]\tLoss: 2.9410\tLR: 1.028900\n",
      "Training Epoch: 11 [14720/50000]\tLoss: 3.2644\tLR: 1.029156\n",
      "Training Epoch: 11 [14848/50000]\tLoss: 3.0081\tLR: 1.029412\n",
      "Training Epoch: 11 [14976/50000]\tLoss: 3.1026\tLR: 1.029668\n",
      "Training Epoch: 11 [15104/50000]\tLoss: 2.8071\tLR: 1.029923\n",
      "Training Epoch: 11 [15232/50000]\tLoss: 3.0687\tLR: 1.030179\n",
      "Training Epoch: 11 [15360/50000]\tLoss: 3.2912\tLR: 1.030435\n",
      "Training Epoch: 11 [15488/50000]\tLoss: 3.1828\tLR: 1.030691\n",
      "Training Epoch: 11 [15616/50000]\tLoss: 3.0851\tLR: 1.030946\n",
      "Training Epoch: 11 [15744/50000]\tLoss: 2.9360\tLR: 1.031202\n",
      "Training Epoch: 11 [15872/50000]\tLoss: 3.1109\tLR: 1.031458\n",
      "Training Epoch: 11 [16000/50000]\tLoss: 2.7798\tLR: 1.031714\n",
      "Training Epoch: 11 [16128/50000]\tLoss: 2.8105\tLR: 1.031969\n",
      "Training Epoch: 11 [16256/50000]\tLoss: 2.9965\tLR: 1.032225\n",
      "Training Epoch: 11 [16384/50000]\tLoss: 2.9864\tLR: 1.032481\n",
      "Training Epoch: 11 [16512/50000]\tLoss: 2.7996\tLR: 1.032737\n",
      "Training Epoch: 11 [16640/50000]\tLoss: 2.9317\tLR: 1.032992\n",
      "Training Epoch: 11 [16768/50000]\tLoss: 2.6130\tLR: 1.033248\n",
      "Training Epoch: 11 [16896/50000]\tLoss: 3.0436\tLR: 1.033504\n",
      "Training Epoch: 11 [17024/50000]\tLoss: 3.0653\tLR: 1.033760\n",
      "Training Epoch: 11 [17152/50000]\tLoss: 2.9138\tLR: 1.034015\n",
      "Training Epoch: 11 [17280/50000]\tLoss: 2.7023\tLR: 1.034271\n",
      "Training Epoch: 11 [17408/50000]\tLoss: 3.0446\tLR: 1.034527\n",
      "Training Epoch: 11 [17536/50000]\tLoss: 2.6932\tLR: 1.034783\n",
      "Training Epoch: 11 [17664/50000]\tLoss: 3.0541\tLR: 1.035038\n",
      "Training Epoch: 11 [17792/50000]\tLoss: 2.7737\tLR: 1.035294\n",
      "Training Epoch: 11 [17920/50000]\tLoss: 2.9702\tLR: 1.035550\n",
      "Training Epoch: 11 [18048/50000]\tLoss: 3.0290\tLR: 1.035806\n",
      "Training Epoch: 11 [18176/50000]\tLoss: 3.1850\tLR: 1.036061\n",
      "Training Epoch: 11 [18304/50000]\tLoss: 2.8700\tLR: 1.036317\n",
      "Training Epoch: 11 [18432/50000]\tLoss: 3.1549\tLR: 1.036573\n",
      "Training Epoch: 11 [18560/50000]\tLoss: 3.1107\tLR: 1.036829\n",
      "Training Epoch: 11 [18688/50000]\tLoss: 2.6734\tLR: 1.037084\n",
      "Training Epoch: 11 [18816/50000]\tLoss: 3.0513\tLR: 1.037340\n",
      "Training Epoch: 11 [18944/50000]\tLoss: 2.8944\tLR: 1.037596\n",
      "Training Epoch: 11 [19072/50000]\tLoss: 2.9361\tLR: 1.037852\n",
      "Training Epoch: 11 [19200/50000]\tLoss: 3.1395\tLR: 1.038107\n",
      "Training Epoch: 11 [19328/50000]\tLoss: 3.1803\tLR: 1.038363\n",
      "Training Epoch: 11 [19456/50000]\tLoss: 2.8727\tLR: 1.038619\n",
      "Training Epoch: 11 [19584/50000]\tLoss: 2.9325\tLR: 1.038875\n",
      "Training Epoch: 11 [19712/50000]\tLoss: 2.7969\tLR: 1.039130\n",
      "Training Epoch: 11 [19840/50000]\tLoss: 3.0917\tLR: 1.039386\n",
      "Training Epoch: 11 [19968/50000]\tLoss: 2.7891\tLR: 1.039642\n",
      "Training Epoch: 11 [20096/50000]\tLoss: 3.2176\tLR: 1.039898\n",
      "Training Epoch: 11 [20224/50000]\tLoss: 2.7229\tLR: 1.040153\n",
      "Training Epoch: 11 [20352/50000]\tLoss: 2.9224\tLR: 1.040409\n",
      "Training Epoch: 11 [20480/50000]\tLoss: 2.9391\tLR: 1.040665\n",
      "Training Epoch: 11 [20608/50000]\tLoss: 2.9850\tLR: 1.040921\n",
      "Training Epoch: 11 [20736/50000]\tLoss: 2.8864\tLR: 1.041176\n",
      "Training Epoch: 11 [20864/50000]\tLoss: 2.8324\tLR: 1.041432\n",
      "Training Epoch: 11 [20992/50000]\tLoss: 2.8059\tLR: 1.041688\n",
      "Training Epoch: 11 [21120/50000]\tLoss: 2.8432\tLR: 1.041944\n",
      "Training Epoch: 11 [21248/50000]\tLoss: 2.9000\tLR: 1.042199\n",
      "Training Epoch: 11 [21376/50000]\tLoss: 3.0219\tLR: 1.042455\n",
      "Training Epoch: 11 [21504/50000]\tLoss: 2.9597\tLR: 1.042711\n",
      "Training Epoch: 11 [21632/50000]\tLoss: 3.0292\tLR: 1.042967\n",
      "Training Epoch: 11 [21760/50000]\tLoss: 2.9006\tLR: 1.043223\n",
      "Training Epoch: 11 [21888/50000]\tLoss: 2.9627\tLR: 1.043478\n",
      "Training Epoch: 11 [22016/50000]\tLoss: 2.9317\tLR: 1.043734\n",
      "Training Epoch: 11 [22144/50000]\tLoss: 2.8791\tLR: 1.043990\n",
      "Training Epoch: 11 [22272/50000]\tLoss: 2.8094\tLR: 1.044246\n",
      "Training Epoch: 11 [22400/50000]\tLoss: 2.9056\tLR: 1.044501\n",
      "Training Epoch: 11 [22528/50000]\tLoss: 2.9895\tLR: 1.044757\n",
      "Training Epoch: 11 [22656/50000]\tLoss: 2.7441\tLR: 1.045013\n",
      "Training Epoch: 11 [22784/50000]\tLoss: 2.9608\tLR: 1.045269\n",
      "Training Epoch: 11 [22912/50000]\tLoss: 2.7633\tLR: 1.045524\n",
      "Training Epoch: 11 [23040/50000]\tLoss: 2.8165\tLR: 1.045780\n",
      "Training Epoch: 11 [23168/50000]\tLoss: 3.0754\tLR: 1.046036\n",
      "Training Epoch: 11 [23296/50000]\tLoss: 2.8249\tLR: 1.046292\n",
      "Training Epoch: 11 [23424/50000]\tLoss: 3.0441\tLR: 1.046547\n",
      "Training Epoch: 11 [23552/50000]\tLoss: 2.7933\tLR: 1.046803\n",
      "Training Epoch: 11 [23680/50000]\tLoss: 2.8193\tLR: 1.047059\n",
      "Training Epoch: 11 [23808/50000]\tLoss: 2.6583\tLR: 1.047315\n",
      "Training Epoch: 11 [23936/50000]\tLoss: 3.0907\tLR: 1.047570\n",
      "Training Epoch: 11 [24064/50000]\tLoss: 2.9829\tLR: 1.047826\n",
      "Training Epoch: 11 [24192/50000]\tLoss: 2.8604\tLR: 1.048082\n",
      "Training Epoch: 11 [24320/50000]\tLoss: 3.0435\tLR: 1.048338\n",
      "Training Epoch: 11 [24448/50000]\tLoss: 3.0958\tLR: 1.048593\n",
      "Training Epoch: 11 [24576/50000]\tLoss: 3.0905\tLR: 1.048849\n",
      "Training Epoch: 11 [24704/50000]\tLoss: 3.1663\tLR: 1.049105\n",
      "Training Epoch: 11 [24832/50000]\tLoss: 3.0895\tLR: 1.049361\n",
      "Training Epoch: 11 [24960/50000]\tLoss: 3.0175\tLR: 1.049616\n",
      "Training Epoch: 11 [25088/50000]\tLoss: 2.9356\tLR: 1.049872\n",
      "Training Epoch: 11 [25216/50000]\tLoss: 2.8404\tLR: 1.050128\n",
      "Training Epoch: 11 [25344/50000]\tLoss: 3.2506\tLR: 1.050384\n",
      "Training Epoch: 11 [25472/50000]\tLoss: 2.6403\tLR: 1.050639\n",
      "Training Epoch: 11 [25600/50000]\tLoss: 2.9436\tLR: 1.050895\n",
      "Training Epoch: 11 [25728/50000]\tLoss: 3.2792\tLR: 1.051151\n",
      "Training Epoch: 11 [25856/50000]\tLoss: 3.1658\tLR: 1.051407\n",
      "Training Epoch: 11 [25984/50000]\tLoss: 2.7393\tLR: 1.051662\n",
      "Training Epoch: 11 [26112/50000]\tLoss: 2.9943\tLR: 1.051918\n",
      "Training Epoch: 11 [26240/50000]\tLoss: 2.8801\tLR: 1.052174\n",
      "Training Epoch: 11 [26368/50000]\tLoss: 2.9371\tLR: 1.052430\n",
      "Training Epoch: 11 [26496/50000]\tLoss: 3.0250\tLR: 1.052685\n",
      "Training Epoch: 11 [26624/50000]\tLoss: 3.1148\tLR: 1.052941\n",
      "Training Epoch: 11 [26752/50000]\tLoss: 2.9465\tLR: 1.053197\n",
      "Training Epoch: 11 [26880/50000]\tLoss: 3.0004\tLR: 1.053453\n",
      "Training Epoch: 11 [27008/50000]\tLoss: 2.9827\tLR: 1.053708\n",
      "Training Epoch: 11 [27136/50000]\tLoss: 2.7317\tLR: 1.053964\n",
      "Training Epoch: 11 [27264/50000]\tLoss: 3.0561\tLR: 1.054220\n",
      "Training Epoch: 11 [27392/50000]\tLoss: 3.0553\tLR: 1.054476\n",
      "Training Epoch: 11 [27520/50000]\tLoss: 2.9035\tLR: 1.054731\n",
      "Training Epoch: 11 [27648/50000]\tLoss: 2.8964\tLR: 1.054987\n",
      "Training Epoch: 11 [27776/50000]\tLoss: 2.8461\tLR: 1.055243\n",
      "Training Epoch: 11 [27904/50000]\tLoss: 2.7723\tLR: 1.055499\n",
      "Training Epoch: 11 [28032/50000]\tLoss: 3.0165\tLR: 1.055754\n",
      "Training Epoch: 11 [28160/50000]\tLoss: 2.7477\tLR: 1.056010\n",
      "Training Epoch: 11 [28288/50000]\tLoss: 2.8511\tLR: 1.056266\n",
      "Training Epoch: 11 [28416/50000]\tLoss: 2.8499\tLR: 1.056522\n",
      "Training Epoch: 11 [28544/50000]\tLoss: 3.0375\tLR: 1.056777\n",
      "Training Epoch: 11 [28672/50000]\tLoss: 2.7055\tLR: 1.057033\n",
      "Training Epoch: 11 [28800/50000]\tLoss: 2.8270\tLR: 1.057289\n",
      "Training Epoch: 11 [28928/50000]\tLoss: 3.2458\tLR: 1.057545\n",
      "Training Epoch: 11 [29056/50000]\tLoss: 2.9920\tLR: 1.057801\n",
      "Training Epoch: 11 [29184/50000]\tLoss: 3.1371\tLR: 1.058056\n",
      "Training Epoch: 11 [29312/50000]\tLoss: 2.8135\tLR: 1.058312\n",
      "Training Epoch: 11 [29440/50000]\tLoss: 2.8750\tLR: 1.058568\n",
      "Training Epoch: 11 [29568/50000]\tLoss: 3.0706\tLR: 1.058824\n",
      "Training Epoch: 11 [29696/50000]\tLoss: 3.0857\tLR: 1.059079\n",
      "Training Epoch: 11 [29824/50000]\tLoss: 3.0716\tLR: 1.059335\n",
      "Training Epoch: 11 [29952/50000]\tLoss: 3.0205\tLR: 1.059591\n",
      "Training Epoch: 11 [30080/50000]\tLoss: 2.7756\tLR: 1.059847\n",
      "Training Epoch: 11 [30208/50000]\tLoss: 3.1362\tLR: 1.060102\n",
      "Training Epoch: 11 [30336/50000]\tLoss: 2.9487\tLR: 1.060358\n",
      "Training Epoch: 11 [30464/50000]\tLoss: 2.9385\tLR: 1.060614\n",
      "Training Epoch: 11 [30592/50000]\tLoss: 2.8873\tLR: 1.060870\n",
      "Training Epoch: 11 [30720/50000]\tLoss: 2.9904\tLR: 1.061125\n",
      "Training Epoch: 11 [30848/50000]\tLoss: 3.1535\tLR: 1.061381\n",
      "Training Epoch: 11 [30976/50000]\tLoss: 3.0154\tLR: 1.061637\n",
      "Training Epoch: 11 [31104/50000]\tLoss: 3.2594\tLR: 1.061893\n",
      "Training Epoch: 11 [31232/50000]\tLoss: 3.1222\tLR: 1.062148\n",
      "Training Epoch: 11 [31360/50000]\tLoss: 2.8359\tLR: 1.062404\n",
      "Training Epoch: 11 [31488/50000]\tLoss: 3.0685\tLR: 1.062660\n",
      "Training Epoch: 11 [31616/50000]\tLoss: 3.1404\tLR: 1.062916\n",
      "Training Epoch: 11 [31744/50000]\tLoss: 3.1132\tLR: 1.063171\n",
      "Training Epoch: 11 [31872/50000]\tLoss: 3.0592\tLR: 1.063427\n",
      "Training Epoch: 11 [32000/50000]\tLoss: 2.5537\tLR: 1.063683\n",
      "Training Epoch: 11 [32128/50000]\tLoss: 2.8200\tLR: 1.063939\n",
      "Training Epoch: 11 [32256/50000]\tLoss: 2.6883\tLR: 1.064194\n",
      "Training Epoch: 11 [32384/50000]\tLoss: 3.1427\tLR: 1.064450\n",
      "Training Epoch: 11 [32512/50000]\tLoss: 3.2098\tLR: 1.064706\n",
      "Training Epoch: 11 [32640/50000]\tLoss: 2.7881\tLR: 1.064962\n",
      "Training Epoch: 11 [32768/50000]\tLoss: 2.9706\tLR: 1.065217\n",
      "Training Epoch: 11 [32896/50000]\tLoss: 2.9033\tLR: 1.065473\n",
      "Training Epoch: 11 [33024/50000]\tLoss: 2.8018\tLR: 1.065729\n",
      "Training Epoch: 11 [33152/50000]\tLoss: 3.0258\tLR: 1.065985\n",
      "Training Epoch: 11 [33280/50000]\tLoss: 3.0734\tLR: 1.066240\n",
      "Training Epoch: 11 [33408/50000]\tLoss: 2.7266\tLR: 1.066496\n",
      "Training Epoch: 11 [33536/50000]\tLoss: 2.9542\tLR: 1.066752\n",
      "Training Epoch: 11 [33664/50000]\tLoss: 2.7894\tLR: 1.067008\n",
      "Training Epoch: 11 [33792/50000]\tLoss: 2.9210\tLR: 1.067263\n",
      "Training Epoch: 11 [33920/50000]\tLoss: 2.8858\tLR: 1.067519\n",
      "Training Epoch: 11 [34048/50000]\tLoss: 3.1850\tLR: 1.067775\n",
      "Training Epoch: 11 [34176/50000]\tLoss: 3.1210\tLR: 1.068031\n",
      "Training Epoch: 11 [34304/50000]\tLoss: 2.8086\tLR: 1.068286\n",
      "Training Epoch: 11 [34432/50000]\tLoss: 2.9783\tLR: 1.068542\n",
      "Training Epoch: 11 [34560/50000]\tLoss: 3.1579\tLR: 1.068798\n",
      "Training Epoch: 11 [34688/50000]\tLoss: 2.9944\tLR: 1.069054\n",
      "Training Epoch: 11 [34816/50000]\tLoss: 3.2435\tLR: 1.069309\n",
      "Training Epoch: 11 [34944/50000]\tLoss: 3.0094\tLR: 1.069565\n",
      "Training Epoch: 11 [35072/50000]\tLoss: 3.1169\tLR: 1.069821\n",
      "Training Epoch: 11 [35200/50000]\tLoss: 2.7897\tLR: 1.070077\n",
      "Training Epoch: 11 [35328/50000]\tLoss: 2.9815\tLR: 1.070332\n",
      "Training Epoch: 11 [35456/50000]\tLoss: 3.0373\tLR: 1.070588\n",
      "Training Epoch: 11 [35584/50000]\tLoss: 3.1134\tLR: 1.070844\n",
      "Training Epoch: 11 [35712/50000]\tLoss: 3.0786\tLR: 1.071100\n",
      "Training Epoch: 11 [35840/50000]\tLoss: 3.1864\tLR: 1.071355\n",
      "Training Epoch: 11 [35968/50000]\tLoss: 2.8769\tLR: 1.071611\n",
      "Training Epoch: 11 [36096/50000]\tLoss: 2.9441\tLR: 1.071867\n",
      "Training Epoch: 11 [36224/50000]\tLoss: 2.9623\tLR: 1.072123\n",
      "Training Epoch: 11 [36352/50000]\tLoss: 2.9887\tLR: 1.072379\n",
      "Training Epoch: 11 [36480/50000]\tLoss: 3.1053\tLR: 1.072634\n",
      "Training Epoch: 11 [36608/50000]\tLoss: 3.1640\tLR: 1.072890\n",
      "Training Epoch: 11 [36736/50000]\tLoss: 3.2087\tLR: 1.073146\n",
      "Training Epoch: 11 [36864/50000]\tLoss: 3.1312\tLR: 1.073402\n",
      "Training Epoch: 11 [36992/50000]\tLoss: 3.1424\tLR: 1.073657\n",
      "Training Epoch: 11 [37120/50000]\tLoss: 3.1072\tLR: 1.073913\n",
      "Training Epoch: 11 [37248/50000]\tLoss: 2.8904\tLR: 1.074169\n",
      "Training Epoch: 11 [37376/50000]\tLoss: 3.0549\tLR: 1.074425\n",
      "Training Epoch: 11 [37504/50000]\tLoss: 3.0297\tLR: 1.074680\n",
      "Training Epoch: 11 [37632/50000]\tLoss: 3.3078\tLR: 1.074936\n",
      "Training Epoch: 11 [37760/50000]\tLoss: 3.3100\tLR: 1.075192\n",
      "Training Epoch: 11 [37888/50000]\tLoss: 3.0612\tLR: 1.075448\n",
      "Training Epoch: 11 [38016/50000]\tLoss: 2.8648\tLR: 1.075703\n",
      "Training Epoch: 11 [38144/50000]\tLoss: 2.7574\tLR: 1.075959\n",
      "Training Epoch: 11 [38272/50000]\tLoss: 2.8347\tLR: 1.076215\n",
      "Training Epoch: 11 [38400/50000]\tLoss: 3.1094\tLR: 1.076471\n",
      "Training Epoch: 11 [38528/50000]\tLoss: 2.6928\tLR: 1.076726\n",
      "Training Epoch: 11 [38656/50000]\tLoss: 2.8825\tLR: 1.076982\n",
      "Training Epoch: 11 [38784/50000]\tLoss: 3.0052\tLR: 1.077238\n",
      "Training Epoch: 11 [38912/50000]\tLoss: 2.9193\tLR: 1.077494\n",
      "Training Epoch: 11 [39040/50000]\tLoss: 2.8829\tLR: 1.077749\n",
      "Training Epoch: 11 [39168/50000]\tLoss: 2.8882\tLR: 1.078005\n",
      "Training Epoch: 11 [39296/50000]\tLoss: 2.6602\tLR: 1.078261\n",
      "Training Epoch: 11 [39424/50000]\tLoss: 3.1419\tLR: 1.078517\n",
      "Training Epoch: 11 [39552/50000]\tLoss: 2.8475\tLR: 1.078772\n",
      "Training Epoch: 11 [39680/50000]\tLoss: 3.0745\tLR: 1.079028\n",
      "Training Epoch: 11 [39808/50000]\tLoss: 3.0521\tLR: 1.079284\n",
      "Training Epoch: 11 [39936/50000]\tLoss: 2.7661\tLR: 1.079540\n",
      "Training Epoch: 11 [40064/50000]\tLoss: 3.0606\tLR: 1.079795\n",
      "Training Epoch: 11 [40192/50000]\tLoss: 3.3090\tLR: 1.080051\n",
      "Training Epoch: 11 [40320/50000]\tLoss: 3.0600\tLR: 1.080307\n",
      "Training Epoch: 11 [40448/50000]\tLoss: 2.5431\tLR: 1.080563\n",
      "Training Epoch: 11 [40576/50000]\tLoss: 3.1263\tLR: 1.080818\n",
      "Training Epoch: 11 [40704/50000]\tLoss: 2.9667\tLR: 1.081074\n",
      "Training Epoch: 11 [40832/50000]\tLoss: 3.1598\tLR: 1.081330\n",
      "Training Epoch: 11 [40960/50000]\tLoss: 3.0127\tLR: 1.081586\n",
      "Training Epoch: 11 [41088/50000]\tLoss: 2.9454\tLR: 1.081841\n",
      "Training Epoch: 11 [41216/50000]\tLoss: 2.7542\tLR: 1.082097\n",
      "Training Epoch: 11 [41344/50000]\tLoss: 2.9533\tLR: 1.082353\n",
      "Training Epoch: 11 [41472/50000]\tLoss: 2.9233\tLR: 1.082609\n",
      "Training Epoch: 11 [41600/50000]\tLoss: 2.6427\tLR: 1.082864\n",
      "Training Epoch: 11 [41728/50000]\tLoss: 3.0785\tLR: 1.083120\n",
      "Training Epoch: 11 [41856/50000]\tLoss: 3.1139\tLR: 1.083376\n",
      "Training Epoch: 11 [41984/50000]\tLoss: 2.7845\tLR: 1.083632\n",
      "Training Epoch: 11 [42112/50000]\tLoss: 2.7165\tLR: 1.083887\n",
      "Training Epoch: 11 [42240/50000]\tLoss: 3.0546\tLR: 1.084143\n",
      "Training Epoch: 11 [42368/50000]\tLoss: 3.0845\tLR: 1.084399\n",
      "Training Epoch: 11 [42496/50000]\tLoss: 2.9254\tLR: 1.084655\n",
      "Training Epoch: 11 [42624/50000]\tLoss: 3.0744\tLR: 1.084910\n",
      "Training Epoch: 11 [42752/50000]\tLoss: 2.9881\tLR: 1.085166\n",
      "Training Epoch: 11 [42880/50000]\tLoss: 3.0516\tLR: 1.085422\n",
      "Training Epoch: 11 [43008/50000]\tLoss: 2.9532\tLR: 1.085678\n",
      "Training Epoch: 11 [43136/50000]\tLoss: 3.1710\tLR: 1.085934\n",
      "Training Epoch: 11 [43264/50000]\tLoss: 3.0355\tLR: 1.086189\n",
      "Training Epoch: 11 [43392/50000]\tLoss: 2.9050\tLR: 1.086445\n",
      "Training Epoch: 11 [43520/50000]\tLoss: 2.9341\tLR: 1.086701\n",
      "Training Epoch: 11 [43648/50000]\tLoss: 2.8577\tLR: 1.086957\n",
      "Training Epoch: 11 [43776/50000]\tLoss: 3.0736\tLR: 1.087212\n",
      "Training Epoch: 11 [43904/50000]\tLoss: 2.9592\tLR: 1.087468\n",
      "Training Epoch: 11 [44032/50000]\tLoss: 2.6886\tLR: 1.087724\n",
      "Training Epoch: 11 [44160/50000]\tLoss: 3.0508\tLR: 1.087980\n",
      "Training Epoch: 11 [44288/50000]\tLoss: 3.1207\tLR: 1.088235\n",
      "Training Epoch: 11 [44416/50000]\tLoss: 3.1510\tLR: 1.088491\n",
      "Training Epoch: 11 [44544/50000]\tLoss: 2.8864\tLR: 1.088747\n",
      "Training Epoch: 11 [44672/50000]\tLoss: 2.8965\tLR: 1.089003\n",
      "Training Epoch: 11 [44800/50000]\tLoss: 3.2331\tLR: 1.089258\n",
      "Training Epoch: 11 [44928/50000]\tLoss: 2.9144\tLR: 1.089514\n",
      "Training Epoch: 11 [45056/50000]\tLoss: 2.9265\tLR: 1.089770\n",
      "Training Epoch: 11 [45184/50000]\tLoss: 2.9739\tLR: 1.090026\n",
      "Training Epoch: 11 [45312/50000]\tLoss: 2.9509\tLR: 1.090281\n",
      "Training Epoch: 11 [45440/50000]\tLoss: 2.9365\tLR: 1.090537\n",
      "Training Epoch: 11 [45568/50000]\tLoss: 2.9292\tLR: 1.090793\n",
      "Training Epoch: 11 [45696/50000]\tLoss: 2.8937\tLR: 1.091049\n",
      "Training Epoch: 11 [45824/50000]\tLoss: 3.1428\tLR: 1.091304\n",
      "Training Epoch: 11 [45952/50000]\tLoss: 3.1521\tLR: 1.091560\n",
      "Training Epoch: 11 [46080/50000]\tLoss: 3.1128\tLR: 1.091816\n",
      "Training Epoch: 11 [46208/50000]\tLoss: 2.9667\tLR: 1.092072\n",
      "Training Epoch: 11 [46336/50000]\tLoss: 2.8810\tLR: 1.092327\n",
      "Training Epoch: 11 [46464/50000]\tLoss: 2.9090\tLR: 1.092583\n",
      "Training Epoch: 11 [46592/50000]\tLoss: 2.8586\tLR: 1.092839\n",
      "Training Epoch: 11 [46720/50000]\tLoss: 3.1848\tLR: 1.093095\n",
      "Training Epoch: 11 [46848/50000]\tLoss: 3.0550\tLR: 1.093350\n",
      "Training Epoch: 11 [46976/50000]\tLoss: 3.1206\tLR: 1.093606\n",
      "Training Epoch: 11 [47104/50000]\tLoss: 2.9983\tLR: 1.093862\n",
      "Training Epoch: 11 [47232/50000]\tLoss: 3.0300\tLR: 1.094118\n",
      "Training Epoch: 11 [47360/50000]\tLoss: 3.1961\tLR: 1.094373\n",
      "Training Epoch: 11 [47488/50000]\tLoss: 3.0164\tLR: 1.094629\n",
      "Training Epoch: 11 [47616/50000]\tLoss: 3.2301\tLR: 1.094885\n",
      "Training Epoch: 11 [47744/50000]\tLoss: 3.0121\tLR: 1.095141\n",
      "Training Epoch: 11 [47872/50000]\tLoss: 2.9174\tLR: 1.095396\n",
      "Training Epoch: 11 [48000/50000]\tLoss: 3.1536\tLR: 1.095652\n",
      "Training Epoch: 11 [48128/50000]\tLoss: 3.1250\tLR: 1.095908\n",
      "Training Epoch: 11 [48256/50000]\tLoss: 3.0478\tLR: 1.096164\n",
      "Training Epoch: 11 [48384/50000]\tLoss: 3.1671\tLR: 1.096419\n",
      "Training Epoch: 11 [48512/50000]\tLoss: 3.2895\tLR: 1.096675\n",
      "Training Epoch: 11 [48640/50000]\tLoss: 2.9942\tLR: 1.096931\n",
      "Training Epoch: 11 [48768/50000]\tLoss: 3.0839\tLR: 1.097187\n",
      "Training Epoch: 11 [48896/50000]\tLoss: 2.9478\tLR: 1.097442\n",
      "Training Epoch: 11 [49024/50000]\tLoss: 3.0984\tLR: 1.097698\n",
      "Training Epoch: 11 [49152/50000]\tLoss: 3.0165\tLR: 1.097954\n",
      "Training Epoch: 11 [49280/50000]\tLoss: 3.1438\tLR: 1.098210\n",
      "Training Epoch: 11 [49408/50000]\tLoss: 2.8776\tLR: 1.098465\n",
      "Training Epoch: 11 [49536/50000]\tLoss: 3.2199\tLR: 1.098721\n",
      "Training Epoch: 11 [49664/50000]\tLoss: 3.0360\tLR: 1.098977\n",
      "Training Epoch: 11 [49792/50000]\tLoss: 2.9440\tLR: 1.099233\n",
      "Training Epoch: 11 [49920/50000]\tLoss: 2.8733\tLR: 1.099488\n",
      "Training Epoch: 11 [50000/50000]\tLoss: 3.2909\tLR: 1.099744\n",
      "epoch 11 training time consumed: 489.17s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   15422 GB |   15422 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   15375 GB |   15375 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      47 GB |      47 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   15422 GB |   15422 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   15375 GB |   15375 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      47 GB |      47 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   15202 GB |   15202 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   15155 GB |   15155 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |      47 GB |      47 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    1635 K  |    1635 K  |\n",
      "|       from large pool |      24    |      65    |     697 K  |     697 K  |\n",
      "|       from small pool |     231    |     274    |     938 K  |     938 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    1635 K  |    1635 K  |\n",
      "|       from large pool |      24    |      65    |     697 K  |     697 K  |\n",
      "|       from small pool |     231    |     274    |     938 K  |     938 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      45    |     947 K  |     947 K  |\n",
      "|       from large pool |      10    |      23    |     335 K  |     335 K  |\n",
      "|       from small pool |      27    |      35    |     612 K  |     612 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 11, Average loss: 0.0405, Accuracy: 0.1196, Time consumed:30.78s\n",
      "\n",
      "Training Epoch: 12 [128/50000]\tLoss: 3.3296\tLR: 0.100000\n",
      "Training Epoch: 12 [256/50000]\tLoss: 3.0365\tLR: 1.100256\n",
      "Training Epoch: 12 [384/50000]\tLoss: 3.1210\tLR: 1.100512\n",
      "Training Epoch: 12 [512/50000]\tLoss: 2.8865\tLR: 1.100767\n",
      "Training Epoch: 12 [640/50000]\tLoss: 2.8869\tLR: 1.101023\n",
      "Training Epoch: 12 [768/50000]\tLoss: 3.3373\tLR: 1.101279\n",
      "Training Epoch: 12 [896/50000]\tLoss: 3.0625\tLR: 1.101535\n",
      "Training Epoch: 12 [1024/50000]\tLoss: 2.9915\tLR: 1.101790\n",
      "Training Epoch: 12 [1152/50000]\tLoss: 2.9311\tLR: 1.102046\n",
      "Training Epoch: 12 [1280/50000]\tLoss: 2.8822\tLR: 1.102302\n",
      "Training Epoch: 12 [1408/50000]\tLoss: 2.7636\tLR: 1.102558\n",
      "Training Epoch: 12 [1536/50000]\tLoss: 2.7340\tLR: 1.102813\n",
      "Training Epoch: 12 [1664/50000]\tLoss: 2.9535\tLR: 1.103069\n",
      "Training Epoch: 12 [1792/50000]\tLoss: 3.1024\tLR: 1.103325\n",
      "Training Epoch: 12 [1920/50000]\tLoss: 2.9110\tLR: 1.103581\n",
      "Training Epoch: 12 [2048/50000]\tLoss: 2.8906\tLR: 1.103836\n",
      "Training Epoch: 12 [2176/50000]\tLoss: 3.1965\tLR: 1.104092\n",
      "Training Epoch: 12 [2304/50000]\tLoss: 3.0050\tLR: 1.104348\n",
      "Training Epoch: 12 [2432/50000]\tLoss: 2.8271\tLR: 1.104604\n",
      "Training Epoch: 12 [2560/50000]\tLoss: 2.7923\tLR: 1.104859\n",
      "Training Epoch: 12 [2688/50000]\tLoss: 2.7823\tLR: 1.105115\n",
      "Training Epoch: 12 [2816/50000]\tLoss: 2.9903\tLR: 1.105371\n",
      "Training Epoch: 12 [2944/50000]\tLoss: 2.8789\tLR: 1.105627\n",
      "Training Epoch: 12 [3072/50000]\tLoss: 2.8657\tLR: 1.105882\n",
      "Training Epoch: 12 [3200/50000]\tLoss: 3.0047\tLR: 1.106138\n",
      "Training Epoch: 12 [3328/50000]\tLoss: 3.2025\tLR: 1.106394\n",
      "Training Epoch: 12 [3456/50000]\tLoss: 2.8962\tLR: 1.106650\n",
      "Training Epoch: 12 [3584/50000]\tLoss: 2.8850\tLR: 1.106905\n",
      "Training Epoch: 12 [3712/50000]\tLoss: 3.0470\tLR: 1.107161\n",
      "Training Epoch: 12 [3840/50000]\tLoss: 3.1227\tLR: 1.107417\n",
      "Training Epoch: 12 [3968/50000]\tLoss: 2.9826\tLR: 1.107673\n",
      "Training Epoch: 12 [4096/50000]\tLoss: 3.0096\tLR: 1.107928\n",
      "Training Epoch: 12 [4224/50000]\tLoss: 3.1031\tLR: 1.108184\n",
      "Training Epoch: 12 [4352/50000]\tLoss: 3.1166\tLR: 1.108440\n",
      "Training Epoch: 12 [4480/50000]\tLoss: 2.7922\tLR: 1.108696\n",
      "Training Epoch: 12 [4608/50000]\tLoss: 2.9801\tLR: 1.108951\n",
      "Training Epoch: 12 [4736/50000]\tLoss: 2.9292\tLR: 1.109207\n",
      "Training Epoch: 12 [4864/50000]\tLoss: 3.0648\tLR: 1.109463\n",
      "Training Epoch: 12 [4992/50000]\tLoss: 3.2507\tLR: 1.109719\n",
      "Training Epoch: 12 [5120/50000]\tLoss: 3.3563\tLR: 1.109974\n",
      "Training Epoch: 12 [5248/50000]\tLoss: 3.0755\tLR: 1.110230\n",
      "Training Epoch: 12 [5376/50000]\tLoss: 2.9681\tLR: 1.110486\n",
      "Training Epoch: 12 [5504/50000]\tLoss: 2.9657\tLR: 1.110742\n",
      "Training Epoch: 12 [5632/50000]\tLoss: 2.6824\tLR: 1.110997\n",
      "Training Epoch: 12 [5760/50000]\tLoss: 3.1959\tLR: 1.111253\n",
      "Training Epoch: 12 [5888/50000]\tLoss: 3.0782\tLR: 1.111509\n",
      "Training Epoch: 12 [6016/50000]\tLoss: 2.8454\tLR: 1.111765\n",
      "Training Epoch: 12 [6144/50000]\tLoss: 3.0686\tLR: 1.112020\n",
      "Training Epoch: 12 [6272/50000]\tLoss: 2.9478\tLR: 1.112276\n",
      "Training Epoch: 12 [6400/50000]\tLoss: 3.0518\tLR: 1.112532\n",
      "Training Epoch: 12 [6528/50000]\tLoss: 2.9548\tLR: 1.112788\n",
      "Training Epoch: 12 [6656/50000]\tLoss: 2.5020\tLR: 1.113043\n",
      "Training Epoch: 12 [6784/50000]\tLoss: 3.0845\tLR: 1.113299\n",
      "Training Epoch: 12 [6912/50000]\tLoss: 3.2118\tLR: 1.113555\n",
      "Training Epoch: 12 [7040/50000]\tLoss: 2.7097\tLR: 1.113811\n",
      "Training Epoch: 12 [7168/50000]\tLoss: 2.8786\tLR: 1.114066\n",
      "Training Epoch: 12 [7296/50000]\tLoss: 2.9811\tLR: 1.114322\n",
      "Training Epoch: 12 [7424/50000]\tLoss: 3.0677\tLR: 1.114578\n",
      "Training Epoch: 12 [7552/50000]\tLoss: 2.8488\tLR: 1.114834\n",
      "Training Epoch: 12 [7680/50000]\tLoss: 2.7719\tLR: 1.115090\n",
      "Training Epoch: 12 [7808/50000]\tLoss: 2.9614\tLR: 1.115345\n",
      "Training Epoch: 12 [7936/50000]\tLoss: 3.1747\tLR: 1.115601\n",
      "Training Epoch: 12 [8064/50000]\tLoss: 2.8193\tLR: 1.115857\n",
      "Training Epoch: 12 [8192/50000]\tLoss: 2.8654\tLR: 1.116113\n",
      "Training Epoch: 12 [8320/50000]\tLoss: 2.9293\tLR: 1.116368\n",
      "Training Epoch: 12 [8448/50000]\tLoss: 2.9171\tLR: 1.116624\n",
      "Training Epoch: 12 [8576/50000]\tLoss: 3.0372\tLR: 1.116880\n",
      "Training Epoch: 12 [8704/50000]\tLoss: 2.8037\tLR: 1.117136\n",
      "Training Epoch: 12 [8832/50000]\tLoss: 3.0265\tLR: 1.117391\n",
      "Training Epoch: 12 [8960/50000]\tLoss: 2.8047\tLR: 1.117647\n",
      "Training Epoch: 12 [9088/50000]\tLoss: 2.9983\tLR: 1.117903\n",
      "Training Epoch: 12 [9216/50000]\tLoss: 2.9913\tLR: 1.118159\n",
      "Training Epoch: 12 [9344/50000]\tLoss: 2.9781\tLR: 1.118414\n",
      "Training Epoch: 12 [9472/50000]\tLoss: 2.9624\tLR: 1.118670\n",
      "Training Epoch: 12 [9600/50000]\tLoss: 3.0106\tLR: 1.118926\n",
      "Training Epoch: 12 [9728/50000]\tLoss: 3.2855\tLR: 1.119182\n",
      "Training Epoch: 12 [9856/50000]\tLoss: 2.9183\tLR: 1.119437\n",
      "Training Epoch: 12 [9984/50000]\tLoss: 3.0629\tLR: 1.119693\n",
      "Training Epoch: 12 [10112/50000]\tLoss: 2.8388\tLR: 1.119949\n",
      "Training Epoch: 12 [10240/50000]\tLoss: 3.0369\tLR: 1.120205\n",
      "Training Epoch: 12 [10368/50000]\tLoss: 3.1214\tLR: 1.120460\n",
      "Training Epoch: 12 [10496/50000]\tLoss: 3.1866\tLR: 1.120716\n",
      "Training Epoch: 12 [10624/50000]\tLoss: 2.7916\tLR: 1.120972\n",
      "Training Epoch: 12 [10752/50000]\tLoss: 3.1697\tLR: 1.121228\n",
      "Training Epoch: 12 [10880/50000]\tLoss: 2.8100\tLR: 1.121483\n",
      "Training Epoch: 12 [11008/50000]\tLoss: 3.1935\tLR: 1.121739\n",
      "Training Epoch: 12 [11136/50000]\tLoss: 3.2986\tLR: 1.121995\n",
      "Training Epoch: 12 [11264/50000]\tLoss: 3.0672\tLR: 1.122251\n",
      "Training Epoch: 12 [11392/50000]\tLoss: 3.2251\tLR: 1.122506\n",
      "Training Epoch: 12 [11520/50000]\tLoss: 3.1151\tLR: 1.122762\n",
      "Training Epoch: 12 [11648/50000]\tLoss: 3.0005\tLR: 1.123018\n",
      "Training Epoch: 12 [11776/50000]\tLoss: 3.0291\tLR: 1.123274\n",
      "Training Epoch: 12 [11904/50000]\tLoss: 3.2310\tLR: 1.123529\n",
      "Training Epoch: 12 [12032/50000]\tLoss: 3.2399\tLR: 1.123785\n",
      "Training Epoch: 12 [12160/50000]\tLoss: 3.0802\tLR: 1.124041\n",
      "Training Epoch: 12 [12288/50000]\tLoss: 2.7426\tLR: 1.124297\n",
      "Training Epoch: 12 [12416/50000]\tLoss: 2.9157\tLR: 1.124552\n",
      "Training Epoch: 12 [12544/50000]\tLoss: 2.9816\tLR: 1.124808\n",
      "Training Epoch: 12 [12672/50000]\tLoss: 3.1037\tLR: 1.125064\n",
      "Training Epoch: 12 [12800/50000]\tLoss: 2.8941\tLR: 1.125320\n",
      "Training Epoch: 12 [12928/50000]\tLoss: 3.2211\tLR: 1.125575\n",
      "Training Epoch: 12 [13056/50000]\tLoss: 2.9200\tLR: 1.125831\n",
      "Training Epoch: 12 [13184/50000]\tLoss: 2.9473\tLR: 1.126087\n",
      "Training Epoch: 12 [13312/50000]\tLoss: 3.0251\tLR: 1.126343\n",
      "Training Epoch: 12 [13440/50000]\tLoss: 2.9918\tLR: 1.126598\n",
      "Training Epoch: 12 [13568/50000]\tLoss: 2.9458\tLR: 1.126854\n",
      "Training Epoch: 12 [13696/50000]\tLoss: 2.8476\tLR: 1.127110\n",
      "Training Epoch: 12 [13824/50000]\tLoss: 2.8521\tLR: 1.127366\n",
      "Training Epoch: 12 [13952/50000]\tLoss: 3.3766\tLR: 1.127621\n",
      "Training Epoch: 12 [14080/50000]\tLoss: 2.9321\tLR: 1.127877\n",
      "Training Epoch: 12 [14208/50000]\tLoss: 3.0748\tLR: 1.128133\n",
      "Training Epoch: 12 [14336/50000]\tLoss: 2.7655\tLR: 1.128389\n",
      "Training Epoch: 12 [14464/50000]\tLoss: 3.1592\tLR: 1.128645\n",
      "Training Epoch: 12 [14592/50000]\tLoss: 3.0637\tLR: 1.128900\n",
      "Training Epoch: 12 [14720/50000]\tLoss: 2.9023\tLR: 1.129156\n",
      "Training Epoch: 12 [14848/50000]\tLoss: 2.8693\tLR: 1.129412\n",
      "Training Epoch: 12 [14976/50000]\tLoss: 3.1317\tLR: 1.129668\n",
      "Training Epoch: 12 [15104/50000]\tLoss: 3.0935\tLR: 1.129923\n",
      "Training Epoch: 12 [15232/50000]\tLoss: 3.1969\tLR: 1.130179\n",
      "Training Epoch: 12 [15360/50000]\tLoss: 2.8784\tLR: 1.130435\n",
      "Training Epoch: 12 [15488/50000]\tLoss: 3.0019\tLR: 1.130691\n",
      "Training Epoch: 12 [15616/50000]\tLoss: 2.9822\tLR: 1.130946\n",
      "Training Epoch: 12 [15744/50000]\tLoss: 3.1736\tLR: 1.131202\n",
      "Training Epoch: 12 [15872/50000]\tLoss: 3.0123\tLR: 1.131458\n",
      "Training Epoch: 12 [16000/50000]\tLoss: 3.0566\tLR: 1.131714\n",
      "Training Epoch: 12 [16128/50000]\tLoss: 3.1167\tLR: 1.131969\n",
      "Training Epoch: 12 [16256/50000]\tLoss: 2.8914\tLR: 1.132225\n",
      "Training Epoch: 12 [16384/50000]\tLoss: 2.8274\tLR: 1.132481\n",
      "Training Epoch: 12 [16512/50000]\tLoss: 3.1770\tLR: 1.132737\n",
      "Training Epoch: 12 [16640/50000]\tLoss: 3.0834\tLR: 1.132992\n",
      "Training Epoch: 12 [16768/50000]\tLoss: 3.1337\tLR: 1.133248\n",
      "Training Epoch: 12 [16896/50000]\tLoss: 2.9153\tLR: 1.133504\n",
      "Training Epoch: 12 [17024/50000]\tLoss: 3.0689\tLR: 1.133760\n",
      "Training Epoch: 12 [17152/50000]\tLoss: 3.1862\tLR: 1.134015\n",
      "Training Epoch: 12 [17280/50000]\tLoss: 2.9330\tLR: 1.134271\n",
      "Training Epoch: 12 [17408/50000]\tLoss: 3.0271\tLR: 1.134527\n",
      "Training Epoch: 12 [17536/50000]\tLoss: 3.0612\tLR: 1.134783\n",
      "Training Epoch: 12 [17664/50000]\tLoss: 3.3258\tLR: 1.135038\n",
      "Training Epoch: 12 [17792/50000]\tLoss: 3.2628\tLR: 1.135294\n",
      "Training Epoch: 12 [17920/50000]\tLoss: 3.1076\tLR: 1.135550\n",
      "Training Epoch: 12 [18048/50000]\tLoss: 3.0674\tLR: 1.135806\n",
      "Training Epoch: 12 [18176/50000]\tLoss: 3.0738\tLR: 1.136061\n",
      "Training Epoch: 12 [18304/50000]\tLoss: 3.0658\tLR: 1.136317\n",
      "Training Epoch: 12 [18432/50000]\tLoss: 3.1112\tLR: 1.136573\n",
      "Training Epoch: 12 [18560/50000]\tLoss: 3.0387\tLR: 1.136829\n",
      "Training Epoch: 12 [18688/50000]\tLoss: 3.1014\tLR: 1.137084\n",
      "Training Epoch: 12 [18816/50000]\tLoss: 3.0589\tLR: 1.137340\n",
      "Training Epoch: 12 [18944/50000]\tLoss: 3.3190\tLR: 1.137596\n",
      "Training Epoch: 12 [19072/50000]\tLoss: 3.1642\tLR: 1.137852\n",
      "Training Epoch: 12 [19200/50000]\tLoss: 2.9000\tLR: 1.138107\n",
      "Training Epoch: 12 [19328/50000]\tLoss: 3.0253\tLR: 1.138363\n",
      "Training Epoch: 12 [19456/50000]\tLoss: 2.9737\tLR: 1.138619\n",
      "Training Epoch: 12 [19584/50000]\tLoss: 3.2400\tLR: 1.138875\n",
      "Training Epoch: 12 [19712/50000]\tLoss: 3.1467\tLR: 1.139130\n",
      "Training Epoch: 12 [19840/50000]\tLoss: 2.9599\tLR: 1.139386\n",
      "Training Epoch: 12 [19968/50000]\tLoss: 2.9404\tLR: 1.139642\n",
      "Training Epoch: 12 [20096/50000]\tLoss: 3.5608\tLR: 1.139898\n",
      "Training Epoch: 12 [20224/50000]\tLoss: 3.0886\tLR: 1.140153\n",
      "Training Epoch: 12 [20352/50000]\tLoss: 3.2821\tLR: 1.140409\n",
      "Training Epoch: 12 [20480/50000]\tLoss: 2.8472\tLR: 1.140665\n",
      "Training Epoch: 12 [20608/50000]\tLoss: 3.0317\tLR: 1.140921\n",
      "Training Epoch: 12 [20736/50000]\tLoss: 3.0831\tLR: 1.141176\n",
      "Training Epoch: 12 [20864/50000]\tLoss: 3.2163\tLR: 1.141432\n",
      "Training Epoch: 12 [20992/50000]\tLoss: 3.0979\tLR: 1.141688\n",
      "Training Epoch: 12 [21120/50000]\tLoss: 2.8303\tLR: 1.141944\n",
      "Training Epoch: 12 [21248/50000]\tLoss: 2.9767\tLR: 1.142199\n",
      "Training Epoch: 12 [21376/50000]\tLoss: 3.0345\tLR: 1.142455\n",
      "Training Epoch: 12 [21504/50000]\tLoss: 3.0136\tLR: 1.142711\n",
      "Training Epoch: 12 [21632/50000]\tLoss: 3.2327\tLR: 1.142967\n",
      "Training Epoch: 12 [21760/50000]\tLoss: 2.8154\tLR: 1.143223\n",
      "Training Epoch: 12 [21888/50000]\tLoss: 3.0174\tLR: 1.143478\n",
      "Training Epoch: 12 [22016/50000]\tLoss: 3.1417\tLR: 1.143734\n",
      "Training Epoch: 12 [22144/50000]\tLoss: 2.9281\tLR: 1.143990\n",
      "Training Epoch: 12 [22272/50000]\tLoss: 2.8197\tLR: 1.144246\n",
      "Training Epoch: 12 [22400/50000]\tLoss: 2.9034\tLR: 1.144501\n",
      "Training Epoch: 12 [22528/50000]\tLoss: 3.1537\tLR: 1.144757\n",
      "Training Epoch: 12 [22656/50000]\tLoss: 3.1279\tLR: 1.145013\n",
      "Training Epoch: 12 [22784/50000]\tLoss: 3.0168\tLR: 1.145269\n",
      "Training Epoch: 12 [22912/50000]\tLoss: 2.9953\tLR: 1.145524\n",
      "Training Epoch: 12 [23040/50000]\tLoss: 3.0750\tLR: 1.145780\n",
      "Training Epoch: 12 [23168/50000]\tLoss: 2.6744\tLR: 1.146036\n",
      "Training Epoch: 12 [23296/50000]\tLoss: 2.7785\tLR: 1.146292\n",
      "Training Epoch: 12 [23424/50000]\tLoss: 2.9195\tLR: 1.146547\n",
      "Training Epoch: 12 [23552/50000]\tLoss: 3.1161\tLR: 1.146803\n",
      "Training Epoch: 12 [23680/50000]\tLoss: 2.9806\tLR: 1.147059\n",
      "Training Epoch: 12 [23808/50000]\tLoss: 3.0054\tLR: 1.147315\n",
      "Training Epoch: 12 [23936/50000]\tLoss: 3.3805\tLR: 1.147570\n",
      "Training Epoch: 12 [24064/50000]\tLoss: 2.9972\tLR: 1.147826\n",
      "Training Epoch: 12 [24192/50000]\tLoss: 2.9599\tLR: 1.148082\n",
      "Training Epoch: 12 [24320/50000]\tLoss: 2.8990\tLR: 1.148338\n",
      "Training Epoch: 12 [24448/50000]\tLoss: 2.9716\tLR: 1.148593\n",
      "Training Epoch: 12 [24576/50000]\tLoss: 3.0904\tLR: 1.148849\n",
      "Training Epoch: 12 [24704/50000]\tLoss: 3.1470\tLR: 1.149105\n",
      "Training Epoch: 12 [24832/50000]\tLoss: 2.7395\tLR: 1.149361\n",
      "Training Epoch: 12 [24960/50000]\tLoss: 3.0488\tLR: 1.149616\n",
      "Training Epoch: 12 [25088/50000]\tLoss: 3.1218\tLR: 1.149872\n",
      "Training Epoch: 12 [25216/50000]\tLoss: 3.1932\tLR: 1.150128\n",
      "Training Epoch: 12 [25344/50000]\tLoss: 3.1312\tLR: 1.150384\n",
      "Training Epoch: 12 [25472/50000]\tLoss: 3.1032\tLR: 1.150639\n",
      "Training Epoch: 12 [25600/50000]\tLoss: 3.3915\tLR: 1.150895\n",
      "Training Epoch: 12 [25728/50000]\tLoss: 2.9111\tLR: 1.151151\n",
      "Training Epoch: 12 [25856/50000]\tLoss: 2.6665\tLR: 1.151407\n",
      "Training Epoch: 12 [25984/50000]\tLoss: 3.0219\tLR: 1.151662\n",
      "Training Epoch: 12 [26112/50000]\tLoss: 3.2823\tLR: 1.151918\n",
      "Training Epoch: 12 [26240/50000]\tLoss: 3.0461\tLR: 1.152174\n",
      "Training Epoch: 12 [26368/50000]\tLoss: 2.9767\tLR: 1.152430\n",
      "Training Epoch: 12 [26496/50000]\tLoss: 3.3221\tLR: 1.152685\n",
      "Training Epoch: 12 [26624/50000]\tLoss: 3.0634\tLR: 1.152941\n",
      "Training Epoch: 12 [26752/50000]\tLoss: 2.9274\tLR: 1.153197\n",
      "Training Epoch: 12 [26880/50000]\tLoss: 3.0788\tLR: 1.153453\n",
      "Training Epoch: 12 [27008/50000]\tLoss: 3.2607\tLR: 1.153708\n",
      "Training Epoch: 12 [27136/50000]\tLoss: 3.1106\tLR: 1.153964\n",
      "Training Epoch: 12 [27264/50000]\tLoss: 3.1128\tLR: 1.154220\n",
      "Training Epoch: 12 [27392/50000]\tLoss: 2.8957\tLR: 1.154476\n",
      "Training Epoch: 12 [27520/50000]\tLoss: 2.9915\tLR: 1.154731\n",
      "Training Epoch: 12 [27648/50000]\tLoss: 2.8237\tLR: 1.154987\n",
      "Training Epoch: 12 [27776/50000]\tLoss: 2.9081\tLR: 1.155243\n",
      "Training Epoch: 12 [27904/50000]\tLoss: 3.2733\tLR: 1.155499\n",
      "Training Epoch: 12 [28032/50000]\tLoss: 3.3457\tLR: 1.155754\n",
      "Training Epoch: 12 [28160/50000]\tLoss: 3.0370\tLR: 1.156010\n",
      "Training Epoch: 12 [28288/50000]\tLoss: 3.1096\tLR: 1.156266\n",
      "Training Epoch: 12 [28416/50000]\tLoss: 3.1309\tLR: 1.156522\n",
      "Training Epoch: 12 [28544/50000]\tLoss: 3.2051\tLR: 1.156777\n",
      "Training Epoch: 12 [28672/50000]\tLoss: 3.2532\tLR: 1.157033\n",
      "Training Epoch: 12 [28800/50000]\tLoss: 3.1427\tLR: 1.157289\n",
      "Training Epoch: 12 [28928/50000]\tLoss: 2.9852\tLR: 1.157545\n",
      "Training Epoch: 12 [29056/50000]\tLoss: 3.3193\tLR: 1.157801\n",
      "Training Epoch: 12 [29184/50000]\tLoss: 3.1044\tLR: 1.158056\n",
      "Training Epoch: 12 [29312/50000]\tLoss: 3.2372\tLR: 1.158312\n",
      "Training Epoch: 12 [29440/50000]\tLoss: 2.9747\tLR: 1.158568\n",
      "Training Epoch: 12 [29568/50000]\tLoss: 3.0101\tLR: 1.158824\n",
      "Training Epoch: 12 [29696/50000]\tLoss: 3.0873\tLR: 1.159079\n",
      "Training Epoch: 12 [29824/50000]\tLoss: 3.0339\tLR: 1.159335\n",
      "Training Epoch: 12 [29952/50000]\tLoss: 3.1211\tLR: 1.159591\n",
      "Training Epoch: 12 [30080/50000]\tLoss: 3.0991\tLR: 1.159847\n",
      "Training Epoch: 12 [30208/50000]\tLoss: 2.7275\tLR: 1.160102\n",
      "Training Epoch: 12 [30336/50000]\tLoss: 3.0066\tLR: 1.160358\n",
      "Training Epoch: 12 [30464/50000]\tLoss: 2.9849\tLR: 1.160614\n",
      "Training Epoch: 12 [30592/50000]\tLoss: 2.5927\tLR: 1.160870\n",
      "Training Epoch: 12 [30720/50000]\tLoss: 2.9825\tLR: 1.161125\n",
      "Training Epoch: 12 [30848/50000]\tLoss: 2.9775\tLR: 1.161381\n",
      "Training Epoch: 12 [30976/50000]\tLoss: 3.1114\tLR: 1.161637\n",
      "Training Epoch: 12 [31104/50000]\tLoss: 2.9131\tLR: 1.161893\n",
      "Training Epoch: 12 [31232/50000]\tLoss: 2.9656\tLR: 1.162148\n",
      "Training Epoch: 12 [31360/50000]\tLoss: 3.1329\tLR: 1.162404\n",
      "Training Epoch: 12 [31488/50000]\tLoss: 2.9297\tLR: 1.162660\n",
      "Training Epoch: 12 [31616/50000]\tLoss: 3.2025\tLR: 1.162916\n",
      "Training Epoch: 12 [31744/50000]\tLoss: 3.0055\tLR: 1.163171\n",
      "Training Epoch: 12 [31872/50000]\tLoss: 3.0828\tLR: 1.163427\n",
      "Training Epoch: 12 [32000/50000]\tLoss: 3.0140\tLR: 1.163683\n",
      "Training Epoch: 12 [32128/50000]\tLoss: 3.0406\tLR: 1.163939\n",
      "Training Epoch: 12 [32256/50000]\tLoss: 3.0243\tLR: 1.164194\n",
      "Training Epoch: 12 [32384/50000]\tLoss: 2.8432\tLR: 1.164450\n",
      "Training Epoch: 12 [32512/50000]\tLoss: 3.2195\tLR: 1.164706\n",
      "Training Epoch: 12 [32640/50000]\tLoss: 3.1647\tLR: 1.164962\n",
      "Training Epoch: 12 [32768/50000]\tLoss: 2.8641\tLR: 1.165217\n",
      "Training Epoch: 12 [32896/50000]\tLoss: 3.0237\tLR: 1.165473\n",
      "Training Epoch: 12 [33024/50000]\tLoss: 3.1526\tLR: 1.165729\n",
      "Training Epoch: 12 [33152/50000]\tLoss: 3.1404\tLR: 1.165985\n",
      "Training Epoch: 12 [33280/50000]\tLoss: 3.4641\tLR: 1.166240\n",
      "Training Epoch: 12 [33408/50000]\tLoss: 3.3671\tLR: 1.166496\n",
      "Training Epoch: 12 [33536/50000]\tLoss: 3.1873\tLR: 1.166752\n",
      "Training Epoch: 12 [33664/50000]\tLoss: 3.0259\tLR: 1.167008\n",
      "Training Epoch: 12 [33792/50000]\tLoss: 3.0735\tLR: 1.167263\n",
      "Training Epoch: 12 [33920/50000]\tLoss: 2.9903\tLR: 1.167519\n",
      "Training Epoch: 12 [34048/50000]\tLoss: 3.1691\tLR: 1.167775\n",
      "Training Epoch: 12 [34176/50000]\tLoss: 3.4441\tLR: 1.168031\n",
      "Training Epoch: 12 [34304/50000]\tLoss: 3.1325\tLR: 1.168286\n",
      "Training Epoch: 12 [34432/50000]\tLoss: 3.2709\tLR: 1.168542\n",
      "Training Epoch: 12 [34560/50000]\tLoss: 3.1492\tLR: 1.168798\n",
      "Training Epoch: 12 [34688/50000]\tLoss: 2.9198\tLR: 1.169054\n",
      "Training Epoch: 12 [34816/50000]\tLoss: 3.0283\tLR: 1.169309\n",
      "Training Epoch: 12 [34944/50000]\tLoss: 3.1061\tLR: 1.169565\n",
      "Training Epoch: 12 [35072/50000]\tLoss: 3.1340\tLR: 1.169821\n",
      "Training Epoch: 12 [35200/50000]\tLoss: 3.1127\tLR: 1.170077\n",
      "Training Epoch: 12 [35328/50000]\tLoss: 2.9436\tLR: 1.170332\n",
      "Training Epoch: 12 [35456/50000]\tLoss: 3.1810\tLR: 1.170588\n",
      "Training Epoch: 12 [35584/50000]\tLoss: 3.1050\tLR: 1.170844\n",
      "Training Epoch: 12 [35712/50000]\tLoss: 3.1139\tLR: 1.171100\n",
      "Training Epoch: 12 [35840/50000]\tLoss: 3.2433\tLR: 1.171355\n",
      "Training Epoch: 12 [35968/50000]\tLoss: 2.8588\tLR: 1.171611\n",
      "Training Epoch: 12 [36096/50000]\tLoss: 2.8668\tLR: 1.171867\n",
      "Training Epoch: 12 [36224/50000]\tLoss: 3.1429\tLR: 1.172123\n",
      "Training Epoch: 12 [36352/50000]\tLoss: 3.2555\tLR: 1.172379\n",
      "Training Epoch: 12 [36480/50000]\tLoss: 2.8721\tLR: 1.172634\n",
      "Training Epoch: 12 [36608/50000]\tLoss: 2.9829\tLR: 1.172890\n",
      "Training Epoch: 12 [36736/50000]\tLoss: 2.9424\tLR: 1.173146\n",
      "Training Epoch: 12 [36864/50000]\tLoss: 3.2009\tLR: 1.173402\n",
      "Training Epoch: 12 [36992/50000]\tLoss: 3.1543\tLR: 1.173657\n",
      "Training Epoch: 12 [37120/50000]\tLoss: 3.1183\tLR: 1.173913\n",
      "Training Epoch: 12 [37248/50000]\tLoss: 2.8357\tLR: 1.174169\n",
      "Training Epoch: 12 [37376/50000]\tLoss: 3.1563\tLR: 1.174425\n",
      "Training Epoch: 12 [37504/50000]\tLoss: 3.1181\tLR: 1.174680\n",
      "Training Epoch: 12 [37632/50000]\tLoss: 3.2343\tLR: 1.174936\n",
      "Training Epoch: 12 [37760/50000]\tLoss: 3.0578\tLR: 1.175192\n",
      "Training Epoch: 12 [37888/50000]\tLoss: 3.1372\tLR: 1.175448\n",
      "Training Epoch: 12 [38016/50000]\tLoss: 3.1393\tLR: 1.175703\n",
      "Training Epoch: 12 [38144/50000]\tLoss: 2.9737\tLR: 1.175959\n",
      "Training Epoch: 12 [38272/50000]\tLoss: 3.2299\tLR: 1.176215\n",
      "Training Epoch: 12 [38400/50000]\tLoss: 3.1061\tLR: 1.176471\n",
      "Training Epoch: 12 [38528/50000]\tLoss: 3.1915\tLR: 1.176726\n",
      "Training Epoch: 12 [38656/50000]\tLoss: 3.2706\tLR: 1.176982\n",
      "Training Epoch: 12 [38784/50000]\tLoss: 3.0116\tLR: 1.177238\n",
      "Training Epoch: 12 [38912/50000]\tLoss: 3.0453\tLR: 1.177494\n",
      "Training Epoch: 12 [39040/50000]\tLoss: 3.3058\tLR: 1.177749\n",
      "Training Epoch: 12 [39168/50000]\tLoss: 3.0908\tLR: 1.178005\n",
      "Training Epoch: 12 [39296/50000]\tLoss: 3.0641\tLR: 1.178261\n",
      "Training Epoch: 12 [39424/50000]\tLoss: 2.9018\tLR: 1.178517\n",
      "Training Epoch: 12 [39552/50000]\tLoss: 3.2082\tLR: 1.178772\n",
      "Training Epoch: 12 [39680/50000]\tLoss: 2.8959\tLR: 1.179028\n",
      "Training Epoch: 12 [39808/50000]\tLoss: 2.7094\tLR: 1.179284\n",
      "Training Epoch: 12 [39936/50000]\tLoss: 3.1647\tLR: 1.179540\n",
      "Training Epoch: 12 [40064/50000]\tLoss: 3.1969\tLR: 1.179795\n",
      "Training Epoch: 12 [40192/50000]\tLoss: 3.0897\tLR: 1.180051\n",
      "Training Epoch: 12 [40320/50000]\tLoss: 2.8496\tLR: 1.180307\n",
      "Training Epoch: 12 [40448/50000]\tLoss: 3.1429\tLR: 1.180563\n",
      "Training Epoch: 12 [40576/50000]\tLoss: 2.9996\tLR: 1.180818\n",
      "Training Epoch: 12 [40704/50000]\tLoss: 3.2855\tLR: 1.181074\n",
      "Training Epoch: 12 [40832/50000]\tLoss: 3.1250\tLR: 1.181330\n",
      "Training Epoch: 12 [40960/50000]\tLoss: 3.1068\tLR: 1.181586\n",
      "Training Epoch: 12 [41088/50000]\tLoss: 3.0456\tLR: 1.181841\n",
      "Training Epoch: 12 [41216/50000]\tLoss: 2.9038\tLR: 1.182097\n",
      "Training Epoch: 12 [41344/50000]\tLoss: 2.9789\tLR: 1.182353\n",
      "Training Epoch: 12 [41472/50000]\tLoss: 3.0886\tLR: 1.182609\n",
      "Training Epoch: 12 [41600/50000]\tLoss: 3.1917\tLR: 1.182864\n",
      "Training Epoch: 12 [41728/50000]\tLoss: 3.2625\tLR: 1.183120\n",
      "Training Epoch: 12 [41856/50000]\tLoss: 2.9413\tLR: 1.183376\n",
      "Training Epoch: 12 [41984/50000]\tLoss: 2.8501\tLR: 1.183632\n",
      "Training Epoch: 12 [42112/50000]\tLoss: 2.8597\tLR: 1.183887\n",
      "Training Epoch: 12 [42240/50000]\tLoss: 3.1004\tLR: 1.184143\n",
      "Training Epoch: 12 [42368/50000]\tLoss: 2.7840\tLR: 1.184399\n",
      "Training Epoch: 12 [42496/50000]\tLoss: 2.9092\tLR: 1.184655\n",
      "Training Epoch: 12 [42624/50000]\tLoss: 3.0743\tLR: 1.184910\n",
      "Training Epoch: 12 [42752/50000]\tLoss: 3.3529\tLR: 1.185166\n",
      "Training Epoch: 12 [42880/50000]\tLoss: 3.0658\tLR: 1.185422\n",
      "Training Epoch: 12 [43008/50000]\tLoss: 3.1147\tLR: 1.185678\n",
      "Training Epoch: 12 [43136/50000]\tLoss: 2.9558\tLR: 1.185934\n",
      "Training Epoch: 12 [43264/50000]\tLoss: 2.9442\tLR: 1.186189\n",
      "Training Epoch: 12 [43392/50000]\tLoss: 3.1328\tLR: 1.186445\n",
      "Training Epoch: 12 [43520/50000]\tLoss: 3.0872\tLR: 1.186701\n",
      "Training Epoch: 12 [43648/50000]\tLoss: 3.3519\tLR: 1.186957\n",
      "Training Epoch: 12 [43776/50000]\tLoss: 3.2481\tLR: 1.187212\n",
      "Training Epoch: 12 [43904/50000]\tLoss: 3.0964\tLR: 1.187468\n",
      "Training Epoch: 12 [44032/50000]\tLoss: 2.8955\tLR: 1.187724\n",
      "Training Epoch: 12 [44160/50000]\tLoss: 2.8580\tLR: 1.187980\n",
      "Training Epoch: 12 [44288/50000]\tLoss: 3.0234\tLR: 1.188235\n",
      "Training Epoch: 12 [44416/50000]\tLoss: 3.0839\tLR: 1.188491\n",
      "Training Epoch: 12 [44544/50000]\tLoss: 3.0211\tLR: 1.188747\n",
      "Training Epoch: 12 [44672/50000]\tLoss: 3.2169\tLR: 1.189003\n",
      "Training Epoch: 12 [44800/50000]\tLoss: 2.9852\tLR: 1.189258\n",
      "Training Epoch: 12 [44928/50000]\tLoss: 3.1165\tLR: 1.189514\n",
      "Training Epoch: 12 [45056/50000]\tLoss: 3.2139\tLR: 1.189770\n",
      "Training Epoch: 12 [45184/50000]\tLoss: 2.9948\tLR: 1.190026\n",
      "Training Epoch: 12 [45312/50000]\tLoss: 2.9471\tLR: 1.190281\n",
      "Training Epoch: 12 [45440/50000]\tLoss: 3.5549\tLR: 1.190537\n",
      "Training Epoch: 12 [45568/50000]\tLoss: 2.9732\tLR: 1.190793\n",
      "Training Epoch: 12 [45696/50000]\tLoss: 3.1582\tLR: 1.191049\n",
      "Training Epoch: 12 [45824/50000]\tLoss: 3.0003\tLR: 1.191304\n",
      "Training Epoch: 12 [45952/50000]\tLoss: 3.0438\tLR: 1.191560\n",
      "Training Epoch: 12 [46080/50000]\tLoss: 2.8940\tLR: 1.191816\n",
      "Training Epoch: 12 [46208/50000]\tLoss: 3.3299\tLR: 1.192072\n",
      "Training Epoch: 12 [46336/50000]\tLoss: 2.9989\tLR: 1.192327\n",
      "Training Epoch: 12 [46464/50000]\tLoss: 3.3332\tLR: 1.192583\n",
      "Training Epoch: 12 [46592/50000]\tLoss: 3.1662\tLR: 1.192839\n",
      "Training Epoch: 12 [46720/50000]\tLoss: 3.0666\tLR: 1.193095\n",
      "Training Epoch: 12 [46848/50000]\tLoss: 2.8739\tLR: 1.193350\n",
      "Training Epoch: 12 [46976/50000]\tLoss: 2.9369\tLR: 1.193606\n",
      "Training Epoch: 12 [47104/50000]\tLoss: 3.1211\tLR: 1.193862\n",
      "Training Epoch: 12 [47232/50000]\tLoss: 2.8603\tLR: 1.194118\n",
      "Training Epoch: 12 [47360/50000]\tLoss: 3.1432\tLR: 1.194373\n",
      "Training Epoch: 12 [47488/50000]\tLoss: 3.1598\tLR: 1.194629\n",
      "Training Epoch: 12 [47616/50000]\tLoss: 3.0984\tLR: 1.194885\n",
      "Training Epoch: 12 [47744/50000]\tLoss: 3.2338\tLR: 1.195141\n",
      "Training Epoch: 12 [47872/50000]\tLoss: 3.1904\tLR: 1.195396\n",
      "Training Epoch: 12 [48000/50000]\tLoss: 3.1714\tLR: 1.195652\n",
      "Training Epoch: 12 [48128/50000]\tLoss: 3.1945\tLR: 1.195908\n",
      "Training Epoch: 12 [48256/50000]\tLoss: 2.9116\tLR: 1.196164\n",
      "Training Epoch: 12 [48384/50000]\tLoss: 3.0700\tLR: 1.196419\n",
      "Training Epoch: 12 [48512/50000]\tLoss: 3.2125\tLR: 1.196675\n",
      "Training Epoch: 12 [48640/50000]\tLoss: 3.0743\tLR: 1.196931\n",
      "Training Epoch: 12 [48768/50000]\tLoss: 3.2544\tLR: 1.197187\n",
      "Training Epoch: 12 [48896/50000]\tLoss: 3.1809\tLR: 1.197442\n",
      "Training Epoch: 12 [49024/50000]\tLoss: 3.1239\tLR: 1.197698\n",
      "Training Epoch: 12 [49152/50000]\tLoss: 2.8904\tLR: 1.197954\n",
      "Training Epoch: 12 [49280/50000]\tLoss: 2.9671\tLR: 1.198210\n",
      "Training Epoch: 12 [49408/50000]\tLoss: 3.0594\tLR: 1.198465\n",
      "Training Epoch: 12 [49536/50000]\tLoss: 2.9358\tLR: 1.198721\n",
      "Training Epoch: 12 [49664/50000]\tLoss: 3.2001\tLR: 1.198977\n",
      "Training Epoch: 12 [49792/50000]\tLoss: 3.1595\tLR: 1.199233\n",
      "Training Epoch: 12 [49920/50000]\tLoss: 3.2554\tLR: 1.199488\n",
      "Training Epoch: 12 [50000/50000]\tLoss: 3.0952\tLR: 1.199744\n",
      "epoch 12 training time consumed: 488.68s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   16824 GB |   16824 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   16772 GB |   16772 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      51 GB |      51 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   16824 GB |   16824 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   16772 GB |   16772 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      51 GB |      51 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   16585 GB |   16584 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   16533 GB |   16533 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |      51 GB |      51 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    1784 K  |    1784 K  |\n",
      "|       from large pool |      24    |      65    |     760 K  |     760 K  |\n",
      "|       from small pool |     231    |     274    |    1023 K  |    1023 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    1784 K  |    1784 K  |\n",
      "|       from large pool |      24    |      65    |     760 K  |     760 K  |\n",
      "|       from small pool |     231    |     274    |    1023 K  |    1023 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      45    |    1033 K  |    1033 K  |\n",
      "|       from large pool |      10    |      23    |     365 K  |     365 K  |\n",
      "|       from small pool |      27    |      35    |     667 K  |     667 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 12, Average loss: 0.0523, Accuracy: 0.0623, Time consumed:30.85s\n",
      "\n",
      "Training Epoch: 13 [128/50000]\tLoss: 3.1010\tLR: 0.100000\n",
      "Training Epoch: 13 [256/50000]\tLoss: 3.1270\tLR: 1.200256\n",
      "Training Epoch: 13 [384/50000]\tLoss: 3.1178\tLR: 1.200512\n",
      "Training Epoch: 13 [512/50000]\tLoss: 2.8915\tLR: 1.200767\n",
      "Training Epoch: 13 [640/50000]\tLoss: 3.1647\tLR: 1.201023\n",
      "Training Epoch: 13 [768/50000]\tLoss: 3.1628\tLR: 1.201279\n",
      "Training Epoch: 13 [896/50000]\tLoss: 2.9291\tLR: 1.201535\n",
      "Training Epoch: 13 [1024/50000]\tLoss: 3.1061\tLR: 1.201790\n",
      "Training Epoch: 13 [1152/50000]\tLoss: 3.0770\tLR: 1.202046\n",
      "Training Epoch: 13 [1280/50000]\tLoss: 2.9625\tLR: 1.202302\n",
      "Training Epoch: 13 [1408/50000]\tLoss: 2.9445\tLR: 1.202558\n",
      "Training Epoch: 13 [1536/50000]\tLoss: 2.9906\tLR: 1.202813\n",
      "Training Epoch: 13 [1664/50000]\tLoss: 3.1059\tLR: 1.203069\n",
      "Training Epoch: 13 [1792/50000]\tLoss: 2.9542\tLR: 1.203325\n",
      "Training Epoch: 13 [1920/50000]\tLoss: 3.0026\tLR: 1.203581\n",
      "Training Epoch: 13 [2048/50000]\tLoss: 3.2466\tLR: 1.203836\n",
      "Training Epoch: 13 [2176/50000]\tLoss: 3.1508\tLR: 1.204092\n",
      "Training Epoch: 13 [2304/50000]\tLoss: 2.9584\tLR: 1.204348\n",
      "Training Epoch: 13 [2432/50000]\tLoss: 3.1284\tLR: 1.204604\n",
      "Training Epoch: 13 [2560/50000]\tLoss: 3.2590\tLR: 1.204859\n",
      "Training Epoch: 13 [2688/50000]\tLoss: 2.7386\tLR: 1.205115\n",
      "Training Epoch: 13 [2816/50000]\tLoss: 3.1575\tLR: 1.205371\n",
      "Training Epoch: 13 [2944/50000]\tLoss: 2.8758\tLR: 1.205627\n",
      "Training Epoch: 13 [3072/50000]\tLoss: 2.9561\tLR: 1.205882\n",
      "Training Epoch: 13 [3200/50000]\tLoss: 2.9517\tLR: 1.206138\n",
      "Training Epoch: 13 [3328/50000]\tLoss: 2.9786\tLR: 1.206394\n",
      "Training Epoch: 13 [3456/50000]\tLoss: 3.1043\tLR: 1.206650\n",
      "Training Epoch: 13 [3584/50000]\tLoss: 3.1417\tLR: 1.206905\n",
      "Training Epoch: 13 [3712/50000]\tLoss: 3.0101\tLR: 1.207161\n",
      "Training Epoch: 13 [3840/50000]\tLoss: 3.3486\tLR: 1.207417\n",
      "Training Epoch: 13 [3968/50000]\tLoss: 2.9742\tLR: 1.207673\n",
      "Training Epoch: 13 [4096/50000]\tLoss: 2.9667\tLR: 1.207928\n",
      "Training Epoch: 13 [4224/50000]\tLoss: 3.1552\tLR: 1.208184\n",
      "Training Epoch: 13 [4352/50000]\tLoss: 2.9436\tLR: 1.208440\n",
      "Training Epoch: 13 [4480/50000]\tLoss: 3.0459\tLR: 1.208696\n",
      "Training Epoch: 13 [4608/50000]\tLoss: 2.8692\tLR: 1.208951\n",
      "Training Epoch: 13 [4736/50000]\tLoss: 3.1252\tLR: 1.209207\n",
      "Training Epoch: 13 [4864/50000]\tLoss: 3.1443\tLR: 1.209463\n",
      "Training Epoch: 13 [4992/50000]\tLoss: 3.2686\tLR: 1.209719\n",
      "Training Epoch: 13 [5120/50000]\tLoss: 3.1759\tLR: 1.209974\n",
      "Training Epoch: 13 [5248/50000]\tLoss: 2.9111\tLR: 1.210230\n",
      "Training Epoch: 13 [5376/50000]\tLoss: 3.0546\tLR: 1.210486\n",
      "Training Epoch: 13 [5504/50000]\tLoss: 3.3811\tLR: 1.210742\n",
      "Training Epoch: 13 [5632/50000]\tLoss: 3.2935\tLR: 1.210997\n",
      "Training Epoch: 13 [5760/50000]\tLoss: 3.0237\tLR: 1.211253\n",
      "Training Epoch: 13 [5888/50000]\tLoss: 2.9583\tLR: 1.211509\n",
      "Training Epoch: 13 [6016/50000]\tLoss: 3.2966\tLR: 1.211765\n",
      "Training Epoch: 13 [6144/50000]\tLoss: 3.1086\tLR: 1.212020\n",
      "Training Epoch: 13 [6272/50000]\tLoss: 2.9983\tLR: 1.212276\n",
      "Training Epoch: 13 [6400/50000]\tLoss: 3.0584\tLR: 1.212532\n",
      "Training Epoch: 13 [6528/50000]\tLoss: 3.2663\tLR: 1.212788\n",
      "Training Epoch: 13 [6656/50000]\tLoss: 2.9759\tLR: 1.213043\n",
      "Training Epoch: 13 [6784/50000]\tLoss: 3.3210\tLR: 1.213299\n",
      "Training Epoch: 13 [6912/50000]\tLoss: 3.1093\tLR: 1.213555\n",
      "Training Epoch: 13 [7040/50000]\tLoss: 3.1457\tLR: 1.213811\n",
      "Training Epoch: 13 [7168/50000]\tLoss: 3.0516\tLR: 1.214066\n",
      "Training Epoch: 13 [7296/50000]\tLoss: 3.0561\tLR: 1.214322\n",
      "Training Epoch: 13 [7424/50000]\tLoss: 3.0345\tLR: 1.214578\n",
      "Training Epoch: 13 [7552/50000]\tLoss: 3.2010\tLR: 1.214834\n",
      "Training Epoch: 13 [7680/50000]\tLoss: 3.1392\tLR: 1.215090\n",
      "Training Epoch: 13 [7808/50000]\tLoss: 3.0545\tLR: 1.215345\n",
      "Training Epoch: 13 [7936/50000]\tLoss: 3.3331\tLR: 1.215601\n",
      "Training Epoch: 13 [8064/50000]\tLoss: 2.9844\tLR: 1.215857\n",
      "Training Epoch: 13 [8192/50000]\tLoss: 2.9392\tLR: 1.216113\n",
      "Training Epoch: 13 [8320/50000]\tLoss: 3.3884\tLR: 1.216368\n",
      "Training Epoch: 13 [8448/50000]\tLoss: 3.0503\tLR: 1.216624\n",
      "Training Epoch: 13 [8576/50000]\tLoss: 3.0662\tLR: 1.216880\n",
      "Training Epoch: 13 [8704/50000]\tLoss: 3.0926\tLR: 1.217136\n",
      "Training Epoch: 13 [8832/50000]\tLoss: 3.2453\tLR: 1.217391\n",
      "Training Epoch: 13 [8960/50000]\tLoss: 3.0081\tLR: 1.217647\n",
      "Training Epoch: 13 [9088/50000]\tLoss: 3.1857\tLR: 1.217903\n",
      "Training Epoch: 13 [9216/50000]\tLoss: 2.6971\tLR: 1.218159\n",
      "Training Epoch: 13 [9344/50000]\tLoss: 2.9187\tLR: 1.218414\n",
      "Training Epoch: 13 [9472/50000]\tLoss: 3.0454\tLR: 1.218670\n",
      "Training Epoch: 13 [9600/50000]\tLoss: 3.1848\tLR: 1.218926\n",
      "Training Epoch: 13 [9728/50000]\tLoss: 3.1277\tLR: 1.219182\n",
      "Training Epoch: 13 [9856/50000]\tLoss: 3.1243\tLR: 1.219437\n",
      "Training Epoch: 13 [9984/50000]\tLoss: 3.1617\tLR: 1.219693\n",
      "Training Epoch: 13 [10112/50000]\tLoss: 3.2314\tLR: 1.219949\n",
      "Training Epoch: 13 [10240/50000]\tLoss: 2.8981\tLR: 1.220205\n",
      "Training Epoch: 13 [10368/50000]\tLoss: 3.3992\tLR: 1.220460\n",
      "Training Epoch: 13 [10496/50000]\tLoss: 3.0491\tLR: 1.220716\n",
      "Training Epoch: 13 [10624/50000]\tLoss: 3.3099\tLR: 1.220972\n",
      "Training Epoch: 13 [10752/50000]\tLoss: 2.9038\tLR: 1.221228\n",
      "Training Epoch: 13 [10880/50000]\tLoss: 3.1073\tLR: 1.221483\n",
      "Training Epoch: 13 [11008/50000]\tLoss: 3.0784\tLR: 1.221739\n",
      "Training Epoch: 13 [11136/50000]\tLoss: 3.4576\tLR: 1.221995\n",
      "Training Epoch: 13 [11264/50000]\tLoss: 3.2680\tLR: 1.222251\n",
      "Training Epoch: 13 [11392/50000]\tLoss: 3.3028\tLR: 1.222506\n",
      "Training Epoch: 13 [11520/50000]\tLoss: 3.0432\tLR: 1.222762\n",
      "Training Epoch: 13 [11648/50000]\tLoss: 2.8503\tLR: 1.223018\n",
      "Training Epoch: 13 [11776/50000]\tLoss: 2.9763\tLR: 1.223274\n",
      "Training Epoch: 13 [11904/50000]\tLoss: 3.1264\tLR: 1.223529\n",
      "Training Epoch: 13 [12032/50000]\tLoss: 2.8163\tLR: 1.223785\n",
      "Training Epoch: 13 [12160/50000]\tLoss: 2.9554\tLR: 1.224041\n",
      "Training Epoch: 13 [12288/50000]\tLoss: 2.9018\tLR: 1.224297\n",
      "Training Epoch: 13 [12416/50000]\tLoss: 2.7603\tLR: 1.224552\n",
      "Training Epoch: 13 [12544/50000]\tLoss: 3.0754\tLR: 1.224808\n",
      "Training Epoch: 13 [12672/50000]\tLoss: 2.9764\tLR: 1.225064\n",
      "Training Epoch: 13 [12800/50000]\tLoss: 3.0263\tLR: 1.225320\n",
      "Training Epoch: 13 [12928/50000]\tLoss: 3.0251\tLR: 1.225575\n",
      "Training Epoch: 13 [13056/50000]\tLoss: 2.8064\tLR: 1.225831\n",
      "Training Epoch: 13 [13184/50000]\tLoss: 3.1015\tLR: 1.226087\n",
      "Training Epoch: 13 [13312/50000]\tLoss: 2.8399\tLR: 1.226343\n",
      "Training Epoch: 13 [13440/50000]\tLoss: 3.0206\tLR: 1.226598\n",
      "Training Epoch: 13 [13568/50000]\tLoss: 3.3669\tLR: 1.226854\n",
      "Training Epoch: 13 [13696/50000]\tLoss: 3.2265\tLR: 1.227110\n",
      "Training Epoch: 13 [13824/50000]\tLoss: 2.8977\tLR: 1.227366\n",
      "Training Epoch: 13 [13952/50000]\tLoss: 3.3555\tLR: 1.227621\n",
      "Training Epoch: 13 [14080/50000]\tLoss: 3.5559\tLR: 1.227877\n",
      "Training Epoch: 13 [14208/50000]\tLoss: 3.5426\tLR: 1.228133\n",
      "Training Epoch: 13 [14336/50000]\tLoss: 3.1991\tLR: 1.228389\n",
      "Training Epoch: 13 [14464/50000]\tLoss: 3.0134\tLR: 1.228645\n",
      "Training Epoch: 13 [14592/50000]\tLoss: 3.2521\tLR: 1.228900\n",
      "Training Epoch: 13 [14720/50000]\tLoss: 2.9043\tLR: 1.229156\n",
      "Training Epoch: 13 [14848/50000]\tLoss: 2.9553\tLR: 1.229412\n",
      "Training Epoch: 13 [14976/50000]\tLoss: 3.1679\tLR: 1.229668\n",
      "Training Epoch: 13 [15104/50000]\tLoss: 3.1959\tLR: 1.229923\n",
      "Training Epoch: 13 [15232/50000]\tLoss: 3.4285\tLR: 1.230179\n",
      "Training Epoch: 13 [15360/50000]\tLoss: 3.2931\tLR: 1.230435\n",
      "Training Epoch: 13 [15488/50000]\tLoss: 3.0801\tLR: 1.230691\n",
      "Training Epoch: 13 [15616/50000]\tLoss: 3.1345\tLR: 1.230946\n",
      "Training Epoch: 13 [15744/50000]\tLoss: 2.9027\tLR: 1.231202\n",
      "Training Epoch: 13 [15872/50000]\tLoss: 3.0807\tLR: 1.231458\n",
      "Training Epoch: 13 [16000/50000]\tLoss: 3.1649\tLR: 1.231714\n",
      "Training Epoch: 13 [16128/50000]\tLoss: 3.2017\tLR: 1.231969\n",
      "Training Epoch: 13 [16256/50000]\tLoss: 3.2855\tLR: 1.232225\n",
      "Training Epoch: 13 [16384/50000]\tLoss: 3.2642\tLR: 1.232481\n",
      "Training Epoch: 13 [16512/50000]\tLoss: 3.1986\tLR: 1.232737\n",
      "Training Epoch: 13 [16640/50000]\tLoss: 3.1117\tLR: 1.232992\n",
      "Training Epoch: 13 [16768/50000]\tLoss: 3.0041\tLR: 1.233248\n",
      "Training Epoch: 13 [16896/50000]\tLoss: 3.3185\tLR: 1.233504\n",
      "Training Epoch: 13 [17024/50000]\tLoss: 3.0068\tLR: 1.233760\n",
      "Training Epoch: 13 [17152/50000]\tLoss: 3.1908\tLR: 1.234015\n",
      "Training Epoch: 13 [17280/50000]\tLoss: 3.1645\tLR: 1.234271\n",
      "Training Epoch: 13 [17408/50000]\tLoss: 3.0397\tLR: 1.234527\n",
      "Training Epoch: 13 [17536/50000]\tLoss: 3.4574\tLR: 1.234783\n",
      "Training Epoch: 13 [17664/50000]\tLoss: 3.2465\tLR: 1.235038\n",
      "Training Epoch: 13 [17792/50000]\tLoss: 3.0202\tLR: 1.235294\n",
      "Training Epoch: 13 [17920/50000]\tLoss: 3.3516\tLR: 1.235550\n",
      "Training Epoch: 13 [18048/50000]\tLoss: 3.2958\tLR: 1.235806\n",
      "Training Epoch: 13 [18176/50000]\tLoss: 3.2676\tLR: 1.236061\n",
      "Training Epoch: 13 [18304/50000]\tLoss: 3.1535\tLR: 1.236317\n",
      "Training Epoch: 13 [18432/50000]\tLoss: 3.1594\tLR: 1.236573\n",
      "Training Epoch: 13 [18560/50000]\tLoss: 3.1312\tLR: 1.236829\n",
      "Training Epoch: 13 [18688/50000]\tLoss: 2.9193\tLR: 1.237084\n",
      "Training Epoch: 13 [18816/50000]\tLoss: 3.2738\tLR: 1.237340\n",
      "Training Epoch: 13 [18944/50000]\tLoss: 2.9692\tLR: 1.237596\n",
      "Training Epoch: 13 [19072/50000]\tLoss: 2.9663\tLR: 1.237852\n",
      "Training Epoch: 13 [19200/50000]\tLoss: 3.4428\tLR: 1.238107\n",
      "Training Epoch: 13 [19328/50000]\tLoss: 3.1644\tLR: 1.238363\n",
      "Training Epoch: 13 [19456/50000]\tLoss: 3.0583\tLR: 1.238619\n",
      "Training Epoch: 13 [19584/50000]\tLoss: 2.9243\tLR: 1.238875\n",
      "Training Epoch: 13 [19712/50000]\tLoss: 2.9930\tLR: 1.239130\n",
      "Training Epoch: 13 [19840/50000]\tLoss: 3.2148\tLR: 1.239386\n",
      "Training Epoch: 13 [19968/50000]\tLoss: 2.9678\tLR: 1.239642\n",
      "Training Epoch: 13 [20096/50000]\tLoss: 3.0371\tLR: 1.239898\n",
      "Training Epoch: 13 [20224/50000]\tLoss: 3.3248\tLR: 1.240153\n",
      "Training Epoch: 13 [20352/50000]\tLoss: 3.1763\tLR: 1.240409\n",
      "Training Epoch: 13 [20480/50000]\tLoss: 3.0818\tLR: 1.240665\n",
      "Training Epoch: 13 [20608/50000]\tLoss: 3.0761\tLR: 1.240921\n",
      "Training Epoch: 13 [20736/50000]\tLoss: 3.0694\tLR: 1.241176\n",
      "Training Epoch: 13 [20864/50000]\tLoss: 2.9596\tLR: 1.241432\n",
      "Training Epoch: 13 [20992/50000]\tLoss: 3.2979\tLR: 1.241688\n",
      "Training Epoch: 13 [21120/50000]\tLoss: 3.2703\tLR: 1.241944\n",
      "Training Epoch: 13 [21248/50000]\tLoss: 3.2887\tLR: 1.242199\n",
      "Training Epoch: 13 [21376/50000]\tLoss: 3.0326\tLR: 1.242455\n",
      "Training Epoch: 13 [21504/50000]\tLoss: 3.3472\tLR: 1.242711\n",
      "Training Epoch: 13 [21632/50000]\tLoss: 3.3405\tLR: 1.242967\n",
      "Training Epoch: 13 [21760/50000]\tLoss: 3.1703\tLR: 1.243223\n",
      "Training Epoch: 13 [21888/50000]\tLoss: 3.1723\tLR: 1.243478\n",
      "Training Epoch: 13 [22016/50000]\tLoss: 3.1522\tLR: 1.243734\n",
      "Training Epoch: 13 [22144/50000]\tLoss: 3.4163\tLR: 1.243990\n",
      "Training Epoch: 13 [22272/50000]\tLoss: 3.0439\tLR: 1.244246\n",
      "Training Epoch: 13 [22400/50000]\tLoss: 3.0189\tLR: 1.244501\n",
      "Training Epoch: 13 [22528/50000]\tLoss: 3.0002\tLR: 1.244757\n",
      "Training Epoch: 13 [22656/50000]\tLoss: 3.0816\tLR: 1.245013\n",
      "Training Epoch: 13 [22784/50000]\tLoss: 3.0443\tLR: 1.245269\n",
      "Training Epoch: 13 [22912/50000]\tLoss: 2.8981\tLR: 1.245524\n",
      "Training Epoch: 13 [23040/50000]\tLoss: 3.2255\tLR: 1.245780\n",
      "Training Epoch: 13 [23168/50000]\tLoss: 3.1281\tLR: 1.246036\n",
      "Training Epoch: 13 [23296/50000]\tLoss: 3.2848\tLR: 1.246292\n",
      "Training Epoch: 13 [23424/50000]\tLoss: 2.9543\tLR: 1.246547\n",
      "Training Epoch: 13 [23552/50000]\tLoss: 3.3231\tLR: 1.246803\n",
      "Training Epoch: 13 [23680/50000]\tLoss: 3.4230\tLR: 1.247059\n",
      "Training Epoch: 13 [23808/50000]\tLoss: 3.3201\tLR: 1.247315\n",
      "Training Epoch: 13 [23936/50000]\tLoss: 3.3734\tLR: 1.247570\n",
      "Training Epoch: 13 [24064/50000]\tLoss: 3.3245\tLR: 1.247826\n",
      "Training Epoch: 13 [24192/50000]\tLoss: 3.4092\tLR: 1.248082\n",
      "Training Epoch: 13 [24320/50000]\tLoss: 3.1971\tLR: 1.248338\n",
      "Training Epoch: 13 [24448/50000]\tLoss: 3.0287\tLR: 1.248593\n",
      "Training Epoch: 13 [24576/50000]\tLoss: 3.0977\tLR: 1.248849\n",
      "Training Epoch: 13 [24704/50000]\tLoss: 3.3560\tLR: 1.249105\n",
      "Training Epoch: 13 [24832/50000]\tLoss: 3.2045\tLR: 1.249361\n",
      "Training Epoch: 13 [24960/50000]\tLoss: 3.0617\tLR: 1.249616\n",
      "Training Epoch: 13 [25088/50000]\tLoss: 3.1449\tLR: 1.249872\n",
      "Training Epoch: 13 [25216/50000]\tLoss: 3.4298\tLR: 1.250128\n",
      "Training Epoch: 13 [25344/50000]\tLoss: 3.1254\tLR: 1.250384\n",
      "Training Epoch: 13 [25472/50000]\tLoss: 3.1029\tLR: 1.250639\n",
      "Training Epoch: 13 [25600/50000]\tLoss: 3.0371\tLR: 1.250895\n",
      "Training Epoch: 13 [25728/50000]\tLoss: 3.4500\tLR: 1.251151\n",
      "Training Epoch: 13 [25856/50000]\tLoss: 2.9400\tLR: 1.251407\n",
      "Training Epoch: 13 [25984/50000]\tLoss: 3.0205\tLR: 1.251662\n",
      "Training Epoch: 13 [26112/50000]\tLoss: 3.3441\tLR: 1.251918\n",
      "Training Epoch: 13 [26240/50000]\tLoss: 3.1018\tLR: 1.252174\n",
      "Training Epoch: 13 [26368/50000]\tLoss: 3.0818\tLR: 1.252430\n",
      "Training Epoch: 13 [26496/50000]\tLoss: 3.0566\tLR: 1.252685\n",
      "Training Epoch: 13 [26624/50000]\tLoss: 3.1667\tLR: 1.252941\n",
      "Training Epoch: 13 [26752/50000]\tLoss: 3.0827\tLR: 1.253197\n",
      "Training Epoch: 13 [26880/50000]\tLoss: 3.3610\tLR: 1.253453\n",
      "Training Epoch: 13 [27008/50000]\tLoss: 3.2023\tLR: 1.253708\n",
      "Training Epoch: 13 [27136/50000]\tLoss: 2.8785\tLR: 1.253964\n",
      "Training Epoch: 13 [27264/50000]\tLoss: 2.9391\tLR: 1.254220\n",
      "Training Epoch: 13 [27392/50000]\tLoss: 3.0953\tLR: 1.254476\n",
      "Training Epoch: 13 [27520/50000]\tLoss: 3.1302\tLR: 1.254731\n",
      "Training Epoch: 13 [27648/50000]\tLoss: 3.2132\tLR: 1.254987\n",
      "Training Epoch: 13 [27776/50000]\tLoss: 3.3083\tLR: 1.255243\n",
      "Training Epoch: 13 [27904/50000]\tLoss: 3.4493\tLR: 1.255499\n",
      "Training Epoch: 13 [28032/50000]\tLoss: 3.2409\tLR: 1.255754\n",
      "Training Epoch: 13 [28160/50000]\tLoss: 3.0026\tLR: 1.256010\n",
      "Training Epoch: 13 [28288/50000]\tLoss: 3.4338\tLR: 1.256266\n",
      "Training Epoch: 13 [28416/50000]\tLoss: 3.2820\tLR: 1.256522\n",
      "Training Epoch: 13 [28544/50000]\tLoss: 3.0968\tLR: 1.256777\n",
      "Training Epoch: 13 [28672/50000]\tLoss: 3.3077\tLR: 1.257033\n",
      "Training Epoch: 13 [28800/50000]\tLoss: 3.2723\tLR: 1.257289\n",
      "Training Epoch: 13 [28928/50000]\tLoss: 2.8752\tLR: 1.257545\n",
      "Training Epoch: 13 [29056/50000]\tLoss: 3.0911\tLR: 1.257801\n",
      "Training Epoch: 13 [29184/50000]\tLoss: 3.3843\tLR: 1.258056\n",
      "Training Epoch: 13 [29312/50000]\tLoss: 3.1912\tLR: 1.258312\n",
      "Training Epoch: 13 [29440/50000]\tLoss: 2.8788\tLR: 1.258568\n",
      "Training Epoch: 13 [29568/50000]\tLoss: 2.8375\tLR: 1.258824\n",
      "Training Epoch: 13 [29696/50000]\tLoss: 3.3127\tLR: 1.259079\n",
      "Training Epoch: 13 [29824/50000]\tLoss: 3.2024\tLR: 1.259335\n",
      "Training Epoch: 13 [29952/50000]\tLoss: 3.0836\tLR: 1.259591\n",
      "Training Epoch: 13 [30080/50000]\tLoss: 3.2502\tLR: 1.259847\n",
      "Training Epoch: 13 [30208/50000]\tLoss: 2.8678\tLR: 1.260102\n",
      "Training Epoch: 13 [30336/50000]\tLoss: 3.2095\tLR: 1.260358\n",
      "Training Epoch: 13 [30464/50000]\tLoss: 3.1545\tLR: 1.260614\n",
      "Training Epoch: 13 [30592/50000]\tLoss: 2.8253\tLR: 1.260870\n",
      "Training Epoch: 13 [30720/50000]\tLoss: 3.0395\tLR: 1.261125\n",
      "Training Epoch: 13 [30848/50000]\tLoss: 3.0728\tLR: 1.261381\n",
      "Training Epoch: 13 [30976/50000]\tLoss: 3.0519\tLR: 1.261637\n",
      "Training Epoch: 13 [31104/50000]\tLoss: 3.0411\tLR: 1.261893\n",
      "Training Epoch: 13 [31232/50000]\tLoss: 3.0354\tLR: 1.262148\n",
      "Training Epoch: 13 [31360/50000]\tLoss: 2.9484\tLR: 1.262404\n",
      "Training Epoch: 13 [31488/50000]\tLoss: 3.1749\tLR: 1.262660\n",
      "Training Epoch: 13 [31616/50000]\tLoss: 2.9894\tLR: 1.262916\n",
      "Training Epoch: 13 [31744/50000]\tLoss: 3.1729\tLR: 1.263171\n",
      "Training Epoch: 13 [31872/50000]\tLoss: 3.2005\tLR: 1.263427\n",
      "Training Epoch: 13 [32000/50000]\tLoss: 3.2563\tLR: 1.263683\n",
      "Training Epoch: 13 [32128/50000]\tLoss: 3.1777\tLR: 1.263939\n",
      "Training Epoch: 13 [32256/50000]\tLoss: 3.1640\tLR: 1.264194\n",
      "Training Epoch: 13 [32384/50000]\tLoss: 3.1417\tLR: 1.264450\n",
      "Training Epoch: 13 [32512/50000]\tLoss: 3.0843\tLR: 1.264706\n",
      "Training Epoch: 13 [32640/50000]\tLoss: 3.2441\tLR: 1.264962\n",
      "Training Epoch: 13 [32768/50000]\tLoss: 3.2145\tLR: 1.265217\n",
      "Training Epoch: 13 [32896/50000]\tLoss: 2.9607\tLR: 1.265473\n",
      "Training Epoch: 13 [33024/50000]\tLoss: 3.3028\tLR: 1.265729\n",
      "Training Epoch: 13 [33152/50000]\tLoss: 3.0333\tLR: 1.265985\n",
      "Training Epoch: 13 [33280/50000]\tLoss: 3.1163\tLR: 1.266240\n",
      "Training Epoch: 13 [33408/50000]\tLoss: 2.8547\tLR: 1.266496\n",
      "Training Epoch: 13 [33536/50000]\tLoss: 3.0697\tLR: 1.266752\n",
      "Training Epoch: 13 [33664/50000]\tLoss: 2.8454\tLR: 1.267008\n",
      "Training Epoch: 13 [33792/50000]\tLoss: 3.2000\tLR: 1.267263\n",
      "Training Epoch: 13 [33920/50000]\tLoss: 2.8154\tLR: 1.267519\n",
      "Training Epoch: 13 [34048/50000]\tLoss: 2.9321\tLR: 1.267775\n",
      "Training Epoch: 13 [34176/50000]\tLoss: 2.8428\tLR: 1.268031\n",
      "Training Epoch: 13 [34304/50000]\tLoss: 3.0050\tLR: 1.268286\n",
      "Training Epoch: 13 [34432/50000]\tLoss: 3.0975\tLR: 1.268542\n",
      "Training Epoch: 13 [34560/50000]\tLoss: 3.1180\tLR: 1.268798\n",
      "Training Epoch: 13 [34688/50000]\tLoss: 3.1545\tLR: 1.269054\n",
      "Training Epoch: 13 [34816/50000]\tLoss: 3.0663\tLR: 1.269309\n",
      "Training Epoch: 13 [34944/50000]\tLoss: 2.8672\tLR: 1.269565\n",
      "Training Epoch: 13 [35072/50000]\tLoss: 2.9386\tLR: 1.269821\n",
      "Training Epoch: 13 [35200/50000]\tLoss: 3.0249\tLR: 1.270077\n",
      "Training Epoch: 13 [35328/50000]\tLoss: 3.2959\tLR: 1.270332\n",
      "Training Epoch: 13 [35456/50000]\tLoss: 3.1077\tLR: 1.270588\n",
      "Training Epoch: 13 [35584/50000]\tLoss: 3.2606\tLR: 1.270844\n",
      "Training Epoch: 13 [35712/50000]\tLoss: 2.9980\tLR: 1.271100\n",
      "Training Epoch: 13 [35840/50000]\tLoss: 2.9570\tLR: 1.271355\n",
      "Training Epoch: 13 [35968/50000]\tLoss: 3.1457\tLR: 1.271611\n",
      "Training Epoch: 13 [36096/50000]\tLoss: 3.1865\tLR: 1.271867\n",
      "Training Epoch: 13 [36224/50000]\tLoss: 3.2396\tLR: 1.272123\n",
      "Training Epoch: 13 [36352/50000]\tLoss: 3.0450\tLR: 1.272379\n",
      "Training Epoch: 13 [36480/50000]\tLoss: 3.2261\tLR: 1.272634\n",
      "Training Epoch: 13 [36608/50000]\tLoss: 3.1414\tLR: 1.272890\n",
      "Training Epoch: 13 [36736/50000]\tLoss: 3.3030\tLR: 1.273146\n",
      "Training Epoch: 13 [36864/50000]\tLoss: 3.1752\tLR: 1.273402\n",
      "Training Epoch: 13 [36992/50000]\tLoss: 3.1665\tLR: 1.273657\n",
      "Training Epoch: 13 [37120/50000]\tLoss: 3.5158\tLR: 1.273913\n",
      "Training Epoch: 13 [37248/50000]\tLoss: 3.3004\tLR: 1.274169\n",
      "Training Epoch: 13 [37376/50000]\tLoss: 3.0086\tLR: 1.274425\n",
      "Training Epoch: 13 [37504/50000]\tLoss: 3.1606\tLR: 1.274680\n",
      "Training Epoch: 13 [37632/50000]\tLoss: 3.2895\tLR: 1.274936\n",
      "Training Epoch: 13 [37760/50000]\tLoss: 3.1487\tLR: 1.275192\n",
      "Training Epoch: 13 [37888/50000]\tLoss: 3.3162\tLR: 1.275448\n",
      "Training Epoch: 13 [38016/50000]\tLoss: 3.2043\tLR: 1.275703\n",
      "Training Epoch: 13 [38144/50000]\tLoss: 3.0292\tLR: 1.275959\n",
      "Training Epoch: 13 [38272/50000]\tLoss: 3.0004\tLR: 1.276215\n",
      "Training Epoch: 13 [38400/50000]\tLoss: 3.2489\tLR: 1.276471\n",
      "Training Epoch: 13 [38528/50000]\tLoss: 3.1720\tLR: 1.276726\n",
      "Training Epoch: 13 [38656/50000]\tLoss: 2.9730\tLR: 1.276982\n",
      "Training Epoch: 13 [38784/50000]\tLoss: 2.9705\tLR: 1.277238\n",
      "Training Epoch: 13 [38912/50000]\tLoss: 3.2147\tLR: 1.277494\n",
      "Training Epoch: 13 [39040/50000]\tLoss: 3.1711\tLR: 1.277749\n",
      "Training Epoch: 13 [39168/50000]\tLoss: 3.3673\tLR: 1.278005\n",
      "Training Epoch: 13 [39296/50000]\tLoss: 3.1105\tLR: 1.278261\n",
      "Training Epoch: 13 [39424/50000]\tLoss: 3.0020\tLR: 1.278517\n",
      "Training Epoch: 13 [39552/50000]\tLoss: 3.1189\tLR: 1.278772\n",
      "Training Epoch: 13 [39680/50000]\tLoss: 3.0134\tLR: 1.279028\n",
      "Training Epoch: 13 [39808/50000]\tLoss: 3.0263\tLR: 1.279284\n",
      "Training Epoch: 13 [39936/50000]\tLoss: 3.1661\tLR: 1.279540\n",
      "Training Epoch: 13 [40064/50000]\tLoss: 3.1483\tLR: 1.279795\n",
      "Training Epoch: 13 [40192/50000]\tLoss: 3.2192\tLR: 1.280051\n",
      "Training Epoch: 13 [40320/50000]\tLoss: 3.3092\tLR: 1.280307\n",
      "Training Epoch: 13 [40448/50000]\tLoss: 3.1760\tLR: 1.280563\n",
      "Training Epoch: 13 [40576/50000]\tLoss: 3.1756\tLR: 1.280818\n",
      "Training Epoch: 13 [40704/50000]\tLoss: 3.0456\tLR: 1.281074\n",
      "Training Epoch: 13 [40832/50000]\tLoss: 3.3542\tLR: 1.281330\n",
      "Training Epoch: 13 [40960/50000]\tLoss: 3.1938\tLR: 1.281586\n",
      "Training Epoch: 13 [41088/50000]\tLoss: 2.8276\tLR: 1.281841\n",
      "Training Epoch: 13 [41216/50000]\tLoss: 3.0456\tLR: 1.282097\n",
      "Training Epoch: 13 [41344/50000]\tLoss: 3.1309\tLR: 1.282353\n",
      "Training Epoch: 13 [41472/50000]\tLoss: 3.1447\tLR: 1.282609\n",
      "Training Epoch: 13 [41600/50000]\tLoss: 2.9745\tLR: 1.282864\n",
      "Training Epoch: 13 [41728/50000]\tLoss: 3.3852\tLR: 1.283120\n",
      "Training Epoch: 13 [41856/50000]\tLoss: 3.4185\tLR: 1.283376\n",
      "Training Epoch: 13 [41984/50000]\tLoss: 3.4692\tLR: 1.283632\n",
      "Training Epoch: 13 [42112/50000]\tLoss: 3.2377\tLR: 1.283887\n",
      "Training Epoch: 13 [42240/50000]\tLoss: 3.2486\tLR: 1.284143\n",
      "Training Epoch: 13 [42368/50000]\tLoss: 3.2396\tLR: 1.284399\n",
      "Training Epoch: 13 [42496/50000]\tLoss: 2.9967\tLR: 1.284655\n",
      "Training Epoch: 13 [42624/50000]\tLoss: 3.3489\tLR: 1.284910\n",
      "Training Epoch: 13 [42752/50000]\tLoss: 3.3812\tLR: 1.285166\n",
      "Training Epoch: 13 [42880/50000]\tLoss: 3.0651\tLR: 1.285422\n",
      "Training Epoch: 13 [43008/50000]\tLoss: 2.9259\tLR: 1.285678\n",
      "Training Epoch: 13 [43136/50000]\tLoss: 2.9621\tLR: 1.285934\n",
      "Training Epoch: 13 [43264/50000]\tLoss: 2.8957\tLR: 1.286189\n",
      "Training Epoch: 13 [43392/50000]\tLoss: 2.8077\tLR: 1.286445\n",
      "Training Epoch: 13 [43520/50000]\tLoss: 3.2782\tLR: 1.286701\n",
      "Training Epoch: 13 [43648/50000]\tLoss: 3.3584\tLR: 1.286957\n",
      "Training Epoch: 13 [43776/50000]\tLoss: 2.8213\tLR: 1.287212\n",
      "Training Epoch: 13 [43904/50000]\tLoss: 3.1731\tLR: 1.287468\n",
      "Training Epoch: 13 [44032/50000]\tLoss: 2.9821\tLR: 1.287724\n",
      "Training Epoch: 13 [44160/50000]\tLoss: 3.3354\tLR: 1.287980\n",
      "Training Epoch: 13 [44288/50000]\tLoss: 3.3119\tLR: 1.288235\n",
      "Training Epoch: 13 [44416/50000]\tLoss: 3.1691\tLR: 1.288491\n",
      "Training Epoch: 13 [44544/50000]\tLoss: 3.1529\tLR: 1.288747\n",
      "Training Epoch: 13 [44672/50000]\tLoss: 3.1208\tLR: 1.289003\n",
      "Training Epoch: 13 [44800/50000]\tLoss: 3.1038\tLR: 1.289258\n",
      "Training Epoch: 13 [44928/50000]\tLoss: 3.5302\tLR: 1.289514\n",
      "Training Epoch: 13 [45056/50000]\tLoss: 3.0025\tLR: 1.289770\n",
      "Training Epoch: 13 [45184/50000]\tLoss: 3.1537\tLR: 1.290026\n",
      "Training Epoch: 13 [45312/50000]\tLoss: 3.1850\tLR: 1.290281\n",
      "Training Epoch: 13 [45440/50000]\tLoss: 3.1869\tLR: 1.290537\n",
      "Training Epoch: 13 [45568/50000]\tLoss: 3.0312\tLR: 1.290793\n",
      "Training Epoch: 13 [45696/50000]\tLoss: 3.1114\tLR: 1.291049\n",
      "Training Epoch: 13 [45824/50000]\tLoss: 3.1646\tLR: 1.291304\n",
      "Training Epoch: 13 [45952/50000]\tLoss: 3.0127\tLR: 1.291560\n",
      "Training Epoch: 13 [46080/50000]\tLoss: 2.9936\tLR: 1.291816\n",
      "Training Epoch: 13 [46208/50000]\tLoss: 2.7912\tLR: 1.292072\n",
      "Training Epoch: 13 [46336/50000]\tLoss: 3.0023\tLR: 1.292327\n",
      "Training Epoch: 13 [46464/50000]\tLoss: 3.1995\tLR: 1.292583\n",
      "Training Epoch: 13 [46592/50000]\tLoss: 3.2198\tLR: 1.292839\n",
      "Training Epoch: 13 [46720/50000]\tLoss: 3.3014\tLR: 1.293095\n",
      "Training Epoch: 13 [46848/50000]\tLoss: 3.0878\tLR: 1.293350\n",
      "Training Epoch: 13 [46976/50000]\tLoss: 3.0695\tLR: 1.293606\n",
      "Training Epoch: 13 [47104/50000]\tLoss: 3.4371\tLR: 1.293862\n",
      "Training Epoch: 13 [47232/50000]\tLoss: 3.1726\tLR: 1.294118\n",
      "Training Epoch: 13 [47360/50000]\tLoss: 3.1126\tLR: 1.294373\n",
      "Training Epoch: 13 [47488/50000]\tLoss: 3.3020\tLR: 1.294629\n",
      "Training Epoch: 13 [47616/50000]\tLoss: 3.3376\tLR: 1.294885\n",
      "Training Epoch: 13 [47744/50000]\tLoss: 3.3440\tLR: 1.295141\n",
      "Training Epoch: 13 [47872/50000]\tLoss: 3.2440\tLR: 1.295396\n",
      "Training Epoch: 13 [48000/50000]\tLoss: 3.3951\tLR: 1.295652\n",
      "Training Epoch: 13 [48128/50000]\tLoss: 3.3154\tLR: 1.295908\n",
      "Training Epoch: 13 [48256/50000]\tLoss: 3.0983\tLR: 1.296164\n",
      "Training Epoch: 13 [48384/50000]\tLoss: 3.5160\tLR: 1.296419\n",
      "Training Epoch: 13 [48512/50000]\tLoss: 3.1324\tLR: 1.296675\n",
      "Training Epoch: 13 [48640/50000]\tLoss: 3.2653\tLR: 1.296931\n",
      "Training Epoch: 13 [48768/50000]\tLoss: 2.8745\tLR: 1.297187\n",
      "Training Epoch: 13 [48896/50000]\tLoss: 3.1657\tLR: 1.297442\n",
      "Training Epoch: 13 [49024/50000]\tLoss: 3.2120\tLR: 1.297698\n",
      "Training Epoch: 13 [49152/50000]\tLoss: 3.1563\tLR: 1.297954\n",
      "Training Epoch: 13 [49280/50000]\tLoss: 3.3621\tLR: 1.298210\n",
      "Training Epoch: 13 [49408/50000]\tLoss: 3.1352\tLR: 1.298465\n",
      "Training Epoch: 13 [49536/50000]\tLoss: 3.2338\tLR: 1.298721\n",
      "Training Epoch: 13 [49664/50000]\tLoss: 3.1326\tLR: 1.298977\n",
      "Training Epoch: 13 [49792/50000]\tLoss: 3.4315\tLR: 1.299233\n",
      "Training Epoch: 13 [49920/50000]\tLoss: 3.2495\tLR: 1.299488\n",
      "Training Epoch: 13 [50000/50000]\tLoss: 3.4034\tLR: 1.299744\n",
      "epoch 13 training time consumed: 488.83s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   18226 GB |   18226 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   18170 GB |   18170 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      55 GB |      55 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   18226 GB |   18226 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   18170 GB |   18170 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      55 GB |      55 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   17967 GB |   17967 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   17911 GB |   17911 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |      56 GB |      56 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    1932 K  |    1932 K  |\n",
      "|       from large pool |      24    |      65    |     823 K  |     823 K  |\n",
      "|       from small pool |     231    |     274    |    1109 K  |    1108 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    1932 K  |    1932 K  |\n",
      "|       from large pool |      24    |      65    |     823 K  |     823 K  |\n",
      "|       from small pool |     231    |     274    |    1109 K  |    1108 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      33    |      45    |    1119 K  |    1119 K  |\n",
      "|       from large pool |      10    |      23    |     396 K  |     395 K  |\n",
      "|       from small pool |      23    |      35    |     723 K  |     723 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 13, Average loss: 0.0311, Accuracy: 0.1530, Time consumed:31.11s\n",
      "\n",
      "Training Epoch: 14 [128/50000]\tLoss: 3.1507\tLR: 0.100000\n",
      "Training Epoch: 14 [256/50000]\tLoss: 3.0836\tLR: 1.300256\n",
      "Training Epoch: 14 [384/50000]\tLoss: 3.1892\tLR: 1.300512\n",
      "Training Epoch: 14 [512/50000]\tLoss: 3.3508\tLR: 1.300767\n",
      "Training Epoch: 14 [640/50000]\tLoss: 3.0150\tLR: 1.301023\n",
      "Training Epoch: 14 [768/50000]\tLoss: 2.9206\tLR: 1.301279\n",
      "Training Epoch: 14 [896/50000]\tLoss: 3.0950\tLR: 1.301535\n",
      "Training Epoch: 14 [1024/50000]\tLoss: 3.2189\tLR: 1.301790\n",
      "Training Epoch: 14 [1152/50000]\tLoss: 3.1507\tLR: 1.302046\n",
      "Training Epoch: 14 [1280/50000]\tLoss: 3.0191\tLR: 1.302302\n",
      "Training Epoch: 14 [1408/50000]\tLoss: 2.7988\tLR: 1.302558\n",
      "Training Epoch: 14 [1536/50000]\tLoss: 3.2474\tLR: 1.302813\n",
      "Training Epoch: 14 [1664/50000]\tLoss: 3.2468\tLR: 1.303069\n",
      "Training Epoch: 14 [1792/50000]\tLoss: 3.0136\tLR: 1.303325\n",
      "Training Epoch: 14 [1920/50000]\tLoss: 3.2395\tLR: 1.303581\n",
      "Training Epoch: 14 [2048/50000]\tLoss: 3.0170\tLR: 1.303836\n",
      "Training Epoch: 14 [2176/50000]\tLoss: 3.0992\tLR: 1.304092\n",
      "Training Epoch: 14 [2304/50000]\tLoss: 3.0321\tLR: 1.304348\n",
      "Training Epoch: 14 [2432/50000]\tLoss: 3.0437\tLR: 1.304604\n",
      "Training Epoch: 14 [2560/50000]\tLoss: 3.1844\tLR: 1.304859\n",
      "Training Epoch: 14 [2688/50000]\tLoss: 3.2926\tLR: 1.305115\n",
      "Training Epoch: 14 [2816/50000]\tLoss: 3.1228\tLR: 1.305371\n",
      "Training Epoch: 14 [2944/50000]\tLoss: 2.9269\tLR: 1.305627\n",
      "Training Epoch: 14 [3072/50000]\tLoss: 3.1066\tLR: 1.305882\n",
      "Training Epoch: 14 [3200/50000]\tLoss: 3.0546\tLR: 1.306138\n",
      "Training Epoch: 14 [3328/50000]\tLoss: 3.1010\tLR: 1.306394\n",
      "Training Epoch: 14 [3456/50000]\tLoss: 3.2128\tLR: 1.306650\n",
      "Training Epoch: 14 [3584/50000]\tLoss: 3.0675\tLR: 1.306905\n",
      "Training Epoch: 14 [3712/50000]\tLoss: 3.2832\tLR: 1.307161\n",
      "Training Epoch: 14 [3840/50000]\tLoss: 3.2976\tLR: 1.307417\n",
      "Training Epoch: 14 [3968/50000]\tLoss: 2.8912\tLR: 1.307673\n",
      "Training Epoch: 14 [4096/50000]\tLoss: 3.2002\tLR: 1.307928\n",
      "Training Epoch: 14 [4224/50000]\tLoss: 3.3005\tLR: 1.308184\n",
      "Training Epoch: 14 [4352/50000]\tLoss: 2.9184\tLR: 1.308440\n",
      "Training Epoch: 14 [4480/50000]\tLoss: 3.1834\tLR: 1.308696\n",
      "Training Epoch: 14 [4608/50000]\tLoss: 2.7965\tLR: 1.308951\n",
      "Training Epoch: 14 [4736/50000]\tLoss: 2.9591\tLR: 1.309207\n",
      "Training Epoch: 14 [4864/50000]\tLoss: 3.0185\tLR: 1.309463\n",
      "Training Epoch: 14 [4992/50000]\tLoss: 3.0927\tLR: 1.309719\n",
      "Training Epoch: 14 [5120/50000]\tLoss: 3.3239\tLR: 1.309974\n",
      "Training Epoch: 14 [5248/50000]\tLoss: 3.2048\tLR: 1.310230\n",
      "Training Epoch: 14 [5376/50000]\tLoss: 2.9752\tLR: 1.310486\n",
      "Training Epoch: 14 [5504/50000]\tLoss: 3.3727\tLR: 1.310742\n",
      "Training Epoch: 14 [5632/50000]\tLoss: 3.4431\tLR: 1.310997\n",
      "Training Epoch: 14 [5760/50000]\tLoss: 3.2097\tLR: 1.311253\n",
      "Training Epoch: 14 [5888/50000]\tLoss: 2.9546\tLR: 1.311509\n",
      "Training Epoch: 14 [6016/50000]\tLoss: 3.2631\tLR: 1.311765\n",
      "Training Epoch: 14 [6144/50000]\tLoss: 3.2149\tLR: 1.312020\n",
      "Training Epoch: 14 [6272/50000]\tLoss: 3.4110\tLR: 1.312276\n",
      "Training Epoch: 14 [6400/50000]\tLoss: 3.3293\tLR: 1.312532\n",
      "Training Epoch: 14 [6528/50000]\tLoss: 2.9246\tLR: 1.312788\n",
      "Training Epoch: 14 [6656/50000]\tLoss: 2.9487\tLR: 1.313043\n",
      "Training Epoch: 14 [6784/50000]\tLoss: 3.0701\tLR: 1.313299\n",
      "Training Epoch: 14 [6912/50000]\tLoss: 2.8429\tLR: 1.313555\n",
      "Training Epoch: 14 [7040/50000]\tLoss: 3.2024\tLR: 1.313811\n",
      "Training Epoch: 14 [7168/50000]\tLoss: 2.9222\tLR: 1.314066\n",
      "Training Epoch: 14 [7296/50000]\tLoss: 3.0813\tLR: 1.314322\n",
      "Training Epoch: 14 [7424/50000]\tLoss: 3.1075\tLR: 1.314578\n",
      "Training Epoch: 14 [7552/50000]\tLoss: 2.9760\tLR: 1.314834\n",
      "Training Epoch: 14 [7680/50000]\tLoss: 3.0417\tLR: 1.315090\n",
      "Training Epoch: 14 [7808/50000]\tLoss: 3.1192\tLR: 1.315345\n",
      "Training Epoch: 14 [7936/50000]\tLoss: 2.9789\tLR: 1.315601\n",
      "Training Epoch: 14 [8064/50000]\tLoss: 3.0585\tLR: 1.315857\n",
      "Training Epoch: 14 [8192/50000]\tLoss: 3.4472\tLR: 1.316113\n",
      "Training Epoch: 14 [8320/50000]\tLoss: 2.8345\tLR: 1.316368\n",
      "Training Epoch: 14 [8448/50000]\tLoss: 3.0056\tLR: 1.316624\n",
      "Training Epoch: 14 [8576/50000]\tLoss: 3.1890\tLR: 1.316880\n",
      "Training Epoch: 14 [8704/50000]\tLoss: 3.1125\tLR: 1.317136\n",
      "Training Epoch: 14 [8832/50000]\tLoss: 3.0145\tLR: 1.317391\n",
      "Training Epoch: 14 [8960/50000]\tLoss: 3.2907\tLR: 1.317647\n",
      "Training Epoch: 14 [9088/50000]\tLoss: 3.1350\tLR: 1.317903\n",
      "Training Epoch: 14 [9216/50000]\tLoss: 3.0001\tLR: 1.318159\n",
      "Training Epoch: 14 [9344/50000]\tLoss: 3.1916\tLR: 1.318414\n",
      "Training Epoch: 14 [9472/50000]\tLoss: 3.4022\tLR: 1.318670\n",
      "Training Epoch: 14 [9600/50000]\tLoss: 3.0536\tLR: 1.318926\n",
      "Training Epoch: 14 [9728/50000]\tLoss: 3.2218\tLR: 1.319182\n",
      "Training Epoch: 14 [9856/50000]\tLoss: 3.0079\tLR: 1.319437\n",
      "Training Epoch: 14 [9984/50000]\tLoss: 3.1058\tLR: 1.319693\n",
      "Training Epoch: 14 [10112/50000]\tLoss: 3.1692\tLR: 1.319949\n",
      "Training Epoch: 14 [10240/50000]\tLoss: 3.0708\tLR: 1.320205\n",
      "Training Epoch: 14 [10368/50000]\tLoss: 2.9891\tLR: 1.320460\n",
      "Training Epoch: 14 [10496/50000]\tLoss: 3.1735\tLR: 1.320716\n",
      "Training Epoch: 14 [10624/50000]\tLoss: 3.1803\tLR: 1.320972\n",
      "Training Epoch: 14 [10752/50000]\tLoss: 3.3618\tLR: 1.321228\n",
      "Training Epoch: 14 [10880/50000]\tLoss: 2.7811\tLR: 1.321483\n",
      "Training Epoch: 14 [11008/50000]\tLoss: 2.9286\tLR: 1.321739\n",
      "Training Epoch: 14 [11136/50000]\tLoss: 3.3676\tLR: 1.321995\n",
      "Training Epoch: 14 [11264/50000]\tLoss: 3.1851\tLR: 1.322251\n",
      "Training Epoch: 14 [11392/50000]\tLoss: 3.0193\tLR: 1.322506\n",
      "Training Epoch: 14 [11520/50000]\tLoss: 3.2520\tLR: 1.322762\n",
      "Training Epoch: 14 [11648/50000]\tLoss: 2.9527\tLR: 1.323018\n",
      "Training Epoch: 14 [11776/50000]\tLoss: 3.0789\tLR: 1.323274\n",
      "Training Epoch: 14 [11904/50000]\tLoss: 2.9728\tLR: 1.323529\n",
      "Training Epoch: 14 [12032/50000]\tLoss: 3.1849\tLR: 1.323785\n",
      "Training Epoch: 14 [12160/50000]\tLoss: 3.4513\tLR: 1.324041\n",
      "Training Epoch: 14 [12288/50000]\tLoss: 3.0865\tLR: 1.324297\n",
      "Training Epoch: 14 [12416/50000]\tLoss: 2.9811\tLR: 1.324552\n",
      "Training Epoch: 14 [12544/50000]\tLoss: 3.1958\tLR: 1.324808\n",
      "Training Epoch: 14 [12672/50000]\tLoss: 3.2700\tLR: 1.325064\n",
      "Training Epoch: 14 [12800/50000]\tLoss: 3.3295\tLR: 1.325320\n",
      "Training Epoch: 14 [12928/50000]\tLoss: 3.1915\tLR: 1.325575\n",
      "Training Epoch: 14 [13056/50000]\tLoss: 3.1339\tLR: 1.325831\n",
      "Training Epoch: 14 [13184/50000]\tLoss: 2.8520\tLR: 1.326087\n",
      "Training Epoch: 14 [13312/50000]\tLoss: 2.9488\tLR: 1.326343\n",
      "Training Epoch: 14 [13440/50000]\tLoss: 3.2900\tLR: 1.326598\n",
      "Training Epoch: 14 [13568/50000]\tLoss: 3.3731\tLR: 1.326854\n",
      "Training Epoch: 14 [13696/50000]\tLoss: 3.1798\tLR: 1.327110\n",
      "Training Epoch: 14 [13824/50000]\tLoss: 2.9861\tLR: 1.327366\n",
      "Training Epoch: 14 [13952/50000]\tLoss: 2.9789\tLR: 1.327621\n",
      "Training Epoch: 14 [14080/50000]\tLoss: 3.4739\tLR: 1.327877\n",
      "Training Epoch: 14 [14208/50000]\tLoss: 3.4518\tLR: 1.328133\n",
      "Training Epoch: 14 [14336/50000]\tLoss: 3.1651\tLR: 1.328389\n",
      "Training Epoch: 14 [14464/50000]\tLoss: 3.3245\tLR: 1.328645\n",
      "Training Epoch: 14 [14592/50000]\tLoss: 3.0908\tLR: 1.328900\n",
      "Training Epoch: 14 [14720/50000]\tLoss: 3.2984\tLR: 1.329156\n",
      "Training Epoch: 14 [14848/50000]\tLoss: 3.1602\tLR: 1.329412\n",
      "Training Epoch: 14 [14976/50000]\tLoss: 3.2356\tLR: 1.329668\n",
      "Training Epoch: 14 [15104/50000]\tLoss: 3.2242\tLR: 1.329923\n",
      "Training Epoch: 14 [15232/50000]\tLoss: 3.0210\tLR: 1.330179\n",
      "Training Epoch: 14 [15360/50000]\tLoss: 3.0839\tLR: 1.330435\n",
      "Training Epoch: 14 [15488/50000]\tLoss: 3.2599\tLR: 1.330691\n",
      "Training Epoch: 14 [15616/50000]\tLoss: 3.0914\tLR: 1.330946\n",
      "Training Epoch: 14 [15744/50000]\tLoss: 3.1353\tLR: 1.331202\n",
      "Training Epoch: 14 [15872/50000]\tLoss: 2.9519\tLR: 1.331458\n",
      "Training Epoch: 14 [16000/50000]\tLoss: 3.1282\tLR: 1.331714\n",
      "Training Epoch: 14 [16128/50000]\tLoss: 3.3005\tLR: 1.331969\n",
      "Training Epoch: 14 [16256/50000]\tLoss: 3.1133\tLR: 1.332225\n",
      "Training Epoch: 14 [16384/50000]\tLoss: 3.1313\tLR: 1.332481\n",
      "Training Epoch: 14 [16512/50000]\tLoss: 3.1004\tLR: 1.332737\n",
      "Training Epoch: 14 [16640/50000]\tLoss: 2.9999\tLR: 1.332992\n",
      "Training Epoch: 14 [16768/50000]\tLoss: 3.3100\tLR: 1.333248\n",
      "Training Epoch: 14 [16896/50000]\tLoss: 3.0887\tLR: 1.333504\n",
      "Training Epoch: 14 [17024/50000]\tLoss: 3.0633\tLR: 1.333760\n",
      "Training Epoch: 14 [17152/50000]\tLoss: 3.3149\tLR: 1.334015\n",
      "Training Epoch: 14 [17280/50000]\tLoss: 3.2480\tLR: 1.334271\n",
      "Training Epoch: 14 [17408/50000]\tLoss: 3.2205\tLR: 1.334527\n",
      "Training Epoch: 14 [17536/50000]\tLoss: 3.2993\tLR: 1.334783\n",
      "Training Epoch: 14 [17664/50000]\tLoss: 3.3232\tLR: 1.335038\n",
      "Training Epoch: 14 [17792/50000]\tLoss: 3.1524\tLR: 1.335294\n",
      "Training Epoch: 14 [17920/50000]\tLoss: 3.2896\tLR: 1.335550\n",
      "Training Epoch: 14 [18048/50000]\tLoss: 3.1345\tLR: 1.335806\n",
      "Training Epoch: 14 [18176/50000]\tLoss: 3.3861\tLR: 1.336061\n",
      "Training Epoch: 14 [18304/50000]\tLoss: 3.2247\tLR: 1.336317\n",
      "Training Epoch: 14 [18432/50000]\tLoss: 3.6818\tLR: 1.336573\n",
      "Training Epoch: 14 [18560/50000]\tLoss: 3.4675\tLR: 1.336829\n",
      "Training Epoch: 14 [18688/50000]\tLoss: 3.1468\tLR: 1.337084\n",
      "Training Epoch: 14 [18816/50000]\tLoss: 3.2184\tLR: 1.337340\n",
      "Training Epoch: 14 [18944/50000]\tLoss: 3.2529\tLR: 1.337596\n",
      "Training Epoch: 14 [19072/50000]\tLoss: 3.4449\tLR: 1.337852\n",
      "Training Epoch: 14 [19200/50000]\tLoss: 2.9899\tLR: 1.338107\n",
      "Training Epoch: 14 [19328/50000]\tLoss: 3.3294\tLR: 1.338363\n",
      "Training Epoch: 14 [19456/50000]\tLoss: 2.6842\tLR: 1.338619\n",
      "Training Epoch: 14 [19584/50000]\tLoss: 3.2887\tLR: 1.338875\n",
      "Training Epoch: 14 [19712/50000]\tLoss: 3.3745\tLR: 1.339130\n",
      "Training Epoch: 14 [19840/50000]\tLoss: 3.2289\tLR: 1.339386\n",
      "Training Epoch: 14 [19968/50000]\tLoss: 3.3083\tLR: 1.339642\n",
      "Training Epoch: 14 [20096/50000]\tLoss: 3.4730\tLR: 1.339898\n",
      "Training Epoch: 14 [20224/50000]\tLoss: 3.1238\tLR: 1.340153\n",
      "Training Epoch: 14 [20352/50000]\tLoss: 3.3036\tLR: 1.340409\n",
      "Training Epoch: 14 [20480/50000]\tLoss: 3.4952\tLR: 1.340665\n",
      "Training Epoch: 14 [20608/50000]\tLoss: 3.3811\tLR: 1.340921\n",
      "Training Epoch: 14 [20736/50000]\tLoss: 3.3290\tLR: 1.341176\n",
      "Training Epoch: 14 [20864/50000]\tLoss: 3.0745\tLR: 1.341432\n",
      "Training Epoch: 14 [20992/50000]\tLoss: 3.1184\tLR: 1.341688\n",
      "Training Epoch: 14 [21120/50000]\tLoss: 3.2483\tLR: 1.341944\n",
      "Training Epoch: 14 [21248/50000]\tLoss: 3.3548\tLR: 1.342199\n",
      "Training Epoch: 14 [21376/50000]\tLoss: 3.1188\tLR: 1.342455\n",
      "Training Epoch: 14 [21504/50000]\tLoss: 3.1064\tLR: 1.342711\n",
      "Training Epoch: 14 [21632/50000]\tLoss: 3.4029\tLR: 1.342967\n",
      "Training Epoch: 14 [21760/50000]\tLoss: 3.1500\tLR: 1.343223\n",
      "Training Epoch: 14 [21888/50000]\tLoss: 3.1583\tLR: 1.343478\n",
      "Training Epoch: 14 [22016/50000]\tLoss: 3.3244\tLR: 1.343734\n",
      "Training Epoch: 14 [22144/50000]\tLoss: 2.9412\tLR: 1.343990\n",
      "Training Epoch: 14 [22272/50000]\tLoss: 3.1795\tLR: 1.344246\n",
      "Training Epoch: 14 [22400/50000]\tLoss: 3.3710\tLR: 1.344501\n",
      "Training Epoch: 14 [22528/50000]\tLoss: 3.0237\tLR: 1.344757\n",
      "Training Epoch: 14 [22656/50000]\tLoss: 3.3446\tLR: 1.345013\n",
      "Training Epoch: 14 [22784/50000]\tLoss: 3.0553\tLR: 1.345269\n",
      "Training Epoch: 14 [22912/50000]\tLoss: 3.5506\tLR: 1.345524\n",
      "Training Epoch: 14 [23040/50000]\tLoss: 3.1488\tLR: 1.345780\n",
      "Training Epoch: 14 [23168/50000]\tLoss: 3.0632\tLR: 1.346036\n",
      "Training Epoch: 14 [23296/50000]\tLoss: 3.0548\tLR: 1.346292\n",
      "Training Epoch: 14 [23424/50000]\tLoss: 3.1113\tLR: 1.346547\n",
      "Training Epoch: 14 [23552/50000]\tLoss: 3.2120\tLR: 1.346803\n",
      "Training Epoch: 14 [23680/50000]\tLoss: 3.2593\tLR: 1.347059\n",
      "Training Epoch: 14 [23808/50000]\tLoss: 3.1274\tLR: 1.347315\n",
      "Training Epoch: 14 [23936/50000]\tLoss: 3.3043\tLR: 1.347570\n",
      "Training Epoch: 14 [24064/50000]\tLoss: 3.2009\tLR: 1.347826\n",
      "Training Epoch: 14 [24192/50000]\tLoss: 3.5111\tLR: 1.348082\n",
      "Training Epoch: 14 [24320/50000]\tLoss: 3.2045\tLR: 1.348338\n",
      "Training Epoch: 14 [24448/50000]\tLoss: 3.0336\tLR: 1.348593\n",
      "Training Epoch: 14 [24576/50000]\tLoss: 3.0984\tLR: 1.348849\n",
      "Training Epoch: 14 [24704/50000]\tLoss: 3.3874\tLR: 1.349105\n",
      "Training Epoch: 14 [24832/50000]\tLoss: 3.3078\tLR: 1.349361\n",
      "Training Epoch: 14 [24960/50000]\tLoss: 3.2395\tLR: 1.349616\n",
      "Training Epoch: 14 [25088/50000]\tLoss: 3.0649\tLR: 1.349872\n",
      "Training Epoch: 14 [25216/50000]\tLoss: 3.0568\tLR: 1.350128\n",
      "Training Epoch: 14 [25344/50000]\tLoss: 3.1024\tLR: 1.350384\n",
      "Training Epoch: 14 [25472/50000]\tLoss: 3.3895\tLR: 1.350639\n",
      "Training Epoch: 14 [25600/50000]\tLoss: 3.4233\tLR: 1.350895\n",
      "Training Epoch: 14 [25728/50000]\tLoss: 3.3384\tLR: 1.351151\n",
      "Training Epoch: 14 [25856/50000]\tLoss: 3.0591\tLR: 1.351407\n",
      "Training Epoch: 14 [25984/50000]\tLoss: 3.3188\tLR: 1.351662\n",
      "Training Epoch: 14 [26112/50000]\tLoss: 3.1241\tLR: 1.351918\n",
      "Training Epoch: 14 [26240/50000]\tLoss: 3.1793\tLR: 1.352174\n",
      "Training Epoch: 14 [26368/50000]\tLoss: 3.3322\tLR: 1.352430\n",
      "Training Epoch: 14 [26496/50000]\tLoss: 3.1370\tLR: 1.352685\n",
      "Training Epoch: 14 [26624/50000]\tLoss: 3.4333\tLR: 1.352941\n",
      "Training Epoch: 14 [26752/50000]\tLoss: 3.1575\tLR: 1.353197\n",
      "Training Epoch: 14 [26880/50000]\tLoss: 3.2235\tLR: 1.353453\n",
      "Training Epoch: 14 [27008/50000]\tLoss: 3.0909\tLR: 1.353708\n",
      "Training Epoch: 14 [27136/50000]\tLoss: 3.2190\tLR: 1.353964\n",
      "Training Epoch: 14 [27264/50000]\tLoss: 3.2298\tLR: 1.354220\n",
      "Training Epoch: 14 [27392/50000]\tLoss: 3.3881\tLR: 1.354476\n",
      "Training Epoch: 14 [27520/50000]\tLoss: 3.2559\tLR: 1.354731\n",
      "Training Epoch: 14 [27648/50000]\tLoss: 3.3048\tLR: 1.354987\n",
      "Training Epoch: 14 [27776/50000]\tLoss: 3.2785\tLR: 1.355243\n",
      "Training Epoch: 14 [27904/50000]\tLoss: 3.1721\tLR: 1.355499\n",
      "Training Epoch: 14 [28032/50000]\tLoss: 3.3049\tLR: 1.355754\n",
      "Training Epoch: 14 [28160/50000]\tLoss: 3.6919\tLR: 1.356010\n",
      "Training Epoch: 14 [28288/50000]\tLoss: 3.3101\tLR: 1.356266\n",
      "Training Epoch: 14 [28416/50000]\tLoss: 3.2047\tLR: 1.356522\n",
      "Training Epoch: 14 [28544/50000]\tLoss: 3.4542\tLR: 1.356777\n",
      "Training Epoch: 14 [28672/50000]\tLoss: 3.3766\tLR: 1.357033\n",
      "Training Epoch: 14 [28800/50000]\tLoss: 3.1581\tLR: 1.357289\n",
      "Training Epoch: 14 [28928/50000]\tLoss: 3.0066\tLR: 1.357545\n",
      "Training Epoch: 14 [29056/50000]\tLoss: 3.3490\tLR: 1.357801\n",
      "Training Epoch: 14 [29184/50000]\tLoss: 3.3969\tLR: 1.358056\n",
      "Training Epoch: 14 [29312/50000]\tLoss: 3.0806\tLR: 1.358312\n",
      "Training Epoch: 14 [29440/50000]\tLoss: 3.2868\tLR: 1.358568\n",
      "Training Epoch: 14 [29568/50000]\tLoss: 3.4075\tLR: 1.358824\n",
      "Training Epoch: 14 [29696/50000]\tLoss: 3.3796\tLR: 1.359079\n",
      "Training Epoch: 14 [29824/50000]\tLoss: 3.3271\tLR: 1.359335\n",
      "Training Epoch: 14 [29952/50000]\tLoss: 3.3167\tLR: 1.359591\n",
      "Training Epoch: 14 [30080/50000]\tLoss: 3.1031\tLR: 1.359847\n",
      "Training Epoch: 14 [30208/50000]\tLoss: 3.1389\tLR: 1.360102\n",
      "Training Epoch: 14 [30336/50000]\tLoss: 2.9991\tLR: 1.360358\n",
      "Training Epoch: 14 [30464/50000]\tLoss: 3.2641\tLR: 1.360614\n",
      "Training Epoch: 14 [30592/50000]\tLoss: 3.2894\tLR: 1.360870\n",
      "Training Epoch: 14 [30720/50000]\tLoss: 3.0730\tLR: 1.361125\n",
      "Training Epoch: 14 [30848/50000]\tLoss: 3.1689\tLR: 1.361381\n",
      "Training Epoch: 14 [30976/50000]\tLoss: 2.9613\tLR: 1.361637\n",
      "Training Epoch: 14 [31104/50000]\tLoss: 3.1743\tLR: 1.361893\n",
      "Training Epoch: 14 [31232/50000]\tLoss: 3.1498\tLR: 1.362148\n",
      "Training Epoch: 14 [31360/50000]\tLoss: 3.3596\tLR: 1.362404\n",
      "Training Epoch: 14 [31488/50000]\tLoss: 3.4228\tLR: 1.362660\n",
      "Training Epoch: 14 [31616/50000]\tLoss: 3.2031\tLR: 1.362916\n",
      "Training Epoch: 14 [31744/50000]\tLoss: 3.1504\tLR: 1.363171\n",
      "Training Epoch: 14 [31872/50000]\tLoss: 3.2035\tLR: 1.363427\n",
      "Training Epoch: 14 [32000/50000]\tLoss: 3.1873\tLR: 1.363683\n",
      "Training Epoch: 14 [32128/50000]\tLoss: 3.4345\tLR: 1.363939\n",
      "Training Epoch: 14 [32256/50000]\tLoss: 3.0433\tLR: 1.364194\n",
      "Training Epoch: 14 [32384/50000]\tLoss: 3.3316\tLR: 1.364450\n",
      "Training Epoch: 14 [32512/50000]\tLoss: 3.1151\tLR: 1.364706\n",
      "Training Epoch: 14 [32640/50000]\tLoss: 3.1294\tLR: 1.364962\n",
      "Training Epoch: 14 [32768/50000]\tLoss: 3.2444\tLR: 1.365217\n",
      "Training Epoch: 14 [32896/50000]\tLoss: 3.0550\tLR: 1.365473\n",
      "Training Epoch: 14 [33024/50000]\tLoss: 3.3295\tLR: 1.365729\n",
      "Training Epoch: 14 [33152/50000]\tLoss: 3.2242\tLR: 1.365985\n",
      "Training Epoch: 14 [33280/50000]\tLoss: 3.2790\tLR: 1.366240\n",
      "Training Epoch: 14 [33408/50000]\tLoss: 3.4555\tLR: 1.366496\n",
      "Training Epoch: 14 [33536/50000]\tLoss: 3.1964\tLR: 1.366752\n",
      "Training Epoch: 14 [33664/50000]\tLoss: 3.0647\tLR: 1.367008\n",
      "Training Epoch: 14 [33792/50000]\tLoss: 3.3039\tLR: 1.367263\n",
      "Training Epoch: 14 [33920/50000]\tLoss: 3.3229\tLR: 1.367519\n",
      "Training Epoch: 14 [34048/50000]\tLoss: 3.2027\tLR: 1.367775\n",
      "Training Epoch: 14 [34176/50000]\tLoss: 3.5190\tLR: 1.368031\n",
      "Training Epoch: 14 [34304/50000]\tLoss: 3.2090\tLR: 1.368286\n",
      "Training Epoch: 14 [34432/50000]\tLoss: 3.1454\tLR: 1.368542\n",
      "Training Epoch: 14 [34560/50000]\tLoss: 3.5012\tLR: 1.368798\n",
      "Training Epoch: 14 [34688/50000]\tLoss: 3.3550\tLR: 1.369054\n",
      "Training Epoch: 14 [34816/50000]\tLoss: 3.3888\tLR: 1.369309\n",
      "Training Epoch: 14 [34944/50000]\tLoss: 3.2099\tLR: 1.369565\n",
      "Training Epoch: 14 [35072/50000]\tLoss: 3.1221\tLR: 1.369821\n",
      "Training Epoch: 14 [35200/50000]\tLoss: 3.1601\tLR: 1.370077\n",
      "Training Epoch: 14 [35328/50000]\tLoss: 3.1393\tLR: 1.370332\n",
      "Training Epoch: 14 [35456/50000]\tLoss: 3.2474\tLR: 1.370588\n",
      "Training Epoch: 14 [35584/50000]\tLoss: 3.1234\tLR: 1.370844\n",
      "Training Epoch: 14 [35712/50000]\tLoss: 3.4261\tLR: 1.371100\n",
      "Training Epoch: 14 [35840/50000]\tLoss: 3.2589\tLR: 1.371355\n",
      "Training Epoch: 14 [35968/50000]\tLoss: 3.4380\tLR: 1.371611\n",
      "Training Epoch: 14 [36096/50000]\tLoss: 3.2438\tLR: 1.371867\n",
      "Training Epoch: 14 [36224/50000]\tLoss: 3.4263\tLR: 1.372123\n",
      "Training Epoch: 14 [36352/50000]\tLoss: 3.4086\tLR: 1.372379\n",
      "Training Epoch: 14 [36480/50000]\tLoss: 3.6237\tLR: 1.372634\n",
      "Training Epoch: 14 [36608/50000]\tLoss: 3.5110\tLR: 1.372890\n",
      "Training Epoch: 14 [36736/50000]\tLoss: 3.5990\tLR: 1.373146\n",
      "Training Epoch: 14 [36864/50000]\tLoss: 3.1288\tLR: 1.373402\n",
      "Training Epoch: 14 [36992/50000]\tLoss: 3.3770\tLR: 1.373657\n",
      "Training Epoch: 14 [37120/50000]\tLoss: 3.3012\tLR: 1.373913\n",
      "Training Epoch: 14 [37248/50000]\tLoss: 3.3440\tLR: 1.374169\n",
      "Training Epoch: 14 [37376/50000]\tLoss: 3.4398\tLR: 1.374425\n",
      "Training Epoch: 14 [37504/50000]\tLoss: 3.2883\tLR: 1.374680\n",
      "Training Epoch: 14 [37632/50000]\tLoss: 3.3148\tLR: 1.374936\n",
      "Training Epoch: 14 [37760/50000]\tLoss: 3.3486\tLR: 1.375192\n",
      "Training Epoch: 14 [37888/50000]\tLoss: 3.3027\tLR: 1.375448\n",
      "Training Epoch: 14 [38016/50000]\tLoss: 3.5085\tLR: 1.375703\n",
      "Training Epoch: 14 [38144/50000]\tLoss: 3.5519\tLR: 1.375959\n",
      "Training Epoch: 14 [38272/50000]\tLoss: 3.2482\tLR: 1.376215\n",
      "Training Epoch: 14 [38400/50000]\tLoss: 3.2329\tLR: 1.376471\n",
      "Training Epoch: 14 [38528/50000]\tLoss: 3.2697\tLR: 1.376726\n",
      "Training Epoch: 14 [38656/50000]\tLoss: 2.9969\tLR: 1.376982\n",
      "Training Epoch: 14 [38784/50000]\tLoss: 3.2905\tLR: 1.377238\n",
      "Training Epoch: 14 [38912/50000]\tLoss: 3.4554\tLR: 1.377494\n",
      "Training Epoch: 14 [39040/50000]\tLoss: 3.2961\tLR: 1.377749\n",
      "Training Epoch: 14 [39168/50000]\tLoss: 3.2923\tLR: 1.378005\n",
      "Training Epoch: 14 [39296/50000]\tLoss: 3.4945\tLR: 1.378261\n",
      "Training Epoch: 14 [39424/50000]\tLoss: 3.0887\tLR: 1.378517\n",
      "Training Epoch: 14 [39552/50000]\tLoss: 3.5158\tLR: 1.378772\n",
      "Training Epoch: 14 [39680/50000]\tLoss: 3.0869\tLR: 1.379028\n",
      "Training Epoch: 14 [39808/50000]\tLoss: 3.1939\tLR: 1.379284\n",
      "Training Epoch: 14 [39936/50000]\tLoss: 3.4353\tLR: 1.379540\n",
      "Training Epoch: 14 [40064/50000]\tLoss: 3.5422\tLR: 1.379795\n",
      "Training Epoch: 14 [40192/50000]\tLoss: 3.5050\tLR: 1.380051\n",
      "Training Epoch: 14 [40320/50000]\tLoss: 3.4702\tLR: 1.380307\n",
      "Training Epoch: 14 [40448/50000]\tLoss: 3.3619\tLR: 1.380563\n",
      "Training Epoch: 14 [40576/50000]\tLoss: 3.3516\tLR: 1.380818\n",
      "Training Epoch: 14 [40704/50000]\tLoss: 3.5725\tLR: 1.381074\n",
      "Training Epoch: 14 [40832/50000]\tLoss: 3.3537\tLR: 1.381330\n",
      "Training Epoch: 14 [40960/50000]\tLoss: 3.2106\tLR: 1.381586\n",
      "Training Epoch: 14 [41088/50000]\tLoss: 3.3251\tLR: 1.381841\n",
      "Training Epoch: 14 [41216/50000]\tLoss: 3.3353\tLR: 1.382097\n",
      "Training Epoch: 14 [41344/50000]\tLoss: 3.2000\tLR: 1.382353\n",
      "Training Epoch: 14 [41472/50000]\tLoss: 3.2564\tLR: 1.382609\n",
      "Training Epoch: 14 [41600/50000]\tLoss: 3.3626\tLR: 1.382864\n",
      "Training Epoch: 14 [41728/50000]\tLoss: 3.2952\tLR: 1.383120\n",
      "Training Epoch: 14 [41856/50000]\tLoss: 3.2710\tLR: 1.383376\n",
      "Training Epoch: 14 [41984/50000]\tLoss: 3.2119\tLR: 1.383632\n",
      "Training Epoch: 14 [42112/50000]\tLoss: 3.3307\tLR: 1.383887\n",
      "Training Epoch: 14 [42240/50000]\tLoss: 3.2292\tLR: 1.384143\n",
      "Training Epoch: 14 [42368/50000]\tLoss: 3.3072\tLR: 1.384399\n",
      "Training Epoch: 14 [42496/50000]\tLoss: 3.2126\tLR: 1.384655\n",
      "Training Epoch: 14 [42624/50000]\tLoss: 3.4143\tLR: 1.384910\n",
      "Training Epoch: 14 [42752/50000]\tLoss: 3.2578\tLR: 1.385166\n",
      "Training Epoch: 14 [42880/50000]\tLoss: 3.4011\tLR: 1.385422\n",
      "Training Epoch: 14 [43008/50000]\tLoss: 3.6743\tLR: 1.385678\n",
      "Training Epoch: 14 [43136/50000]\tLoss: 2.9645\tLR: 1.385934\n",
      "Training Epoch: 14 [43264/50000]\tLoss: 3.0373\tLR: 1.386189\n",
      "Training Epoch: 14 [43392/50000]\tLoss: 3.2508\tLR: 1.386445\n",
      "Training Epoch: 14 [43520/50000]\tLoss: 3.1531\tLR: 1.386701\n",
      "Training Epoch: 14 [43648/50000]\tLoss: 3.4434\tLR: 1.386957\n",
      "Training Epoch: 14 [43776/50000]\tLoss: 2.9996\tLR: 1.387212\n",
      "Training Epoch: 14 [43904/50000]\tLoss: 3.1103\tLR: 1.387468\n",
      "Training Epoch: 14 [44032/50000]\tLoss: 3.2497\tLR: 1.387724\n",
      "Training Epoch: 14 [44160/50000]\tLoss: 3.3957\tLR: 1.387980\n",
      "Training Epoch: 14 [44288/50000]\tLoss: 3.3884\tLR: 1.388235\n",
      "Training Epoch: 14 [44416/50000]\tLoss: 3.4324\tLR: 1.388491\n",
      "Training Epoch: 14 [44544/50000]\tLoss: 3.1133\tLR: 1.388747\n",
      "Training Epoch: 14 [44672/50000]\tLoss: 3.3254\tLR: 1.389003\n",
      "Training Epoch: 14 [44800/50000]\tLoss: 3.2171\tLR: 1.389258\n",
      "Training Epoch: 14 [44928/50000]\tLoss: 3.4626\tLR: 1.389514\n",
      "Training Epoch: 14 [45056/50000]\tLoss: 3.2475\tLR: 1.389770\n",
      "Training Epoch: 14 [45184/50000]\tLoss: 3.2773\tLR: 1.390026\n",
      "Training Epoch: 14 [45312/50000]\tLoss: 3.4259\tLR: 1.390281\n",
      "Training Epoch: 14 [45440/50000]\tLoss: 3.2971\tLR: 1.390537\n",
      "Training Epoch: 14 [45568/50000]\tLoss: 3.7768\tLR: 1.390793\n",
      "Training Epoch: 14 [45696/50000]\tLoss: 3.3194\tLR: 1.391049\n",
      "Training Epoch: 14 [45824/50000]\tLoss: 3.3376\tLR: 1.391304\n",
      "Training Epoch: 14 [45952/50000]\tLoss: 3.4141\tLR: 1.391560\n",
      "Training Epoch: 14 [46080/50000]\tLoss: 3.2888\tLR: 1.391816\n",
      "Training Epoch: 14 [46208/50000]\tLoss: 3.4040\tLR: 1.392072\n",
      "Training Epoch: 14 [46336/50000]\tLoss: 3.2408\tLR: 1.392327\n",
      "Training Epoch: 14 [46464/50000]\tLoss: 3.0476\tLR: 1.392583\n",
      "Training Epoch: 14 [46592/50000]\tLoss: 3.4685\tLR: 1.392839\n",
      "Training Epoch: 14 [46720/50000]\tLoss: 3.5090\tLR: 1.393095\n",
      "Training Epoch: 14 [46848/50000]\tLoss: 3.3718\tLR: 1.393350\n",
      "Training Epoch: 14 [46976/50000]\tLoss: 3.3856\tLR: 1.393606\n",
      "Training Epoch: 14 [47104/50000]\tLoss: 3.2771\tLR: 1.393862\n",
      "Training Epoch: 14 [47232/50000]\tLoss: 3.5029\tLR: 1.394118\n",
      "Training Epoch: 14 [47360/50000]\tLoss: 3.3061\tLR: 1.394373\n",
      "Training Epoch: 14 [47488/50000]\tLoss: 2.8676\tLR: 1.394629\n",
      "Training Epoch: 14 [47616/50000]\tLoss: 3.1800\tLR: 1.394885\n",
      "Training Epoch: 14 [47744/50000]\tLoss: 3.2236\tLR: 1.395141\n",
      "Training Epoch: 14 [47872/50000]\tLoss: 3.3238\tLR: 1.395396\n",
      "Training Epoch: 14 [48000/50000]\tLoss: 3.1612\tLR: 1.395652\n",
      "Training Epoch: 14 [48128/50000]\tLoss: 3.2673\tLR: 1.395908\n",
      "Training Epoch: 14 [48256/50000]\tLoss: 3.1167\tLR: 1.396164\n",
      "Training Epoch: 14 [48384/50000]\tLoss: 3.2529\tLR: 1.396419\n",
      "Training Epoch: 14 [48512/50000]\tLoss: 3.0172\tLR: 1.396675\n",
      "Training Epoch: 14 [48640/50000]\tLoss: 3.4487\tLR: 1.396931\n",
      "Training Epoch: 14 [48768/50000]\tLoss: 3.1265\tLR: 1.397187\n",
      "Training Epoch: 14 [48896/50000]\tLoss: 3.2391\tLR: 1.397442\n",
      "Training Epoch: 14 [49024/50000]\tLoss: 3.3359\tLR: 1.397698\n",
      "Training Epoch: 14 [49152/50000]\tLoss: 3.5113\tLR: 1.397954\n",
      "Training Epoch: 14 [49280/50000]\tLoss: 3.0538\tLR: 1.398210\n",
      "Training Epoch: 14 [49408/50000]\tLoss: 3.2328\tLR: 1.398465\n",
      "Training Epoch: 14 [49536/50000]\tLoss: 3.3887\tLR: 1.398721\n",
      "Training Epoch: 14 [49664/50000]\tLoss: 3.1617\tLR: 1.398977\n",
      "Training Epoch: 14 [49792/50000]\tLoss: 3.2920\tLR: 1.399233\n",
      "Training Epoch: 14 [49920/50000]\tLoss: 3.0164\tLR: 1.399488\n",
      "Training Epoch: 14 [50000/50000]\tLoss: 3.2670\tLR: 1.399744\n",
      "epoch 14 training time consumed: 488.84s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   19628 GB |   19628 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   19568 GB |   19568 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      60 GB |      60 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   19628 GB |   19628 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   19568 GB |   19568 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      60 GB |      60 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   19349 GB |   19349 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   19289 GB |   19289 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |      60 GB |      60 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    2081 K  |    2081 K  |\n",
      "|       from large pool |      24    |      65    |     887 K  |     887 K  |\n",
      "|       from small pool |     231    |     274    |    1194 K  |    1194 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    2081 K  |    2081 K  |\n",
      "|       from large pool |      24    |      65    |     887 K  |     887 K  |\n",
      "|       from small pool |     231    |     274    |    1194 K  |    1194 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      45    |    1205 K  |    1205 K  |\n",
      "|       from large pool |      10    |      23    |     426 K  |     426 K  |\n",
      "|       from small pool |      27    |      35    |     778 K  |     778 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 14, Average loss: 0.0421, Accuracy: 0.0739, Time consumed:31.40s\n",
      "\n",
      "Training Epoch: 15 [128/50000]\tLoss: 3.3165\tLR: 0.100000\n",
      "Training Epoch: 15 [256/50000]\tLoss: 3.1778\tLR: 1.400256\n",
      "Training Epoch: 15 [384/50000]\tLoss: 3.2098\tLR: 1.400512\n",
      "Training Epoch: 15 [512/50000]\tLoss: 3.0536\tLR: 1.400767\n",
      "Training Epoch: 15 [640/50000]\tLoss: 3.2289\tLR: 1.401023\n",
      "Training Epoch: 15 [768/50000]\tLoss: 3.1235\tLR: 1.401279\n",
      "Training Epoch: 15 [896/50000]\tLoss: 3.2801\tLR: 1.401535\n",
      "Training Epoch: 15 [1024/50000]\tLoss: 3.4050\tLR: 1.401790\n",
      "Training Epoch: 15 [1152/50000]\tLoss: 3.2704\tLR: 1.402046\n",
      "Training Epoch: 15 [1280/50000]\tLoss: 3.4213\tLR: 1.402302\n",
      "Training Epoch: 15 [1408/50000]\tLoss: 3.5426\tLR: 1.402558\n",
      "Training Epoch: 15 [1536/50000]\tLoss: 3.3564\tLR: 1.402813\n",
      "Training Epoch: 15 [1664/50000]\tLoss: 3.3420\tLR: 1.403069\n",
      "Training Epoch: 15 [1792/50000]\tLoss: 3.2577\tLR: 1.403325\n",
      "Training Epoch: 15 [1920/50000]\tLoss: 3.2919\tLR: 1.403581\n",
      "Training Epoch: 15 [2048/50000]\tLoss: 3.5027\tLR: 1.403836\n",
      "Training Epoch: 15 [2176/50000]\tLoss: 3.3566\tLR: 1.404092\n",
      "Training Epoch: 15 [2304/50000]\tLoss: 3.3672\tLR: 1.404348\n",
      "Training Epoch: 15 [2432/50000]\tLoss: 3.3805\tLR: 1.404604\n",
      "Training Epoch: 15 [2560/50000]\tLoss: 3.3721\tLR: 1.404859\n",
      "Training Epoch: 15 [2688/50000]\tLoss: 3.3057\tLR: 1.405115\n",
      "Training Epoch: 15 [2816/50000]\tLoss: 3.4501\tLR: 1.405371\n",
      "Training Epoch: 15 [2944/50000]\tLoss: 3.3645\tLR: 1.405627\n",
      "Training Epoch: 15 [3072/50000]\tLoss: 2.9392\tLR: 1.405882\n",
      "Training Epoch: 15 [3200/50000]\tLoss: 3.3756\tLR: 1.406138\n",
      "Training Epoch: 15 [3328/50000]\tLoss: 3.0857\tLR: 1.406394\n",
      "Training Epoch: 15 [3456/50000]\tLoss: 3.6017\tLR: 1.406650\n",
      "Training Epoch: 15 [3584/50000]\tLoss: 3.3172\tLR: 1.406905\n",
      "Training Epoch: 15 [3712/50000]\tLoss: 3.4911\tLR: 1.407161\n",
      "Training Epoch: 15 [3840/50000]\tLoss: 3.0409\tLR: 1.407417\n",
      "Training Epoch: 15 [3968/50000]\tLoss: 3.4324\tLR: 1.407673\n",
      "Training Epoch: 15 [4096/50000]\tLoss: 2.9999\tLR: 1.407928\n",
      "Training Epoch: 15 [4224/50000]\tLoss: 3.0123\tLR: 1.408184\n",
      "Training Epoch: 15 [4352/50000]\tLoss: 3.1926\tLR: 1.408440\n",
      "Training Epoch: 15 [4480/50000]\tLoss: 3.0933\tLR: 1.408696\n",
      "Training Epoch: 15 [4608/50000]\tLoss: 3.5563\tLR: 1.408951\n",
      "Training Epoch: 15 [4736/50000]\tLoss: 3.2035\tLR: 1.409207\n",
      "Training Epoch: 15 [4864/50000]\tLoss: 3.2798\tLR: 1.409463\n",
      "Training Epoch: 15 [4992/50000]\tLoss: 3.2103\tLR: 1.409719\n",
      "Training Epoch: 15 [5120/50000]\tLoss: 3.3376\tLR: 1.409974\n",
      "Training Epoch: 15 [5248/50000]\tLoss: 3.4134\tLR: 1.410230\n",
      "Training Epoch: 15 [5376/50000]\tLoss: 3.5841\tLR: 1.410486\n",
      "Training Epoch: 15 [5504/50000]\tLoss: 3.0468\tLR: 1.410742\n",
      "Training Epoch: 15 [5632/50000]\tLoss: 3.2731\tLR: 1.410997\n",
      "Training Epoch: 15 [5760/50000]\tLoss: 3.2794\tLR: 1.411253\n",
      "Training Epoch: 15 [5888/50000]\tLoss: 3.1886\tLR: 1.411509\n",
      "Training Epoch: 15 [6016/50000]\tLoss: 3.1429\tLR: 1.411765\n",
      "Training Epoch: 15 [6144/50000]\tLoss: 3.4196\tLR: 1.412020\n",
      "Training Epoch: 15 [6272/50000]\tLoss: 3.0972\tLR: 1.412276\n",
      "Training Epoch: 15 [6400/50000]\tLoss: 3.1554\tLR: 1.412532\n",
      "Training Epoch: 15 [6528/50000]\tLoss: 3.4223\tLR: 1.412788\n",
      "Training Epoch: 15 [6656/50000]\tLoss: 3.2068\tLR: 1.413043\n",
      "Training Epoch: 15 [6784/50000]\tLoss: 3.4373\tLR: 1.413299\n",
      "Training Epoch: 15 [6912/50000]\tLoss: 3.3224\tLR: 1.413555\n",
      "Training Epoch: 15 [7040/50000]\tLoss: 3.3835\tLR: 1.413811\n",
      "Training Epoch: 15 [7168/50000]\tLoss: 3.3862\tLR: 1.414066\n",
      "Training Epoch: 15 [7296/50000]\tLoss: 3.0120\tLR: 1.414322\n",
      "Training Epoch: 15 [7424/50000]\tLoss: 3.4520\tLR: 1.414578\n",
      "Training Epoch: 15 [7552/50000]\tLoss: 3.5046\tLR: 1.414834\n",
      "Training Epoch: 15 [7680/50000]\tLoss: 3.4652\tLR: 1.415090\n",
      "Training Epoch: 15 [7808/50000]\tLoss: 3.2149\tLR: 1.415345\n",
      "Training Epoch: 15 [7936/50000]\tLoss: 3.6124\tLR: 1.415601\n",
      "Training Epoch: 15 [8064/50000]\tLoss: 3.3786\tLR: 1.415857\n",
      "Training Epoch: 15 [8192/50000]\tLoss: 3.4645\tLR: 1.416113\n",
      "Training Epoch: 15 [8320/50000]\tLoss: 3.3817\tLR: 1.416368\n",
      "Training Epoch: 15 [8448/50000]\tLoss: 3.1377\tLR: 1.416624\n",
      "Training Epoch: 15 [8576/50000]\tLoss: 3.0732\tLR: 1.416880\n",
      "Training Epoch: 15 [8704/50000]\tLoss: 3.2657\tLR: 1.417136\n",
      "Training Epoch: 15 [8832/50000]\tLoss: 3.1973\tLR: 1.417391\n",
      "Training Epoch: 15 [8960/50000]\tLoss: 3.1281\tLR: 1.417647\n",
      "Training Epoch: 15 [9088/50000]\tLoss: 3.1577\tLR: 1.417903\n",
      "Training Epoch: 15 [9216/50000]\tLoss: 3.0511\tLR: 1.418159\n",
      "Training Epoch: 15 [9344/50000]\tLoss: 3.1434\tLR: 1.418414\n",
      "Training Epoch: 15 [9472/50000]\tLoss: 3.0915\tLR: 1.418670\n",
      "Training Epoch: 15 [9600/50000]\tLoss: 3.0254\tLR: 1.418926\n",
      "Training Epoch: 15 [9728/50000]\tLoss: 3.3043\tLR: 1.419182\n",
      "Training Epoch: 15 [9856/50000]\tLoss: 2.9600\tLR: 1.419437\n",
      "Training Epoch: 15 [9984/50000]\tLoss: 3.1559\tLR: 1.419693\n",
      "Training Epoch: 15 [10112/50000]\tLoss: 3.1774\tLR: 1.419949\n",
      "Training Epoch: 15 [10240/50000]\tLoss: 3.3910\tLR: 1.420205\n",
      "Training Epoch: 15 [10368/50000]\tLoss: 3.3283\tLR: 1.420460\n",
      "Training Epoch: 15 [10496/50000]\tLoss: 3.1092\tLR: 1.420716\n",
      "Training Epoch: 15 [10624/50000]\tLoss: 3.2867\tLR: 1.420972\n",
      "Training Epoch: 15 [10752/50000]\tLoss: 3.4535\tLR: 1.421228\n",
      "Training Epoch: 15 [10880/50000]\tLoss: 3.3888\tLR: 1.421483\n",
      "Training Epoch: 15 [11008/50000]\tLoss: 3.2319\tLR: 1.421739\n",
      "Training Epoch: 15 [11136/50000]\tLoss: 2.8758\tLR: 1.421995\n",
      "Training Epoch: 15 [11264/50000]\tLoss: 3.4721\tLR: 1.422251\n",
      "Training Epoch: 15 [11392/50000]\tLoss: 3.4544\tLR: 1.422506\n",
      "Training Epoch: 15 [11520/50000]\tLoss: 3.5864\tLR: 1.422762\n",
      "Training Epoch: 15 [11648/50000]\tLoss: 3.3210\tLR: 1.423018\n",
      "Training Epoch: 15 [11776/50000]\tLoss: 3.1524\tLR: 1.423274\n",
      "Training Epoch: 15 [11904/50000]\tLoss: 2.9482\tLR: 1.423529\n",
      "Training Epoch: 15 [12032/50000]\tLoss: 3.4267\tLR: 1.423785\n",
      "Training Epoch: 15 [12160/50000]\tLoss: 3.5129\tLR: 1.424041\n",
      "Training Epoch: 15 [12288/50000]\tLoss: 3.2665\tLR: 1.424297\n",
      "Training Epoch: 15 [12416/50000]\tLoss: 3.2650\tLR: 1.424552\n",
      "Training Epoch: 15 [12544/50000]\tLoss: 3.0987\tLR: 1.424808\n",
      "Training Epoch: 15 [12672/50000]\tLoss: 3.3782\tLR: 1.425064\n",
      "Training Epoch: 15 [12800/50000]\tLoss: 2.8988\tLR: 1.425320\n",
      "Training Epoch: 15 [12928/50000]\tLoss: 3.4108\tLR: 1.425575\n",
      "Training Epoch: 15 [13056/50000]\tLoss: 3.2230\tLR: 1.425831\n",
      "Training Epoch: 15 [13184/50000]\tLoss: 3.1352\tLR: 1.426087\n",
      "Training Epoch: 15 [13312/50000]\tLoss: 3.1274\tLR: 1.426343\n",
      "Training Epoch: 15 [13440/50000]\tLoss: 3.0672\tLR: 1.426598\n",
      "Training Epoch: 15 [13568/50000]\tLoss: 3.2667\tLR: 1.426854\n",
      "Training Epoch: 15 [13696/50000]\tLoss: 3.1714\tLR: 1.427110\n",
      "Training Epoch: 15 [13824/50000]\tLoss: 3.3471\tLR: 1.427366\n",
      "Training Epoch: 15 [13952/50000]\tLoss: 3.3338\tLR: 1.427621\n",
      "Training Epoch: 15 [14080/50000]\tLoss: 3.1754\tLR: 1.427877\n",
      "Training Epoch: 15 [14208/50000]\tLoss: 3.2702\tLR: 1.428133\n",
      "Training Epoch: 15 [14336/50000]\tLoss: 3.3719\tLR: 1.428389\n",
      "Training Epoch: 15 [14464/50000]\tLoss: 3.3058\tLR: 1.428645\n",
      "Training Epoch: 15 [14592/50000]\tLoss: 3.3972\tLR: 1.428900\n",
      "Training Epoch: 15 [14720/50000]\tLoss: 3.1594\tLR: 1.429156\n",
      "Training Epoch: 15 [14848/50000]\tLoss: 3.0303\tLR: 1.429412\n",
      "Training Epoch: 15 [14976/50000]\tLoss: 3.3756\tLR: 1.429668\n",
      "Training Epoch: 15 [15104/50000]\tLoss: 3.3029\tLR: 1.429923\n",
      "Training Epoch: 15 [15232/50000]\tLoss: 2.8336\tLR: 1.430179\n",
      "Training Epoch: 15 [15360/50000]\tLoss: 3.5673\tLR: 1.430435\n",
      "Training Epoch: 15 [15488/50000]\tLoss: 3.3525\tLR: 1.430691\n",
      "Training Epoch: 15 [15616/50000]\tLoss: 3.3025\tLR: 1.430946\n",
      "Training Epoch: 15 [15744/50000]\tLoss: 3.3654\tLR: 1.431202\n",
      "Training Epoch: 15 [15872/50000]\tLoss: 3.2242\tLR: 1.431458\n",
      "Training Epoch: 15 [16000/50000]\tLoss: 3.3310\tLR: 1.431714\n",
      "Training Epoch: 15 [16128/50000]\tLoss: 3.4889\tLR: 1.431969\n",
      "Training Epoch: 15 [16256/50000]\tLoss: 3.2872\tLR: 1.432225\n",
      "Training Epoch: 15 [16384/50000]\tLoss: 3.5939\tLR: 1.432481\n",
      "Training Epoch: 15 [16512/50000]\tLoss: 3.1674\tLR: 1.432737\n",
      "Training Epoch: 15 [16640/50000]\tLoss: 3.3302\tLR: 1.432992\n",
      "Training Epoch: 15 [16768/50000]\tLoss: 3.2560\tLR: 1.433248\n",
      "Training Epoch: 15 [16896/50000]\tLoss: 3.1588\tLR: 1.433504\n",
      "Training Epoch: 15 [17024/50000]\tLoss: 3.4083\tLR: 1.433760\n",
      "Training Epoch: 15 [17152/50000]\tLoss: 3.3327\tLR: 1.434015\n",
      "Training Epoch: 15 [17280/50000]\tLoss: 3.5481\tLR: 1.434271\n",
      "Training Epoch: 15 [17408/50000]\tLoss: 3.2584\tLR: 1.434527\n",
      "Training Epoch: 15 [17536/50000]\tLoss: 3.4562\tLR: 1.434783\n",
      "Training Epoch: 15 [17664/50000]\tLoss: 3.3938\tLR: 1.435038\n",
      "Training Epoch: 15 [17792/50000]\tLoss: 3.3227\tLR: 1.435294\n",
      "Training Epoch: 15 [17920/50000]\tLoss: 3.6611\tLR: 1.435550\n",
      "Training Epoch: 15 [18048/50000]\tLoss: 3.1021\tLR: 1.435806\n",
      "Training Epoch: 15 [18176/50000]\tLoss: 3.2434\tLR: 1.436061\n",
      "Training Epoch: 15 [18304/50000]\tLoss: 3.3628\tLR: 1.436317\n",
      "Training Epoch: 15 [18432/50000]\tLoss: 3.3393\tLR: 1.436573\n",
      "Training Epoch: 15 [18560/50000]\tLoss: 3.6345\tLR: 1.436829\n",
      "Training Epoch: 15 [18688/50000]\tLoss: 3.2078\tLR: 1.437084\n",
      "Training Epoch: 15 [18816/50000]\tLoss: 3.0872\tLR: 1.437340\n",
      "Training Epoch: 15 [18944/50000]\tLoss: 3.2969\tLR: 1.437596\n",
      "Training Epoch: 15 [19072/50000]\tLoss: 3.3317\tLR: 1.437852\n",
      "Training Epoch: 15 [19200/50000]\tLoss: 3.3315\tLR: 1.438107\n",
      "Training Epoch: 15 [19328/50000]\tLoss: 3.3140\tLR: 1.438363\n",
      "Training Epoch: 15 [19456/50000]\tLoss: 3.6110\tLR: 1.438619\n",
      "Training Epoch: 15 [19584/50000]\tLoss: 3.1751\tLR: 1.438875\n",
      "Training Epoch: 15 [19712/50000]\tLoss: 3.5216\tLR: 1.439130\n",
      "Training Epoch: 15 [19840/50000]\tLoss: 3.4424\tLR: 1.439386\n",
      "Training Epoch: 15 [19968/50000]\tLoss: 3.4844\tLR: 1.439642\n",
      "Training Epoch: 15 [20096/50000]\tLoss: 3.2040\tLR: 1.439898\n",
      "Training Epoch: 15 [20224/50000]\tLoss: 3.2927\tLR: 1.440153\n",
      "Training Epoch: 15 [20352/50000]\tLoss: 3.4588\tLR: 1.440409\n",
      "Training Epoch: 15 [20480/50000]\tLoss: 3.2256\tLR: 1.440665\n",
      "Training Epoch: 15 [20608/50000]\tLoss: 3.2950\tLR: 1.440921\n",
      "Training Epoch: 15 [20736/50000]\tLoss: 2.9833\tLR: 1.441176\n",
      "Training Epoch: 15 [20864/50000]\tLoss: 3.1516\tLR: 1.441432\n",
      "Training Epoch: 15 [20992/50000]\tLoss: 3.2569\tLR: 1.441688\n",
      "Training Epoch: 15 [21120/50000]\tLoss: 3.1526\tLR: 1.441944\n",
      "Training Epoch: 15 [21248/50000]\tLoss: 3.4996\tLR: 1.442199\n",
      "Training Epoch: 15 [21376/50000]\tLoss: 3.3595\tLR: 1.442455\n",
      "Training Epoch: 15 [21504/50000]\tLoss: 3.5092\tLR: 1.442711\n",
      "Training Epoch: 15 [21632/50000]\tLoss: 3.0454\tLR: 1.442967\n",
      "Training Epoch: 15 [21760/50000]\tLoss: 3.4209\tLR: 1.443223\n",
      "Training Epoch: 15 [21888/50000]\tLoss: 3.3992\tLR: 1.443478\n",
      "Training Epoch: 15 [22016/50000]\tLoss: 3.3940\tLR: 1.443734\n",
      "Training Epoch: 15 [22144/50000]\tLoss: 3.2182\tLR: 1.443990\n",
      "Training Epoch: 15 [22272/50000]\tLoss: 3.6406\tLR: 1.444246\n",
      "Training Epoch: 15 [22400/50000]\tLoss: 3.4780\tLR: 1.444501\n",
      "Training Epoch: 15 [22528/50000]\tLoss: 3.2219\tLR: 1.444757\n",
      "Training Epoch: 15 [22656/50000]\tLoss: 3.4825\tLR: 1.445013\n",
      "Training Epoch: 15 [22784/50000]\tLoss: 3.4416\tLR: 1.445269\n",
      "Training Epoch: 15 [22912/50000]\tLoss: 3.4846\tLR: 1.445524\n",
      "Training Epoch: 15 [23040/50000]\tLoss: 3.4621\tLR: 1.445780\n",
      "Training Epoch: 15 [23168/50000]\tLoss: 3.1993\tLR: 1.446036\n",
      "Training Epoch: 15 [23296/50000]\tLoss: 3.3232\tLR: 1.446292\n",
      "Training Epoch: 15 [23424/50000]\tLoss: 3.2822\tLR: 1.446547\n",
      "Training Epoch: 15 [23552/50000]\tLoss: 2.9678\tLR: 1.446803\n",
      "Training Epoch: 15 [23680/50000]\tLoss: 3.0780\tLR: 1.447059\n",
      "Training Epoch: 15 [23808/50000]\tLoss: 3.3925\tLR: 1.447315\n",
      "Training Epoch: 15 [23936/50000]\tLoss: 3.2554\tLR: 1.447570\n",
      "Training Epoch: 15 [24064/50000]\tLoss: 3.4019\tLR: 1.447826\n",
      "Training Epoch: 15 [24192/50000]\tLoss: 3.2788\tLR: 1.448082\n",
      "Training Epoch: 15 [24320/50000]\tLoss: 3.3949\tLR: 1.448338\n",
      "Training Epoch: 15 [24448/50000]\tLoss: 3.2860\tLR: 1.448593\n",
      "Training Epoch: 15 [24576/50000]\tLoss: 3.4957\tLR: 1.448849\n",
      "Training Epoch: 15 [24704/50000]\tLoss: 3.1073\tLR: 1.449105\n",
      "Training Epoch: 15 [24832/50000]\tLoss: 3.1879\tLR: 1.449361\n",
      "Training Epoch: 15 [24960/50000]\tLoss: 3.3700\tLR: 1.449616\n",
      "Training Epoch: 15 [25088/50000]\tLoss: 3.3137\tLR: 1.449872\n",
      "Training Epoch: 15 [25216/50000]\tLoss: 3.3440\tLR: 1.450128\n",
      "Training Epoch: 15 [25344/50000]\tLoss: 3.5117\tLR: 1.450384\n",
      "Training Epoch: 15 [25472/50000]\tLoss: 3.4278\tLR: 1.450639\n",
      "Training Epoch: 15 [25600/50000]\tLoss: 3.1712\tLR: 1.450895\n",
      "Training Epoch: 15 [25728/50000]\tLoss: 3.3816\tLR: 1.451151\n",
      "Training Epoch: 15 [25856/50000]\tLoss: 3.4266\tLR: 1.451407\n",
      "Training Epoch: 15 [25984/50000]\tLoss: 3.2502\tLR: 1.451662\n",
      "Training Epoch: 15 [26112/50000]\tLoss: 3.4437\tLR: 1.451918\n",
      "Training Epoch: 15 [26240/50000]\tLoss: 3.2765\tLR: 1.452174\n",
      "Training Epoch: 15 [26368/50000]\tLoss: 3.4675\tLR: 1.452430\n",
      "Training Epoch: 15 [26496/50000]\tLoss: 3.1681\tLR: 1.452685\n",
      "Training Epoch: 15 [26624/50000]\tLoss: 3.3017\tLR: 1.452941\n",
      "Training Epoch: 15 [26752/50000]\tLoss: 3.4137\tLR: 1.453197\n",
      "Training Epoch: 15 [26880/50000]\tLoss: 3.4616\tLR: 1.453453\n",
      "Training Epoch: 15 [27008/50000]\tLoss: 3.3690\tLR: 1.453708\n",
      "Training Epoch: 15 [27136/50000]\tLoss: 3.1921\tLR: 1.453964\n",
      "Training Epoch: 15 [27264/50000]\tLoss: 3.3038\tLR: 1.454220\n",
      "Training Epoch: 15 [27392/50000]\tLoss: 3.4385\tLR: 1.454476\n",
      "Training Epoch: 15 [27520/50000]\tLoss: 3.3558\tLR: 1.454731\n",
      "Training Epoch: 15 [27648/50000]\tLoss: 3.3610\tLR: 1.454987\n",
      "Training Epoch: 15 [27776/50000]\tLoss: 3.4241\tLR: 1.455243\n",
      "Training Epoch: 15 [27904/50000]\tLoss: 3.4320\tLR: 1.455499\n",
      "Training Epoch: 15 [28032/50000]\tLoss: 3.1816\tLR: 1.455754\n",
      "Training Epoch: 15 [28160/50000]\tLoss: 3.3826\tLR: 1.456010\n",
      "Training Epoch: 15 [28288/50000]\tLoss: 3.3603\tLR: 1.456266\n",
      "Training Epoch: 15 [28416/50000]\tLoss: 3.3658\tLR: 1.456522\n",
      "Training Epoch: 15 [28544/50000]\tLoss: 3.1518\tLR: 1.456777\n",
      "Training Epoch: 15 [28672/50000]\tLoss: 3.2128\tLR: 1.457033\n",
      "Training Epoch: 15 [28800/50000]\tLoss: 3.3745\tLR: 1.457289\n",
      "Training Epoch: 15 [28928/50000]\tLoss: 3.4437\tLR: 1.457545\n",
      "Training Epoch: 15 [29056/50000]\tLoss: 3.1677\tLR: 1.457801\n",
      "Training Epoch: 15 [29184/50000]\tLoss: 3.4113\tLR: 1.458056\n",
      "Training Epoch: 15 [29312/50000]\tLoss: 3.0018\tLR: 1.458312\n",
      "Training Epoch: 15 [29440/50000]\tLoss: 3.1277\tLR: 1.458568\n",
      "Training Epoch: 15 [29568/50000]\tLoss: 3.3834\tLR: 1.458824\n",
      "Training Epoch: 15 [29696/50000]\tLoss: 3.5845\tLR: 1.459079\n",
      "Training Epoch: 15 [29824/50000]\tLoss: 3.5169\tLR: 1.459335\n",
      "Training Epoch: 15 [29952/50000]\tLoss: 3.5139\tLR: 1.459591\n",
      "Training Epoch: 15 [30080/50000]\tLoss: 3.4169\tLR: 1.459847\n",
      "Training Epoch: 15 [30208/50000]\tLoss: 3.5948\tLR: 1.460102\n",
      "Training Epoch: 15 [30336/50000]\tLoss: 3.3351\tLR: 1.460358\n",
      "Training Epoch: 15 [30464/50000]\tLoss: 3.4964\tLR: 1.460614\n",
      "Training Epoch: 15 [30592/50000]\tLoss: 3.3611\tLR: 1.460870\n",
      "Training Epoch: 15 [30720/50000]\tLoss: 3.3712\tLR: 1.461125\n",
      "Training Epoch: 15 [30848/50000]\tLoss: 3.3552\tLR: 1.461381\n",
      "Training Epoch: 15 [30976/50000]\tLoss: 3.4162\tLR: 1.461637\n",
      "Training Epoch: 15 [31104/50000]\tLoss: 3.3290\tLR: 1.461893\n",
      "Training Epoch: 15 [31232/50000]\tLoss: 3.2938\tLR: 1.462148\n",
      "Training Epoch: 15 [31360/50000]\tLoss: 3.3989\tLR: 1.462404\n",
      "Training Epoch: 15 [31488/50000]\tLoss: 3.4302\tLR: 1.462660\n",
      "Training Epoch: 15 [31616/50000]\tLoss: 3.4466\tLR: 1.462916\n",
      "Training Epoch: 15 [31744/50000]\tLoss: 3.3425\tLR: 1.463171\n",
      "Training Epoch: 15 [31872/50000]\tLoss: 3.3320\tLR: 1.463427\n",
      "Training Epoch: 15 [32000/50000]\tLoss: 3.1986\tLR: 1.463683\n",
      "Training Epoch: 15 [32128/50000]\tLoss: 3.1526\tLR: 1.463939\n",
      "Training Epoch: 15 [32256/50000]\tLoss: 3.2841\tLR: 1.464194\n",
      "Training Epoch: 15 [32384/50000]\tLoss: 3.4823\tLR: 1.464450\n",
      "Training Epoch: 15 [32512/50000]\tLoss: 3.2278\tLR: 1.464706\n",
      "Training Epoch: 15 [32640/50000]\tLoss: 3.3664\tLR: 1.464962\n",
      "Training Epoch: 15 [32768/50000]\tLoss: 3.3386\tLR: 1.465217\n",
      "Training Epoch: 15 [32896/50000]\tLoss: 3.3850\tLR: 1.465473\n",
      "Training Epoch: 15 [33024/50000]\tLoss: 3.1790\tLR: 1.465729\n",
      "Training Epoch: 15 [33152/50000]\tLoss: 3.4470\tLR: 1.465985\n",
      "Training Epoch: 15 [33280/50000]\tLoss: 3.5632\tLR: 1.466240\n",
      "Training Epoch: 15 [33408/50000]\tLoss: 3.1882\tLR: 1.466496\n",
      "Training Epoch: 15 [33536/50000]\tLoss: 3.4376\tLR: 1.466752\n",
      "Training Epoch: 15 [33664/50000]\tLoss: 3.5685\tLR: 1.467008\n",
      "Training Epoch: 15 [33792/50000]\tLoss: 3.4461\tLR: 1.467263\n",
      "Training Epoch: 15 [33920/50000]\tLoss: 3.5053\tLR: 1.467519\n",
      "Training Epoch: 15 [34048/50000]\tLoss: 3.3639\tLR: 1.467775\n",
      "Training Epoch: 15 [34176/50000]\tLoss: 3.2618\tLR: 1.468031\n",
      "Training Epoch: 15 [34304/50000]\tLoss: 3.2799\tLR: 1.468286\n",
      "Training Epoch: 15 [34432/50000]\tLoss: 3.0565\tLR: 1.468542\n",
      "Training Epoch: 15 [34560/50000]\tLoss: 3.2718\tLR: 1.468798\n",
      "Training Epoch: 15 [34688/50000]\tLoss: 3.5529\tLR: 1.469054\n",
      "Training Epoch: 15 [34816/50000]\tLoss: 3.4231\tLR: 1.469309\n",
      "Training Epoch: 15 [34944/50000]\tLoss: 3.4762\tLR: 1.469565\n",
      "Training Epoch: 15 [35072/50000]\tLoss: 3.3675\tLR: 1.469821\n",
      "Training Epoch: 15 [35200/50000]\tLoss: 3.4830\tLR: 1.470077\n",
      "Training Epoch: 15 [35328/50000]\tLoss: 3.2377\tLR: 1.470332\n",
      "Training Epoch: 15 [35456/50000]\tLoss: 3.1723\tLR: 1.470588\n",
      "Training Epoch: 15 [35584/50000]\tLoss: 3.5206\tLR: 1.470844\n",
      "Training Epoch: 15 [35712/50000]\tLoss: 3.4105\tLR: 1.471100\n",
      "Training Epoch: 15 [35840/50000]\tLoss: 3.3139\tLR: 1.471355\n",
      "Training Epoch: 15 [35968/50000]\tLoss: 3.3971\tLR: 1.471611\n",
      "Training Epoch: 15 [36096/50000]\tLoss: 3.3929\tLR: 1.471867\n",
      "Training Epoch: 15 [36224/50000]\tLoss: 3.3474\tLR: 1.472123\n",
      "Training Epoch: 15 [36352/50000]\tLoss: 3.1237\tLR: 1.472379\n",
      "Training Epoch: 15 [36480/50000]\tLoss: 3.3621\tLR: 1.472634\n",
      "Training Epoch: 15 [36608/50000]\tLoss: 3.4860\tLR: 1.472890\n",
      "Training Epoch: 15 [36736/50000]\tLoss: 3.1990\tLR: 1.473146\n",
      "Training Epoch: 15 [36864/50000]\tLoss: 3.4682\tLR: 1.473402\n",
      "Training Epoch: 15 [36992/50000]\tLoss: 3.4098\tLR: 1.473657\n",
      "Training Epoch: 15 [37120/50000]\tLoss: 3.5651\tLR: 1.473913\n",
      "Training Epoch: 15 [37248/50000]\tLoss: 3.1363\tLR: 1.474169\n",
      "Training Epoch: 15 [37376/50000]\tLoss: 3.5900\tLR: 1.474425\n",
      "Training Epoch: 15 [37504/50000]\tLoss: 3.3204\tLR: 1.474680\n",
      "Training Epoch: 15 [37632/50000]\tLoss: 3.1846\tLR: 1.474936\n",
      "Training Epoch: 15 [37760/50000]\tLoss: 3.1927\tLR: 1.475192\n",
      "Training Epoch: 15 [37888/50000]\tLoss: 3.2807\tLR: 1.475448\n",
      "Training Epoch: 15 [38016/50000]\tLoss: 3.2254\tLR: 1.475703\n",
      "Training Epoch: 15 [38144/50000]\tLoss: 3.3859\tLR: 1.475959\n",
      "Training Epoch: 15 [38272/50000]\tLoss: 3.4924\tLR: 1.476215\n",
      "Training Epoch: 15 [38400/50000]\tLoss: 3.4770\tLR: 1.476471\n",
      "Training Epoch: 15 [38528/50000]\tLoss: 3.4805\tLR: 1.476726\n",
      "Training Epoch: 15 [38656/50000]\tLoss: 3.1414\tLR: 1.476982\n",
      "Training Epoch: 15 [38784/50000]\tLoss: 3.6709\tLR: 1.477238\n",
      "Training Epoch: 15 [38912/50000]\tLoss: 3.3770\tLR: 1.477494\n",
      "Training Epoch: 15 [39040/50000]\tLoss: 3.1303\tLR: 1.477749\n",
      "Training Epoch: 15 [39168/50000]\tLoss: 3.4319\tLR: 1.478005\n",
      "Training Epoch: 15 [39296/50000]\tLoss: 3.4098\tLR: 1.478261\n",
      "Training Epoch: 15 [39424/50000]\tLoss: 3.0809\tLR: 1.478517\n",
      "Training Epoch: 15 [39552/50000]\tLoss: 3.3312\tLR: 1.478772\n",
      "Training Epoch: 15 [39680/50000]\tLoss: 3.3623\tLR: 1.479028\n",
      "Training Epoch: 15 [39808/50000]\tLoss: 3.1145\tLR: 1.479284\n",
      "Training Epoch: 15 [39936/50000]\tLoss: 3.0472\tLR: 1.479540\n",
      "Training Epoch: 15 [40064/50000]\tLoss: 3.4472\tLR: 1.479795\n",
      "Training Epoch: 15 [40192/50000]\tLoss: 3.4659\tLR: 1.480051\n",
      "Training Epoch: 15 [40320/50000]\tLoss: 3.5724\tLR: 1.480307\n",
      "Training Epoch: 15 [40448/50000]\tLoss: 3.3900\tLR: 1.480563\n",
      "Training Epoch: 15 [40576/50000]\tLoss: 3.2849\tLR: 1.480818\n",
      "Training Epoch: 15 [40704/50000]\tLoss: 3.5268\tLR: 1.481074\n",
      "Training Epoch: 15 [40832/50000]\tLoss: 3.2435\tLR: 1.481330\n",
      "Training Epoch: 15 [40960/50000]\tLoss: 3.5473\tLR: 1.481586\n",
      "Training Epoch: 15 [41088/50000]\tLoss: 3.3429\tLR: 1.481841\n",
      "Training Epoch: 15 [41216/50000]\tLoss: 3.5505\tLR: 1.482097\n",
      "Training Epoch: 15 [41344/50000]\tLoss: 3.3680\tLR: 1.482353\n",
      "Training Epoch: 15 [41472/50000]\tLoss: 3.1237\tLR: 1.482609\n",
      "Training Epoch: 15 [41600/50000]\tLoss: 3.3898\tLR: 1.482864\n",
      "Training Epoch: 15 [41728/50000]\tLoss: 3.4175\tLR: 1.483120\n",
      "Training Epoch: 15 [41856/50000]\tLoss: 3.5238\tLR: 1.483376\n",
      "Training Epoch: 15 [41984/50000]\tLoss: 3.2023\tLR: 1.483632\n",
      "Training Epoch: 15 [42112/50000]\tLoss: 3.5238\tLR: 1.483887\n",
      "Training Epoch: 15 [42240/50000]\tLoss: 3.4725\tLR: 1.484143\n",
      "Training Epoch: 15 [42368/50000]\tLoss: 3.3224\tLR: 1.484399\n",
      "Training Epoch: 15 [42496/50000]\tLoss: 3.4362\tLR: 1.484655\n",
      "Training Epoch: 15 [42624/50000]\tLoss: 3.6319\tLR: 1.484910\n",
      "Training Epoch: 15 [42752/50000]\tLoss: 3.3567\tLR: 1.485166\n",
      "Training Epoch: 15 [42880/50000]\tLoss: 3.3132\tLR: 1.485422\n",
      "Training Epoch: 15 [43008/50000]\tLoss: 3.6404\tLR: 1.485678\n",
      "Training Epoch: 15 [43136/50000]\tLoss: 3.4314\tLR: 1.485934\n",
      "Training Epoch: 15 [43264/50000]\tLoss: 3.4689\tLR: 1.486189\n",
      "Training Epoch: 15 [43392/50000]\tLoss: 3.4971\tLR: 1.486445\n",
      "Training Epoch: 15 [43520/50000]\tLoss: 3.5061\tLR: 1.486701\n",
      "Training Epoch: 15 [43648/50000]\tLoss: 3.5006\tLR: 1.486957\n",
      "Training Epoch: 15 [43776/50000]\tLoss: 3.1933\tLR: 1.487212\n",
      "Training Epoch: 15 [43904/50000]\tLoss: 3.6262\tLR: 1.487468\n",
      "Training Epoch: 15 [44032/50000]\tLoss: 3.2788\tLR: 1.487724\n",
      "Training Epoch: 15 [44160/50000]\tLoss: 3.2993\tLR: 1.487980\n",
      "Training Epoch: 15 [44288/50000]\tLoss: 3.3962\tLR: 1.488235\n",
      "Training Epoch: 15 [44416/50000]\tLoss: 3.3772\tLR: 1.488491\n",
      "Training Epoch: 15 [44544/50000]\tLoss: 3.1834\tLR: 1.488747\n",
      "Training Epoch: 15 [44672/50000]\tLoss: 3.3143\tLR: 1.489003\n",
      "Training Epoch: 15 [44800/50000]\tLoss: 3.3031\tLR: 1.489258\n",
      "Training Epoch: 15 [44928/50000]\tLoss: 3.3192\tLR: 1.489514\n",
      "Training Epoch: 15 [45056/50000]\tLoss: 3.3969\tLR: 1.489770\n",
      "Training Epoch: 15 [45184/50000]\tLoss: 2.9486\tLR: 1.490026\n",
      "Training Epoch: 15 [45312/50000]\tLoss: 3.3373\tLR: 1.490281\n",
      "Training Epoch: 15 [45440/50000]\tLoss: 3.4243\tLR: 1.490537\n",
      "Training Epoch: 15 [45568/50000]\tLoss: 3.1581\tLR: 1.490793\n",
      "Training Epoch: 15 [45696/50000]\tLoss: 3.5046\tLR: 1.491049\n",
      "Training Epoch: 15 [45824/50000]\tLoss: 3.3370\tLR: 1.491304\n",
      "Training Epoch: 15 [45952/50000]\tLoss: 3.2469\tLR: 1.491560\n",
      "Training Epoch: 15 [46080/50000]\tLoss: 3.4277\tLR: 1.491816\n",
      "Training Epoch: 15 [46208/50000]\tLoss: 3.3393\tLR: 1.492072\n",
      "Training Epoch: 15 [46336/50000]\tLoss: 3.1667\tLR: 1.492327\n",
      "Training Epoch: 15 [46464/50000]\tLoss: 3.7470\tLR: 1.492583\n",
      "Training Epoch: 15 [46592/50000]\tLoss: 3.3378\tLR: 1.492839\n",
      "Training Epoch: 15 [46720/50000]\tLoss: 3.3702\tLR: 1.493095\n",
      "Training Epoch: 15 [46848/50000]\tLoss: 3.2988\tLR: 1.493350\n",
      "Training Epoch: 15 [46976/50000]\tLoss: 3.4456\tLR: 1.493606\n",
      "Training Epoch: 15 [47104/50000]\tLoss: 3.4131\tLR: 1.493862\n",
      "Training Epoch: 15 [47232/50000]\tLoss: 3.0773\tLR: 1.494118\n",
      "Training Epoch: 15 [47360/50000]\tLoss: 3.3773\tLR: 1.494373\n",
      "Training Epoch: 15 [47488/50000]\tLoss: 3.3618\tLR: 1.494629\n",
      "Training Epoch: 15 [47616/50000]\tLoss: 3.3199\tLR: 1.494885\n",
      "Training Epoch: 15 [47744/50000]\tLoss: 3.2704\tLR: 1.495141\n",
      "Training Epoch: 15 [47872/50000]\tLoss: 3.4125\tLR: 1.495396\n",
      "Training Epoch: 15 [48000/50000]\tLoss: 3.6533\tLR: 1.495652\n",
      "Training Epoch: 15 [48128/50000]\tLoss: 3.2710\tLR: 1.495908\n",
      "Training Epoch: 15 [48256/50000]\tLoss: 3.3612\tLR: 1.496164\n",
      "Training Epoch: 15 [48384/50000]\tLoss: 3.4415\tLR: 1.496419\n",
      "Training Epoch: 15 [48512/50000]\tLoss: 3.3194\tLR: 1.496675\n",
      "Training Epoch: 15 [48640/50000]\tLoss: 3.1071\tLR: 1.496931\n",
      "Training Epoch: 15 [48768/50000]\tLoss: 3.5636\tLR: 1.497187\n",
      "Training Epoch: 15 [48896/50000]\tLoss: 3.1231\tLR: 1.497442\n",
      "Training Epoch: 15 [49024/50000]\tLoss: 3.4366\tLR: 1.497698\n",
      "Training Epoch: 15 [49152/50000]\tLoss: 3.5764\tLR: 1.497954\n",
      "Training Epoch: 15 [49280/50000]\tLoss: 3.5307\tLR: 1.498210\n",
      "Training Epoch: 15 [49408/50000]\tLoss: 3.3138\tLR: 1.498465\n",
      "Training Epoch: 15 [49536/50000]\tLoss: 3.4545\tLR: 1.498721\n",
      "Training Epoch: 15 [49664/50000]\tLoss: 3.3655\tLR: 1.498977\n",
      "Training Epoch: 15 [49792/50000]\tLoss: 3.3907\tLR: 1.499233\n",
      "Training Epoch: 15 [49920/50000]\tLoss: 3.5354\tLR: 1.499488\n",
      "Training Epoch: 15 [50000/50000]\tLoss: 3.4950\tLR: 1.499744\n",
      "epoch 15 training time consumed: 488.83s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   21030 GB |   21030 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   20965 GB |   20965 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      64 GB |      64 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   21030 GB |   21030 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   20965 GB |   20965 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      64 GB |      64 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   20731 GB |   20731 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   20667 GB |   20667 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |      64 GB |      64 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    2230 K  |    2230 K  |\n",
      "|       from large pool |      24    |      65    |     950 K  |     950 K  |\n",
      "|       from small pool |     231    |     274    |    1279 K  |    1279 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    2230 K  |    2230 K  |\n",
      "|       from large pool |      24    |      65    |     950 K  |     950 K  |\n",
      "|       from small pool |     231    |     274    |    1279 K  |    1279 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      45    |    1291 K  |    1291 K  |\n",
      "|       from large pool |      10    |      23    |     456 K  |     456 K  |\n",
      "|       from small pool |      27    |      35    |     834 K  |     834 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 15, Average loss: 0.0337, Accuracy: 0.1183, Time consumed:31.05s\n",
      "\n",
      "Training Epoch: 16 [128/50000]\tLoss: 3.5201\tLR: 0.100000\n",
      "Training Epoch: 16 [256/50000]\tLoss: 3.5415\tLR: 1.500256\n",
      "Training Epoch: 16 [384/50000]\tLoss: 3.2093\tLR: 1.500512\n",
      "Training Epoch: 16 [512/50000]\tLoss: 3.1537\tLR: 1.500767\n",
      "Training Epoch: 16 [640/50000]\tLoss: 3.2802\tLR: 1.501023\n",
      "Training Epoch: 16 [768/50000]\tLoss: 3.4116\tLR: 1.501279\n",
      "Training Epoch: 16 [896/50000]\tLoss: 2.9621\tLR: 1.501535\n",
      "Training Epoch: 16 [1024/50000]\tLoss: 3.3024\tLR: 1.501790\n",
      "Training Epoch: 16 [1152/50000]\tLoss: 3.5226\tLR: 1.502046\n",
      "Training Epoch: 16 [1280/50000]\tLoss: 3.4145\tLR: 1.502302\n",
      "Training Epoch: 16 [1408/50000]\tLoss: 3.3823\tLR: 1.502558\n",
      "Training Epoch: 16 [1536/50000]\tLoss: 3.4720\tLR: 1.502813\n",
      "Training Epoch: 16 [1664/50000]\tLoss: 3.5357\tLR: 1.503069\n",
      "Training Epoch: 16 [1792/50000]\tLoss: 3.5531\tLR: 1.503325\n",
      "Training Epoch: 16 [1920/50000]\tLoss: 3.4752\tLR: 1.503581\n",
      "Training Epoch: 16 [2048/50000]\tLoss: 3.4418\tLR: 1.503836\n",
      "Training Epoch: 16 [2176/50000]\tLoss: 3.4521\tLR: 1.504092\n",
      "Training Epoch: 16 [2304/50000]\tLoss: 3.2902\tLR: 1.504348\n",
      "Training Epoch: 16 [2432/50000]\tLoss: 3.5735\tLR: 1.504604\n",
      "Training Epoch: 16 [2560/50000]\tLoss: 3.5032\tLR: 1.504859\n",
      "Training Epoch: 16 [2688/50000]\tLoss: 3.6607\tLR: 1.505115\n",
      "Training Epoch: 16 [2816/50000]\tLoss: 3.3027\tLR: 1.505371\n",
      "Training Epoch: 16 [2944/50000]\tLoss: 3.5284\tLR: 1.505627\n",
      "Training Epoch: 16 [3072/50000]\tLoss: 3.4941\tLR: 1.505882\n",
      "Training Epoch: 16 [3200/50000]\tLoss: 3.3451\tLR: 1.506138\n",
      "Training Epoch: 16 [3328/50000]\tLoss: 3.3479\tLR: 1.506394\n",
      "Training Epoch: 16 [3456/50000]\tLoss: 3.4594\tLR: 1.506650\n",
      "Training Epoch: 16 [3584/50000]\tLoss: 3.2164\tLR: 1.506905\n",
      "Training Epoch: 16 [3712/50000]\tLoss: 3.6679\tLR: 1.507161\n",
      "Training Epoch: 16 [3840/50000]\tLoss: 3.5646\tLR: 1.507417\n",
      "Training Epoch: 16 [3968/50000]\tLoss: 3.3995\tLR: 1.507673\n",
      "Training Epoch: 16 [4096/50000]\tLoss: 3.4488\tLR: 1.507928\n",
      "Training Epoch: 16 [4224/50000]\tLoss: 3.5457\tLR: 1.508184\n",
      "Training Epoch: 16 [4352/50000]\tLoss: 3.2510\tLR: 1.508440\n",
      "Training Epoch: 16 [4480/50000]\tLoss: 3.6412\tLR: 1.508696\n",
      "Training Epoch: 16 [4608/50000]\tLoss: 3.4179\tLR: 1.508951\n",
      "Training Epoch: 16 [4736/50000]\tLoss: 3.3423\tLR: 1.509207\n",
      "Training Epoch: 16 [4864/50000]\tLoss: 3.3469\tLR: 1.509463\n",
      "Training Epoch: 16 [4992/50000]\tLoss: 3.3136\tLR: 1.509719\n",
      "Training Epoch: 16 [5120/50000]\tLoss: 3.1898\tLR: 1.509974\n",
      "Training Epoch: 16 [5248/50000]\tLoss: 3.5177\tLR: 1.510230\n",
      "Training Epoch: 16 [5376/50000]\tLoss: 3.1254\tLR: 1.510486\n",
      "Training Epoch: 16 [5504/50000]\tLoss: 3.0528\tLR: 1.510742\n",
      "Training Epoch: 16 [5632/50000]\tLoss: 3.1987\tLR: 1.510997\n",
      "Training Epoch: 16 [5760/50000]\tLoss: 3.3169\tLR: 1.511253\n",
      "Training Epoch: 16 [5888/50000]\tLoss: 3.4198\tLR: 1.511509\n",
      "Training Epoch: 16 [6016/50000]\tLoss: 3.7080\tLR: 1.511765\n",
      "Training Epoch: 16 [6144/50000]\tLoss: 3.4901\tLR: 1.512020\n",
      "Training Epoch: 16 [6272/50000]\tLoss: 3.4337\tLR: 1.512276\n",
      "Training Epoch: 16 [6400/50000]\tLoss: 3.4108\tLR: 1.512532\n",
      "Training Epoch: 16 [6528/50000]\tLoss: 3.6882\tLR: 1.512788\n",
      "Training Epoch: 16 [6656/50000]\tLoss: 3.3167\tLR: 1.513043\n",
      "Training Epoch: 16 [6784/50000]\tLoss: 3.5018\tLR: 1.513299\n",
      "Training Epoch: 16 [6912/50000]\tLoss: 3.3807\tLR: 1.513555\n",
      "Training Epoch: 16 [7040/50000]\tLoss: 3.5382\tLR: 1.513811\n",
      "Training Epoch: 16 [7168/50000]\tLoss: 3.2543\tLR: 1.514066\n",
      "Training Epoch: 16 [7296/50000]\tLoss: 3.2813\tLR: 1.514322\n",
      "Training Epoch: 16 [7424/50000]\tLoss: 3.4261\tLR: 1.514578\n",
      "Training Epoch: 16 [7552/50000]\tLoss: 3.2466\tLR: 1.514834\n",
      "Training Epoch: 16 [7680/50000]\tLoss: 3.2054\tLR: 1.515090\n",
      "Training Epoch: 16 [7808/50000]\tLoss: 3.4616\tLR: 1.515345\n",
      "Training Epoch: 16 [7936/50000]\tLoss: 3.0651\tLR: 1.515601\n",
      "Training Epoch: 16 [8064/50000]\tLoss: 3.4712\tLR: 1.515857\n",
      "Training Epoch: 16 [8192/50000]\tLoss: 3.2767\tLR: 1.516113\n",
      "Training Epoch: 16 [8320/50000]\tLoss: 3.4118\tLR: 1.516368\n",
      "Training Epoch: 16 [8448/50000]\tLoss: 3.3567\tLR: 1.516624\n",
      "Training Epoch: 16 [8576/50000]\tLoss: 3.4287\tLR: 1.516880\n",
      "Training Epoch: 16 [8704/50000]\tLoss: 3.2490\tLR: 1.517136\n",
      "Training Epoch: 16 [8832/50000]\tLoss: 3.3545\tLR: 1.517391\n",
      "Training Epoch: 16 [8960/50000]\tLoss: 3.4029\tLR: 1.517647\n",
      "Training Epoch: 16 [9088/50000]\tLoss: 3.2896\tLR: 1.517903\n",
      "Training Epoch: 16 [9216/50000]\tLoss: 3.5752\tLR: 1.518159\n",
      "Training Epoch: 16 [9344/50000]\tLoss: 3.2669\tLR: 1.518414\n",
      "Training Epoch: 16 [9472/50000]\tLoss: 3.4334\tLR: 1.518670\n",
      "Training Epoch: 16 [9600/50000]\tLoss: 3.4591\tLR: 1.518926\n",
      "Training Epoch: 16 [9728/50000]\tLoss: 3.3870\tLR: 1.519182\n",
      "Training Epoch: 16 [9856/50000]\tLoss: 3.2500\tLR: 1.519437\n",
      "Training Epoch: 16 [9984/50000]\tLoss: 3.6532\tLR: 1.519693\n",
      "Training Epoch: 16 [10112/50000]\tLoss: 3.3780\tLR: 1.519949\n",
      "Training Epoch: 16 [10240/50000]\tLoss: 3.3398\tLR: 1.520205\n",
      "Training Epoch: 16 [10368/50000]\tLoss: 3.2878\tLR: 1.520460\n",
      "Training Epoch: 16 [10496/50000]\tLoss: 3.3642\tLR: 1.520716\n",
      "Training Epoch: 16 [10624/50000]\tLoss: 3.3795\tLR: 1.520972\n",
      "Training Epoch: 16 [10752/50000]\tLoss: 3.3189\tLR: 1.521228\n",
      "Training Epoch: 16 [10880/50000]\tLoss: 3.5054\tLR: 1.521483\n",
      "Training Epoch: 16 [11008/50000]\tLoss: 3.3117\tLR: 1.521739\n",
      "Training Epoch: 16 [11136/50000]\tLoss: 3.1492\tLR: 1.521995\n",
      "Training Epoch: 16 [11264/50000]\tLoss: 3.5282\tLR: 1.522251\n",
      "Training Epoch: 16 [11392/50000]\tLoss: 2.9272\tLR: 1.522506\n",
      "Training Epoch: 16 [11520/50000]\tLoss: 3.0397\tLR: 1.522762\n",
      "Training Epoch: 16 [11648/50000]\tLoss: 3.4023\tLR: 1.523018\n",
      "Training Epoch: 16 [11776/50000]\tLoss: 3.3291\tLR: 1.523274\n",
      "Training Epoch: 16 [11904/50000]\tLoss: 3.4304\tLR: 1.523529\n",
      "Training Epoch: 16 [12032/50000]\tLoss: 3.3626\tLR: 1.523785\n",
      "Training Epoch: 16 [12160/50000]\tLoss: 3.5036\tLR: 1.524041\n",
      "Training Epoch: 16 [12288/50000]\tLoss: 3.4765\tLR: 1.524297\n",
      "Training Epoch: 16 [12416/50000]\tLoss: 3.4354\tLR: 1.524552\n",
      "Training Epoch: 16 [12544/50000]\tLoss: 3.3958\tLR: 1.524808\n",
      "Training Epoch: 16 [12672/50000]\tLoss: 3.6368\tLR: 1.525064\n",
      "Training Epoch: 16 [12800/50000]\tLoss: 3.4528\tLR: 1.525320\n",
      "Training Epoch: 16 [12928/50000]\tLoss: 3.3407\tLR: 1.525575\n",
      "Training Epoch: 16 [13056/50000]\tLoss: 3.4660\tLR: 1.525831\n",
      "Training Epoch: 16 [13184/50000]\tLoss: 3.3068\tLR: 1.526087\n",
      "Training Epoch: 16 [13312/50000]\tLoss: 3.5766\tLR: 1.526343\n",
      "Training Epoch: 16 [13440/50000]\tLoss: 3.4692\tLR: 1.526598\n",
      "Training Epoch: 16 [13568/50000]\tLoss: 3.4648\tLR: 1.526854\n",
      "Training Epoch: 16 [13696/50000]\tLoss: 3.6018\tLR: 1.527110\n",
      "Training Epoch: 16 [13824/50000]\tLoss: 3.4772\tLR: 1.527366\n",
      "Training Epoch: 16 [13952/50000]\tLoss: 3.5843\tLR: 1.527621\n",
      "Training Epoch: 16 [14080/50000]\tLoss: 3.3351\tLR: 1.527877\n",
      "Training Epoch: 16 [14208/50000]\tLoss: 3.4669\tLR: 1.528133\n",
      "Training Epoch: 16 [14336/50000]\tLoss: 3.4754\tLR: 1.528389\n",
      "Training Epoch: 16 [14464/50000]\tLoss: 3.2003\tLR: 1.528645\n",
      "Training Epoch: 16 [14592/50000]\tLoss: 3.3436\tLR: 1.528900\n",
      "Training Epoch: 16 [14720/50000]\tLoss: 3.3812\tLR: 1.529156\n",
      "Training Epoch: 16 [14848/50000]\tLoss: 3.4087\tLR: 1.529412\n",
      "Training Epoch: 16 [14976/50000]\tLoss: 3.5496\tLR: 1.529668\n",
      "Training Epoch: 16 [15104/50000]\tLoss: 3.3292\tLR: 1.529923\n",
      "Training Epoch: 16 [15232/50000]\tLoss: 3.4331\tLR: 1.530179\n",
      "Training Epoch: 16 [15360/50000]\tLoss: 3.6702\tLR: 1.530435\n",
      "Training Epoch: 16 [15488/50000]\tLoss: 3.4766\tLR: 1.530691\n",
      "Training Epoch: 16 [15616/50000]\tLoss: 3.1965\tLR: 1.530946\n",
      "Training Epoch: 16 [15744/50000]\tLoss: 3.4843\tLR: 1.531202\n",
      "Training Epoch: 16 [15872/50000]\tLoss: 3.1319\tLR: 1.531458\n",
      "Training Epoch: 16 [16000/50000]\tLoss: 3.3003\tLR: 1.531714\n",
      "Training Epoch: 16 [16128/50000]\tLoss: 3.5732\tLR: 1.531969\n",
      "Training Epoch: 16 [16256/50000]\tLoss: 3.3805\tLR: 1.532225\n",
      "Training Epoch: 16 [16384/50000]\tLoss: 3.5538\tLR: 1.532481\n",
      "Training Epoch: 16 [16512/50000]\tLoss: 3.2235\tLR: 1.532737\n",
      "Training Epoch: 16 [16640/50000]\tLoss: 3.2215\tLR: 1.532992\n",
      "Training Epoch: 16 [16768/50000]\tLoss: 3.4157\tLR: 1.533248\n",
      "Training Epoch: 16 [16896/50000]\tLoss: 3.3014\tLR: 1.533504\n",
      "Training Epoch: 16 [17024/50000]\tLoss: 3.5232\tLR: 1.533760\n",
      "Training Epoch: 16 [17152/50000]\tLoss: 3.4702\tLR: 1.534015\n",
      "Training Epoch: 16 [17280/50000]\tLoss: 3.4155\tLR: 1.534271\n",
      "Training Epoch: 16 [17408/50000]\tLoss: 3.3611\tLR: 1.534527\n",
      "Training Epoch: 16 [17536/50000]\tLoss: 3.4615\tLR: 1.534783\n",
      "Training Epoch: 16 [17664/50000]\tLoss: 3.4459\tLR: 1.535038\n",
      "Training Epoch: 16 [17792/50000]\tLoss: 3.5559\tLR: 1.535294\n",
      "Training Epoch: 16 [17920/50000]\tLoss: 3.1900\tLR: 1.535550\n",
      "Training Epoch: 16 [18048/50000]\tLoss: 3.2375\tLR: 1.535806\n",
      "Training Epoch: 16 [18176/50000]\tLoss: 3.2713\tLR: 1.536061\n",
      "Training Epoch: 16 [18304/50000]\tLoss: 3.3743\tLR: 1.536317\n",
      "Training Epoch: 16 [18432/50000]\tLoss: 3.3455\tLR: 1.536573\n",
      "Training Epoch: 16 [18560/50000]\tLoss: 3.3033\tLR: 1.536829\n",
      "Training Epoch: 16 [18688/50000]\tLoss: 3.2717\tLR: 1.537084\n",
      "Training Epoch: 16 [18816/50000]\tLoss: 3.1203\tLR: 1.537340\n",
      "Training Epoch: 16 [18944/50000]\tLoss: 3.5284\tLR: 1.537596\n",
      "Training Epoch: 16 [19072/50000]\tLoss: 3.4458\tLR: 1.537852\n",
      "Training Epoch: 16 [19200/50000]\tLoss: 3.3525\tLR: 1.538107\n",
      "Training Epoch: 16 [19328/50000]\tLoss: 3.8179\tLR: 1.538363\n",
      "Training Epoch: 16 [19456/50000]\tLoss: 3.4065\tLR: 1.538619\n",
      "Training Epoch: 16 [19584/50000]\tLoss: 3.3661\tLR: 1.538875\n",
      "Training Epoch: 16 [19712/50000]\tLoss: 3.4234\tLR: 1.539130\n",
      "Training Epoch: 16 [19840/50000]\tLoss: 3.4263\tLR: 1.539386\n",
      "Training Epoch: 16 [19968/50000]\tLoss: 3.4548\tLR: 1.539642\n",
      "Training Epoch: 16 [20096/50000]\tLoss: 3.3215\tLR: 1.539898\n",
      "Training Epoch: 16 [20224/50000]\tLoss: 3.3106\tLR: 1.540153\n",
      "Training Epoch: 16 [20352/50000]\tLoss: 3.5098\tLR: 1.540409\n",
      "Training Epoch: 16 [20480/50000]\tLoss: 3.6358\tLR: 1.540665\n",
      "Training Epoch: 16 [20608/50000]\tLoss: 3.6545\tLR: 1.540921\n",
      "Training Epoch: 16 [20736/50000]\tLoss: 3.4666\tLR: 1.541176\n",
      "Training Epoch: 16 [20864/50000]\tLoss: 3.5146\tLR: 1.541432\n",
      "Training Epoch: 16 [20992/50000]\tLoss: 3.5347\tLR: 1.541688\n",
      "Training Epoch: 16 [21120/50000]\tLoss: 3.2500\tLR: 1.541944\n",
      "Training Epoch: 16 [21248/50000]\tLoss: 3.3754\tLR: 1.542199\n",
      "Training Epoch: 16 [21376/50000]\tLoss: 3.4340\tLR: 1.542455\n",
      "Training Epoch: 16 [21504/50000]\tLoss: 3.1457\tLR: 1.542711\n",
      "Training Epoch: 16 [21632/50000]\tLoss: 3.6420\tLR: 1.542967\n",
      "Training Epoch: 16 [21760/50000]\tLoss: 3.4652\tLR: 1.543223\n",
      "Training Epoch: 16 [21888/50000]\tLoss: 3.2233\tLR: 1.543478\n",
      "Training Epoch: 16 [22016/50000]\tLoss: 3.3850\tLR: 1.543734\n",
      "Training Epoch: 16 [22144/50000]\tLoss: 3.2625\tLR: 1.543990\n",
      "Training Epoch: 16 [22272/50000]\tLoss: 3.4408\tLR: 1.544246\n",
      "Training Epoch: 16 [22400/50000]\tLoss: 3.4325\tLR: 1.544501\n",
      "Training Epoch: 16 [22528/50000]\tLoss: 3.2445\tLR: 1.544757\n",
      "Training Epoch: 16 [22656/50000]\tLoss: 3.2354\tLR: 1.545013\n",
      "Training Epoch: 16 [22784/50000]\tLoss: 3.1537\tLR: 1.545269\n",
      "Training Epoch: 16 [22912/50000]\tLoss: 3.5268\tLR: 1.545524\n",
      "Training Epoch: 16 [23040/50000]\tLoss: 3.4355\tLR: 1.545780\n",
      "Training Epoch: 16 [23168/50000]\tLoss: 3.5687\tLR: 1.546036\n",
      "Training Epoch: 16 [23296/50000]\tLoss: 3.3229\tLR: 1.546292\n",
      "Training Epoch: 16 [23424/50000]\tLoss: 3.6362\tLR: 1.546547\n",
      "Training Epoch: 16 [23552/50000]\tLoss: 3.7116\tLR: 1.546803\n",
      "Training Epoch: 16 [23680/50000]\tLoss: 3.4427\tLR: 1.547059\n",
      "Training Epoch: 16 [23808/50000]\tLoss: 3.4328\tLR: 1.547315\n",
      "Training Epoch: 16 [23936/50000]\tLoss: 3.4684\tLR: 1.547570\n",
      "Training Epoch: 16 [24064/50000]\tLoss: 3.4146\tLR: 1.547826\n",
      "Training Epoch: 16 [24192/50000]\tLoss: 3.4988\tLR: 1.548082\n",
      "Training Epoch: 16 [24320/50000]\tLoss: 3.5022\tLR: 1.548338\n",
      "Training Epoch: 16 [24448/50000]\tLoss: 3.8294\tLR: 1.548593\n",
      "Training Epoch: 16 [24576/50000]\tLoss: 3.4901\tLR: 1.548849\n",
      "Training Epoch: 16 [24704/50000]\tLoss: 3.3906\tLR: 1.549105\n",
      "Training Epoch: 16 [24832/50000]\tLoss: 3.4097\tLR: 1.549361\n",
      "Training Epoch: 16 [24960/50000]\tLoss: 3.1064\tLR: 1.549616\n",
      "Training Epoch: 16 [25088/50000]\tLoss: 3.4531\tLR: 1.549872\n",
      "Training Epoch: 16 [25216/50000]\tLoss: 3.6396\tLR: 1.550128\n",
      "Training Epoch: 16 [25344/50000]\tLoss: 3.3560\tLR: 1.550384\n",
      "Training Epoch: 16 [25472/50000]\tLoss: 3.4553\tLR: 1.550639\n",
      "Training Epoch: 16 [25600/50000]\tLoss: 3.2752\tLR: 1.550895\n",
      "Training Epoch: 16 [25728/50000]\tLoss: 3.4325\tLR: 1.551151\n",
      "Training Epoch: 16 [25856/50000]\tLoss: 3.6580\tLR: 1.551407\n",
      "Training Epoch: 16 [25984/50000]\tLoss: 3.3953\tLR: 1.551662\n",
      "Training Epoch: 16 [26112/50000]\tLoss: 3.2724\tLR: 1.551918\n",
      "Training Epoch: 16 [26240/50000]\tLoss: 3.6464\tLR: 1.552174\n",
      "Training Epoch: 16 [26368/50000]\tLoss: 3.4332\tLR: 1.552430\n",
      "Training Epoch: 16 [26496/50000]\tLoss: 3.5602\tLR: 1.552685\n",
      "Training Epoch: 16 [26624/50000]\tLoss: 3.3856\tLR: 1.552941\n",
      "Training Epoch: 16 [26752/50000]\tLoss: 3.4978\tLR: 1.553197\n",
      "Training Epoch: 16 [26880/50000]\tLoss: 3.2884\tLR: 1.553453\n",
      "Training Epoch: 16 [27008/50000]\tLoss: 3.3178\tLR: 1.553708\n",
      "Training Epoch: 16 [27136/50000]\tLoss: 3.3194\tLR: 1.553964\n",
      "Training Epoch: 16 [27264/50000]\tLoss: 3.5966\tLR: 1.554220\n",
      "Training Epoch: 16 [27392/50000]\tLoss: 3.6830\tLR: 1.554476\n",
      "Training Epoch: 16 [27520/50000]\tLoss: 3.3600\tLR: 1.554731\n",
      "Training Epoch: 16 [27648/50000]\tLoss: 3.5654\tLR: 1.554987\n",
      "Training Epoch: 16 [27776/50000]\tLoss: 3.3840\tLR: 1.555243\n",
      "Training Epoch: 16 [27904/50000]\tLoss: 3.3429\tLR: 1.555499\n",
      "Training Epoch: 16 [28032/50000]\tLoss: 3.3441\tLR: 1.555754\n",
      "Training Epoch: 16 [28160/50000]\tLoss: 3.5401\tLR: 1.556010\n",
      "Training Epoch: 16 [28288/50000]\tLoss: 3.3549\tLR: 1.556266\n",
      "Training Epoch: 16 [28416/50000]\tLoss: 3.4079\tLR: 1.556522\n",
      "Training Epoch: 16 [28544/50000]\tLoss: 3.6787\tLR: 1.556777\n",
      "Training Epoch: 16 [28672/50000]\tLoss: 3.2750\tLR: 1.557033\n",
      "Training Epoch: 16 [28800/50000]\tLoss: 3.3222\tLR: 1.557289\n",
      "Training Epoch: 16 [28928/50000]\tLoss: 3.6472\tLR: 1.557545\n",
      "Training Epoch: 16 [29056/50000]\tLoss: 3.3061\tLR: 1.557801\n",
      "Training Epoch: 16 [29184/50000]\tLoss: 3.4792\tLR: 1.558056\n",
      "Training Epoch: 16 [29312/50000]\tLoss: 3.3326\tLR: 1.558312\n",
      "Training Epoch: 16 [29440/50000]\tLoss: 3.5066\tLR: 1.558568\n",
      "Training Epoch: 16 [29568/50000]\tLoss: 3.3433\tLR: 1.558824\n",
      "Training Epoch: 16 [29696/50000]\tLoss: 3.7897\tLR: 1.559079\n",
      "Training Epoch: 16 [29824/50000]\tLoss: 3.4099\tLR: 1.559335\n",
      "Training Epoch: 16 [29952/50000]\tLoss: 3.3405\tLR: 1.559591\n",
      "Training Epoch: 16 [30080/50000]\tLoss: 3.3741\tLR: 1.559847\n",
      "Training Epoch: 16 [30208/50000]\tLoss: 3.4888\tLR: 1.560102\n",
      "Training Epoch: 16 [30336/50000]\tLoss: 3.3996\tLR: 1.560358\n",
      "Training Epoch: 16 [30464/50000]\tLoss: 3.6049\tLR: 1.560614\n",
      "Training Epoch: 16 [30592/50000]\tLoss: 3.4268\tLR: 1.560870\n",
      "Training Epoch: 16 [30720/50000]\tLoss: 3.5524\tLR: 1.561125\n",
      "Training Epoch: 16 [30848/50000]\tLoss: 3.5347\tLR: 1.561381\n",
      "Training Epoch: 16 [30976/50000]\tLoss: 3.1891\tLR: 1.561637\n",
      "Training Epoch: 16 [31104/50000]\tLoss: 3.6179\tLR: 1.561893\n",
      "Training Epoch: 16 [31232/50000]\tLoss: 3.5959\tLR: 1.562148\n",
      "Training Epoch: 16 [31360/50000]\tLoss: 3.7273\tLR: 1.562404\n",
      "Training Epoch: 16 [31488/50000]\tLoss: 3.3435\tLR: 1.562660\n",
      "Training Epoch: 16 [31616/50000]\tLoss: 3.5755\tLR: 1.562916\n",
      "Training Epoch: 16 [31744/50000]\tLoss: 3.5028\tLR: 1.563171\n",
      "Training Epoch: 16 [31872/50000]\tLoss: 3.3348\tLR: 1.563427\n",
      "Training Epoch: 16 [32000/50000]\tLoss: 3.7415\tLR: 1.563683\n",
      "Training Epoch: 16 [32128/50000]\tLoss: 3.4051\tLR: 1.563939\n",
      "Training Epoch: 16 [32256/50000]\tLoss: 3.3710\tLR: 1.564194\n",
      "Training Epoch: 16 [32384/50000]\tLoss: 3.3722\tLR: 1.564450\n",
      "Training Epoch: 16 [32512/50000]\tLoss: 3.8919\tLR: 1.564706\n",
      "Training Epoch: 16 [32640/50000]\tLoss: 3.4935\tLR: 1.564962\n",
      "Training Epoch: 16 [32768/50000]\tLoss: 3.6528\tLR: 1.565217\n",
      "Training Epoch: 16 [32896/50000]\tLoss: 3.6233\tLR: 1.565473\n",
      "Training Epoch: 16 [33024/50000]\tLoss: 3.4106\tLR: 1.565729\n",
      "Training Epoch: 16 [33152/50000]\tLoss: 3.8767\tLR: 1.565985\n",
      "Training Epoch: 16 [33280/50000]\tLoss: 3.5893\tLR: 1.566240\n",
      "Training Epoch: 16 [33408/50000]\tLoss: 3.5636\tLR: 1.566496\n",
      "Training Epoch: 16 [33536/50000]\tLoss: 3.4513\tLR: 1.566752\n",
      "Training Epoch: 16 [33664/50000]\tLoss: 3.7136\tLR: 1.567008\n",
      "Training Epoch: 16 [33792/50000]\tLoss: 3.6139\tLR: 1.567263\n",
      "Training Epoch: 16 [33920/50000]\tLoss: 3.5746\tLR: 1.567519\n",
      "Training Epoch: 16 [34048/50000]\tLoss: 3.4363\tLR: 1.567775\n",
      "Training Epoch: 16 [34176/50000]\tLoss: 3.2603\tLR: 1.568031\n",
      "Training Epoch: 16 [34304/50000]\tLoss: 3.2879\tLR: 1.568286\n",
      "Training Epoch: 16 [34432/50000]\tLoss: 3.4521\tLR: 1.568542\n",
      "Training Epoch: 16 [34560/50000]\tLoss: 3.4057\tLR: 1.568798\n",
      "Training Epoch: 16 [34688/50000]\tLoss: 3.2774\tLR: 1.569054\n",
      "Training Epoch: 16 [34816/50000]\tLoss: 3.5697\tLR: 1.569309\n",
      "Training Epoch: 16 [34944/50000]\tLoss: 3.5395\tLR: 1.569565\n",
      "Training Epoch: 16 [35072/50000]\tLoss: 3.6323\tLR: 1.569821\n",
      "Training Epoch: 16 [35200/50000]\tLoss: 3.3441\tLR: 1.570077\n",
      "Training Epoch: 16 [35328/50000]\tLoss: 3.4918\tLR: 1.570332\n",
      "Training Epoch: 16 [35456/50000]\tLoss: 3.4637\tLR: 1.570588\n",
      "Training Epoch: 16 [35584/50000]\tLoss: 3.1999\tLR: 1.570844\n",
      "Training Epoch: 16 [35712/50000]\tLoss: 3.3562\tLR: 1.571100\n",
      "Training Epoch: 16 [35840/50000]\tLoss: 3.4252\tLR: 1.571355\n",
      "Training Epoch: 16 [35968/50000]\tLoss: 3.3105\tLR: 1.571611\n",
      "Training Epoch: 16 [36096/50000]\tLoss: 3.6258\tLR: 1.571867\n",
      "Training Epoch: 16 [36224/50000]\tLoss: 3.5607\tLR: 1.572123\n",
      "Training Epoch: 16 [36352/50000]\tLoss: 3.5533\tLR: 1.572379\n",
      "Training Epoch: 16 [36480/50000]\tLoss: 3.3974\tLR: 1.572634\n",
      "Training Epoch: 16 [36608/50000]\tLoss: 3.6382\tLR: 1.572890\n",
      "Training Epoch: 16 [36736/50000]\tLoss: 3.5118\tLR: 1.573146\n",
      "Training Epoch: 16 [36864/50000]\tLoss: 3.5846\tLR: 1.573402\n",
      "Training Epoch: 16 [36992/50000]\tLoss: 3.2995\tLR: 1.573657\n",
      "Training Epoch: 16 [37120/50000]\tLoss: 3.4798\tLR: 1.573913\n",
      "Training Epoch: 16 [37248/50000]\tLoss: 3.4509\tLR: 1.574169\n",
      "Training Epoch: 16 [37376/50000]\tLoss: 3.4295\tLR: 1.574425\n",
      "Training Epoch: 16 [37504/50000]\tLoss: 3.6389\tLR: 1.574680\n",
      "Training Epoch: 16 [37632/50000]\tLoss: 3.3482\tLR: 1.574936\n",
      "Training Epoch: 16 [37760/50000]\tLoss: 3.6637\tLR: 1.575192\n",
      "Training Epoch: 16 [37888/50000]\tLoss: 3.4814\tLR: 1.575448\n",
      "Training Epoch: 16 [38016/50000]\tLoss: 3.8147\tLR: 1.575703\n",
      "Training Epoch: 16 [38144/50000]\tLoss: 3.6430\tLR: 1.575959\n",
      "Training Epoch: 16 [38272/50000]\tLoss: 3.4680\tLR: 1.576215\n",
      "Training Epoch: 16 [38400/50000]\tLoss: 3.5090\tLR: 1.576471\n",
      "Training Epoch: 16 [38528/50000]\tLoss: 3.3120\tLR: 1.576726\n",
      "Training Epoch: 16 [38656/50000]\tLoss: 3.3092\tLR: 1.576982\n",
      "Training Epoch: 16 [38784/50000]\tLoss: 3.1462\tLR: 1.577238\n",
      "Training Epoch: 16 [38912/50000]\tLoss: 3.3433\tLR: 1.577494\n",
      "Training Epoch: 16 [39040/50000]\tLoss: 3.1356\tLR: 1.577749\n",
      "Training Epoch: 16 [39168/50000]\tLoss: 3.3672\tLR: 1.578005\n",
      "Training Epoch: 16 [39296/50000]\tLoss: 3.3423\tLR: 1.578261\n",
      "Training Epoch: 16 [39424/50000]\tLoss: 3.5046\tLR: 1.578517\n",
      "Training Epoch: 16 [39552/50000]\tLoss: 3.2273\tLR: 1.578772\n",
      "Training Epoch: 16 [39680/50000]\tLoss: 3.2364\tLR: 1.579028\n",
      "Training Epoch: 16 [39808/50000]\tLoss: 3.4649\tLR: 1.579284\n",
      "Training Epoch: 16 [39936/50000]\tLoss: 3.3379\tLR: 1.579540\n",
      "Training Epoch: 16 [40064/50000]\tLoss: 3.2867\tLR: 1.579795\n",
      "Training Epoch: 16 [40192/50000]\tLoss: 3.4871\tLR: 1.580051\n",
      "Training Epoch: 16 [40320/50000]\tLoss: 3.2798\tLR: 1.580307\n",
      "Training Epoch: 16 [40448/50000]\tLoss: 3.6261\tLR: 1.580563\n",
      "Training Epoch: 16 [40576/50000]\tLoss: 3.3066\tLR: 1.580818\n",
      "Training Epoch: 16 [40704/50000]\tLoss: 3.5210\tLR: 1.581074\n",
      "Training Epoch: 16 [40832/50000]\tLoss: 3.3505\tLR: 1.581330\n",
      "Training Epoch: 16 [40960/50000]\tLoss: 3.4539\tLR: 1.581586\n",
      "Training Epoch: 16 [41088/50000]\tLoss: 3.4576\tLR: 1.581841\n",
      "Training Epoch: 16 [41216/50000]\tLoss: 3.4009\tLR: 1.582097\n",
      "Training Epoch: 16 [41344/50000]\tLoss: 3.3633\tLR: 1.582353\n",
      "Training Epoch: 16 [41472/50000]\tLoss: 3.5196\tLR: 1.582609\n",
      "Training Epoch: 16 [41600/50000]\tLoss: 3.6542\tLR: 1.582864\n",
      "Training Epoch: 16 [41728/50000]\tLoss: 3.3914\tLR: 1.583120\n",
      "Training Epoch: 16 [41856/50000]\tLoss: 3.4622\tLR: 1.583376\n",
      "Training Epoch: 16 [41984/50000]\tLoss: 3.4587\tLR: 1.583632\n",
      "Training Epoch: 16 [42112/50000]\tLoss: 3.3657\tLR: 1.583887\n",
      "Training Epoch: 16 [42240/50000]\tLoss: 3.3621\tLR: 1.584143\n",
      "Training Epoch: 16 [42368/50000]\tLoss: 3.4567\tLR: 1.584399\n",
      "Training Epoch: 16 [42496/50000]\tLoss: 3.6302\tLR: 1.584655\n",
      "Training Epoch: 16 [42624/50000]\tLoss: 3.4371\tLR: 1.584910\n",
      "Training Epoch: 16 [42752/50000]\tLoss: 3.4557\tLR: 1.585166\n",
      "Training Epoch: 16 [42880/50000]\tLoss: 3.5590\tLR: 1.585422\n",
      "Training Epoch: 16 [43008/50000]\tLoss: 3.3760\tLR: 1.585678\n",
      "Training Epoch: 16 [43136/50000]\tLoss: 3.2672\tLR: 1.585934\n",
      "Training Epoch: 16 [43264/50000]\tLoss: 3.5322\tLR: 1.586189\n",
      "Training Epoch: 16 [43392/50000]\tLoss: 3.5312\tLR: 1.586445\n",
      "Training Epoch: 16 [43520/50000]\tLoss: 3.2418\tLR: 1.586701\n",
      "Training Epoch: 16 [43648/50000]\tLoss: 3.3860\tLR: 1.586957\n",
      "Training Epoch: 16 [43776/50000]\tLoss: 3.3711\tLR: 1.587212\n",
      "Training Epoch: 16 [43904/50000]\tLoss: 3.1938\tLR: 1.587468\n",
      "Training Epoch: 16 [44032/50000]\tLoss: 3.4111\tLR: 1.587724\n",
      "Training Epoch: 16 [44160/50000]\tLoss: 3.2908\tLR: 1.587980\n",
      "Training Epoch: 16 [44288/50000]\tLoss: 3.3098\tLR: 1.588235\n",
      "Training Epoch: 16 [44416/50000]\tLoss: 3.4018\tLR: 1.588491\n",
      "Training Epoch: 16 [44544/50000]\tLoss: 3.6186\tLR: 1.588747\n",
      "Training Epoch: 16 [44672/50000]\tLoss: 3.2270\tLR: 1.589003\n",
      "Training Epoch: 16 [44800/50000]\tLoss: 3.3936\tLR: 1.589258\n",
      "Training Epoch: 16 [44928/50000]\tLoss: 3.2603\tLR: 1.589514\n",
      "Training Epoch: 16 [45056/50000]\tLoss: 3.3291\tLR: 1.589770\n",
      "Training Epoch: 16 [45184/50000]\tLoss: 3.4752\tLR: 1.590026\n",
      "Training Epoch: 16 [45312/50000]\tLoss: 3.4172\tLR: 1.590281\n",
      "Training Epoch: 16 [45440/50000]\tLoss: 3.4118\tLR: 1.590537\n",
      "Training Epoch: 16 [45568/50000]\tLoss: 3.4040\tLR: 1.590793\n",
      "Training Epoch: 16 [45696/50000]\tLoss: 3.2057\tLR: 1.591049\n",
      "Training Epoch: 16 [45824/50000]\tLoss: 3.5384\tLR: 1.591304\n",
      "Training Epoch: 16 [45952/50000]\tLoss: 3.4156\tLR: 1.591560\n",
      "Training Epoch: 16 [46080/50000]\tLoss: 3.4014\tLR: 1.591816\n",
      "Training Epoch: 16 [46208/50000]\tLoss: 3.3778\tLR: 1.592072\n",
      "Training Epoch: 16 [46336/50000]\tLoss: 3.6341\tLR: 1.592327\n",
      "Training Epoch: 16 [46464/50000]\tLoss: 3.3776\tLR: 1.592583\n",
      "Training Epoch: 16 [46592/50000]\tLoss: 3.3347\tLR: 1.592839\n",
      "Training Epoch: 16 [46720/50000]\tLoss: 3.4217\tLR: 1.593095\n",
      "Training Epoch: 16 [46848/50000]\tLoss: 3.2304\tLR: 1.593350\n",
      "Training Epoch: 16 [46976/50000]\tLoss: 3.3447\tLR: 1.593606\n",
      "Training Epoch: 16 [47104/50000]\tLoss: 3.3711\tLR: 1.593862\n",
      "Training Epoch: 16 [47232/50000]\tLoss: 3.4286\tLR: 1.594118\n",
      "Training Epoch: 16 [47360/50000]\tLoss: 3.6911\tLR: 1.594373\n",
      "Training Epoch: 16 [47488/50000]\tLoss: 3.5908\tLR: 1.594629\n",
      "Training Epoch: 16 [47616/50000]\tLoss: 3.7689\tLR: 1.594885\n",
      "Training Epoch: 16 [47744/50000]\tLoss: 3.5969\tLR: 1.595141\n",
      "Training Epoch: 16 [47872/50000]\tLoss: 3.5519\tLR: 1.595396\n",
      "Training Epoch: 16 [48000/50000]\tLoss: 3.5539\tLR: 1.595652\n",
      "Training Epoch: 16 [48128/50000]\tLoss: 3.5820\tLR: 1.595908\n",
      "Training Epoch: 16 [48256/50000]\tLoss: 3.4578\tLR: 1.596164\n",
      "Training Epoch: 16 [48384/50000]\tLoss: 3.4314\tLR: 1.596419\n",
      "Training Epoch: 16 [48512/50000]\tLoss: 3.7008\tLR: 1.596675\n",
      "Training Epoch: 16 [48640/50000]\tLoss: 3.6826\tLR: 1.596931\n",
      "Training Epoch: 16 [48768/50000]\tLoss: 3.5264\tLR: 1.597187\n",
      "Training Epoch: 16 [48896/50000]\tLoss: 3.6302\tLR: 1.597442\n",
      "Training Epoch: 16 [49024/50000]\tLoss: 3.5685\tLR: 1.597698\n",
      "Training Epoch: 16 [49152/50000]\tLoss: 3.5478\tLR: 1.597954\n",
      "Training Epoch: 16 [49280/50000]\tLoss: 3.5700\tLR: 1.598210\n",
      "Training Epoch: 16 [49408/50000]\tLoss: 3.6042\tLR: 1.598465\n",
      "Training Epoch: 16 [49536/50000]\tLoss: 3.6247\tLR: 1.598721\n",
      "Training Epoch: 16 [49664/50000]\tLoss: 3.5501\tLR: 1.598977\n",
      "Training Epoch: 16 [49792/50000]\tLoss: 3.6453\tLR: 1.599233\n",
      "Training Epoch: 16 [49920/50000]\tLoss: 3.4210\tLR: 1.599488\n",
      "Training Epoch: 16 [50000/50000]\tLoss: 3.4693\tLR: 1.599744\n",
      "epoch 16 training time consumed: 489.11s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   22432 GB |   22432 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   22363 GB |   22363 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      68 GB |      68 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   22432 GB |   22432 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   22363 GB |   22363 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      68 GB |      68 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   22114 GB |   22114 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   22045 GB |   22045 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |      68 GB |      68 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    2378 K  |    2378 K  |\n",
      "|       from large pool |      24    |      65    |    1013 K  |    1013 K  |\n",
      "|       from small pool |     231    |     274    |    1364 K  |    1364 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    2378 K  |    2378 K  |\n",
      "|       from large pool |      24    |      65    |    1013 K  |    1013 K  |\n",
      "|       from small pool |     231    |     274    |    1364 K  |    1364 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      45    |    1377 K  |    1377 K  |\n",
      "|       from large pool |      10    |      23    |     487 K  |     487 K  |\n",
      "|       from small pool |      25    |      35    |     890 K  |     890 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 16, Average loss: 0.0336, Accuracy: 0.0868, Time consumed:30.90s\n",
      "\n",
      "Training Epoch: 17 [128/50000]\tLoss: 3.2535\tLR: 0.100000\n",
      "Training Epoch: 17 [256/50000]\tLoss: 3.5769\tLR: 1.600256\n",
      "Training Epoch: 17 [384/50000]\tLoss: 3.4684\tLR: 1.600512\n",
      "Training Epoch: 17 [512/50000]\tLoss: 3.4574\tLR: 1.600767\n",
      "Training Epoch: 17 [640/50000]\tLoss: 3.5674\tLR: 1.601023\n",
      "Training Epoch: 17 [768/50000]\tLoss: 3.1022\tLR: 1.601279\n",
      "Training Epoch: 17 [896/50000]\tLoss: 3.5004\tLR: 1.601535\n",
      "Training Epoch: 17 [1024/50000]\tLoss: 3.4498\tLR: 1.601790\n",
      "Training Epoch: 17 [1152/50000]\tLoss: 3.2829\tLR: 1.602046\n",
      "Training Epoch: 17 [1280/50000]\tLoss: 3.4419\tLR: 1.602302\n",
      "Training Epoch: 17 [1408/50000]\tLoss: 3.4725\tLR: 1.602558\n",
      "Training Epoch: 17 [1536/50000]\tLoss: 3.5502\tLR: 1.602813\n",
      "Training Epoch: 17 [1664/50000]\tLoss: 3.2877\tLR: 1.603069\n",
      "Training Epoch: 17 [1792/50000]\tLoss: 3.3366\tLR: 1.603325\n",
      "Training Epoch: 17 [1920/50000]\tLoss: 3.3675\tLR: 1.603581\n",
      "Training Epoch: 17 [2048/50000]\tLoss: 3.5286\tLR: 1.603836\n",
      "Training Epoch: 17 [2176/50000]\tLoss: 3.6240\tLR: 1.604092\n",
      "Training Epoch: 17 [2304/50000]\tLoss: 3.6217\tLR: 1.604348\n",
      "Training Epoch: 17 [2432/50000]\tLoss: 3.4868\tLR: 1.604604\n",
      "Training Epoch: 17 [2560/50000]\tLoss: 3.4993\tLR: 1.604859\n",
      "Training Epoch: 17 [2688/50000]\tLoss: 3.6595\tLR: 1.605115\n",
      "Training Epoch: 17 [2816/50000]\tLoss: 3.5734\tLR: 1.605371\n",
      "Training Epoch: 17 [2944/50000]\tLoss: 3.5691\tLR: 1.605627\n",
      "Training Epoch: 17 [3072/50000]\tLoss: 3.6606\tLR: 1.605882\n",
      "Training Epoch: 17 [3200/50000]\tLoss: 3.5369\tLR: 1.606138\n",
      "Training Epoch: 17 [3328/50000]\tLoss: 3.4376\tLR: 1.606394\n",
      "Training Epoch: 17 [3456/50000]\tLoss: 3.4498\tLR: 1.606650\n",
      "Training Epoch: 17 [3584/50000]\tLoss: 3.5787\tLR: 1.606905\n",
      "Training Epoch: 17 [3712/50000]\tLoss: 3.3745\tLR: 1.607161\n",
      "Training Epoch: 17 [3840/50000]\tLoss: 3.5317\tLR: 1.607417\n",
      "Training Epoch: 17 [3968/50000]\tLoss: 3.3767\tLR: 1.607673\n",
      "Training Epoch: 17 [4096/50000]\tLoss: 3.4949\tLR: 1.607928\n",
      "Training Epoch: 17 [4224/50000]\tLoss: 3.4806\tLR: 1.608184\n",
      "Training Epoch: 17 [4352/50000]\tLoss: 3.3301\tLR: 1.608440\n",
      "Training Epoch: 17 [4480/50000]\tLoss: 3.6402\tLR: 1.608696\n",
      "Training Epoch: 17 [4608/50000]\tLoss: 3.4562\tLR: 1.608951\n",
      "Training Epoch: 17 [4736/50000]\tLoss: 3.7931\tLR: 1.609207\n",
      "Training Epoch: 17 [4864/50000]\tLoss: 3.4640\tLR: 1.609463\n",
      "Training Epoch: 17 [4992/50000]\tLoss: 3.6695\tLR: 1.609719\n",
      "Training Epoch: 17 [5120/50000]\tLoss: 3.4004\tLR: 1.609974\n",
      "Training Epoch: 17 [5248/50000]\tLoss: 3.6166\tLR: 1.610230\n",
      "Training Epoch: 17 [5376/50000]\tLoss: 3.4530\tLR: 1.610486\n",
      "Training Epoch: 17 [5504/50000]\tLoss: 3.3969\tLR: 1.610742\n",
      "Training Epoch: 17 [5632/50000]\tLoss: 3.3952\tLR: 1.610997\n",
      "Training Epoch: 17 [5760/50000]\tLoss: 3.4800\tLR: 1.611253\n",
      "Training Epoch: 17 [5888/50000]\tLoss: 3.4381\tLR: 1.611509\n",
      "Training Epoch: 17 [6016/50000]\tLoss: 3.3626\tLR: 1.611765\n",
      "Training Epoch: 17 [6144/50000]\tLoss: 3.5147\tLR: 1.612020\n",
      "Training Epoch: 17 [6272/50000]\tLoss: 3.5709\tLR: 1.612276\n",
      "Training Epoch: 17 [6400/50000]\tLoss: 3.4014\tLR: 1.612532\n",
      "Training Epoch: 17 [6528/50000]\tLoss: 3.5522\tLR: 1.612788\n",
      "Training Epoch: 17 [6656/50000]\tLoss: 3.5290\tLR: 1.613043\n",
      "Training Epoch: 17 [6784/50000]\tLoss: 3.6949\tLR: 1.613299\n",
      "Training Epoch: 17 [6912/50000]\tLoss: 3.4383\tLR: 1.613555\n",
      "Training Epoch: 17 [7040/50000]\tLoss: 3.5825\tLR: 1.613811\n",
      "Training Epoch: 17 [7168/50000]\tLoss: 3.3834\tLR: 1.614066\n",
      "Training Epoch: 17 [7296/50000]\tLoss: 3.4759\tLR: 1.614322\n",
      "Training Epoch: 17 [7424/50000]\tLoss: 3.1953\tLR: 1.614578\n",
      "Training Epoch: 17 [7552/50000]\tLoss: 3.4537\tLR: 1.614834\n",
      "Training Epoch: 17 [7680/50000]\tLoss: 3.6018\tLR: 1.615090\n",
      "Training Epoch: 17 [7808/50000]\tLoss: 3.5255\tLR: 1.615345\n",
      "Training Epoch: 17 [7936/50000]\tLoss: 3.3804\tLR: 1.615601\n",
      "Training Epoch: 17 [8064/50000]\tLoss: 3.6458\tLR: 1.615857\n",
      "Training Epoch: 17 [8192/50000]\tLoss: 3.4862\tLR: 1.616113\n",
      "Training Epoch: 17 [8320/50000]\tLoss: 3.3077\tLR: 1.616368\n",
      "Training Epoch: 17 [8448/50000]\tLoss: 3.4601\tLR: 1.616624\n",
      "Training Epoch: 17 [8576/50000]\tLoss: 3.6682\tLR: 1.616880\n",
      "Training Epoch: 17 [8704/50000]\tLoss: 3.3874\tLR: 1.617136\n",
      "Training Epoch: 17 [8832/50000]\tLoss: 3.4604\tLR: 1.617391\n",
      "Training Epoch: 17 [8960/50000]\tLoss: 3.2491\tLR: 1.617647\n",
      "Training Epoch: 17 [9088/50000]\tLoss: 3.4234\tLR: 1.617903\n",
      "Training Epoch: 17 [9216/50000]\tLoss: 3.3263\tLR: 1.618159\n",
      "Training Epoch: 17 [9344/50000]\tLoss: 3.6734\tLR: 1.618414\n",
      "Training Epoch: 17 [9472/50000]\tLoss: 3.7482\tLR: 1.618670\n",
      "Training Epoch: 17 [9600/50000]\tLoss: 3.5917\tLR: 1.618926\n",
      "Training Epoch: 17 [9728/50000]\tLoss: 3.4028\tLR: 1.619182\n",
      "Training Epoch: 17 [9856/50000]\tLoss: 3.4893\tLR: 1.619437\n",
      "Training Epoch: 17 [9984/50000]\tLoss: 3.5194\tLR: 1.619693\n",
      "Training Epoch: 17 [10112/50000]\tLoss: 3.4838\tLR: 1.619949\n",
      "Training Epoch: 17 [10240/50000]\tLoss: 3.2977\tLR: 1.620205\n",
      "Training Epoch: 17 [10368/50000]\tLoss: 3.5173\tLR: 1.620460\n",
      "Training Epoch: 17 [10496/50000]\tLoss: 3.1918\tLR: 1.620716\n",
      "Training Epoch: 17 [10624/50000]\tLoss: 3.3494\tLR: 1.620972\n",
      "Training Epoch: 17 [10752/50000]\tLoss: 3.4091\tLR: 1.621228\n",
      "Training Epoch: 17 [10880/50000]\tLoss: 3.4351\tLR: 1.621483\n",
      "Training Epoch: 17 [11008/50000]\tLoss: 3.3544\tLR: 1.621739\n",
      "Training Epoch: 17 [11136/50000]\tLoss: 3.3286\tLR: 1.621995\n",
      "Training Epoch: 17 [11264/50000]\tLoss: 3.6003\tLR: 1.622251\n",
      "Training Epoch: 17 [11392/50000]\tLoss: 3.2607\tLR: 1.622506\n",
      "Training Epoch: 17 [11520/50000]\tLoss: 3.5924\tLR: 1.622762\n",
      "Training Epoch: 17 [11648/50000]\tLoss: 3.6657\tLR: 1.623018\n",
      "Training Epoch: 17 [11776/50000]\tLoss: 3.4959\tLR: 1.623274\n",
      "Training Epoch: 17 [11904/50000]\tLoss: 3.5144\tLR: 1.623529\n",
      "Training Epoch: 17 [12032/50000]\tLoss: 3.4592\tLR: 1.623785\n",
      "Training Epoch: 17 [12160/50000]\tLoss: 3.4351\tLR: 1.624041\n",
      "Training Epoch: 17 [12288/50000]\tLoss: 3.4528\tLR: 1.624297\n",
      "Training Epoch: 17 [12416/50000]\tLoss: 3.4287\tLR: 1.624552\n",
      "Training Epoch: 17 [12544/50000]\tLoss: 3.5313\tLR: 1.624808\n",
      "Training Epoch: 17 [12672/50000]\tLoss: 3.5031\tLR: 1.625064\n",
      "Training Epoch: 17 [12800/50000]\tLoss: 3.3729\tLR: 1.625320\n",
      "Training Epoch: 17 [12928/50000]\tLoss: 3.7185\tLR: 1.625575\n",
      "Training Epoch: 17 [13056/50000]\tLoss: 3.4536\tLR: 1.625831\n",
      "Training Epoch: 17 [13184/50000]\tLoss: 3.6388\tLR: 1.626087\n",
      "Training Epoch: 17 [13312/50000]\tLoss: 3.3631\tLR: 1.626343\n",
      "Training Epoch: 17 [13440/50000]\tLoss: 3.4654\tLR: 1.626598\n",
      "Training Epoch: 17 [13568/50000]\tLoss: 3.3264\tLR: 1.626854\n",
      "Training Epoch: 17 [13696/50000]\tLoss: 3.3165\tLR: 1.627110\n",
      "Training Epoch: 17 [13824/50000]\tLoss: 3.5405\tLR: 1.627366\n",
      "Training Epoch: 17 [13952/50000]\tLoss: 3.7646\tLR: 1.627621\n",
      "Training Epoch: 17 [14080/50000]\tLoss: 3.4745\tLR: 1.627877\n",
      "Training Epoch: 17 [14208/50000]\tLoss: 3.6351\tLR: 1.628133\n",
      "Training Epoch: 17 [14336/50000]\tLoss: 3.5547\tLR: 1.628389\n",
      "Training Epoch: 17 [14464/50000]\tLoss: 3.3829\tLR: 1.628645\n",
      "Training Epoch: 17 [14592/50000]\tLoss: 3.6074\tLR: 1.628900\n",
      "Training Epoch: 17 [14720/50000]\tLoss: 3.4576\tLR: 1.629156\n",
      "Training Epoch: 17 [14848/50000]\tLoss: 3.6543\tLR: 1.629412\n",
      "Training Epoch: 17 [14976/50000]\tLoss: 3.8027\tLR: 1.629668\n",
      "Training Epoch: 17 [15104/50000]\tLoss: 3.4005\tLR: 1.629923\n",
      "Training Epoch: 17 [15232/50000]\tLoss: 3.6556\tLR: 1.630179\n",
      "Training Epoch: 17 [15360/50000]\tLoss: 3.5368\tLR: 1.630435\n",
      "Training Epoch: 17 [15488/50000]\tLoss: 3.4300\tLR: 1.630691\n",
      "Training Epoch: 17 [15616/50000]\tLoss: 3.6365\tLR: 1.630946\n",
      "Training Epoch: 17 [15744/50000]\tLoss: 3.7015\tLR: 1.631202\n",
      "Training Epoch: 17 [15872/50000]\tLoss: 3.5391\tLR: 1.631458\n",
      "Training Epoch: 17 [16000/50000]\tLoss: 3.5946\tLR: 1.631714\n",
      "Training Epoch: 17 [16128/50000]\tLoss: 3.7574\tLR: 1.631969\n",
      "Training Epoch: 17 [16256/50000]\tLoss: 3.5937\tLR: 1.632225\n",
      "Training Epoch: 17 [16384/50000]\tLoss: 3.7299\tLR: 1.632481\n",
      "Training Epoch: 17 [16512/50000]\tLoss: 3.4487\tLR: 1.632737\n",
      "Training Epoch: 17 [16640/50000]\tLoss: 3.4913\tLR: 1.632992\n",
      "Training Epoch: 17 [16768/50000]\tLoss: 3.3212\tLR: 1.633248\n",
      "Training Epoch: 17 [16896/50000]\tLoss: 3.4032\tLR: 1.633504\n",
      "Training Epoch: 17 [17024/50000]\tLoss: 3.3556\tLR: 1.633760\n",
      "Training Epoch: 17 [17152/50000]\tLoss: 3.8180\tLR: 1.634015\n",
      "Training Epoch: 17 [17280/50000]\tLoss: 3.4353\tLR: 1.634271\n",
      "Training Epoch: 17 [17408/50000]\tLoss: 3.4173\tLR: 1.634527\n",
      "Training Epoch: 17 [17536/50000]\tLoss: 3.4833\tLR: 1.634783\n",
      "Training Epoch: 17 [17664/50000]\tLoss: 3.6654\tLR: 1.635038\n",
      "Training Epoch: 17 [17792/50000]\tLoss: 3.4639\tLR: 1.635294\n",
      "Training Epoch: 17 [17920/50000]\tLoss: 3.5537\tLR: 1.635550\n",
      "Training Epoch: 17 [18048/50000]\tLoss: 3.7641\tLR: 1.635806\n",
      "Training Epoch: 17 [18176/50000]\tLoss: 3.5638\tLR: 1.636061\n",
      "Training Epoch: 17 [18304/50000]\tLoss: 3.6018\tLR: 1.636317\n",
      "Training Epoch: 17 [18432/50000]\tLoss: 3.9591\tLR: 1.636573\n",
      "Training Epoch: 17 [18560/50000]\tLoss: 3.5419\tLR: 1.636829\n",
      "Training Epoch: 17 [18688/50000]\tLoss: 3.6729\tLR: 1.637084\n",
      "Training Epoch: 17 [18816/50000]\tLoss: 3.5939\tLR: 1.637340\n",
      "Training Epoch: 17 [18944/50000]\tLoss: 3.6177\tLR: 1.637596\n",
      "Training Epoch: 17 [19072/50000]\tLoss: 3.7868\tLR: 1.637852\n",
      "Training Epoch: 17 [19200/50000]\tLoss: 3.6456\tLR: 1.638107\n",
      "Training Epoch: 17 [19328/50000]\tLoss: 3.6284\tLR: 1.638363\n",
      "Training Epoch: 17 [19456/50000]\tLoss: 3.4197\tLR: 1.638619\n",
      "Training Epoch: 17 [19584/50000]\tLoss: 3.2718\tLR: 1.638875\n",
      "Training Epoch: 17 [19712/50000]\tLoss: 3.7781\tLR: 1.639130\n",
      "Training Epoch: 17 [19840/50000]\tLoss: 3.3615\tLR: 1.639386\n",
      "Training Epoch: 17 [19968/50000]\tLoss: 3.5327\tLR: 1.639642\n",
      "Training Epoch: 17 [20096/50000]\tLoss: 3.5641\tLR: 1.639898\n",
      "Training Epoch: 17 [20224/50000]\tLoss: 3.5773\tLR: 1.640153\n",
      "Training Epoch: 17 [20352/50000]\tLoss: 3.7377\tLR: 1.640409\n",
      "Training Epoch: 17 [20480/50000]\tLoss: 3.3334\tLR: 1.640665\n",
      "Training Epoch: 17 [20608/50000]\tLoss: 3.6379\tLR: 1.640921\n",
      "Training Epoch: 17 [20736/50000]\tLoss: 3.6139\tLR: 1.641176\n",
      "Training Epoch: 17 [20864/50000]\tLoss: 3.5809\tLR: 1.641432\n",
      "Training Epoch: 17 [20992/50000]\tLoss: 3.5888\tLR: 1.641688\n",
      "Training Epoch: 17 [21120/50000]\tLoss: 3.3344\tLR: 1.641944\n",
      "Training Epoch: 17 [21248/50000]\tLoss: 3.8748\tLR: 1.642199\n",
      "Training Epoch: 17 [21376/50000]\tLoss: 3.5702\tLR: 1.642455\n",
      "Training Epoch: 17 [21504/50000]\tLoss: 3.7636\tLR: 1.642711\n",
      "Training Epoch: 17 [21632/50000]\tLoss: 3.5916\tLR: 1.642967\n",
      "Training Epoch: 17 [21760/50000]\tLoss: 3.5335\tLR: 1.643223\n",
      "Training Epoch: 17 [21888/50000]\tLoss: 3.3851\tLR: 1.643478\n",
      "Training Epoch: 17 [22016/50000]\tLoss: 3.6154\tLR: 1.643734\n",
      "Training Epoch: 17 [22144/50000]\tLoss: 4.0140\tLR: 1.643990\n",
      "Training Epoch: 17 [22272/50000]\tLoss: 3.4546\tLR: 1.644246\n",
      "Training Epoch: 17 [22400/50000]\tLoss: 3.7877\tLR: 1.644501\n",
      "Training Epoch: 17 [22528/50000]\tLoss: 3.9607\tLR: 1.644757\n",
      "Training Epoch: 17 [22656/50000]\tLoss: 3.7578\tLR: 1.645013\n",
      "Training Epoch: 17 [22784/50000]\tLoss: 3.6331\tLR: 1.645269\n",
      "Training Epoch: 17 [22912/50000]\tLoss: 3.4038\tLR: 1.645524\n",
      "Training Epoch: 17 [23040/50000]\tLoss: 3.6063\tLR: 1.645780\n",
      "Training Epoch: 17 [23168/50000]\tLoss: 3.6739\tLR: 1.646036\n",
      "Training Epoch: 17 [23296/50000]\tLoss: 3.5246\tLR: 1.646292\n",
      "Training Epoch: 17 [23424/50000]\tLoss: 3.6874\tLR: 1.646547\n",
      "Training Epoch: 17 [23552/50000]\tLoss: 3.3901\tLR: 1.646803\n",
      "Training Epoch: 17 [23680/50000]\tLoss: 3.8079\tLR: 1.647059\n",
      "Training Epoch: 17 [23808/50000]\tLoss: 3.6715\tLR: 1.647315\n",
      "Training Epoch: 17 [23936/50000]\tLoss: 3.5949\tLR: 1.647570\n",
      "Training Epoch: 17 [24064/50000]\tLoss: 3.6857\tLR: 1.647826\n",
      "Training Epoch: 17 [24192/50000]\tLoss: 3.3263\tLR: 1.648082\n",
      "Training Epoch: 17 [24320/50000]\tLoss: 3.5902\tLR: 1.648338\n",
      "Training Epoch: 17 [24448/50000]\tLoss: 3.5976\tLR: 1.648593\n",
      "Training Epoch: 17 [24576/50000]\tLoss: 3.6643\tLR: 1.648849\n",
      "Training Epoch: 17 [24704/50000]\tLoss: 3.5605\tLR: 1.649105\n",
      "Training Epoch: 17 [24832/50000]\tLoss: 3.6546\tLR: 1.649361\n",
      "Training Epoch: 17 [24960/50000]\tLoss: 3.4740\tLR: 1.649616\n",
      "Training Epoch: 17 [25088/50000]\tLoss: 3.5975\tLR: 1.649872\n",
      "Training Epoch: 17 [25216/50000]\tLoss: 3.7015\tLR: 1.650128\n",
      "Training Epoch: 17 [25344/50000]\tLoss: 3.4432\tLR: 1.650384\n",
      "Training Epoch: 17 [25472/50000]\tLoss: 3.3931\tLR: 1.650639\n",
      "Training Epoch: 17 [25600/50000]\tLoss: 3.8297\tLR: 1.650895\n",
      "Training Epoch: 17 [25728/50000]\tLoss: 3.3395\tLR: 1.651151\n",
      "Training Epoch: 17 [25856/50000]\tLoss: 3.3954\tLR: 1.651407\n",
      "Training Epoch: 17 [25984/50000]\tLoss: 3.4725\tLR: 1.651662\n",
      "Training Epoch: 17 [26112/50000]\tLoss: 3.2771\tLR: 1.651918\n",
      "Training Epoch: 17 [26240/50000]\tLoss: 3.4154\tLR: 1.652174\n",
      "Training Epoch: 17 [26368/50000]\tLoss: 3.6745\tLR: 1.652430\n",
      "Training Epoch: 17 [26496/50000]\tLoss: 3.4140\tLR: 1.652685\n",
      "Training Epoch: 17 [26624/50000]\tLoss: 3.4876\tLR: 1.652941\n",
      "Training Epoch: 17 [26752/50000]\tLoss: 3.4501\tLR: 1.653197\n",
      "Training Epoch: 17 [26880/50000]\tLoss: 3.5858\tLR: 1.653453\n",
      "Training Epoch: 17 [27008/50000]\tLoss: 3.5911\tLR: 1.653708\n",
      "Training Epoch: 17 [27136/50000]\tLoss: 3.4018\tLR: 1.653964\n",
      "Training Epoch: 17 [27264/50000]\tLoss: 3.6025\tLR: 1.654220\n",
      "Training Epoch: 17 [27392/50000]\tLoss: 3.3127\tLR: 1.654476\n",
      "Training Epoch: 17 [27520/50000]\tLoss: 3.3523\tLR: 1.654731\n",
      "Training Epoch: 17 [27648/50000]\tLoss: 3.5528\tLR: 1.654987\n",
      "Training Epoch: 17 [27776/50000]\tLoss: 3.7130\tLR: 1.655243\n",
      "Training Epoch: 17 [27904/50000]\tLoss: 3.4398\tLR: 1.655499\n",
      "Training Epoch: 17 [28032/50000]\tLoss: 3.6469\tLR: 1.655754\n",
      "Training Epoch: 17 [28160/50000]\tLoss: 3.5307\tLR: 1.656010\n",
      "Training Epoch: 17 [28288/50000]\tLoss: 3.7334\tLR: 1.656266\n",
      "Training Epoch: 17 [28416/50000]\tLoss: 3.5256\tLR: 1.656522\n",
      "Training Epoch: 17 [28544/50000]\tLoss: 3.4804\tLR: 1.656777\n",
      "Training Epoch: 17 [28672/50000]\tLoss: 3.6903\tLR: 1.657033\n",
      "Training Epoch: 17 [28800/50000]\tLoss: 3.6539\tLR: 1.657289\n",
      "Training Epoch: 17 [28928/50000]\tLoss: 3.5694\tLR: 1.657545\n",
      "Training Epoch: 17 [29056/50000]\tLoss: 3.8600\tLR: 1.657801\n",
      "Training Epoch: 17 [29184/50000]\tLoss: 3.6291\tLR: 1.658056\n",
      "Training Epoch: 17 [29312/50000]\tLoss: 3.6393\tLR: 1.658312\n",
      "Training Epoch: 17 [29440/50000]\tLoss: 3.6120\tLR: 1.658568\n",
      "Training Epoch: 17 [29568/50000]\tLoss: 3.6369\tLR: 1.658824\n",
      "Training Epoch: 17 [29696/50000]\tLoss: 3.7382\tLR: 1.659079\n",
      "Training Epoch: 17 [29824/50000]\tLoss: 3.4984\tLR: 1.659335\n",
      "Training Epoch: 17 [29952/50000]\tLoss: 3.6642\tLR: 1.659591\n",
      "Training Epoch: 17 [30080/50000]\tLoss: 3.8866\tLR: 1.659847\n",
      "Training Epoch: 17 [30208/50000]\tLoss: 3.2608\tLR: 1.660102\n",
      "Training Epoch: 17 [30336/50000]\tLoss: 3.7821\tLR: 1.660358\n",
      "Training Epoch: 17 [30464/50000]\tLoss: 3.4390\tLR: 1.660614\n",
      "Training Epoch: 17 [30592/50000]\tLoss: 3.6094\tLR: 1.660870\n",
      "Training Epoch: 17 [30720/50000]\tLoss: 3.4111\tLR: 1.661125\n",
      "Training Epoch: 17 [30848/50000]\tLoss: 3.5835\tLR: 1.661381\n",
      "Training Epoch: 17 [30976/50000]\tLoss: 3.6967\tLR: 1.661637\n",
      "Training Epoch: 17 [31104/50000]\tLoss: 3.5787\tLR: 1.661893\n",
      "Training Epoch: 17 [31232/50000]\tLoss: 3.7314\tLR: 1.662148\n",
      "Training Epoch: 17 [31360/50000]\tLoss: 3.9086\tLR: 1.662404\n",
      "Training Epoch: 17 [31488/50000]\tLoss: 3.6317\tLR: 1.662660\n",
      "Training Epoch: 17 [31616/50000]\tLoss: 3.4152\tLR: 1.662916\n",
      "Training Epoch: 17 [31744/50000]\tLoss: 3.4521\tLR: 1.663171\n",
      "Training Epoch: 17 [31872/50000]\tLoss: 3.5338\tLR: 1.663427\n",
      "Training Epoch: 17 [32000/50000]\tLoss: 3.7182\tLR: 1.663683\n",
      "Training Epoch: 17 [32128/50000]\tLoss: 3.6922\tLR: 1.663939\n",
      "Training Epoch: 17 [32256/50000]\tLoss: 3.3761\tLR: 1.664194\n",
      "Training Epoch: 17 [32384/50000]\tLoss: 3.7227\tLR: 1.664450\n",
      "Training Epoch: 17 [32512/50000]\tLoss: 3.7401\tLR: 1.664706\n",
      "Training Epoch: 17 [32640/50000]\tLoss: 3.5082\tLR: 1.664962\n",
      "Training Epoch: 17 [32768/50000]\tLoss: 3.3477\tLR: 1.665217\n",
      "Training Epoch: 17 [32896/50000]\tLoss: 3.3671\tLR: 1.665473\n",
      "Training Epoch: 17 [33024/50000]\tLoss: 3.4516\tLR: 1.665729\n",
      "Training Epoch: 17 [33152/50000]\tLoss: 3.7359\tLR: 1.665985\n",
      "Training Epoch: 17 [33280/50000]\tLoss: 3.4723\tLR: 1.666240\n",
      "Training Epoch: 17 [33408/50000]\tLoss: 3.4503\tLR: 1.666496\n",
      "Training Epoch: 17 [33536/50000]\tLoss: 3.5244\tLR: 1.666752\n",
      "Training Epoch: 17 [33664/50000]\tLoss: 3.7696\tLR: 1.667008\n",
      "Training Epoch: 17 [33792/50000]\tLoss: 3.4080\tLR: 1.667263\n",
      "Training Epoch: 17 [33920/50000]\tLoss: 3.5965\tLR: 1.667519\n",
      "Training Epoch: 17 [34048/50000]\tLoss: 3.4092\tLR: 1.667775\n",
      "Training Epoch: 17 [34176/50000]\tLoss: 3.6040\tLR: 1.668031\n",
      "Training Epoch: 17 [34304/50000]\tLoss: 3.2667\tLR: 1.668286\n",
      "Training Epoch: 17 [34432/50000]\tLoss: 3.3862\tLR: 1.668542\n",
      "Training Epoch: 17 [34560/50000]\tLoss: 3.7534\tLR: 1.668798\n",
      "Training Epoch: 17 [34688/50000]\tLoss: 3.7057\tLR: 1.669054\n",
      "Training Epoch: 17 [34816/50000]\tLoss: 3.5720\tLR: 1.669309\n",
      "Training Epoch: 17 [34944/50000]\tLoss: 3.6804\tLR: 1.669565\n",
      "Training Epoch: 17 [35072/50000]\tLoss: 3.7351\tLR: 1.669821\n",
      "Training Epoch: 17 [35200/50000]\tLoss: 3.7339\tLR: 1.670077\n",
      "Training Epoch: 17 [35328/50000]\tLoss: 3.5837\tLR: 1.670332\n",
      "Training Epoch: 17 [35456/50000]\tLoss: 3.4547\tLR: 1.670588\n",
      "Training Epoch: 17 [35584/50000]\tLoss: 3.5798\tLR: 1.670844\n",
      "Training Epoch: 17 [35712/50000]\tLoss: 3.6483\tLR: 1.671100\n",
      "Training Epoch: 17 [35840/50000]\tLoss: 3.5366\tLR: 1.671355\n",
      "Training Epoch: 17 [35968/50000]\tLoss: 3.5983\tLR: 1.671611\n",
      "Training Epoch: 17 [36096/50000]\tLoss: 3.5952\tLR: 1.671867\n",
      "Training Epoch: 17 [36224/50000]\tLoss: 3.4675\tLR: 1.672123\n",
      "Training Epoch: 17 [36352/50000]\tLoss: 3.3583\tLR: 1.672379\n",
      "Training Epoch: 17 [36480/50000]\tLoss: 3.5467\tLR: 1.672634\n",
      "Training Epoch: 17 [36608/50000]\tLoss: 3.2390\tLR: 1.672890\n",
      "Training Epoch: 17 [36736/50000]\tLoss: 3.5323\tLR: 1.673146\n",
      "Training Epoch: 17 [36864/50000]\tLoss: 3.4326\tLR: 1.673402\n",
      "Training Epoch: 17 [36992/50000]\tLoss: 3.3774\tLR: 1.673657\n",
      "Training Epoch: 17 [37120/50000]\tLoss: 3.5576\tLR: 1.673913\n",
      "Training Epoch: 17 [37248/50000]\tLoss: 3.6392\tLR: 1.674169\n",
      "Training Epoch: 17 [37376/50000]\tLoss: 3.3825\tLR: 1.674425\n",
      "Training Epoch: 17 [37504/50000]\tLoss: 3.8448\tLR: 1.674680\n",
      "Training Epoch: 17 [37632/50000]\tLoss: 3.5827\tLR: 1.674936\n",
      "Training Epoch: 17 [37760/50000]\tLoss: 3.4954\tLR: 1.675192\n",
      "Training Epoch: 17 [37888/50000]\tLoss: 3.5244\tLR: 1.675448\n",
      "Training Epoch: 17 [38016/50000]\tLoss: 3.3449\tLR: 1.675703\n",
      "Training Epoch: 17 [38144/50000]\tLoss: 3.5788\tLR: 1.675959\n",
      "Training Epoch: 17 [38272/50000]\tLoss: 3.7207\tLR: 1.676215\n",
      "Training Epoch: 17 [38400/50000]\tLoss: 3.6950\tLR: 1.676471\n",
      "Training Epoch: 17 [38528/50000]\tLoss: 3.6852\tLR: 1.676726\n",
      "Training Epoch: 17 [38656/50000]\tLoss: 3.6580\tLR: 1.676982\n",
      "Training Epoch: 17 [38784/50000]\tLoss: 3.5978\tLR: 1.677238\n",
      "Training Epoch: 17 [38912/50000]\tLoss: 3.3390\tLR: 1.677494\n",
      "Training Epoch: 17 [39040/50000]\tLoss: 3.4778\tLR: 1.677749\n",
      "Training Epoch: 17 [39168/50000]\tLoss: 3.4895\tLR: 1.678005\n",
      "Training Epoch: 17 [39296/50000]\tLoss: 3.3298\tLR: 1.678261\n",
      "Training Epoch: 17 [39424/50000]\tLoss: 3.6723\tLR: 1.678517\n",
      "Training Epoch: 17 [39552/50000]\tLoss: 3.5772\tLR: 1.678772\n",
      "Training Epoch: 17 [39680/50000]\tLoss: 3.5811\tLR: 1.679028\n",
      "Training Epoch: 17 [39808/50000]\tLoss: 3.5409\tLR: 1.679284\n",
      "Training Epoch: 17 [39936/50000]\tLoss: 3.4315\tLR: 1.679540\n",
      "Training Epoch: 17 [40064/50000]\tLoss: 3.6358\tLR: 1.679795\n",
      "Training Epoch: 17 [40192/50000]\tLoss: 3.5466\tLR: 1.680051\n",
      "Training Epoch: 17 [40320/50000]\tLoss: 3.6071\tLR: 1.680307\n",
      "Training Epoch: 17 [40448/50000]\tLoss: 3.3005\tLR: 1.680563\n",
      "Training Epoch: 17 [40576/50000]\tLoss: 3.5633\tLR: 1.680818\n",
      "Training Epoch: 17 [40704/50000]\tLoss: 3.6022\tLR: 1.681074\n",
      "Training Epoch: 17 [40832/50000]\tLoss: 3.4658\tLR: 1.681330\n",
      "Training Epoch: 17 [40960/50000]\tLoss: 3.6656\tLR: 1.681586\n",
      "Training Epoch: 17 [41088/50000]\tLoss: 3.6881\tLR: 1.681841\n",
      "Training Epoch: 17 [41216/50000]\tLoss: 3.6065\tLR: 1.682097\n",
      "Training Epoch: 17 [41344/50000]\tLoss: 3.4990\tLR: 1.682353\n",
      "Training Epoch: 17 [41472/50000]\tLoss: 3.8213\tLR: 1.682609\n",
      "Training Epoch: 17 [41600/50000]\tLoss: 3.5029\tLR: 1.682864\n",
      "Training Epoch: 17 [41728/50000]\tLoss: 3.3395\tLR: 1.683120\n",
      "Training Epoch: 17 [41856/50000]\tLoss: 3.4642\tLR: 1.683376\n",
      "Training Epoch: 17 [41984/50000]\tLoss: 3.5869\tLR: 1.683632\n",
      "Training Epoch: 17 [42112/50000]\tLoss: 3.5770\tLR: 1.683887\n",
      "Training Epoch: 17 [42240/50000]\tLoss: 3.9932\tLR: 1.684143\n",
      "Training Epoch: 17 [42368/50000]\tLoss: 3.5855\tLR: 1.684399\n",
      "Training Epoch: 17 [42496/50000]\tLoss: 3.4536\tLR: 1.684655\n",
      "Training Epoch: 17 [42624/50000]\tLoss: 3.3265\tLR: 1.684910\n",
      "Training Epoch: 17 [42752/50000]\tLoss: 3.4847\tLR: 1.685166\n",
      "Training Epoch: 17 [42880/50000]\tLoss: 3.4471\tLR: 1.685422\n",
      "Training Epoch: 17 [43008/50000]\tLoss: 3.7222\tLR: 1.685678\n",
      "Training Epoch: 17 [43136/50000]\tLoss: 3.8241\tLR: 1.685934\n",
      "Training Epoch: 17 [43264/50000]\tLoss: 3.3955\tLR: 1.686189\n",
      "Training Epoch: 17 [43392/50000]\tLoss: 3.6488\tLR: 1.686445\n",
      "Training Epoch: 17 [43520/50000]\tLoss: 3.7836\tLR: 1.686701\n",
      "Training Epoch: 17 [43648/50000]\tLoss: 3.7135\tLR: 1.686957\n",
      "Training Epoch: 17 [43776/50000]\tLoss: 3.6866\tLR: 1.687212\n",
      "Training Epoch: 17 [43904/50000]\tLoss: 3.7912\tLR: 1.687468\n",
      "Training Epoch: 17 [44032/50000]\tLoss: 3.7306\tLR: 1.687724\n",
      "Training Epoch: 17 [44160/50000]\tLoss: 3.8722\tLR: 1.687980\n",
      "Training Epoch: 17 [44288/50000]\tLoss: 4.0320\tLR: 1.688235\n",
      "Training Epoch: 17 [44416/50000]\tLoss: 3.4785\tLR: 1.688491\n",
      "Training Epoch: 17 [44544/50000]\tLoss: 3.9095\tLR: 1.688747\n",
      "Training Epoch: 17 [44672/50000]\tLoss: 3.8271\tLR: 1.689003\n",
      "Training Epoch: 17 [44800/50000]\tLoss: 3.8566\tLR: 1.689258\n",
      "Training Epoch: 17 [44928/50000]\tLoss: 3.7630\tLR: 1.689514\n",
      "Training Epoch: 17 [45056/50000]\tLoss: 3.6016\tLR: 1.689770\n",
      "Training Epoch: 17 [45184/50000]\tLoss: 3.6205\tLR: 1.690026\n",
      "Training Epoch: 17 [45312/50000]\tLoss: 3.7174\tLR: 1.690281\n",
      "Training Epoch: 17 [45440/50000]\tLoss: 3.7840\tLR: 1.690537\n",
      "Training Epoch: 17 [45568/50000]\tLoss: 3.5910\tLR: 1.690793\n",
      "Training Epoch: 17 [45696/50000]\tLoss: 3.8257\tLR: 1.691049\n",
      "Training Epoch: 17 [45824/50000]\tLoss: 3.2840\tLR: 1.691304\n",
      "Training Epoch: 17 [45952/50000]\tLoss: 3.6058\tLR: 1.691560\n",
      "Training Epoch: 17 [46080/50000]\tLoss: 3.5200\tLR: 1.691816\n",
      "Training Epoch: 17 [46208/50000]\tLoss: 3.7323\tLR: 1.692072\n",
      "Training Epoch: 17 [46336/50000]\tLoss: 3.6048\tLR: 1.692327\n",
      "Training Epoch: 17 [46464/50000]\tLoss: 3.5505\tLR: 1.692583\n",
      "Training Epoch: 17 [46592/50000]\tLoss: 3.6915\tLR: 1.692839\n",
      "Training Epoch: 17 [46720/50000]\tLoss: 3.5600\tLR: 1.693095\n",
      "Training Epoch: 17 [46848/50000]\tLoss: 3.5330\tLR: 1.693350\n",
      "Training Epoch: 17 [46976/50000]\tLoss: 3.3843\tLR: 1.693606\n",
      "Training Epoch: 17 [47104/50000]\tLoss: 3.6267\tLR: 1.693862\n",
      "Training Epoch: 17 [47232/50000]\tLoss: 3.6500\tLR: 1.694118\n",
      "Training Epoch: 17 [47360/50000]\tLoss: 3.5677\tLR: 1.694373\n",
      "Training Epoch: 17 [47488/50000]\tLoss: 3.7717\tLR: 1.694629\n",
      "Training Epoch: 17 [47616/50000]\tLoss: 3.8245\tLR: 1.694885\n",
      "Training Epoch: 17 [47744/50000]\tLoss: 3.6442\tLR: 1.695141\n",
      "Training Epoch: 17 [47872/50000]\tLoss: 3.7298\tLR: 1.695396\n",
      "Training Epoch: 17 [48000/50000]\tLoss: 3.5761\tLR: 1.695652\n",
      "Training Epoch: 17 [48128/50000]\tLoss: 3.5961\tLR: 1.695908\n",
      "Training Epoch: 17 [48256/50000]\tLoss: 3.9485\tLR: 1.696164\n",
      "Training Epoch: 17 [48384/50000]\tLoss: 3.7118\tLR: 1.696419\n",
      "Training Epoch: 17 [48512/50000]\tLoss: 3.8526\tLR: 1.696675\n",
      "Training Epoch: 17 [48640/50000]\tLoss: 3.7885\tLR: 1.696931\n",
      "Training Epoch: 17 [48768/50000]\tLoss: 3.6257\tLR: 1.697187\n",
      "Training Epoch: 17 [48896/50000]\tLoss: 3.6410\tLR: 1.697442\n",
      "Training Epoch: 17 [49024/50000]\tLoss: 3.6122\tLR: 1.697698\n",
      "Training Epoch: 17 [49152/50000]\tLoss: 3.8569\tLR: 1.697954\n",
      "Training Epoch: 17 [49280/50000]\tLoss: 3.6129\tLR: 1.698210\n",
      "Training Epoch: 17 [49408/50000]\tLoss: 3.6400\tLR: 1.698465\n",
      "Training Epoch: 17 [49536/50000]\tLoss: 3.3515\tLR: 1.698721\n",
      "Training Epoch: 17 [49664/50000]\tLoss: 3.4982\tLR: 1.698977\n",
      "Training Epoch: 17 [49792/50000]\tLoss: 3.4312\tLR: 1.699233\n",
      "Training Epoch: 17 [49920/50000]\tLoss: 3.6401\tLR: 1.699488\n",
      "Training Epoch: 17 [50000/50000]\tLoss: 3.4295\tLR: 1.699744\n",
      "epoch 17 training time consumed: 488.98s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   23834 GB |   23834 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   23761 GB |   23760 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      73 GB |      73 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   23834 GB |   23834 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   23761 GB |   23760 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      73 GB |      73 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   23496 GB |   23496 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   23423 GB |   23423 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |      73 GB |      73 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    2527 K  |    2527 K  |\n",
      "|       from large pool |      24    |      65    |    1077 K  |    1077 K  |\n",
      "|       from small pool |     231    |     274    |    1450 K  |    1450 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    2527 K  |    2527 K  |\n",
      "|       from large pool |      24    |      65    |    1077 K  |    1077 K  |\n",
      "|       from small pool |     231    |     274    |    1450 K  |    1450 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      45    |    1463 K  |    1463 K  |\n",
      "|       from large pool |      10    |      23    |     517 K  |     517 K  |\n",
      "|       from small pool |      25    |      35    |     945 K  |     945 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 17, Average loss: 0.0334, Accuracy: 0.0974, Time consumed:30.91s\n",
      "\n",
      "Training Epoch: 18 [128/50000]\tLoss: 3.6890\tLR: 0.100000\n",
      "Training Epoch: 18 [256/50000]\tLoss: 3.6634\tLR: 1.700256\n",
      "Training Epoch: 18 [384/50000]\tLoss: 3.7149\tLR: 1.700512\n",
      "Training Epoch: 18 [512/50000]\tLoss: 3.6170\tLR: 1.700767\n",
      "Training Epoch: 18 [640/50000]\tLoss: 3.7532\tLR: 1.701023\n",
      "Training Epoch: 18 [768/50000]\tLoss: 3.5510\tLR: 1.701279\n",
      "Training Epoch: 18 [896/50000]\tLoss: 3.7076\tLR: 1.701535\n",
      "Training Epoch: 18 [1024/50000]\tLoss: 3.6132\tLR: 1.701790\n",
      "Training Epoch: 18 [1152/50000]\tLoss: 3.6179\tLR: 1.702046\n",
      "Training Epoch: 18 [1280/50000]\tLoss: 3.4153\tLR: 1.702302\n",
      "Training Epoch: 18 [1408/50000]\tLoss: 3.7019\tLR: 1.702558\n",
      "Training Epoch: 18 [1536/50000]\tLoss: 3.8022\tLR: 1.702813\n",
      "Training Epoch: 18 [1664/50000]\tLoss: 3.5891\tLR: 1.703069\n",
      "Training Epoch: 18 [1792/50000]\tLoss: 3.6063\tLR: 1.703325\n",
      "Training Epoch: 18 [1920/50000]\tLoss: 3.5400\tLR: 1.703581\n",
      "Training Epoch: 18 [2048/50000]\tLoss: 3.7094\tLR: 1.703836\n",
      "Training Epoch: 18 [2176/50000]\tLoss: 3.5540\tLR: 1.704092\n",
      "Training Epoch: 18 [2304/50000]\tLoss: 3.4478\tLR: 1.704348\n",
      "Training Epoch: 18 [2432/50000]\tLoss: 3.6557\tLR: 1.704604\n",
      "Training Epoch: 18 [2560/50000]\tLoss: 3.8132\tLR: 1.704859\n",
      "Training Epoch: 18 [2688/50000]\tLoss: 3.6118\tLR: 1.705115\n",
      "Training Epoch: 18 [2816/50000]\tLoss: 3.4428\tLR: 1.705371\n",
      "Training Epoch: 18 [2944/50000]\tLoss: 3.5377\tLR: 1.705627\n",
      "Training Epoch: 18 [3072/50000]\tLoss: 3.7132\tLR: 1.705882\n",
      "Training Epoch: 18 [3200/50000]\tLoss: 3.5647\tLR: 1.706138\n",
      "Training Epoch: 18 [3328/50000]\tLoss: 3.6420\tLR: 1.706394\n",
      "Training Epoch: 18 [3456/50000]\tLoss: 3.4286\tLR: 1.706650\n",
      "Training Epoch: 18 [3584/50000]\tLoss: 3.5961\tLR: 1.706905\n",
      "Training Epoch: 18 [3712/50000]\tLoss: 3.7354\tLR: 1.707161\n",
      "Training Epoch: 18 [3840/50000]\tLoss: 3.4910\tLR: 1.707417\n",
      "Training Epoch: 18 [3968/50000]\tLoss: 3.6140\tLR: 1.707673\n",
      "Training Epoch: 18 [4096/50000]\tLoss: 3.6384\tLR: 1.707928\n",
      "Training Epoch: 18 [4224/50000]\tLoss: 3.4946\tLR: 1.708184\n",
      "Training Epoch: 18 [4352/50000]\tLoss: 3.5197\tLR: 1.708440\n",
      "Training Epoch: 18 [4480/50000]\tLoss: 3.4060\tLR: 1.708696\n",
      "Training Epoch: 18 [4608/50000]\tLoss: 3.5417\tLR: 1.708951\n",
      "Training Epoch: 18 [4736/50000]\tLoss: 3.5525\tLR: 1.709207\n",
      "Training Epoch: 18 [4864/50000]\tLoss: 3.4212\tLR: 1.709463\n",
      "Training Epoch: 18 [4992/50000]\tLoss: 3.5681\tLR: 1.709719\n",
      "Training Epoch: 18 [5120/50000]\tLoss: 3.6136\tLR: 1.709974\n",
      "Training Epoch: 18 [5248/50000]\tLoss: 3.2234\tLR: 1.710230\n",
      "Training Epoch: 18 [5376/50000]\tLoss: 3.3444\tLR: 1.710486\n",
      "Training Epoch: 18 [5504/50000]\tLoss: 3.6889\tLR: 1.710742\n",
      "Training Epoch: 18 [5632/50000]\tLoss: 3.6230\tLR: 1.710997\n",
      "Training Epoch: 18 [5760/50000]\tLoss: 3.5315\tLR: 1.711253\n",
      "Training Epoch: 18 [5888/50000]\tLoss: 3.5393\tLR: 1.711509\n",
      "Training Epoch: 18 [6016/50000]\tLoss: 3.4326\tLR: 1.711765\n",
      "Training Epoch: 18 [6144/50000]\tLoss: 3.2796\tLR: 1.712020\n",
      "Training Epoch: 18 [6272/50000]\tLoss: 3.8041\tLR: 1.712276\n",
      "Training Epoch: 18 [6400/50000]\tLoss: 3.4362\tLR: 1.712532\n",
      "Training Epoch: 18 [6528/50000]\tLoss: 3.5194\tLR: 1.712788\n",
      "Training Epoch: 18 [6656/50000]\tLoss: 3.7283\tLR: 1.713043\n",
      "Training Epoch: 18 [6784/50000]\tLoss: 3.2622\tLR: 1.713299\n",
      "Training Epoch: 18 [6912/50000]\tLoss: 3.3280\tLR: 1.713555\n",
      "Training Epoch: 18 [7040/50000]\tLoss: 3.5985\tLR: 1.713811\n",
      "Training Epoch: 18 [7168/50000]\tLoss: 3.6026\tLR: 1.714066\n",
      "Training Epoch: 18 [7296/50000]\tLoss: 3.6945\tLR: 1.714322\n",
      "Training Epoch: 18 [7424/50000]\tLoss: 3.6887\tLR: 1.714578\n",
      "Training Epoch: 18 [7552/50000]\tLoss: 3.7775\tLR: 1.714834\n",
      "Training Epoch: 18 [7680/50000]\tLoss: 3.7168\tLR: 1.715090\n",
      "Training Epoch: 18 [7808/50000]\tLoss: 3.8886\tLR: 1.715345\n",
      "Training Epoch: 18 [7936/50000]\tLoss: 3.6825\tLR: 1.715601\n",
      "Training Epoch: 18 [8064/50000]\tLoss: 3.7958\tLR: 1.715857\n",
      "Training Epoch: 18 [8192/50000]\tLoss: 3.4011\tLR: 1.716113\n",
      "Training Epoch: 18 [8320/50000]\tLoss: 3.5754\tLR: 1.716368\n",
      "Training Epoch: 18 [8448/50000]\tLoss: 3.6853\tLR: 1.716624\n",
      "Training Epoch: 18 [8576/50000]\tLoss: 3.5786\tLR: 1.716880\n",
      "Training Epoch: 18 [8704/50000]\tLoss: 3.6666\tLR: 1.717136\n",
      "Training Epoch: 18 [8832/50000]\tLoss: 3.8613\tLR: 1.717391\n",
      "Training Epoch: 18 [8960/50000]\tLoss: 3.4897\tLR: 1.717647\n",
      "Training Epoch: 18 [9088/50000]\tLoss: 3.5655\tLR: 1.717903\n",
      "Training Epoch: 18 [9216/50000]\tLoss: 3.7144\tLR: 1.718159\n",
      "Training Epoch: 18 [9344/50000]\tLoss: 3.4644\tLR: 1.718414\n",
      "Training Epoch: 18 [9472/50000]\tLoss: 3.7073\tLR: 1.718670\n",
      "Training Epoch: 18 [9600/50000]\tLoss: 3.6876\tLR: 1.718926\n",
      "Training Epoch: 18 [9728/50000]\tLoss: 3.5299\tLR: 1.719182\n",
      "Training Epoch: 18 [9856/50000]\tLoss: 3.5441\tLR: 1.719437\n",
      "Training Epoch: 18 [9984/50000]\tLoss: 3.6857\tLR: 1.719693\n",
      "Training Epoch: 18 [10112/50000]\tLoss: 3.7371\tLR: 1.719949\n",
      "Training Epoch: 18 [10240/50000]\tLoss: 3.5116\tLR: 1.720205\n",
      "Training Epoch: 18 [10368/50000]\tLoss: 3.7584\tLR: 1.720460\n",
      "Training Epoch: 18 [10496/50000]\tLoss: 3.4642\tLR: 1.720716\n",
      "Training Epoch: 18 [10624/50000]\tLoss: 3.5049\tLR: 1.720972\n",
      "Training Epoch: 18 [10752/50000]\tLoss: 3.4007\tLR: 1.721228\n",
      "Training Epoch: 18 [10880/50000]\tLoss: 3.3121\tLR: 1.721483\n",
      "Training Epoch: 18 [11008/50000]\tLoss: 3.6921\tLR: 1.721739\n",
      "Training Epoch: 18 [11136/50000]\tLoss: 3.7280\tLR: 1.721995\n",
      "Training Epoch: 18 [11264/50000]\tLoss: 3.6781\tLR: 1.722251\n",
      "Training Epoch: 18 [11392/50000]\tLoss: 3.5773\tLR: 1.722506\n",
      "Training Epoch: 18 [11520/50000]\tLoss: 3.4243\tLR: 1.722762\n",
      "Training Epoch: 18 [11648/50000]\tLoss: 3.6223\tLR: 1.723018\n",
      "Training Epoch: 18 [11776/50000]\tLoss: 3.8979\tLR: 1.723274\n",
      "Training Epoch: 18 [11904/50000]\tLoss: 3.7089\tLR: 1.723529\n",
      "Training Epoch: 18 [12032/50000]\tLoss: 3.7374\tLR: 1.723785\n",
      "Training Epoch: 18 [12160/50000]\tLoss: 3.7847\tLR: 1.724041\n",
      "Training Epoch: 18 [12288/50000]\tLoss: 3.5508\tLR: 1.724297\n",
      "Training Epoch: 18 [12416/50000]\tLoss: 3.4999\tLR: 1.724552\n",
      "Training Epoch: 18 [12544/50000]\tLoss: 3.4985\tLR: 1.724808\n",
      "Training Epoch: 18 [12672/50000]\tLoss: 3.8201\tLR: 1.725064\n",
      "Training Epoch: 18 [12800/50000]\tLoss: 3.6931\tLR: 1.725320\n",
      "Training Epoch: 18 [12928/50000]\tLoss: 3.7660\tLR: 1.725575\n",
      "Training Epoch: 18 [13056/50000]\tLoss: 3.7107\tLR: 1.725831\n",
      "Training Epoch: 18 [13184/50000]\tLoss: 3.4435\tLR: 1.726087\n",
      "Training Epoch: 18 [13312/50000]\tLoss: 3.5059\tLR: 1.726343\n",
      "Training Epoch: 18 [13440/50000]\tLoss: 3.6276\tLR: 1.726598\n",
      "Training Epoch: 18 [13568/50000]\tLoss: 3.7251\tLR: 1.726854\n",
      "Training Epoch: 18 [13696/50000]\tLoss: 3.6863\tLR: 1.727110\n",
      "Training Epoch: 18 [13824/50000]\tLoss: 3.5542\tLR: 1.727366\n",
      "Training Epoch: 18 [13952/50000]\tLoss: 3.6494\tLR: 1.727621\n",
      "Training Epoch: 18 [14080/50000]\tLoss: 3.6193\tLR: 1.727877\n",
      "Training Epoch: 18 [14208/50000]\tLoss: 3.5698\tLR: 1.728133\n",
      "Training Epoch: 18 [14336/50000]\tLoss: 3.3092\tLR: 1.728389\n",
      "Training Epoch: 18 [14464/50000]\tLoss: 3.5374\tLR: 1.728645\n",
      "Training Epoch: 18 [14592/50000]\tLoss: 3.5008\tLR: 1.728900\n",
      "Training Epoch: 18 [14720/50000]\tLoss: 3.5821\tLR: 1.729156\n",
      "Training Epoch: 18 [14848/50000]\tLoss: 3.4895\tLR: 1.729412\n",
      "Training Epoch: 18 [14976/50000]\tLoss: 3.6528\tLR: 1.729668\n",
      "Training Epoch: 18 [15104/50000]\tLoss: 3.7121\tLR: 1.729923\n",
      "Training Epoch: 18 [15232/50000]\tLoss: 3.5566\tLR: 1.730179\n",
      "Training Epoch: 18 [15360/50000]\tLoss: 3.5188\tLR: 1.730435\n",
      "Training Epoch: 18 [15488/50000]\tLoss: 3.9363\tLR: 1.730691\n",
      "Training Epoch: 18 [15616/50000]\tLoss: 3.8242\tLR: 1.730946\n",
      "Training Epoch: 18 [15744/50000]\tLoss: 3.6662\tLR: 1.731202\n",
      "Training Epoch: 18 [15872/50000]\tLoss: 3.6491\tLR: 1.731458\n",
      "Training Epoch: 18 [16000/50000]\tLoss: 3.8527\tLR: 1.731714\n",
      "Training Epoch: 18 [16128/50000]\tLoss: 3.7112\tLR: 1.731969\n",
      "Training Epoch: 18 [16256/50000]\tLoss: 3.5146\tLR: 1.732225\n",
      "Training Epoch: 18 [16384/50000]\tLoss: 3.9626\tLR: 1.732481\n",
      "Training Epoch: 18 [16512/50000]\tLoss: 3.5477\tLR: 1.732737\n",
      "Training Epoch: 18 [16640/50000]\tLoss: 3.6686\tLR: 1.732992\n",
      "Training Epoch: 18 [16768/50000]\tLoss: 3.6326\tLR: 1.733248\n",
      "Training Epoch: 18 [16896/50000]\tLoss: 3.5160\tLR: 1.733504\n",
      "Training Epoch: 18 [17024/50000]\tLoss: 3.5798\tLR: 1.733760\n",
      "Training Epoch: 18 [17152/50000]\tLoss: 3.5627\tLR: 1.734015\n",
      "Training Epoch: 18 [17280/50000]\tLoss: 3.5181\tLR: 1.734271\n",
      "Training Epoch: 18 [17408/50000]\tLoss: 3.7729\tLR: 1.734527\n",
      "Training Epoch: 18 [17536/50000]\tLoss: 3.8004\tLR: 1.734783\n",
      "Training Epoch: 18 [17664/50000]\tLoss: 3.6160\tLR: 1.735038\n",
      "Training Epoch: 18 [17792/50000]\tLoss: 3.5948\tLR: 1.735294\n",
      "Training Epoch: 18 [17920/50000]\tLoss: 3.6181\tLR: 1.735550\n",
      "Training Epoch: 18 [18048/50000]\tLoss: 3.9188\tLR: 1.735806\n",
      "Training Epoch: 18 [18176/50000]\tLoss: 3.6917\tLR: 1.736061\n",
      "Training Epoch: 18 [18304/50000]\tLoss: 3.9116\tLR: 1.736317\n",
      "Training Epoch: 18 [18432/50000]\tLoss: 3.5139\tLR: 1.736573\n",
      "Training Epoch: 18 [18560/50000]\tLoss: 3.5436\tLR: 1.736829\n",
      "Training Epoch: 18 [18688/50000]\tLoss: 3.6917\tLR: 1.737084\n",
      "Training Epoch: 18 [18816/50000]\tLoss: 3.7829\tLR: 1.737340\n",
      "Training Epoch: 18 [18944/50000]\tLoss: 3.6638\tLR: 1.737596\n",
      "Training Epoch: 18 [19072/50000]\tLoss: 3.4716\tLR: 1.737852\n",
      "Training Epoch: 18 [19200/50000]\tLoss: 3.6098\tLR: 1.738107\n",
      "Training Epoch: 18 [19328/50000]\tLoss: 3.8495\tLR: 1.738363\n",
      "Training Epoch: 18 [19456/50000]\tLoss: 3.8683\tLR: 1.738619\n",
      "Training Epoch: 18 [19584/50000]\tLoss: 3.7087\tLR: 1.738875\n",
      "Training Epoch: 18 [19712/50000]\tLoss: 3.6005\tLR: 1.739130\n",
      "Training Epoch: 18 [19840/50000]\tLoss: 3.7366\tLR: 1.739386\n",
      "Training Epoch: 18 [19968/50000]\tLoss: 3.5744\tLR: 1.739642\n",
      "Training Epoch: 18 [20096/50000]\tLoss: 3.5054\tLR: 1.739898\n",
      "Training Epoch: 18 [20224/50000]\tLoss: 3.7459\tLR: 1.740153\n",
      "Training Epoch: 18 [20352/50000]\tLoss: 3.6379\tLR: 1.740409\n",
      "Training Epoch: 18 [20480/50000]\tLoss: 3.6704\tLR: 1.740665\n",
      "Training Epoch: 18 [20608/50000]\tLoss: 3.6711\tLR: 1.740921\n",
      "Training Epoch: 18 [20736/50000]\tLoss: 3.5223\tLR: 1.741176\n",
      "Training Epoch: 18 [20864/50000]\tLoss: 3.4862\tLR: 1.741432\n",
      "Training Epoch: 18 [20992/50000]\tLoss: 3.3393\tLR: 1.741688\n",
      "Training Epoch: 18 [21120/50000]\tLoss: 3.3726\tLR: 1.741944\n",
      "Training Epoch: 18 [21248/50000]\tLoss: 3.4151\tLR: 1.742199\n",
      "Training Epoch: 18 [21376/50000]\tLoss: 3.7481\tLR: 1.742455\n",
      "Training Epoch: 18 [21504/50000]\tLoss: 3.9737\tLR: 1.742711\n",
      "Training Epoch: 18 [21632/50000]\tLoss: 3.5612\tLR: 1.742967\n",
      "Training Epoch: 18 [21760/50000]\tLoss: 4.0105\tLR: 1.743223\n",
      "Training Epoch: 18 [21888/50000]\tLoss: 3.8111\tLR: 1.743478\n",
      "Training Epoch: 18 [22016/50000]\tLoss: 3.6225\tLR: 1.743734\n",
      "Training Epoch: 18 [22144/50000]\tLoss: 3.6930\tLR: 1.743990\n",
      "Training Epoch: 18 [22272/50000]\tLoss: 3.5409\tLR: 1.744246\n",
      "Training Epoch: 18 [22400/50000]\tLoss: 3.6032\tLR: 1.744501\n",
      "Training Epoch: 18 [22528/50000]\tLoss: 3.7576\tLR: 1.744757\n",
      "Training Epoch: 18 [22656/50000]\tLoss: 3.4823\tLR: 1.745013\n",
      "Training Epoch: 18 [22784/50000]\tLoss: 3.4054\tLR: 1.745269\n",
      "Training Epoch: 18 [22912/50000]\tLoss: 3.4890\tLR: 1.745524\n",
      "Training Epoch: 18 [23040/50000]\tLoss: 3.8944\tLR: 1.745780\n",
      "Training Epoch: 18 [23168/50000]\tLoss: 3.9856\tLR: 1.746036\n",
      "Training Epoch: 18 [23296/50000]\tLoss: 3.6822\tLR: 1.746292\n",
      "Training Epoch: 18 [23424/50000]\tLoss: 3.5726\tLR: 1.746547\n",
      "Training Epoch: 18 [23552/50000]\tLoss: 3.8743\tLR: 1.746803\n",
      "Training Epoch: 18 [23680/50000]\tLoss: 3.6056\tLR: 1.747059\n",
      "Training Epoch: 18 [23808/50000]\tLoss: 3.8538\tLR: 1.747315\n",
      "Training Epoch: 18 [23936/50000]\tLoss: 3.8933\tLR: 1.747570\n",
      "Training Epoch: 18 [24064/50000]\tLoss: 3.8343\tLR: 1.747826\n",
      "Training Epoch: 18 [24192/50000]\tLoss: 3.9633\tLR: 1.748082\n",
      "Training Epoch: 18 [24320/50000]\tLoss: 3.7989\tLR: 1.748338\n",
      "Training Epoch: 18 [24448/50000]\tLoss: 3.5490\tLR: 1.748593\n",
      "Training Epoch: 18 [24576/50000]\tLoss: 3.8380\tLR: 1.748849\n",
      "Training Epoch: 18 [24704/50000]\tLoss: 3.7878\tLR: 1.749105\n",
      "Training Epoch: 18 [24832/50000]\tLoss: 3.6872\tLR: 1.749361\n",
      "Training Epoch: 18 [24960/50000]\tLoss: 3.7384\tLR: 1.749616\n",
      "Training Epoch: 18 [25088/50000]\tLoss: 3.6827\tLR: 1.749872\n",
      "Training Epoch: 18 [25216/50000]\tLoss: 3.7765\tLR: 1.750128\n",
      "Training Epoch: 18 [25344/50000]\tLoss: 4.0052\tLR: 1.750384\n",
      "Training Epoch: 18 [25472/50000]\tLoss: 3.6484\tLR: 1.750639\n",
      "Training Epoch: 18 [25600/50000]\tLoss: 3.9192\tLR: 1.750895\n",
      "Training Epoch: 18 [25728/50000]\tLoss: 3.7815\tLR: 1.751151\n",
      "Training Epoch: 18 [25856/50000]\tLoss: 3.6966\tLR: 1.751407\n",
      "Training Epoch: 18 [25984/50000]\tLoss: 3.5416\tLR: 1.751662\n",
      "Training Epoch: 18 [26112/50000]\tLoss: 3.7360\tLR: 1.751918\n",
      "Training Epoch: 18 [26240/50000]\tLoss: 3.8747\tLR: 1.752174\n",
      "Training Epoch: 18 [26368/50000]\tLoss: 3.4889\tLR: 1.752430\n",
      "Training Epoch: 18 [26496/50000]\tLoss: 3.7067\tLR: 1.752685\n",
      "Training Epoch: 18 [26624/50000]\tLoss: 3.8718\tLR: 1.752941\n",
      "Training Epoch: 18 [26752/50000]\tLoss: 3.6544\tLR: 1.753197\n",
      "Training Epoch: 18 [26880/50000]\tLoss: 3.6584\tLR: 1.753453\n",
      "Training Epoch: 18 [27008/50000]\tLoss: 3.6885\tLR: 1.753708\n",
      "Training Epoch: 18 [27136/50000]\tLoss: 3.8301\tLR: 1.753964\n",
      "Training Epoch: 18 [27264/50000]\tLoss: 3.4310\tLR: 1.754220\n",
      "Training Epoch: 18 [27392/50000]\tLoss: 3.6402\tLR: 1.754476\n",
      "Training Epoch: 18 [27520/50000]\tLoss: 3.5680\tLR: 1.754731\n",
      "Training Epoch: 18 [27648/50000]\tLoss: 3.6159\tLR: 1.754987\n",
      "Training Epoch: 18 [27776/50000]\tLoss: 3.4439\tLR: 1.755243\n",
      "Training Epoch: 18 [27904/50000]\tLoss: 3.6960\tLR: 1.755499\n",
      "Training Epoch: 18 [28032/50000]\tLoss: 3.7759\tLR: 1.755754\n",
      "Training Epoch: 18 [28160/50000]\tLoss: 3.7818\tLR: 1.756010\n",
      "Training Epoch: 18 [28288/50000]\tLoss: 3.6544\tLR: 1.756266\n",
      "Training Epoch: 18 [28416/50000]\tLoss: 3.8082\tLR: 1.756522\n",
      "Training Epoch: 18 [28544/50000]\tLoss: 3.4724\tLR: 1.756777\n",
      "Training Epoch: 18 [28672/50000]\tLoss: 3.6762\tLR: 1.757033\n",
      "Training Epoch: 18 [28800/50000]\tLoss: 3.6471\tLR: 1.757289\n",
      "Training Epoch: 18 [28928/50000]\tLoss: 3.6960\tLR: 1.757545\n",
      "Training Epoch: 18 [29056/50000]\tLoss: 3.7409\tLR: 1.757801\n",
      "Training Epoch: 18 [29184/50000]\tLoss: 3.8810\tLR: 1.758056\n",
      "Training Epoch: 18 [29312/50000]\tLoss: 3.6949\tLR: 1.758312\n",
      "Training Epoch: 18 [29440/50000]\tLoss: 3.7290\tLR: 1.758568\n",
      "Training Epoch: 18 [29568/50000]\tLoss: 3.6762\tLR: 1.758824\n",
      "Training Epoch: 18 [29696/50000]\tLoss: 3.9011\tLR: 1.759079\n",
      "Training Epoch: 18 [29824/50000]\tLoss: 3.8119\tLR: 1.759335\n",
      "Training Epoch: 18 [29952/50000]\tLoss: 3.7362\tLR: 1.759591\n",
      "Training Epoch: 18 [30080/50000]\tLoss: 3.5965\tLR: 1.759847\n",
      "Training Epoch: 18 [30208/50000]\tLoss: 3.6290\tLR: 1.760102\n",
      "Training Epoch: 18 [30336/50000]\tLoss: 3.8905\tLR: 1.760358\n",
      "Training Epoch: 18 [30464/50000]\tLoss: 3.8476\tLR: 1.760614\n",
      "Training Epoch: 18 [30592/50000]\tLoss: 3.9062\tLR: 1.760870\n",
      "Training Epoch: 18 [30720/50000]\tLoss: 3.6324\tLR: 1.761125\n",
      "Training Epoch: 18 [30848/50000]\tLoss: 3.6007\tLR: 1.761381\n",
      "Training Epoch: 18 [30976/50000]\tLoss: 3.7308\tLR: 1.761637\n",
      "Training Epoch: 18 [31104/50000]\tLoss: 3.9300\tLR: 1.761893\n",
      "Training Epoch: 18 [31232/50000]\tLoss: 3.7471\tLR: 1.762148\n",
      "Training Epoch: 18 [31360/50000]\tLoss: 3.4737\tLR: 1.762404\n",
      "Training Epoch: 18 [31488/50000]\tLoss: 3.8120\tLR: 1.762660\n",
      "Training Epoch: 18 [31616/50000]\tLoss: 3.8712\tLR: 1.762916\n",
      "Training Epoch: 18 [31744/50000]\tLoss: 3.5905\tLR: 1.763171\n",
      "Training Epoch: 18 [31872/50000]\tLoss: 3.6685\tLR: 1.763427\n",
      "Training Epoch: 18 [32000/50000]\tLoss: 3.5669\tLR: 1.763683\n",
      "Training Epoch: 18 [32128/50000]\tLoss: 3.7472\tLR: 1.763939\n",
      "Training Epoch: 18 [32256/50000]\tLoss: 3.7482\tLR: 1.764194\n",
      "Training Epoch: 18 [32384/50000]\tLoss: 3.5180\tLR: 1.764450\n",
      "Training Epoch: 18 [32512/50000]\tLoss: 3.5991\tLR: 1.764706\n",
      "Training Epoch: 18 [32640/50000]\tLoss: 3.5114\tLR: 1.764962\n",
      "Training Epoch: 18 [32768/50000]\tLoss: 3.7201\tLR: 1.765217\n",
      "Training Epoch: 18 [32896/50000]\tLoss: 3.8352\tLR: 1.765473\n",
      "Training Epoch: 18 [33024/50000]\tLoss: 3.5768\tLR: 1.765729\n",
      "Training Epoch: 18 [33152/50000]\tLoss: 3.8160\tLR: 1.765985\n",
      "Training Epoch: 18 [33280/50000]\tLoss: 3.7941\tLR: 1.766240\n",
      "Training Epoch: 18 [33408/50000]\tLoss: 3.7844\tLR: 1.766496\n",
      "Training Epoch: 18 [33536/50000]\tLoss: 3.6334\tLR: 1.766752\n",
      "Training Epoch: 18 [33664/50000]\tLoss: 3.6939\tLR: 1.767008\n",
      "Training Epoch: 18 [33792/50000]\tLoss: 3.7477\tLR: 1.767263\n",
      "Training Epoch: 18 [33920/50000]\tLoss: 3.6607\tLR: 1.767519\n",
      "Training Epoch: 18 [34048/50000]\tLoss: 3.8213\tLR: 1.767775\n",
      "Training Epoch: 18 [34176/50000]\tLoss: 3.6175\tLR: 1.768031\n",
      "Training Epoch: 18 [34304/50000]\tLoss: 3.6298\tLR: 1.768286\n",
      "Training Epoch: 18 [34432/50000]\tLoss: 3.7210\tLR: 1.768542\n",
      "Training Epoch: 18 [34560/50000]\tLoss: 3.5840\tLR: 1.768798\n",
      "Training Epoch: 18 [34688/50000]\tLoss: 3.7845\tLR: 1.769054\n",
      "Training Epoch: 18 [34816/50000]\tLoss: 3.7732\tLR: 1.769309\n",
      "Training Epoch: 18 [34944/50000]\tLoss: 3.5766\tLR: 1.769565\n",
      "Training Epoch: 18 [35072/50000]\tLoss: 3.9178\tLR: 1.769821\n",
      "Training Epoch: 18 [35200/50000]\tLoss: 3.5381\tLR: 1.770077\n",
      "Training Epoch: 18 [35328/50000]\tLoss: 3.5005\tLR: 1.770332\n",
      "Training Epoch: 18 [35456/50000]\tLoss: 3.8751\tLR: 1.770588\n",
      "Training Epoch: 18 [35584/50000]\tLoss: 3.5057\tLR: 1.770844\n",
      "Training Epoch: 18 [35712/50000]\tLoss: 3.9340\tLR: 1.771100\n",
      "Training Epoch: 18 [35840/50000]\tLoss: 3.6363\tLR: 1.771355\n",
      "Training Epoch: 18 [35968/50000]\tLoss: 3.7513\tLR: 1.771611\n",
      "Training Epoch: 18 [36096/50000]\tLoss: 3.9399\tLR: 1.771867\n",
      "Training Epoch: 18 [36224/50000]\tLoss: 3.7242\tLR: 1.772123\n",
      "Training Epoch: 18 [36352/50000]\tLoss: 3.7067\tLR: 1.772379\n",
      "Training Epoch: 18 [36480/50000]\tLoss: 3.8684\tLR: 1.772634\n",
      "Training Epoch: 18 [36608/50000]\tLoss: 3.8799\tLR: 1.772890\n",
      "Training Epoch: 18 [36736/50000]\tLoss: 3.7779\tLR: 1.773146\n",
      "Training Epoch: 18 [36864/50000]\tLoss: 3.8919\tLR: 1.773402\n",
      "Training Epoch: 18 [36992/50000]\tLoss: 3.6989\tLR: 1.773657\n",
      "Training Epoch: 18 [37120/50000]\tLoss: 3.7696\tLR: 1.773913\n",
      "Training Epoch: 18 [37248/50000]\tLoss: 3.9739\tLR: 1.774169\n",
      "Training Epoch: 18 [37376/50000]\tLoss: 3.4537\tLR: 1.774425\n",
      "Training Epoch: 18 [37504/50000]\tLoss: 3.7542\tLR: 1.774680\n",
      "Training Epoch: 18 [37632/50000]\tLoss: 3.7689\tLR: 1.774936\n",
      "Training Epoch: 18 [37760/50000]\tLoss: 3.8336\tLR: 1.775192\n",
      "Training Epoch: 18 [37888/50000]\tLoss: 3.7109\tLR: 1.775448\n",
      "Training Epoch: 18 [38016/50000]\tLoss: 3.7864\tLR: 1.775703\n",
      "Training Epoch: 18 [38144/50000]\tLoss: 3.3965\tLR: 1.775959\n",
      "Training Epoch: 18 [38272/50000]\tLoss: 3.7927\tLR: 1.776215\n",
      "Training Epoch: 18 [38400/50000]\tLoss: 4.1424\tLR: 1.776471\n",
      "Training Epoch: 18 [38528/50000]\tLoss: 3.8258\tLR: 1.776726\n",
      "Training Epoch: 18 [38656/50000]\tLoss: 3.7530\tLR: 1.776982\n",
      "Training Epoch: 18 [38784/50000]\tLoss: 3.6741\tLR: 1.777238\n",
      "Training Epoch: 18 [38912/50000]\tLoss: 3.6497\tLR: 1.777494\n",
      "Training Epoch: 18 [39040/50000]\tLoss: 3.7071\tLR: 1.777749\n",
      "Training Epoch: 18 [39168/50000]\tLoss: 3.7803\tLR: 1.778005\n",
      "Training Epoch: 18 [39296/50000]\tLoss: 3.8341\tLR: 1.778261\n",
      "Training Epoch: 18 [39424/50000]\tLoss: 3.6778\tLR: 1.778517\n",
      "Training Epoch: 18 [39552/50000]\tLoss: 3.5181\tLR: 1.778772\n",
      "Training Epoch: 18 [39680/50000]\tLoss: 3.6506\tLR: 1.779028\n",
      "Training Epoch: 18 [39808/50000]\tLoss: 3.7348\tLR: 1.779284\n",
      "Training Epoch: 18 [39936/50000]\tLoss: 3.7104\tLR: 1.779540\n",
      "Training Epoch: 18 [40064/50000]\tLoss: 3.7572\tLR: 1.779795\n",
      "Training Epoch: 18 [40192/50000]\tLoss: 3.5926\tLR: 1.780051\n",
      "Training Epoch: 18 [40320/50000]\tLoss: 3.7677\tLR: 1.780307\n",
      "Training Epoch: 18 [40448/50000]\tLoss: 3.8238\tLR: 1.780563\n",
      "Training Epoch: 18 [40576/50000]\tLoss: 3.7908\tLR: 1.780818\n",
      "Training Epoch: 18 [40704/50000]\tLoss: 3.4366\tLR: 1.781074\n",
      "Training Epoch: 18 [40832/50000]\tLoss: 3.7700\tLR: 1.781330\n",
      "Training Epoch: 18 [40960/50000]\tLoss: 3.5527\tLR: 1.781586\n",
      "Training Epoch: 18 [41088/50000]\tLoss: 3.5593\tLR: 1.781841\n",
      "Training Epoch: 18 [41216/50000]\tLoss: 3.6234\tLR: 1.782097\n",
      "Training Epoch: 18 [41344/50000]\tLoss: 3.7588\tLR: 1.782353\n",
      "Training Epoch: 18 [41472/50000]\tLoss: 3.6974\tLR: 1.782609\n",
      "Training Epoch: 18 [41600/50000]\tLoss: 3.5915\tLR: 1.782864\n",
      "Training Epoch: 18 [41728/50000]\tLoss: 3.6203\tLR: 1.783120\n",
      "Training Epoch: 18 [41856/50000]\tLoss: 3.8706\tLR: 1.783376\n",
      "Training Epoch: 18 [41984/50000]\tLoss: 3.6108\tLR: 1.783632\n",
      "Training Epoch: 18 [42112/50000]\tLoss: 3.7389\tLR: 1.783887\n",
      "Training Epoch: 18 [42240/50000]\tLoss: 3.4323\tLR: 1.784143\n",
      "Training Epoch: 18 [42368/50000]\tLoss: 3.7240\tLR: 1.784399\n",
      "Training Epoch: 18 [42496/50000]\tLoss: 3.6386\tLR: 1.784655\n",
      "Training Epoch: 18 [42624/50000]\tLoss: 3.7365\tLR: 1.784910\n",
      "Training Epoch: 18 [42752/50000]\tLoss: 3.6519\tLR: 1.785166\n",
      "Training Epoch: 18 [42880/50000]\tLoss: 3.5982\tLR: 1.785422\n",
      "Training Epoch: 18 [43008/50000]\tLoss: 3.6178\tLR: 1.785678\n",
      "Training Epoch: 18 [43136/50000]\tLoss: 3.9037\tLR: 1.785934\n",
      "Training Epoch: 18 [43264/50000]\tLoss: 3.8224\tLR: 1.786189\n",
      "Training Epoch: 18 [43392/50000]\tLoss: 3.4581\tLR: 1.786445\n",
      "Training Epoch: 18 [43520/50000]\tLoss: 3.8307\tLR: 1.786701\n",
      "Training Epoch: 18 [43648/50000]\tLoss: 3.6267\tLR: 1.786957\n",
      "Training Epoch: 18 [43776/50000]\tLoss: 3.9027\tLR: 1.787212\n",
      "Training Epoch: 18 [43904/50000]\tLoss: 3.8730\tLR: 1.787468\n",
      "Training Epoch: 18 [44032/50000]\tLoss: 3.8114\tLR: 1.787724\n",
      "Training Epoch: 18 [44160/50000]\tLoss: 3.9277\tLR: 1.787980\n",
      "Training Epoch: 18 [44288/50000]\tLoss: 3.8360\tLR: 1.788235\n",
      "Training Epoch: 18 [44416/50000]\tLoss: 3.8578\tLR: 1.788491\n",
      "Training Epoch: 18 [44544/50000]\tLoss: 3.8770\tLR: 1.788747\n",
      "Training Epoch: 18 [44672/50000]\tLoss: 3.6962\tLR: 1.789003\n",
      "Training Epoch: 18 [44800/50000]\tLoss: 3.9440\tLR: 1.789258\n",
      "Training Epoch: 18 [44928/50000]\tLoss: 3.5726\tLR: 1.789514\n",
      "Training Epoch: 18 [45056/50000]\tLoss: 4.1377\tLR: 1.789770\n",
      "Training Epoch: 18 [45184/50000]\tLoss: 3.7285\tLR: 1.790026\n",
      "Training Epoch: 18 [45312/50000]\tLoss: 4.0164\tLR: 1.790281\n",
      "Training Epoch: 18 [45440/50000]\tLoss: 3.6703\tLR: 1.790537\n",
      "Training Epoch: 18 [45568/50000]\tLoss: 3.6593\tLR: 1.790793\n",
      "Training Epoch: 18 [45696/50000]\tLoss: 3.7504\tLR: 1.791049\n",
      "Training Epoch: 18 [45824/50000]\tLoss: 3.8010\tLR: 1.791304\n",
      "Training Epoch: 18 [45952/50000]\tLoss: 3.8725\tLR: 1.791560\n",
      "Training Epoch: 18 [46080/50000]\tLoss: 3.6636\tLR: 1.791816\n",
      "Training Epoch: 18 [46208/50000]\tLoss: 3.5681\tLR: 1.792072\n",
      "Training Epoch: 18 [46336/50000]\tLoss: 3.6962\tLR: 1.792327\n",
      "Training Epoch: 18 [46464/50000]\tLoss: 3.6059\tLR: 1.792583\n",
      "Training Epoch: 18 [46592/50000]\tLoss: 3.6012\tLR: 1.792839\n",
      "Training Epoch: 18 [46720/50000]\tLoss: 3.8531\tLR: 1.793095\n",
      "Training Epoch: 18 [46848/50000]\tLoss: 3.6587\tLR: 1.793350\n",
      "Training Epoch: 18 [46976/50000]\tLoss: 3.4993\tLR: 1.793606\n",
      "Training Epoch: 18 [47104/50000]\tLoss: 3.8322\tLR: 1.793862\n",
      "Training Epoch: 18 [47232/50000]\tLoss: 3.7265\tLR: 1.794118\n",
      "Training Epoch: 18 [47360/50000]\tLoss: 3.8157\tLR: 1.794373\n",
      "Training Epoch: 18 [47488/50000]\tLoss: 3.8469\tLR: 1.794629\n",
      "Training Epoch: 18 [47616/50000]\tLoss: 3.7424\tLR: 1.794885\n",
      "Training Epoch: 18 [47744/50000]\tLoss: 3.8163\tLR: 1.795141\n",
      "Training Epoch: 18 [47872/50000]\tLoss: 3.6978\tLR: 1.795396\n",
      "Training Epoch: 18 [48000/50000]\tLoss: 3.7944\tLR: 1.795652\n",
      "Training Epoch: 18 [48128/50000]\tLoss: 3.8318\tLR: 1.795908\n",
      "Training Epoch: 18 [48256/50000]\tLoss: 3.9353\tLR: 1.796164\n",
      "Training Epoch: 18 [48384/50000]\tLoss: 3.9338\tLR: 1.796419\n",
      "Training Epoch: 18 [48512/50000]\tLoss: 3.7522\tLR: 1.796675\n",
      "Training Epoch: 18 [48640/50000]\tLoss: 3.7378\tLR: 1.796931\n",
      "Training Epoch: 18 [48768/50000]\tLoss: 3.8569\tLR: 1.797187\n",
      "Training Epoch: 18 [48896/50000]\tLoss: 3.8161\tLR: 1.797442\n",
      "Training Epoch: 18 [49024/50000]\tLoss: 3.7440\tLR: 1.797698\n",
      "Training Epoch: 18 [49152/50000]\tLoss: 3.7911\tLR: 1.797954\n",
      "Training Epoch: 18 [49280/50000]\tLoss: 4.0191\tLR: 1.798210\n",
      "Training Epoch: 18 [49408/50000]\tLoss: 3.7984\tLR: 1.798465\n",
      "Training Epoch: 18 [49536/50000]\tLoss: 3.8742\tLR: 1.798721\n",
      "Training Epoch: 18 [49664/50000]\tLoss: 4.0114\tLR: 1.798977\n",
      "Training Epoch: 18 [49792/50000]\tLoss: 3.8895\tLR: 1.799233\n",
      "Training Epoch: 18 [49920/50000]\tLoss: 3.7589\tLR: 1.799488\n",
      "Training Epoch: 18 [50000/50000]\tLoss: 3.8503\tLR: 1.799744\n",
      "epoch 18 training time consumed: 488.97s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   25236 GB |   25235 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   25158 GB |   25158 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      77 GB |      77 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   25236 GB |   25235 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   25158 GB |   25158 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      77 GB |      77 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   24878 GB |   24878 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   24801 GB |   24801 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |      77 GB |      77 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    2676 K  |    2675 K  |\n",
      "|       from large pool |      24    |      65    |    1140 K  |    1140 K  |\n",
      "|       from small pool |     231    |     274    |    1535 K  |    1535 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    2676 K  |    2675 K  |\n",
      "|       from large pool |      24    |      65    |    1140 K  |    1140 K  |\n",
      "|       from small pool |     231    |     274    |    1535 K  |    1535 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      45    |    1549 K  |    1549 K  |\n",
      "|       from large pool |      10    |      23    |     548 K  |     548 K  |\n",
      "|       from small pool |      25    |      35    |    1001 K  |    1001 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 18, Average loss: 0.0330, Accuracy: 0.0938, Time consumed:30.95s\n",
      "\n",
      "Training Epoch: 19 [128/50000]\tLoss: 3.9239\tLR: 0.100000\n",
      "Training Epoch: 19 [256/50000]\tLoss: 3.6548\tLR: 1.800256\n",
      "Training Epoch: 19 [384/50000]\tLoss: 3.6569\tLR: 1.800512\n",
      "Training Epoch: 19 [512/50000]\tLoss: 3.7209\tLR: 1.800767\n",
      "Training Epoch: 19 [640/50000]\tLoss: 3.8364\tLR: 1.801023\n",
      "Training Epoch: 19 [768/50000]\tLoss: 3.6678\tLR: 1.801279\n",
      "Training Epoch: 19 [896/50000]\tLoss: 3.5486\tLR: 1.801535\n",
      "Training Epoch: 19 [1024/50000]\tLoss: 3.7377\tLR: 1.801790\n",
      "Training Epoch: 19 [1152/50000]\tLoss: 3.8077\tLR: 1.802046\n",
      "Training Epoch: 19 [1280/50000]\tLoss: 3.8576\tLR: 1.802302\n",
      "Training Epoch: 19 [1408/50000]\tLoss: 3.6707\tLR: 1.802558\n",
      "Training Epoch: 19 [1536/50000]\tLoss: 3.7097\tLR: 1.802813\n",
      "Training Epoch: 19 [1664/50000]\tLoss: 3.8775\tLR: 1.803069\n",
      "Training Epoch: 19 [1792/50000]\tLoss: 3.5590\tLR: 1.803325\n",
      "Training Epoch: 19 [1920/50000]\tLoss: 3.8619\tLR: 1.803581\n",
      "Training Epoch: 19 [2048/50000]\tLoss: 3.7960\tLR: 1.803836\n",
      "Training Epoch: 19 [2176/50000]\tLoss: 3.6673\tLR: 1.804092\n",
      "Training Epoch: 19 [2304/50000]\tLoss: 3.7273\tLR: 1.804348\n",
      "Training Epoch: 19 [2432/50000]\tLoss: 3.6387\tLR: 1.804604\n",
      "Training Epoch: 19 [2560/50000]\tLoss: 3.6592\tLR: 1.804859\n",
      "Training Epoch: 19 [2688/50000]\tLoss: 3.9375\tLR: 1.805115\n",
      "Training Epoch: 19 [2816/50000]\tLoss: 3.9376\tLR: 1.805371\n",
      "Training Epoch: 19 [2944/50000]\tLoss: 3.9655\tLR: 1.805627\n",
      "Training Epoch: 19 [3072/50000]\tLoss: 3.8993\tLR: 1.805882\n",
      "Training Epoch: 19 [3200/50000]\tLoss: 3.9381\tLR: 1.806138\n",
      "Training Epoch: 19 [3328/50000]\tLoss: 3.7524\tLR: 1.806394\n",
      "Training Epoch: 19 [3456/50000]\tLoss: 3.6502\tLR: 1.806650\n",
      "Training Epoch: 19 [3584/50000]\tLoss: 4.0034\tLR: 1.806905\n",
      "Training Epoch: 19 [3712/50000]\tLoss: 3.6516\tLR: 1.807161\n",
      "Training Epoch: 19 [3840/50000]\tLoss: 3.9882\tLR: 1.807417\n",
      "Training Epoch: 19 [3968/50000]\tLoss: 3.6541\tLR: 1.807673\n",
      "Training Epoch: 19 [4096/50000]\tLoss: 3.8809\tLR: 1.807928\n",
      "Training Epoch: 19 [4224/50000]\tLoss: 3.8679\tLR: 1.808184\n",
      "Training Epoch: 19 [4352/50000]\tLoss: 3.7323\tLR: 1.808440\n",
      "Training Epoch: 19 [4480/50000]\tLoss: 3.9542\tLR: 1.808696\n",
      "Training Epoch: 19 [4608/50000]\tLoss: 3.7222\tLR: 1.808951\n",
      "Training Epoch: 19 [4736/50000]\tLoss: 3.6924\tLR: 1.809207\n",
      "Training Epoch: 19 [4864/50000]\tLoss: 3.6807\tLR: 1.809463\n",
      "Training Epoch: 19 [4992/50000]\tLoss: 3.5684\tLR: 1.809719\n",
      "Training Epoch: 19 [5120/50000]\tLoss: 3.5571\tLR: 1.809974\n",
      "Training Epoch: 19 [5248/50000]\tLoss: 3.5036\tLR: 1.810230\n",
      "Training Epoch: 19 [5376/50000]\tLoss: 3.8597\tLR: 1.810486\n",
      "Training Epoch: 19 [5504/50000]\tLoss: 3.7052\tLR: 1.810742\n",
      "Training Epoch: 19 [5632/50000]\tLoss: 3.8819\tLR: 1.810997\n",
      "Training Epoch: 19 [5760/50000]\tLoss: 3.7733\tLR: 1.811253\n",
      "Training Epoch: 19 [5888/50000]\tLoss: 3.6441\tLR: 1.811509\n",
      "Training Epoch: 19 [6016/50000]\tLoss: 3.8558\tLR: 1.811765\n",
      "Training Epoch: 19 [6144/50000]\tLoss: 3.9759\tLR: 1.812020\n",
      "Training Epoch: 19 [6272/50000]\tLoss: 3.8950\tLR: 1.812276\n",
      "Training Epoch: 19 [6400/50000]\tLoss: 3.5413\tLR: 1.812532\n",
      "Training Epoch: 19 [6528/50000]\tLoss: 3.7119\tLR: 1.812788\n",
      "Training Epoch: 19 [6656/50000]\tLoss: 3.8804\tLR: 1.813043\n",
      "Training Epoch: 19 [6784/50000]\tLoss: 3.9709\tLR: 1.813299\n",
      "Training Epoch: 19 [6912/50000]\tLoss: 3.8000\tLR: 1.813555\n",
      "Training Epoch: 19 [7040/50000]\tLoss: 3.5696\tLR: 1.813811\n",
      "Training Epoch: 19 [7168/50000]\tLoss: 3.6111\tLR: 1.814066\n",
      "Training Epoch: 19 [7296/50000]\tLoss: 3.6494\tLR: 1.814322\n",
      "Training Epoch: 19 [7424/50000]\tLoss: 3.5031\tLR: 1.814578\n",
      "Training Epoch: 19 [7552/50000]\tLoss: 3.7389\tLR: 1.814834\n",
      "Training Epoch: 19 [7680/50000]\tLoss: 3.8597\tLR: 1.815090\n",
      "Training Epoch: 19 [7808/50000]\tLoss: 3.6581\tLR: 1.815345\n",
      "Training Epoch: 19 [7936/50000]\tLoss: 3.7335\tLR: 1.815601\n",
      "Training Epoch: 19 [8064/50000]\tLoss: 3.6113\tLR: 1.815857\n",
      "Training Epoch: 19 [8192/50000]\tLoss: 3.9189\tLR: 1.816113\n",
      "Training Epoch: 19 [8320/50000]\tLoss: 3.6925\tLR: 1.816368\n",
      "Training Epoch: 19 [8448/50000]\tLoss: 3.7096\tLR: 1.816624\n",
      "Training Epoch: 19 [8576/50000]\tLoss: 3.7529\tLR: 1.816880\n",
      "Training Epoch: 19 [8704/50000]\tLoss: 3.7655\tLR: 1.817136\n",
      "Training Epoch: 19 [8832/50000]\tLoss: 3.8316\tLR: 1.817391\n",
      "Training Epoch: 19 [8960/50000]\tLoss: 3.6251\tLR: 1.817647\n",
      "Training Epoch: 19 [9088/50000]\tLoss: 3.7142\tLR: 1.817903\n",
      "Training Epoch: 19 [9216/50000]\tLoss: 3.6747\tLR: 1.818159\n",
      "Training Epoch: 19 [9344/50000]\tLoss: 3.7919\tLR: 1.818414\n",
      "Training Epoch: 19 [9472/50000]\tLoss: 3.6986\tLR: 1.818670\n",
      "Training Epoch: 19 [9600/50000]\tLoss: 3.6741\tLR: 1.818926\n",
      "Training Epoch: 19 [9728/50000]\tLoss: 3.5740\tLR: 1.819182\n",
      "Training Epoch: 19 [9856/50000]\tLoss: 3.7818\tLR: 1.819437\n",
      "Training Epoch: 19 [9984/50000]\tLoss: 3.8345\tLR: 1.819693\n",
      "Training Epoch: 19 [10112/50000]\tLoss: 3.7699\tLR: 1.819949\n",
      "Training Epoch: 19 [10240/50000]\tLoss: 3.4953\tLR: 1.820205\n",
      "Training Epoch: 19 [10368/50000]\tLoss: 3.6497\tLR: 1.820460\n",
      "Training Epoch: 19 [10496/50000]\tLoss: 3.7673\tLR: 1.820716\n",
      "Training Epoch: 19 [10624/50000]\tLoss: 3.7090\tLR: 1.820972\n",
      "Training Epoch: 19 [10752/50000]\tLoss: 3.8487\tLR: 1.821228\n",
      "Training Epoch: 19 [10880/50000]\tLoss: 3.8225\tLR: 1.821483\n",
      "Training Epoch: 19 [11008/50000]\tLoss: 3.6974\tLR: 1.821739\n",
      "Training Epoch: 19 [11136/50000]\tLoss: 3.7926\tLR: 1.821995\n",
      "Training Epoch: 19 [11264/50000]\tLoss: 3.8098\tLR: 1.822251\n",
      "Training Epoch: 19 [11392/50000]\tLoss: 3.6858\tLR: 1.822506\n",
      "Training Epoch: 19 [11520/50000]\tLoss: 3.6550\tLR: 1.822762\n",
      "Training Epoch: 19 [11648/50000]\tLoss: 3.6431\tLR: 1.823018\n",
      "Training Epoch: 19 [11776/50000]\tLoss: 3.7451\tLR: 1.823274\n",
      "Training Epoch: 19 [11904/50000]\tLoss: 3.7026\tLR: 1.823529\n",
      "Training Epoch: 19 [12032/50000]\tLoss: 3.8717\tLR: 1.823785\n",
      "Training Epoch: 19 [12160/50000]\tLoss: 3.7568\tLR: 1.824041\n",
      "Training Epoch: 19 [12288/50000]\tLoss: 3.9561\tLR: 1.824297\n",
      "Training Epoch: 19 [12416/50000]\tLoss: 3.8712\tLR: 1.824552\n",
      "Training Epoch: 19 [12544/50000]\tLoss: 3.8275\tLR: 1.824808\n",
      "Training Epoch: 19 [12672/50000]\tLoss: 3.6516\tLR: 1.825064\n",
      "Training Epoch: 19 [12800/50000]\tLoss: 3.9722\tLR: 1.825320\n",
      "Training Epoch: 19 [12928/50000]\tLoss: 3.6697\tLR: 1.825575\n",
      "Training Epoch: 19 [13056/50000]\tLoss: 3.9530\tLR: 1.825831\n",
      "Training Epoch: 19 [13184/50000]\tLoss: 3.8513\tLR: 1.826087\n",
      "Training Epoch: 19 [13312/50000]\tLoss: 4.2166\tLR: 1.826343\n",
      "Training Epoch: 19 [13440/50000]\tLoss: 3.6241\tLR: 1.826598\n",
      "Training Epoch: 19 [13568/50000]\tLoss: 3.8653\tLR: 1.826854\n",
      "Training Epoch: 19 [13696/50000]\tLoss: 3.9537\tLR: 1.827110\n",
      "Training Epoch: 19 [13824/50000]\tLoss: 3.9832\tLR: 1.827366\n",
      "Training Epoch: 19 [13952/50000]\tLoss: 3.9451\tLR: 1.827621\n",
      "Training Epoch: 19 [14080/50000]\tLoss: 3.6256\tLR: 1.827877\n",
      "Training Epoch: 19 [14208/50000]\tLoss: 3.6583\tLR: 1.828133\n",
      "Training Epoch: 19 [14336/50000]\tLoss: 3.6062\tLR: 1.828389\n",
      "Training Epoch: 19 [14464/50000]\tLoss: 3.6795\tLR: 1.828645\n",
      "Training Epoch: 19 [14592/50000]\tLoss: 3.7279\tLR: 1.828900\n",
      "Training Epoch: 19 [14720/50000]\tLoss: 3.8710\tLR: 1.829156\n",
      "Training Epoch: 19 [14848/50000]\tLoss: 3.6941\tLR: 1.829412\n",
      "Training Epoch: 19 [14976/50000]\tLoss: 4.0340\tLR: 1.829668\n",
      "Training Epoch: 19 [15104/50000]\tLoss: 3.7659\tLR: 1.829923\n",
      "Training Epoch: 19 [15232/50000]\tLoss: 3.6462\tLR: 1.830179\n",
      "Training Epoch: 19 [15360/50000]\tLoss: 3.5934\tLR: 1.830435\n",
      "Training Epoch: 19 [15488/50000]\tLoss: 3.8293\tLR: 1.830691\n",
      "Training Epoch: 19 [15616/50000]\tLoss: 3.6400\tLR: 1.830946\n",
      "Training Epoch: 19 [15744/50000]\tLoss: 3.8757\tLR: 1.831202\n",
      "Training Epoch: 19 [15872/50000]\tLoss: 3.7089\tLR: 1.831458\n",
      "Training Epoch: 19 [16000/50000]\tLoss: 3.8454\tLR: 1.831714\n",
      "Training Epoch: 19 [16128/50000]\tLoss: 3.9466\tLR: 1.831969\n",
      "Training Epoch: 19 [16256/50000]\tLoss: 3.8361\tLR: 1.832225\n",
      "Training Epoch: 19 [16384/50000]\tLoss: 3.7840\tLR: 1.832481\n",
      "Training Epoch: 19 [16512/50000]\tLoss: 3.9073\tLR: 1.832737\n",
      "Training Epoch: 19 [16640/50000]\tLoss: 3.8747\tLR: 1.832992\n",
      "Training Epoch: 19 [16768/50000]\tLoss: 3.7078\tLR: 1.833248\n",
      "Training Epoch: 19 [16896/50000]\tLoss: 3.7075\tLR: 1.833504\n",
      "Training Epoch: 19 [17024/50000]\tLoss: 3.8317\tLR: 1.833760\n",
      "Training Epoch: 19 [17152/50000]\tLoss: 3.9262\tLR: 1.834015\n",
      "Training Epoch: 19 [17280/50000]\tLoss: 3.6714\tLR: 1.834271\n",
      "Training Epoch: 19 [17408/50000]\tLoss: 3.6300\tLR: 1.834527\n",
      "Training Epoch: 19 [17536/50000]\tLoss: 3.7393\tLR: 1.834783\n",
      "Training Epoch: 19 [17664/50000]\tLoss: 4.0358\tLR: 1.835038\n",
      "Training Epoch: 19 [17792/50000]\tLoss: 4.0054\tLR: 1.835294\n",
      "Training Epoch: 19 [17920/50000]\tLoss: 3.7886\tLR: 1.835550\n",
      "Training Epoch: 19 [18048/50000]\tLoss: 3.5961\tLR: 1.835806\n",
      "Training Epoch: 19 [18176/50000]\tLoss: 3.7917\tLR: 1.836061\n",
      "Training Epoch: 19 [18304/50000]\tLoss: 3.7588\tLR: 1.836317\n",
      "Training Epoch: 19 [18432/50000]\tLoss: 3.7062\tLR: 1.836573\n",
      "Training Epoch: 19 [18560/50000]\tLoss: 3.7910\tLR: 1.836829\n",
      "Training Epoch: 19 [18688/50000]\tLoss: 3.7815\tLR: 1.837084\n",
      "Training Epoch: 19 [18816/50000]\tLoss: 3.8578\tLR: 1.837340\n",
      "Training Epoch: 19 [18944/50000]\tLoss: 3.9520\tLR: 1.837596\n",
      "Training Epoch: 19 [19072/50000]\tLoss: 3.7938\tLR: 1.837852\n",
      "Training Epoch: 19 [19200/50000]\tLoss: 3.7456\tLR: 1.838107\n",
      "Training Epoch: 19 [19328/50000]\tLoss: 3.6764\tLR: 1.838363\n",
      "Training Epoch: 19 [19456/50000]\tLoss: 4.0132\tLR: 1.838619\n",
      "Training Epoch: 19 [19584/50000]\tLoss: 3.8245\tLR: 1.838875\n",
      "Training Epoch: 19 [19712/50000]\tLoss: 3.9476\tLR: 1.839130\n",
      "Training Epoch: 19 [19840/50000]\tLoss: 3.6955\tLR: 1.839386\n",
      "Training Epoch: 19 [19968/50000]\tLoss: 3.5711\tLR: 1.839642\n",
      "Training Epoch: 19 [20096/50000]\tLoss: 3.5291\tLR: 1.839898\n",
      "Training Epoch: 19 [20224/50000]\tLoss: 3.5804\tLR: 1.840153\n",
      "Training Epoch: 19 [20352/50000]\tLoss: 3.8017\tLR: 1.840409\n",
      "Training Epoch: 19 [20480/50000]\tLoss: 3.8003\tLR: 1.840665\n",
      "Training Epoch: 19 [20608/50000]\tLoss: 3.6507\tLR: 1.840921\n",
      "Training Epoch: 19 [20736/50000]\tLoss: 3.8691\tLR: 1.841176\n",
      "Training Epoch: 19 [20864/50000]\tLoss: 3.5925\tLR: 1.841432\n",
      "Training Epoch: 19 [20992/50000]\tLoss: 3.7792\tLR: 1.841688\n",
      "Training Epoch: 19 [21120/50000]\tLoss: 3.7080\tLR: 1.841944\n",
      "Training Epoch: 19 [21248/50000]\tLoss: 3.8622\tLR: 1.842199\n",
      "Training Epoch: 19 [21376/50000]\tLoss: 3.6838\tLR: 1.842455\n",
      "Training Epoch: 19 [21504/50000]\tLoss: 3.7485\tLR: 1.842711\n",
      "Training Epoch: 19 [21632/50000]\tLoss: 3.5931\tLR: 1.842967\n",
      "Training Epoch: 19 [21760/50000]\tLoss: 3.9684\tLR: 1.843223\n",
      "Training Epoch: 19 [21888/50000]\tLoss: 3.7790\tLR: 1.843478\n",
      "Training Epoch: 19 [22016/50000]\tLoss: 3.6419\tLR: 1.843734\n",
      "Training Epoch: 19 [22144/50000]\tLoss: 3.6062\tLR: 1.843990\n",
      "Training Epoch: 19 [22272/50000]\tLoss: 3.8188\tLR: 1.844246\n",
      "Training Epoch: 19 [22400/50000]\tLoss: 3.7599\tLR: 1.844501\n",
      "Training Epoch: 19 [22528/50000]\tLoss: 3.8385\tLR: 1.844757\n",
      "Training Epoch: 19 [22656/50000]\tLoss: 4.0281\tLR: 1.845013\n",
      "Training Epoch: 19 [22784/50000]\tLoss: 4.0545\tLR: 1.845269\n",
      "Training Epoch: 19 [22912/50000]\tLoss: 3.7987\tLR: 1.845524\n",
      "Training Epoch: 19 [23040/50000]\tLoss: 3.7771\tLR: 1.845780\n",
      "Training Epoch: 19 [23168/50000]\tLoss: 3.6197\tLR: 1.846036\n",
      "Training Epoch: 19 [23296/50000]\tLoss: 3.9364\tLR: 1.846292\n",
      "Training Epoch: 19 [23424/50000]\tLoss: 3.9735\tLR: 1.846547\n",
      "Training Epoch: 19 [23552/50000]\tLoss: 3.7621\tLR: 1.846803\n",
      "Training Epoch: 19 [23680/50000]\tLoss: 4.0787\tLR: 1.847059\n",
      "Training Epoch: 19 [23808/50000]\tLoss: 3.7582\tLR: 1.847315\n",
      "Training Epoch: 19 [23936/50000]\tLoss: 3.9082\tLR: 1.847570\n",
      "Training Epoch: 19 [24064/50000]\tLoss: 3.7768\tLR: 1.847826\n",
      "Training Epoch: 19 [24192/50000]\tLoss: 3.8873\tLR: 1.848082\n",
      "Training Epoch: 19 [24320/50000]\tLoss: 3.8021\tLR: 1.848338\n",
      "Training Epoch: 19 [24448/50000]\tLoss: 3.9732\tLR: 1.848593\n",
      "Training Epoch: 19 [24576/50000]\tLoss: 3.8541\tLR: 1.848849\n",
      "Training Epoch: 19 [24704/50000]\tLoss: 3.5433\tLR: 1.849105\n",
      "Training Epoch: 19 [24832/50000]\tLoss: 3.9228\tLR: 1.849361\n",
      "Training Epoch: 19 [24960/50000]\tLoss: 3.8601\tLR: 1.849616\n",
      "Training Epoch: 19 [25088/50000]\tLoss: 3.7927\tLR: 1.849872\n",
      "Training Epoch: 19 [25216/50000]\tLoss: 3.7860\tLR: 1.850128\n",
      "Training Epoch: 19 [25344/50000]\tLoss: 3.6042\tLR: 1.850384\n",
      "Training Epoch: 19 [25472/50000]\tLoss: 3.6596\tLR: 1.850639\n",
      "Training Epoch: 19 [25600/50000]\tLoss: 3.5391\tLR: 1.850895\n",
      "Training Epoch: 19 [25728/50000]\tLoss: 3.5204\tLR: 1.851151\n",
      "Training Epoch: 19 [25856/50000]\tLoss: 3.6660\tLR: 1.851407\n",
      "Training Epoch: 19 [25984/50000]\tLoss: 3.6374\tLR: 1.851662\n",
      "Training Epoch: 19 [26112/50000]\tLoss: 3.6160\tLR: 1.851918\n",
      "Training Epoch: 19 [26240/50000]\tLoss: 3.5441\tLR: 1.852174\n",
      "Training Epoch: 19 [26368/50000]\tLoss: 3.5396\tLR: 1.852430\n",
      "Training Epoch: 19 [26496/50000]\tLoss: 3.9753\tLR: 1.852685\n",
      "Training Epoch: 19 [26624/50000]\tLoss: 3.8177\tLR: 1.852941\n",
      "Training Epoch: 19 [26752/50000]\tLoss: 3.9595\tLR: 1.853197\n",
      "Training Epoch: 19 [26880/50000]\tLoss: 3.7033\tLR: 1.853453\n",
      "Training Epoch: 19 [27008/50000]\tLoss: 3.7880\tLR: 1.853708\n",
      "Training Epoch: 19 [27136/50000]\tLoss: 3.9198\tLR: 1.853964\n",
      "Training Epoch: 19 [27264/50000]\tLoss: 3.8952\tLR: 1.854220\n",
      "Training Epoch: 19 [27392/50000]\tLoss: 3.9736\tLR: 1.854476\n",
      "Training Epoch: 19 [27520/50000]\tLoss: 4.0591\tLR: 1.854731\n",
      "Training Epoch: 19 [27648/50000]\tLoss: 3.9692\tLR: 1.854987\n",
      "Training Epoch: 19 [27776/50000]\tLoss: 3.7959\tLR: 1.855243\n",
      "Training Epoch: 19 [27904/50000]\tLoss: 4.1136\tLR: 1.855499\n",
      "Training Epoch: 19 [28032/50000]\tLoss: 3.8264\tLR: 1.855754\n",
      "Training Epoch: 19 [28160/50000]\tLoss: 3.8382\tLR: 1.856010\n",
      "Training Epoch: 19 [28288/50000]\tLoss: 3.7608\tLR: 1.856266\n",
      "Training Epoch: 19 [28416/50000]\tLoss: 3.5442\tLR: 1.856522\n",
      "Training Epoch: 19 [28544/50000]\tLoss: 3.5882\tLR: 1.856777\n",
      "Training Epoch: 19 [28672/50000]\tLoss: 3.6774\tLR: 1.857033\n",
      "Training Epoch: 19 [28800/50000]\tLoss: 3.7070\tLR: 1.857289\n",
      "Training Epoch: 19 [28928/50000]\tLoss: 3.4689\tLR: 1.857545\n",
      "Training Epoch: 19 [29056/50000]\tLoss: 3.6996\tLR: 1.857801\n",
      "Training Epoch: 19 [29184/50000]\tLoss: 3.7298\tLR: 1.858056\n",
      "Training Epoch: 19 [29312/50000]\tLoss: 3.7514\tLR: 1.858312\n",
      "Training Epoch: 19 [29440/50000]\tLoss: 3.6709\tLR: 1.858568\n",
      "Training Epoch: 19 [29568/50000]\tLoss: 3.7723\tLR: 1.858824\n",
      "Training Epoch: 19 [29696/50000]\tLoss: 4.1072\tLR: 1.859079\n",
      "Training Epoch: 19 [29824/50000]\tLoss: 3.8481\tLR: 1.859335\n",
      "Training Epoch: 19 [29952/50000]\tLoss: 3.8690\tLR: 1.859591\n",
      "Training Epoch: 19 [30080/50000]\tLoss: 3.8576\tLR: 1.859847\n",
      "Training Epoch: 19 [30208/50000]\tLoss: 3.7036\tLR: 1.860102\n",
      "Training Epoch: 19 [30336/50000]\tLoss: 3.6361\tLR: 1.860358\n",
      "Training Epoch: 19 [30464/50000]\tLoss: 3.8029\tLR: 1.860614\n",
      "Training Epoch: 19 [30592/50000]\tLoss: 3.9020\tLR: 1.860870\n",
      "Training Epoch: 19 [30720/50000]\tLoss: 3.7954\tLR: 1.861125\n",
      "Training Epoch: 19 [30848/50000]\tLoss: 3.8710\tLR: 1.861381\n",
      "Training Epoch: 19 [30976/50000]\tLoss: 3.7122\tLR: 1.861637\n",
      "Training Epoch: 19 [31104/50000]\tLoss: 3.8319\tLR: 1.861893\n",
      "Training Epoch: 19 [31232/50000]\tLoss: 3.7349\tLR: 1.862148\n",
      "Training Epoch: 19 [31360/50000]\tLoss: 3.8566\tLR: 1.862404\n",
      "Training Epoch: 19 [31488/50000]\tLoss: 3.7016\tLR: 1.862660\n",
      "Training Epoch: 19 [31616/50000]\tLoss: 3.9702\tLR: 1.862916\n",
      "Training Epoch: 19 [31744/50000]\tLoss: 4.1045\tLR: 1.863171\n",
      "Training Epoch: 19 [31872/50000]\tLoss: 3.8824\tLR: 1.863427\n",
      "Training Epoch: 19 [32000/50000]\tLoss: 3.9803\tLR: 1.863683\n",
      "Training Epoch: 19 [32128/50000]\tLoss: 3.7792\tLR: 1.863939\n",
      "Training Epoch: 19 [32256/50000]\tLoss: 3.8036\tLR: 1.864194\n",
      "Training Epoch: 19 [32384/50000]\tLoss: 3.7794\tLR: 1.864450\n",
      "Training Epoch: 19 [32512/50000]\tLoss: 3.6351\tLR: 1.864706\n",
      "Training Epoch: 19 [32640/50000]\tLoss: 3.7168\tLR: 1.864962\n",
      "Training Epoch: 19 [32768/50000]\tLoss: 3.8685\tLR: 1.865217\n",
      "Training Epoch: 19 [32896/50000]\tLoss: 3.8190\tLR: 1.865473\n",
      "Training Epoch: 19 [33024/50000]\tLoss: 3.8181\tLR: 1.865729\n",
      "Training Epoch: 19 [33152/50000]\tLoss: 3.9464\tLR: 1.865985\n",
      "Training Epoch: 19 [33280/50000]\tLoss: 3.8757\tLR: 1.866240\n",
      "Training Epoch: 19 [33408/50000]\tLoss: 3.6242\tLR: 1.866496\n",
      "Training Epoch: 19 [33536/50000]\tLoss: 3.9937\tLR: 1.866752\n",
      "Training Epoch: 19 [33664/50000]\tLoss: 3.8302\tLR: 1.867008\n",
      "Training Epoch: 19 [33792/50000]\tLoss: 3.7708\tLR: 1.867263\n",
      "Training Epoch: 19 [33920/50000]\tLoss: 3.8455\tLR: 1.867519\n",
      "Training Epoch: 19 [34048/50000]\tLoss: 3.8444\tLR: 1.867775\n",
      "Training Epoch: 19 [34176/50000]\tLoss: 3.7886\tLR: 1.868031\n",
      "Training Epoch: 19 [34304/50000]\tLoss: 3.6449\tLR: 1.868286\n",
      "Training Epoch: 19 [34432/50000]\tLoss: 4.0504\tLR: 1.868542\n",
      "Training Epoch: 19 [34560/50000]\tLoss: 3.9249\tLR: 1.868798\n",
      "Training Epoch: 19 [34688/50000]\tLoss: 3.7317\tLR: 1.869054\n",
      "Training Epoch: 19 [34816/50000]\tLoss: 3.6432\tLR: 1.869309\n",
      "Training Epoch: 19 [34944/50000]\tLoss: 3.9415\tLR: 1.869565\n",
      "Training Epoch: 19 [35072/50000]\tLoss: 3.7431\tLR: 1.869821\n",
      "Training Epoch: 19 [35200/50000]\tLoss: 3.9133\tLR: 1.870077\n",
      "Training Epoch: 19 [35328/50000]\tLoss: 3.8900\tLR: 1.870332\n",
      "Training Epoch: 19 [35456/50000]\tLoss: 4.0751\tLR: 1.870588\n",
      "Training Epoch: 19 [35584/50000]\tLoss: 3.7004\tLR: 1.870844\n",
      "Training Epoch: 19 [35712/50000]\tLoss: 3.8725\tLR: 1.871100\n",
      "Training Epoch: 19 [35840/50000]\tLoss: 3.7497\tLR: 1.871355\n",
      "Training Epoch: 19 [35968/50000]\tLoss: 3.8819\tLR: 1.871611\n",
      "Training Epoch: 19 [36096/50000]\tLoss: 3.7288\tLR: 1.871867\n",
      "Training Epoch: 19 [36224/50000]\tLoss: 3.6572\tLR: 1.872123\n",
      "Training Epoch: 19 [36352/50000]\tLoss: 3.7947\tLR: 1.872379\n",
      "Training Epoch: 19 [36480/50000]\tLoss: 3.8774\tLR: 1.872634\n",
      "Training Epoch: 19 [36608/50000]\tLoss: 3.8207\tLR: 1.872890\n",
      "Training Epoch: 19 [36736/50000]\tLoss: 3.8560\tLR: 1.873146\n",
      "Training Epoch: 19 [36864/50000]\tLoss: 3.5491\tLR: 1.873402\n",
      "Training Epoch: 19 [36992/50000]\tLoss: 3.6837\tLR: 1.873657\n",
      "Training Epoch: 19 [37120/50000]\tLoss: 3.7807\tLR: 1.873913\n",
      "Training Epoch: 19 [37248/50000]\tLoss: 3.6694\tLR: 1.874169\n",
      "Training Epoch: 19 [37376/50000]\tLoss: 3.4683\tLR: 1.874425\n",
      "Training Epoch: 19 [37504/50000]\tLoss: 3.7202\tLR: 1.874680\n",
      "Training Epoch: 19 [37632/50000]\tLoss: 3.7912\tLR: 1.874936\n",
      "Training Epoch: 19 [37760/50000]\tLoss: 3.9909\tLR: 1.875192\n",
      "Training Epoch: 19 [37888/50000]\tLoss: 3.7600\tLR: 1.875448\n",
      "Training Epoch: 19 [38016/50000]\tLoss: 3.9150\tLR: 1.875703\n",
      "Training Epoch: 19 [38144/50000]\tLoss: 3.7529\tLR: 1.875959\n",
      "Training Epoch: 19 [38272/50000]\tLoss: 3.7763\tLR: 1.876215\n",
      "Training Epoch: 19 [38400/50000]\tLoss: 3.9679\tLR: 1.876471\n",
      "Training Epoch: 19 [38528/50000]\tLoss: 3.8610\tLR: 1.876726\n",
      "Training Epoch: 19 [38656/50000]\tLoss: 3.8258\tLR: 1.876982\n",
      "Training Epoch: 19 [38784/50000]\tLoss: 3.9215\tLR: 1.877238\n",
      "Training Epoch: 19 [38912/50000]\tLoss: 3.8827\tLR: 1.877494\n",
      "Training Epoch: 19 [39040/50000]\tLoss: 3.8588\tLR: 1.877749\n",
      "Training Epoch: 19 [39168/50000]\tLoss: 3.7584\tLR: 1.878005\n",
      "Training Epoch: 19 [39296/50000]\tLoss: 3.7745\tLR: 1.878261\n",
      "Training Epoch: 19 [39424/50000]\tLoss: 4.0046\tLR: 1.878517\n",
      "Training Epoch: 19 [39552/50000]\tLoss: 4.0174\tLR: 1.878772\n",
      "Training Epoch: 19 [39680/50000]\tLoss: 3.8599\tLR: 1.879028\n",
      "Training Epoch: 19 [39808/50000]\tLoss: 3.7455\tLR: 1.879284\n",
      "Training Epoch: 19 [39936/50000]\tLoss: 3.8013\tLR: 1.879540\n",
      "Training Epoch: 19 [40064/50000]\tLoss: 3.6718\tLR: 1.879795\n",
      "Training Epoch: 19 [40192/50000]\tLoss: 3.9158\tLR: 1.880051\n",
      "Training Epoch: 19 [40320/50000]\tLoss: 4.0416\tLR: 1.880307\n",
      "Training Epoch: 19 [40448/50000]\tLoss: 3.8938\tLR: 1.880563\n",
      "Training Epoch: 19 [40576/50000]\tLoss: 3.8849\tLR: 1.880818\n",
      "Training Epoch: 19 [40704/50000]\tLoss: 3.8603\tLR: 1.881074\n",
      "Training Epoch: 19 [40832/50000]\tLoss: 3.9136\tLR: 1.881330\n",
      "Training Epoch: 19 [40960/50000]\tLoss: 4.1173\tLR: 1.881586\n",
      "Training Epoch: 19 [41088/50000]\tLoss: 3.9860\tLR: 1.881841\n",
      "Training Epoch: 19 [41216/50000]\tLoss: 3.9046\tLR: 1.882097\n",
      "Training Epoch: 19 [41344/50000]\tLoss: 3.9455\tLR: 1.882353\n",
      "Training Epoch: 19 [41472/50000]\tLoss: 3.8209\tLR: 1.882609\n",
      "Training Epoch: 19 [41600/50000]\tLoss: 3.9351\tLR: 1.882864\n",
      "Training Epoch: 19 [41728/50000]\tLoss: 3.9286\tLR: 1.883120\n",
      "Training Epoch: 19 [41856/50000]\tLoss: 3.9762\tLR: 1.883376\n",
      "Training Epoch: 19 [41984/50000]\tLoss: 3.8693\tLR: 1.883632\n",
      "Training Epoch: 19 [42112/50000]\tLoss: 3.8741\tLR: 1.883887\n",
      "Training Epoch: 19 [42240/50000]\tLoss: 3.7471\tLR: 1.884143\n",
      "Training Epoch: 19 [42368/50000]\tLoss: 3.8580\tLR: 1.884399\n",
      "Training Epoch: 19 [42496/50000]\tLoss: 3.8311\tLR: 1.884655\n",
      "Training Epoch: 19 [42624/50000]\tLoss: 3.8770\tLR: 1.884910\n",
      "Training Epoch: 19 [42752/50000]\tLoss: 3.9088\tLR: 1.885166\n",
      "Training Epoch: 19 [42880/50000]\tLoss: 3.8969\tLR: 1.885422\n",
      "Training Epoch: 19 [43008/50000]\tLoss: 3.6956\tLR: 1.885678\n",
      "Training Epoch: 19 [43136/50000]\tLoss: 3.7678\tLR: 1.885934\n",
      "Training Epoch: 19 [43264/50000]\tLoss: 3.7416\tLR: 1.886189\n",
      "Training Epoch: 19 [43392/50000]\tLoss: 3.6901\tLR: 1.886445\n",
      "Training Epoch: 19 [43520/50000]\tLoss: 3.8223\tLR: 1.886701\n",
      "Training Epoch: 19 [43648/50000]\tLoss: 3.8584\tLR: 1.886957\n",
      "Training Epoch: 19 [43776/50000]\tLoss: 4.0101\tLR: 1.887212\n",
      "Training Epoch: 19 [43904/50000]\tLoss: 3.8132\tLR: 1.887468\n",
      "Training Epoch: 19 [44032/50000]\tLoss: 3.8136\tLR: 1.887724\n",
      "Training Epoch: 19 [44160/50000]\tLoss: 3.8483\tLR: 1.887980\n",
      "Training Epoch: 19 [44288/50000]\tLoss: 3.8882\tLR: 1.888235\n",
      "Training Epoch: 19 [44416/50000]\tLoss: 3.9172\tLR: 1.888491\n",
      "Training Epoch: 19 [44544/50000]\tLoss: 3.9281\tLR: 1.888747\n",
      "Training Epoch: 19 [44672/50000]\tLoss: 3.8057\tLR: 1.889003\n",
      "Training Epoch: 19 [44800/50000]\tLoss: 3.6871\tLR: 1.889258\n",
      "Training Epoch: 19 [44928/50000]\tLoss: 3.8883\tLR: 1.889514\n",
      "Training Epoch: 19 [45056/50000]\tLoss: 3.8879\tLR: 1.889770\n",
      "Training Epoch: 19 [45184/50000]\tLoss: 3.7047\tLR: 1.890026\n",
      "Training Epoch: 19 [45312/50000]\tLoss: 3.8328\tLR: 1.890281\n",
      "Training Epoch: 19 [45440/50000]\tLoss: 4.1103\tLR: 1.890537\n",
      "Training Epoch: 19 [45568/50000]\tLoss: 3.8527\tLR: 1.890793\n",
      "Training Epoch: 19 [45696/50000]\tLoss: 3.8758\tLR: 1.891049\n",
      "Training Epoch: 19 [45824/50000]\tLoss: 3.9268\tLR: 1.891304\n",
      "Training Epoch: 19 [45952/50000]\tLoss: 3.8058\tLR: 1.891560\n",
      "Training Epoch: 19 [46080/50000]\tLoss: 4.0822\tLR: 1.891816\n",
      "Training Epoch: 19 [46208/50000]\tLoss: 3.6926\tLR: 1.892072\n",
      "Training Epoch: 19 [46336/50000]\tLoss: 3.7479\tLR: 1.892327\n",
      "Training Epoch: 19 [46464/50000]\tLoss: 3.7029\tLR: 1.892583\n",
      "Training Epoch: 19 [46592/50000]\tLoss: 4.0128\tLR: 1.892839\n",
      "Training Epoch: 19 [46720/50000]\tLoss: 3.7255\tLR: 1.893095\n",
      "Training Epoch: 19 [46848/50000]\tLoss: 3.5865\tLR: 1.893350\n",
      "Training Epoch: 19 [46976/50000]\tLoss: 3.7092\tLR: 1.893606\n",
      "Training Epoch: 19 [47104/50000]\tLoss: 3.7639\tLR: 1.893862\n",
      "Training Epoch: 19 [47232/50000]\tLoss: 3.6658\tLR: 1.894118\n",
      "Training Epoch: 19 [47360/50000]\tLoss: 3.7622\tLR: 1.894373\n",
      "Training Epoch: 19 [47488/50000]\tLoss: 3.6147\tLR: 1.894629\n",
      "Training Epoch: 19 [47616/50000]\tLoss: 3.7815\tLR: 1.894885\n",
      "Training Epoch: 19 [47744/50000]\tLoss: 3.9570\tLR: 1.895141\n",
      "Training Epoch: 19 [47872/50000]\tLoss: 3.7581\tLR: 1.895396\n",
      "Training Epoch: 19 [48000/50000]\tLoss: 4.0840\tLR: 1.895652\n",
      "Training Epoch: 19 [48128/50000]\tLoss: 3.9126\tLR: 1.895908\n",
      "Training Epoch: 19 [48256/50000]\tLoss: 4.0846\tLR: 1.896164\n",
      "Training Epoch: 19 [48384/50000]\tLoss: 4.0268\tLR: 1.896419\n",
      "Training Epoch: 19 [48512/50000]\tLoss: 3.8292\tLR: 1.896675\n",
      "Training Epoch: 19 [48640/50000]\tLoss: 3.8696\tLR: 1.896931\n",
      "Training Epoch: 19 [48768/50000]\tLoss: 3.9786\tLR: 1.897187\n",
      "Training Epoch: 19 [48896/50000]\tLoss: 4.0196\tLR: 1.897442\n",
      "Training Epoch: 19 [49024/50000]\tLoss: 3.7995\tLR: 1.897698\n",
      "Training Epoch: 19 [49152/50000]\tLoss: 3.9732\tLR: 1.897954\n",
      "Training Epoch: 19 [49280/50000]\tLoss: 4.0036\tLR: 1.898210\n",
      "Training Epoch: 19 [49408/50000]\tLoss: 3.6883\tLR: 1.898465\n",
      "Training Epoch: 19 [49536/50000]\tLoss: 3.8046\tLR: 1.898721\n",
      "Training Epoch: 19 [49664/50000]\tLoss: 4.0137\tLR: 1.898977\n",
      "Training Epoch: 19 [49792/50000]\tLoss: 3.8346\tLR: 1.899233\n",
      "Training Epoch: 19 [49920/50000]\tLoss: 3.7597\tLR: 1.899488\n",
      "Training Epoch: 19 [50000/50000]\tLoss: 3.7940\tLR: 1.899744\n",
      "epoch 19 training time consumed: 488.98s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   26638 GB |   26637 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   26556 GB |   26556 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      81 GB |      81 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   26638 GB |   26637 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   26556 GB |   26556 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      81 GB |      81 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   26260 GB |   26260 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   26179 GB |   26178 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |      81 GB |      81 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    2824 K  |    2824 K  |\n",
      "|       from large pool |      24    |      65    |    1204 K  |    1204 K  |\n",
      "|       from small pool |     231    |     274    |    1620 K  |    1620 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    2824 K  |    2824 K  |\n",
      "|       from large pool |      24    |      65    |    1204 K  |    1204 K  |\n",
      "|       from small pool |     231    |     274    |    1620 K  |    1620 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      45    |    1636 K  |    1636 K  |\n",
      "|       from large pool |      10    |      23    |     578 K  |     578 K  |\n",
      "|       from small pool |      26    |      35    |    1057 K  |    1057 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 19, Average loss: 0.0377, Accuracy: 0.0556, Time consumed:31.02s\n",
      "\n",
      "Training Epoch: 20 [128/50000]\tLoss: 3.7598\tLR: 0.100000\n",
      "Training Epoch: 20 [256/50000]\tLoss: 3.9887\tLR: 1.900256\n",
      "Training Epoch: 20 [384/50000]\tLoss: 3.9018\tLR: 1.900512\n",
      "Training Epoch: 20 [512/50000]\tLoss: 3.9472\tLR: 1.900767\n",
      "Training Epoch: 20 [640/50000]\tLoss: 3.8688\tLR: 1.901023\n",
      "Training Epoch: 20 [768/50000]\tLoss: 3.9384\tLR: 1.901279\n",
      "Training Epoch: 20 [896/50000]\tLoss: 3.7883\tLR: 1.901535\n",
      "Training Epoch: 20 [1024/50000]\tLoss: 3.7521\tLR: 1.901790\n",
      "Training Epoch: 20 [1152/50000]\tLoss: 4.0366\tLR: 1.902046\n",
      "Training Epoch: 20 [1280/50000]\tLoss: 3.9376\tLR: 1.902302\n",
      "Training Epoch: 20 [1408/50000]\tLoss: 3.9271\tLR: 1.902558\n",
      "Training Epoch: 20 [1536/50000]\tLoss: 3.7901\tLR: 1.902813\n",
      "Training Epoch: 20 [1664/50000]\tLoss: 3.7924\tLR: 1.903069\n",
      "Training Epoch: 20 [1792/50000]\tLoss: 3.8907\tLR: 1.903325\n",
      "Training Epoch: 20 [1920/50000]\tLoss: 3.8527\tLR: 1.903581\n",
      "Training Epoch: 20 [2048/50000]\tLoss: 3.8717\tLR: 1.903836\n",
      "Training Epoch: 20 [2176/50000]\tLoss: 3.7216\tLR: 1.904092\n",
      "Training Epoch: 20 [2304/50000]\tLoss: 4.0245\tLR: 1.904348\n",
      "Training Epoch: 20 [2432/50000]\tLoss: 3.8307\tLR: 1.904604\n",
      "Training Epoch: 20 [2560/50000]\tLoss: 3.9349\tLR: 1.904859\n",
      "Training Epoch: 20 [2688/50000]\tLoss: 3.7924\tLR: 1.905115\n",
      "Training Epoch: 20 [2816/50000]\tLoss: 4.0469\tLR: 1.905371\n",
      "Training Epoch: 20 [2944/50000]\tLoss: 3.8167\tLR: 1.905627\n",
      "Training Epoch: 20 [3072/50000]\tLoss: 3.7825\tLR: 1.905882\n",
      "Training Epoch: 20 [3200/50000]\tLoss: 3.9688\tLR: 1.906138\n",
      "Training Epoch: 20 [3328/50000]\tLoss: 3.9242\tLR: 1.906394\n",
      "Training Epoch: 20 [3456/50000]\tLoss: 3.8155\tLR: 1.906650\n",
      "Training Epoch: 20 [3584/50000]\tLoss: 3.7798\tLR: 1.906905\n",
      "Training Epoch: 20 [3712/50000]\tLoss: 4.1319\tLR: 1.907161\n",
      "Training Epoch: 20 [3840/50000]\tLoss: 4.0560\tLR: 1.907417\n",
      "Training Epoch: 20 [3968/50000]\tLoss: 3.8959\tLR: 1.907673\n",
      "Training Epoch: 20 [4096/50000]\tLoss: 3.9277\tLR: 1.907928\n",
      "Training Epoch: 20 [4224/50000]\tLoss: 3.9024\tLR: 1.908184\n",
      "Training Epoch: 20 [4352/50000]\tLoss: 3.9675\tLR: 1.908440\n",
      "Training Epoch: 20 [4480/50000]\tLoss: 3.9939\tLR: 1.908696\n",
      "Training Epoch: 20 [4608/50000]\tLoss: 4.0102\tLR: 1.908951\n",
      "Training Epoch: 20 [4736/50000]\tLoss: 4.0834\tLR: 1.909207\n",
      "Training Epoch: 20 [4864/50000]\tLoss: 3.8830\tLR: 1.909463\n",
      "Training Epoch: 20 [4992/50000]\tLoss: 3.9857\tLR: 1.909719\n",
      "Training Epoch: 20 [5120/50000]\tLoss: 4.0097\tLR: 1.909974\n",
      "Training Epoch: 20 [5248/50000]\tLoss: 3.9713\tLR: 1.910230\n",
      "Training Epoch: 20 [5376/50000]\tLoss: 3.8212\tLR: 1.910486\n",
      "Training Epoch: 20 [5504/50000]\tLoss: 3.6709\tLR: 1.910742\n",
      "Training Epoch: 20 [5632/50000]\tLoss: 3.8887\tLR: 1.910997\n",
      "Training Epoch: 20 [5760/50000]\tLoss: 3.7811\tLR: 1.911253\n",
      "Training Epoch: 20 [5888/50000]\tLoss: 4.0593\tLR: 1.911509\n",
      "Training Epoch: 20 [6016/50000]\tLoss: 3.8265\tLR: 1.911765\n",
      "Training Epoch: 20 [6144/50000]\tLoss: 3.7639\tLR: 1.912020\n",
      "Training Epoch: 20 [6272/50000]\tLoss: 3.7280\tLR: 1.912276\n",
      "Training Epoch: 20 [6400/50000]\tLoss: 3.5466\tLR: 1.912532\n",
      "Training Epoch: 20 [6528/50000]\tLoss: 3.7276\tLR: 1.912788\n",
      "Training Epoch: 20 [6656/50000]\tLoss: 3.9697\tLR: 1.913043\n",
      "Training Epoch: 20 [6784/50000]\tLoss: 3.7621\tLR: 1.913299\n",
      "Training Epoch: 20 [6912/50000]\tLoss: 3.9176\tLR: 1.913555\n",
      "Training Epoch: 20 [7040/50000]\tLoss: 3.8557\tLR: 1.913811\n",
      "Training Epoch: 20 [7168/50000]\tLoss: 3.9215\tLR: 1.914066\n",
      "Training Epoch: 20 [7296/50000]\tLoss: 3.8323\tLR: 1.914322\n",
      "Training Epoch: 20 [7424/50000]\tLoss: 3.9060\tLR: 1.914578\n",
      "Training Epoch: 20 [7552/50000]\tLoss: 3.8474\tLR: 1.914834\n",
      "Training Epoch: 20 [7680/50000]\tLoss: 4.1047\tLR: 1.915090\n",
      "Training Epoch: 20 [7808/50000]\tLoss: 3.9615\tLR: 1.915345\n",
      "Training Epoch: 20 [7936/50000]\tLoss: 3.7762\tLR: 1.915601\n",
      "Training Epoch: 20 [8064/50000]\tLoss: 3.8182\tLR: 1.915857\n",
      "Training Epoch: 20 [8192/50000]\tLoss: 3.7620\tLR: 1.916113\n",
      "Training Epoch: 20 [8320/50000]\tLoss: 3.8337\tLR: 1.916368\n",
      "Training Epoch: 20 [8448/50000]\tLoss: 3.8423\tLR: 1.916624\n",
      "Training Epoch: 20 [8576/50000]\tLoss: 3.9330\tLR: 1.916880\n",
      "Training Epoch: 20 [8704/50000]\tLoss: 3.7225\tLR: 1.917136\n",
      "Training Epoch: 20 [8832/50000]\tLoss: 3.9171\tLR: 1.917391\n",
      "Training Epoch: 20 [8960/50000]\tLoss: 3.9572\tLR: 1.917647\n",
      "Training Epoch: 20 [9088/50000]\tLoss: 3.8886\tLR: 1.917903\n",
      "Training Epoch: 20 [9216/50000]\tLoss: 3.9238\tLR: 1.918159\n",
      "Training Epoch: 20 [9344/50000]\tLoss: 3.7702\tLR: 1.918414\n",
      "Training Epoch: 20 [9472/50000]\tLoss: 3.7287\tLR: 1.918670\n",
      "Training Epoch: 20 [9600/50000]\tLoss: 3.7083\tLR: 1.918926\n",
      "Training Epoch: 20 [9728/50000]\tLoss: 3.9795\tLR: 1.919182\n",
      "Training Epoch: 20 [9856/50000]\tLoss: 3.8654\tLR: 1.919437\n",
      "Training Epoch: 20 [9984/50000]\tLoss: 3.8261\tLR: 1.919693\n",
      "Training Epoch: 20 [10112/50000]\tLoss: 3.7197\tLR: 1.919949\n",
      "Training Epoch: 20 [10240/50000]\tLoss: 3.8470\tLR: 1.920205\n",
      "Training Epoch: 20 [10368/50000]\tLoss: 3.8682\tLR: 1.920460\n",
      "Training Epoch: 20 [10496/50000]\tLoss: 4.0642\tLR: 1.920716\n",
      "Training Epoch: 20 [10624/50000]\tLoss: 3.8768\tLR: 1.920972\n",
      "Training Epoch: 20 [10752/50000]\tLoss: 3.7819\tLR: 1.921228\n",
      "Training Epoch: 20 [10880/50000]\tLoss: 3.8203\tLR: 1.921483\n",
      "Training Epoch: 20 [11008/50000]\tLoss: 3.8824\tLR: 1.921739\n",
      "Training Epoch: 20 [11136/50000]\tLoss: 3.8075\tLR: 1.921995\n",
      "Training Epoch: 20 [11264/50000]\tLoss: 4.0335\tLR: 1.922251\n",
      "Training Epoch: 20 [11392/50000]\tLoss: 3.8968\tLR: 1.922506\n",
      "Training Epoch: 20 [11520/50000]\tLoss: 3.9756\tLR: 1.922762\n",
      "Training Epoch: 20 [11648/50000]\tLoss: 3.9984\tLR: 1.923018\n",
      "Training Epoch: 20 [11776/50000]\tLoss: 3.9066\tLR: 1.923274\n",
      "Training Epoch: 20 [11904/50000]\tLoss: 3.8216\tLR: 1.923529\n",
      "Training Epoch: 20 [12032/50000]\tLoss: 3.8562\tLR: 1.923785\n",
      "Training Epoch: 20 [12160/50000]\tLoss: 3.8950\tLR: 1.924041\n",
      "Training Epoch: 20 [12288/50000]\tLoss: 3.7824\tLR: 1.924297\n",
      "Training Epoch: 20 [12416/50000]\tLoss: 3.8163\tLR: 1.924552\n",
      "Training Epoch: 20 [12544/50000]\tLoss: 3.8101\tLR: 1.924808\n",
      "Training Epoch: 20 [12672/50000]\tLoss: 3.9245\tLR: 1.925064\n",
      "Training Epoch: 20 [12800/50000]\tLoss: 3.7638\tLR: 1.925320\n",
      "Training Epoch: 20 [12928/50000]\tLoss: 3.9461\tLR: 1.925575\n",
      "Training Epoch: 20 [13056/50000]\tLoss: 3.8436\tLR: 1.925831\n",
      "Training Epoch: 20 [13184/50000]\tLoss: 3.7982\tLR: 1.926087\n",
      "Training Epoch: 20 [13312/50000]\tLoss: 3.6677\tLR: 1.926343\n",
      "Training Epoch: 20 [13440/50000]\tLoss: 4.0590\tLR: 1.926598\n",
      "Training Epoch: 20 [13568/50000]\tLoss: 3.7159\tLR: 1.926854\n",
      "Training Epoch: 20 [13696/50000]\tLoss: 3.7949\tLR: 1.927110\n",
      "Training Epoch: 20 [13824/50000]\tLoss: 3.8706\tLR: 1.927366\n",
      "Training Epoch: 20 [13952/50000]\tLoss: 4.0664\tLR: 1.927621\n",
      "Training Epoch: 20 [14080/50000]\tLoss: 3.7357\tLR: 1.927877\n",
      "Training Epoch: 20 [14208/50000]\tLoss: 3.8865\tLR: 1.928133\n",
      "Training Epoch: 20 [14336/50000]\tLoss: 3.9560\tLR: 1.928389\n",
      "Training Epoch: 20 [14464/50000]\tLoss: 3.7763\tLR: 1.928645\n",
      "Training Epoch: 20 [14592/50000]\tLoss: 3.7109\tLR: 1.928900\n",
      "Training Epoch: 20 [14720/50000]\tLoss: 3.9143\tLR: 1.929156\n",
      "Training Epoch: 20 [14848/50000]\tLoss: 3.7135\tLR: 1.929412\n",
      "Training Epoch: 20 [14976/50000]\tLoss: 4.0457\tLR: 1.929668\n",
      "Training Epoch: 20 [15104/50000]\tLoss: 4.0508\tLR: 1.929923\n",
      "Training Epoch: 20 [15232/50000]\tLoss: 3.7426\tLR: 1.930179\n",
      "Training Epoch: 20 [15360/50000]\tLoss: 3.8097\tLR: 1.930435\n",
      "Training Epoch: 20 [15488/50000]\tLoss: 3.7890\tLR: 1.930691\n",
      "Training Epoch: 20 [15616/50000]\tLoss: 3.9939\tLR: 1.930946\n",
      "Training Epoch: 20 [15744/50000]\tLoss: 3.8347\tLR: 1.931202\n",
      "Training Epoch: 20 [15872/50000]\tLoss: 3.9108\tLR: 1.931458\n",
      "Training Epoch: 20 [16000/50000]\tLoss: 3.6794\tLR: 1.931714\n",
      "Training Epoch: 20 [16128/50000]\tLoss: 3.8960\tLR: 1.931969\n",
      "Training Epoch: 20 [16256/50000]\tLoss: 4.0106\tLR: 1.932225\n",
      "Training Epoch: 20 [16384/50000]\tLoss: 4.0486\tLR: 1.932481\n",
      "Training Epoch: 20 [16512/50000]\tLoss: 3.7704\tLR: 1.932737\n",
      "Training Epoch: 20 [16640/50000]\tLoss: 3.6324\tLR: 1.932992\n",
      "Training Epoch: 20 [16768/50000]\tLoss: 3.7636\tLR: 1.933248\n",
      "Training Epoch: 20 [16896/50000]\tLoss: 3.8704\tLR: 1.933504\n",
      "Training Epoch: 20 [17024/50000]\tLoss: 4.0142\tLR: 1.933760\n",
      "Training Epoch: 20 [17152/50000]\tLoss: 3.9621\tLR: 1.934015\n",
      "Training Epoch: 20 [17280/50000]\tLoss: 4.0820\tLR: 1.934271\n",
      "Training Epoch: 20 [17408/50000]\tLoss: 3.9726\tLR: 1.934527\n",
      "Training Epoch: 20 [17536/50000]\tLoss: 3.8917\tLR: 1.934783\n",
      "Training Epoch: 20 [17664/50000]\tLoss: 3.9036\tLR: 1.935038\n",
      "Training Epoch: 20 [17792/50000]\tLoss: 4.0075\tLR: 1.935294\n",
      "Training Epoch: 20 [17920/50000]\tLoss: 3.8881\tLR: 1.935550\n",
      "Training Epoch: 20 [18048/50000]\tLoss: 3.8365\tLR: 1.935806\n",
      "Training Epoch: 20 [18176/50000]\tLoss: 3.9155\tLR: 1.936061\n",
      "Training Epoch: 20 [18304/50000]\tLoss: 3.9113\tLR: 1.936317\n",
      "Training Epoch: 20 [18432/50000]\tLoss: 3.7191\tLR: 1.936573\n",
      "Training Epoch: 20 [18560/50000]\tLoss: 4.0340\tLR: 1.936829\n",
      "Training Epoch: 20 [18688/50000]\tLoss: 3.7541\tLR: 1.937084\n",
      "Training Epoch: 20 [18816/50000]\tLoss: 3.7560\tLR: 1.937340\n",
      "Training Epoch: 20 [18944/50000]\tLoss: 3.6701\tLR: 1.937596\n",
      "Training Epoch: 20 [19072/50000]\tLoss: 3.9537\tLR: 1.937852\n",
      "Training Epoch: 20 [19200/50000]\tLoss: 3.9845\tLR: 1.938107\n",
      "Training Epoch: 20 [19328/50000]\tLoss: 3.9499\tLR: 1.938363\n",
      "Training Epoch: 20 [19456/50000]\tLoss: 3.9334\tLR: 1.938619\n",
      "Training Epoch: 20 [19584/50000]\tLoss: 3.8190\tLR: 1.938875\n",
      "Training Epoch: 20 [19712/50000]\tLoss: 3.8484\tLR: 1.939130\n",
      "Training Epoch: 20 [19840/50000]\tLoss: 3.8047\tLR: 1.939386\n",
      "Training Epoch: 20 [19968/50000]\tLoss: 4.0441\tLR: 1.939642\n",
      "Training Epoch: 20 [20096/50000]\tLoss: 3.9405\tLR: 1.939898\n",
      "Training Epoch: 20 [20224/50000]\tLoss: 4.0200\tLR: 1.940153\n",
      "Training Epoch: 20 [20352/50000]\tLoss: 4.0408\tLR: 1.940409\n",
      "Training Epoch: 20 [20480/50000]\tLoss: 3.9504\tLR: 1.940665\n",
      "Training Epoch: 20 [20608/50000]\tLoss: 4.0219\tLR: 1.940921\n",
      "Training Epoch: 20 [20736/50000]\tLoss: 3.9494\tLR: 1.941176\n",
      "Training Epoch: 20 [20864/50000]\tLoss: 4.0593\tLR: 1.941432\n",
      "Training Epoch: 20 [20992/50000]\tLoss: 3.9386\tLR: 1.941688\n",
      "Training Epoch: 20 [21120/50000]\tLoss: 4.2016\tLR: 1.941944\n",
      "Training Epoch: 20 [21248/50000]\tLoss: 3.9044\tLR: 1.942199\n",
      "Training Epoch: 20 [21376/50000]\tLoss: 4.0816\tLR: 1.942455\n",
      "Training Epoch: 20 [21504/50000]\tLoss: 4.0976\tLR: 1.942711\n",
      "Training Epoch: 20 [21632/50000]\tLoss: 3.7977\tLR: 1.942967\n",
      "Training Epoch: 20 [21760/50000]\tLoss: 3.7949\tLR: 1.943223\n",
      "Training Epoch: 20 [21888/50000]\tLoss: 4.0520\tLR: 1.943478\n",
      "Training Epoch: 20 [22016/50000]\tLoss: 4.0115\tLR: 1.943734\n",
      "Training Epoch: 20 [22144/50000]\tLoss: 3.9892\tLR: 1.943990\n",
      "Training Epoch: 20 [22272/50000]\tLoss: 3.9317\tLR: 1.944246\n",
      "Training Epoch: 20 [22400/50000]\tLoss: 4.0267\tLR: 1.944501\n",
      "Training Epoch: 20 [22528/50000]\tLoss: 3.7960\tLR: 1.944757\n",
      "Training Epoch: 20 [22656/50000]\tLoss: 3.8163\tLR: 1.945013\n",
      "Training Epoch: 20 [22784/50000]\tLoss: 3.8111\tLR: 1.945269\n",
      "Training Epoch: 20 [22912/50000]\tLoss: 3.9033\tLR: 1.945524\n",
      "Training Epoch: 20 [23040/50000]\tLoss: 3.7638\tLR: 1.945780\n",
      "Training Epoch: 20 [23168/50000]\tLoss: 3.9497\tLR: 1.946036\n",
      "Training Epoch: 20 [23296/50000]\tLoss: 4.0021\tLR: 1.946292\n",
      "Training Epoch: 20 [23424/50000]\tLoss: 3.9796\tLR: 1.946547\n",
      "Training Epoch: 20 [23552/50000]\tLoss: 4.0764\tLR: 1.946803\n",
      "Training Epoch: 20 [23680/50000]\tLoss: 3.8150\tLR: 1.947059\n",
      "Training Epoch: 20 [23808/50000]\tLoss: 4.0946\tLR: 1.947315\n",
      "Training Epoch: 20 [23936/50000]\tLoss: 4.0236\tLR: 1.947570\n",
      "Training Epoch: 20 [24064/50000]\tLoss: 4.0223\tLR: 1.947826\n",
      "Training Epoch: 20 [24192/50000]\tLoss: 4.0876\tLR: 1.948082\n",
      "Training Epoch: 20 [24320/50000]\tLoss: 3.8503\tLR: 1.948338\n",
      "Training Epoch: 20 [24448/50000]\tLoss: 4.0715\tLR: 1.948593\n",
      "Training Epoch: 20 [24576/50000]\tLoss: 3.9396\tLR: 1.948849\n",
      "Training Epoch: 20 [24704/50000]\tLoss: 3.9604\tLR: 1.949105\n",
      "Training Epoch: 20 [24832/50000]\tLoss: 3.8075\tLR: 1.949361\n",
      "Training Epoch: 20 [24960/50000]\tLoss: 3.8068\tLR: 1.949616\n",
      "Training Epoch: 20 [25088/50000]\tLoss: 4.0091\tLR: 1.949872\n",
      "Training Epoch: 20 [25216/50000]\tLoss: 3.8705\tLR: 1.950128\n",
      "Training Epoch: 20 [25344/50000]\tLoss: 3.7736\tLR: 1.950384\n",
      "Training Epoch: 20 [25472/50000]\tLoss: 3.9133\tLR: 1.950639\n",
      "Training Epoch: 20 [25600/50000]\tLoss: 3.8487\tLR: 1.950895\n",
      "Training Epoch: 20 [25728/50000]\tLoss: 4.1028\tLR: 1.951151\n",
      "Training Epoch: 20 [25856/50000]\tLoss: 3.8636\tLR: 1.951407\n",
      "Training Epoch: 20 [25984/50000]\tLoss: 3.9959\tLR: 1.951662\n",
      "Training Epoch: 20 [26112/50000]\tLoss: 3.9266\tLR: 1.951918\n",
      "Training Epoch: 20 [26240/50000]\tLoss: 3.8810\tLR: 1.952174\n",
      "Training Epoch: 20 [26368/50000]\tLoss: 4.0506\tLR: 1.952430\n",
      "Training Epoch: 20 [26496/50000]\tLoss: 3.9386\tLR: 1.952685\n",
      "Training Epoch: 20 [26624/50000]\tLoss: 4.0392\tLR: 1.952941\n",
      "Training Epoch: 20 [26752/50000]\tLoss: 4.0088\tLR: 1.953197\n",
      "Training Epoch: 20 [26880/50000]\tLoss: 4.0163\tLR: 1.953453\n",
      "Training Epoch: 20 [27008/50000]\tLoss: 4.1418\tLR: 1.953708\n",
      "Training Epoch: 20 [27136/50000]\tLoss: 4.0933\tLR: 1.953964\n",
      "Training Epoch: 20 [27264/50000]\tLoss: 4.0669\tLR: 1.954220\n",
      "Training Epoch: 20 [27392/50000]\tLoss: 4.0329\tLR: 1.954476\n",
      "Training Epoch: 20 [27520/50000]\tLoss: 4.0063\tLR: 1.954731\n",
      "Training Epoch: 20 [27648/50000]\tLoss: 3.9018\tLR: 1.954987\n",
      "Training Epoch: 20 [27776/50000]\tLoss: 4.0821\tLR: 1.955243\n",
      "Training Epoch: 20 [27904/50000]\tLoss: 3.9854\tLR: 1.955499\n",
      "Training Epoch: 20 [28032/50000]\tLoss: 3.8203\tLR: 1.955754\n",
      "Training Epoch: 20 [28160/50000]\tLoss: 4.0024\tLR: 1.956010\n",
      "Training Epoch: 20 [28288/50000]\tLoss: 3.9129\tLR: 1.956266\n",
      "Training Epoch: 20 [28416/50000]\tLoss: 3.9235\tLR: 1.956522\n",
      "Training Epoch: 20 [28544/50000]\tLoss: 4.0582\tLR: 1.956777\n",
      "Training Epoch: 20 [28672/50000]\tLoss: 3.9421\tLR: 1.957033\n",
      "Training Epoch: 20 [28800/50000]\tLoss: 3.9702\tLR: 1.957289\n",
      "Training Epoch: 20 [28928/50000]\tLoss: 3.8867\tLR: 1.957545\n",
      "Training Epoch: 20 [29056/50000]\tLoss: 3.8400\tLR: 1.957801\n",
      "Training Epoch: 20 [29184/50000]\tLoss: 3.8833\tLR: 1.958056\n",
      "Training Epoch: 20 [29312/50000]\tLoss: 3.8639\tLR: 1.958312\n",
      "Training Epoch: 20 [29440/50000]\tLoss: 4.3064\tLR: 1.958568\n",
      "Training Epoch: 20 [29568/50000]\tLoss: 4.0958\tLR: 1.958824\n",
      "Training Epoch: 20 [29696/50000]\tLoss: 4.2137\tLR: 1.959079\n",
      "Training Epoch: 20 [29824/50000]\tLoss: 3.9480\tLR: 1.959335\n",
      "Training Epoch: 20 [29952/50000]\tLoss: 4.1060\tLR: 1.959591\n",
      "Training Epoch: 20 [30080/50000]\tLoss: 3.9679\tLR: 1.959847\n",
      "Training Epoch: 20 [30208/50000]\tLoss: 3.9303\tLR: 1.960102\n",
      "Training Epoch: 20 [30336/50000]\tLoss: 4.1570\tLR: 1.960358\n",
      "Training Epoch: 20 [30464/50000]\tLoss: 4.0408\tLR: 1.960614\n",
      "Training Epoch: 20 [30592/50000]\tLoss: 4.1808\tLR: 1.960870\n",
      "Training Epoch: 20 [30720/50000]\tLoss: 3.9359\tLR: 1.961125\n",
      "Training Epoch: 20 [30848/50000]\tLoss: 4.0138\tLR: 1.961381\n",
      "Training Epoch: 20 [30976/50000]\tLoss: 3.8997\tLR: 1.961637\n",
      "Training Epoch: 20 [31104/50000]\tLoss: 3.7430\tLR: 1.961893\n",
      "Training Epoch: 20 [31232/50000]\tLoss: 3.8356\tLR: 1.962148\n",
      "Training Epoch: 20 [31360/50000]\tLoss: 3.8934\tLR: 1.962404\n",
      "Training Epoch: 20 [31488/50000]\tLoss: 4.1484\tLR: 1.962660\n",
      "Training Epoch: 20 [31616/50000]\tLoss: 3.9722\tLR: 1.962916\n",
      "Training Epoch: 20 [31744/50000]\tLoss: 3.8648\tLR: 1.963171\n",
      "Training Epoch: 20 [31872/50000]\tLoss: 4.1182\tLR: 1.963427\n",
      "Training Epoch: 20 [32000/50000]\tLoss: 4.1208\tLR: 1.963683\n",
      "Training Epoch: 20 [32128/50000]\tLoss: 3.9120\tLR: 1.963939\n",
      "Training Epoch: 20 [32256/50000]\tLoss: 3.9551\tLR: 1.964194\n",
      "Training Epoch: 20 [32384/50000]\tLoss: 3.7838\tLR: 1.964450\n",
      "Training Epoch: 20 [32512/50000]\tLoss: 4.1224\tLR: 1.964706\n",
      "Training Epoch: 20 [32640/50000]\tLoss: 3.8216\tLR: 1.964962\n",
      "Training Epoch: 20 [32768/50000]\tLoss: 3.8917\tLR: 1.965217\n",
      "Training Epoch: 20 [32896/50000]\tLoss: 3.7699\tLR: 1.965473\n",
      "Training Epoch: 20 [33024/50000]\tLoss: 4.0398\tLR: 1.965729\n",
      "Training Epoch: 20 [33152/50000]\tLoss: 3.7858\tLR: 1.965985\n",
      "Training Epoch: 20 [33280/50000]\tLoss: 3.7598\tLR: 1.966240\n",
      "Training Epoch: 20 [33408/50000]\tLoss: 3.8453\tLR: 1.966496\n",
      "Training Epoch: 20 [33536/50000]\tLoss: 4.0334\tLR: 1.966752\n",
      "Training Epoch: 20 [33664/50000]\tLoss: 3.9302\tLR: 1.967008\n",
      "Training Epoch: 20 [33792/50000]\tLoss: 3.9909\tLR: 1.967263\n",
      "Training Epoch: 20 [33920/50000]\tLoss: 3.7780\tLR: 1.967519\n",
      "Training Epoch: 20 [34048/50000]\tLoss: 3.9242\tLR: 1.967775\n",
      "Training Epoch: 20 [34176/50000]\tLoss: 3.9061\tLR: 1.968031\n",
      "Training Epoch: 20 [34304/50000]\tLoss: 3.8263\tLR: 1.968286\n",
      "Training Epoch: 20 [34432/50000]\tLoss: 3.9091\tLR: 1.968542\n",
      "Training Epoch: 20 [34560/50000]\tLoss: 3.7843\tLR: 1.968798\n",
      "Training Epoch: 20 [34688/50000]\tLoss: 3.8978\tLR: 1.969054\n",
      "Training Epoch: 20 [34816/50000]\tLoss: 3.9893\tLR: 1.969309\n",
      "Training Epoch: 20 [34944/50000]\tLoss: 3.8221\tLR: 1.969565\n",
      "Training Epoch: 20 [35072/50000]\tLoss: 3.9304\tLR: 1.969821\n",
      "Training Epoch: 20 [35200/50000]\tLoss: 3.7861\tLR: 1.970077\n",
      "Training Epoch: 20 [35328/50000]\tLoss: 3.7184\tLR: 1.970332\n",
      "Training Epoch: 20 [35456/50000]\tLoss: 3.8913\tLR: 1.970588\n",
      "Training Epoch: 20 [35584/50000]\tLoss: 3.9567\tLR: 1.970844\n",
      "Training Epoch: 20 [35712/50000]\tLoss: 4.1317\tLR: 1.971100\n",
      "Training Epoch: 20 [35840/50000]\tLoss: 3.9214\tLR: 1.971355\n",
      "Training Epoch: 20 [35968/50000]\tLoss: 4.0860\tLR: 1.971611\n",
      "Training Epoch: 20 [36096/50000]\tLoss: 4.1971\tLR: 1.971867\n",
      "Training Epoch: 20 [36224/50000]\tLoss: 3.9793\tLR: 1.972123\n",
      "Training Epoch: 20 [36352/50000]\tLoss: 4.2453\tLR: 1.972379\n",
      "Training Epoch: 20 [36480/50000]\tLoss: 4.0604\tLR: 1.972634\n",
      "Training Epoch: 20 [36608/50000]\tLoss: 4.1034\tLR: 1.972890\n",
      "Training Epoch: 20 [36736/50000]\tLoss: 4.0744\tLR: 1.973146\n",
      "Training Epoch: 20 [36864/50000]\tLoss: 3.8543\tLR: 1.973402\n",
      "Training Epoch: 20 [36992/50000]\tLoss: 3.9206\tLR: 1.973657\n",
      "Training Epoch: 20 [37120/50000]\tLoss: 4.1234\tLR: 1.973913\n",
      "Training Epoch: 20 [37248/50000]\tLoss: 3.9502\tLR: 1.974169\n",
      "Training Epoch: 20 [37376/50000]\tLoss: 3.9560\tLR: 1.974425\n",
      "Training Epoch: 20 [37504/50000]\tLoss: 3.9599\tLR: 1.974680\n",
      "Training Epoch: 20 [37632/50000]\tLoss: 4.0275\tLR: 1.974936\n",
      "Training Epoch: 20 [37760/50000]\tLoss: 3.9753\tLR: 1.975192\n",
      "Training Epoch: 20 [37888/50000]\tLoss: 3.9311\tLR: 1.975448\n",
      "Training Epoch: 20 [38016/50000]\tLoss: 3.9686\tLR: 1.975703\n",
      "Training Epoch: 20 [38144/50000]\tLoss: 3.8490\tLR: 1.975959\n",
      "Training Epoch: 20 [38272/50000]\tLoss: 3.9468\tLR: 1.976215\n",
      "Training Epoch: 20 [38400/50000]\tLoss: 4.1063\tLR: 1.976471\n",
      "Training Epoch: 20 [38528/50000]\tLoss: 3.9845\tLR: 1.976726\n",
      "Training Epoch: 20 [38656/50000]\tLoss: 4.0356\tLR: 1.976982\n",
      "Training Epoch: 20 [38784/50000]\tLoss: 3.9718\tLR: 1.977238\n",
      "Training Epoch: 20 [38912/50000]\tLoss: 3.9732\tLR: 1.977494\n",
      "Training Epoch: 20 [39040/50000]\tLoss: 3.8290\tLR: 1.977749\n",
      "Training Epoch: 20 [39168/50000]\tLoss: 3.8607\tLR: 1.978005\n",
      "Training Epoch: 20 [39296/50000]\tLoss: 3.8444\tLR: 1.978261\n",
      "Training Epoch: 20 [39424/50000]\tLoss: 3.9015\tLR: 1.978517\n",
      "Training Epoch: 20 [39552/50000]\tLoss: 3.9319\tLR: 1.978772\n",
      "Training Epoch: 20 [39680/50000]\tLoss: 3.9422\tLR: 1.979028\n",
      "Training Epoch: 20 [39808/50000]\tLoss: 3.9046\tLR: 1.979284\n",
      "Training Epoch: 20 [39936/50000]\tLoss: 4.0373\tLR: 1.979540\n",
      "Training Epoch: 20 [40064/50000]\tLoss: 3.9834\tLR: 1.979795\n",
      "Training Epoch: 20 [40192/50000]\tLoss: 3.9605\tLR: 1.980051\n",
      "Training Epoch: 20 [40320/50000]\tLoss: 3.8504\tLR: 1.980307\n",
      "Training Epoch: 20 [40448/50000]\tLoss: 3.9264\tLR: 1.980563\n",
      "Training Epoch: 20 [40576/50000]\tLoss: 3.9536\tLR: 1.980818\n",
      "Training Epoch: 20 [40704/50000]\tLoss: 3.7570\tLR: 1.981074\n",
      "Training Epoch: 20 [40832/50000]\tLoss: 3.8964\tLR: 1.981330\n",
      "Training Epoch: 20 [40960/50000]\tLoss: 3.8022\tLR: 1.981586\n",
      "Training Epoch: 20 [41088/50000]\tLoss: 3.8764\tLR: 1.981841\n",
      "Training Epoch: 20 [41216/50000]\tLoss: 3.8100\tLR: 1.982097\n",
      "Training Epoch: 20 [41344/50000]\tLoss: 4.1493\tLR: 1.982353\n",
      "Training Epoch: 20 [41472/50000]\tLoss: 4.0216\tLR: 1.982609\n",
      "Training Epoch: 20 [41600/50000]\tLoss: 4.1193\tLR: 1.982864\n",
      "Training Epoch: 20 [41728/50000]\tLoss: 3.9017\tLR: 1.983120\n",
      "Training Epoch: 20 [41856/50000]\tLoss: 3.8038\tLR: 1.983376\n",
      "Training Epoch: 20 [41984/50000]\tLoss: 4.1735\tLR: 1.983632\n",
      "Training Epoch: 20 [42112/50000]\tLoss: 4.0699\tLR: 1.983887\n",
      "Training Epoch: 20 [42240/50000]\tLoss: 4.2300\tLR: 1.984143\n",
      "Training Epoch: 20 [42368/50000]\tLoss: 3.9656\tLR: 1.984399\n",
      "Training Epoch: 20 [42496/50000]\tLoss: 3.9532\tLR: 1.984655\n",
      "Training Epoch: 20 [42624/50000]\tLoss: 3.9747\tLR: 1.984910\n",
      "Training Epoch: 20 [42752/50000]\tLoss: 4.0303\tLR: 1.985166\n",
      "Training Epoch: 20 [42880/50000]\tLoss: 4.0097\tLR: 1.985422\n",
      "Training Epoch: 20 [43008/50000]\tLoss: 3.9283\tLR: 1.985678\n",
      "Training Epoch: 20 [43136/50000]\tLoss: 3.8569\tLR: 1.985934\n",
      "Training Epoch: 20 [43264/50000]\tLoss: 3.9322\tLR: 1.986189\n",
      "Training Epoch: 20 [43392/50000]\tLoss: 4.0778\tLR: 1.986445\n",
      "Training Epoch: 20 [43520/50000]\tLoss: 3.9307\tLR: 1.986701\n",
      "Training Epoch: 20 [43648/50000]\tLoss: 3.9451\tLR: 1.986957\n",
      "Training Epoch: 20 [43776/50000]\tLoss: 4.1478\tLR: 1.987212\n",
      "Training Epoch: 20 [43904/50000]\tLoss: 3.9652\tLR: 1.987468\n",
      "Training Epoch: 20 [44032/50000]\tLoss: 3.9311\tLR: 1.987724\n",
      "Training Epoch: 20 [44160/50000]\tLoss: 3.8475\tLR: 1.987980\n",
      "Training Epoch: 20 [44288/50000]\tLoss: 3.9113\tLR: 1.988235\n",
      "Training Epoch: 20 [44416/50000]\tLoss: 4.1125\tLR: 1.988491\n",
      "Training Epoch: 20 [44544/50000]\tLoss: 4.0231\tLR: 1.988747\n",
      "Training Epoch: 20 [44672/50000]\tLoss: 3.9939\tLR: 1.989003\n",
      "Training Epoch: 20 [44800/50000]\tLoss: 3.9339\tLR: 1.989258\n",
      "Training Epoch: 20 [44928/50000]\tLoss: 4.1598\tLR: 1.989514\n",
      "Training Epoch: 20 [45056/50000]\tLoss: 3.7364\tLR: 1.989770\n",
      "Training Epoch: 20 [45184/50000]\tLoss: 4.0177\tLR: 1.990026\n",
      "Training Epoch: 20 [45312/50000]\tLoss: 3.9073\tLR: 1.990281\n",
      "Training Epoch: 20 [45440/50000]\tLoss: 3.9006\tLR: 1.990537\n",
      "Training Epoch: 20 [45568/50000]\tLoss: 3.8227\tLR: 1.990793\n",
      "Training Epoch: 20 [45696/50000]\tLoss: 3.8992\tLR: 1.991049\n",
      "Training Epoch: 20 [45824/50000]\tLoss: 4.0006\tLR: 1.991304\n",
      "Training Epoch: 20 [45952/50000]\tLoss: 3.7635\tLR: 1.991560\n",
      "Training Epoch: 20 [46080/50000]\tLoss: 3.9373\tLR: 1.991816\n",
      "Training Epoch: 20 [46208/50000]\tLoss: 3.9380\tLR: 1.992072\n",
      "Training Epoch: 20 [46336/50000]\tLoss: 3.9666\tLR: 1.992327\n",
      "Training Epoch: 20 [46464/50000]\tLoss: 3.9808\tLR: 1.992583\n",
      "Training Epoch: 20 [46592/50000]\tLoss: 4.2068\tLR: 1.992839\n",
      "Training Epoch: 20 [46720/50000]\tLoss: 4.0296\tLR: 1.993095\n",
      "Training Epoch: 20 [46848/50000]\tLoss: 3.8247\tLR: 1.993350\n",
      "Training Epoch: 20 [46976/50000]\tLoss: 4.1430\tLR: 1.993606\n",
      "Training Epoch: 20 [47104/50000]\tLoss: 4.0623\tLR: 1.993862\n",
      "Training Epoch: 20 [47232/50000]\tLoss: 3.8980\tLR: 1.994118\n",
      "Training Epoch: 20 [47360/50000]\tLoss: 3.8729\tLR: 1.994373\n",
      "Training Epoch: 20 [47488/50000]\tLoss: 3.7726\tLR: 1.994629\n",
      "Training Epoch: 20 [47616/50000]\tLoss: 4.0609\tLR: 1.994885\n",
      "Training Epoch: 20 [47744/50000]\tLoss: 4.0429\tLR: 1.995141\n",
      "Training Epoch: 20 [47872/50000]\tLoss: 3.9460\tLR: 1.995396\n",
      "Training Epoch: 20 [48000/50000]\tLoss: 4.0237\tLR: 1.995652\n",
      "Training Epoch: 20 [48128/50000]\tLoss: 4.0236\tLR: 1.995908\n",
      "Training Epoch: 20 [48256/50000]\tLoss: 3.8395\tLR: 1.996164\n",
      "Training Epoch: 20 [48384/50000]\tLoss: 3.9722\tLR: 1.996419\n",
      "Training Epoch: 20 [48512/50000]\tLoss: 4.0455\tLR: 1.996675\n",
      "Training Epoch: 20 [48640/50000]\tLoss: 4.2465\tLR: 1.996931\n",
      "Training Epoch: 20 [48768/50000]\tLoss: 4.0447\tLR: 1.997187\n",
      "Training Epoch: 20 [48896/50000]\tLoss: 4.2729\tLR: 1.997442\n",
      "Training Epoch: 20 [49024/50000]\tLoss: 3.9824\tLR: 1.997698\n",
      "Training Epoch: 20 [49152/50000]\tLoss: 3.9909\tLR: 1.997954\n",
      "Training Epoch: 20 [49280/50000]\tLoss: 3.9014\tLR: 1.998210\n",
      "Training Epoch: 20 [49408/50000]\tLoss: 3.8527\tLR: 1.998465\n",
      "Training Epoch: 20 [49536/50000]\tLoss: 3.9781\tLR: 1.998721\n",
      "Training Epoch: 20 [49664/50000]\tLoss: 3.8980\tLR: 1.998977\n",
      "Training Epoch: 20 [49792/50000]\tLoss: 3.6280\tLR: 1.999233\n",
      "Training Epoch: 20 [49920/50000]\tLoss: 3.8556\tLR: 1.999488\n",
      "Training Epoch: 20 [50000/50000]\tLoss: 4.1868\tLR: 1.999744\n",
      "epoch 20 training time consumed: 488.96s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   28039 GB |   28039 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   27953 GB |   27953 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      86 GB |      86 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   28039 GB |   28039 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   27953 GB |   27953 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      86 GB |      86 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   27643 GB |   27643 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   27556 GB |   27556 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |      86 GB |      86 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    2973 K  |    2973 K  |\n",
      "|       from large pool |      24    |      65    |    1267 K  |    1267 K  |\n",
      "|       from small pool |     231    |     274    |    1706 K  |    1705 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    2973 K  |    2973 K  |\n",
      "|       from large pool |      24    |      65    |    1267 K  |    1267 K  |\n",
      "|       from small pool |     231    |     274    |    1706 K  |    1705 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      46    |    1722 K  |    1722 K  |\n",
      "|       from large pool |      10    |      23    |     609 K  |     609 K  |\n",
      "|       from small pool |      27    |      35    |    1113 K  |    1113 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 20, Average loss: 0.0385, Accuracy: 0.0612, Time consumed:30.96s\n",
      "\n",
      "saving weights file to checkpoint\\resnet18\\Monday_25_July_2022_16h_56m_47s\\resnet18-20-regular.pth\n",
      "Training Epoch: 21 [128/50000]\tLoss: 3.9605\tLR: 0.100000\n",
      "Training Epoch: 21 [256/50000]\tLoss: 4.0015\tLR: 2.000256\n",
      "Training Epoch: 21 [384/50000]\tLoss: 4.0202\tLR: 2.000512\n",
      "Training Epoch: 21 [512/50000]\tLoss: 4.1663\tLR: 2.000767\n",
      "Training Epoch: 21 [640/50000]\tLoss: 4.1120\tLR: 2.001023\n",
      "Training Epoch: 21 [768/50000]\tLoss: 3.9791\tLR: 2.001279\n",
      "Training Epoch: 21 [896/50000]\tLoss: 4.0726\tLR: 2.001535\n",
      "Training Epoch: 21 [1024/50000]\tLoss: 4.0453\tLR: 2.001790\n",
      "Training Epoch: 21 [1152/50000]\tLoss: 3.9783\tLR: 2.002046\n",
      "Training Epoch: 21 [1280/50000]\tLoss: 3.7432\tLR: 2.002302\n",
      "Training Epoch: 21 [1408/50000]\tLoss: 4.1868\tLR: 2.002558\n",
      "Training Epoch: 21 [1536/50000]\tLoss: 4.1405\tLR: 2.002813\n",
      "Training Epoch: 21 [1664/50000]\tLoss: 3.8527\tLR: 2.003069\n",
      "Training Epoch: 21 [1792/50000]\tLoss: 4.1138\tLR: 2.003325\n",
      "Training Epoch: 21 [1920/50000]\tLoss: 4.1541\tLR: 2.003581\n",
      "Training Epoch: 21 [2048/50000]\tLoss: 3.7318\tLR: 2.003836\n",
      "Training Epoch: 21 [2176/50000]\tLoss: 3.9401\tLR: 2.004092\n",
      "Training Epoch: 21 [2304/50000]\tLoss: 4.0880\tLR: 2.004348\n",
      "Training Epoch: 21 [2432/50000]\tLoss: 3.7429\tLR: 2.004604\n",
      "Training Epoch: 21 [2560/50000]\tLoss: 4.2344\tLR: 2.004859\n",
      "Training Epoch: 21 [2688/50000]\tLoss: 3.8103\tLR: 2.005115\n",
      "Training Epoch: 21 [2816/50000]\tLoss: 4.1668\tLR: 2.005371\n",
      "Training Epoch: 21 [2944/50000]\tLoss: 3.8615\tLR: 2.005627\n",
      "Training Epoch: 21 [3072/50000]\tLoss: 3.9303\tLR: 2.005882\n",
      "Training Epoch: 21 [3200/50000]\tLoss: 4.1101\tLR: 2.006138\n",
      "Training Epoch: 21 [3328/50000]\tLoss: 4.0630\tLR: 2.006394\n",
      "Training Epoch: 21 [3456/50000]\tLoss: 4.0617\tLR: 2.006650\n",
      "Training Epoch: 21 [3584/50000]\tLoss: 4.1274\tLR: 2.006905\n",
      "Training Epoch: 21 [3712/50000]\tLoss: 4.2486\tLR: 2.007161\n",
      "Training Epoch: 21 [3840/50000]\tLoss: 3.9138\tLR: 2.007417\n",
      "Training Epoch: 21 [3968/50000]\tLoss: 4.0512\tLR: 2.007673\n",
      "Training Epoch: 21 [4096/50000]\tLoss: 4.0613\tLR: 2.007928\n",
      "Training Epoch: 21 [4224/50000]\tLoss: 4.0125\tLR: 2.008184\n",
      "Training Epoch: 21 [4352/50000]\tLoss: 4.1780\tLR: 2.008440\n",
      "Training Epoch: 21 [4480/50000]\tLoss: 4.2000\tLR: 2.008696\n",
      "Training Epoch: 21 [4608/50000]\tLoss: 4.3739\tLR: 2.008951\n",
      "Training Epoch: 21 [4736/50000]\tLoss: 3.9014\tLR: 2.009207\n",
      "Training Epoch: 21 [4864/50000]\tLoss: 3.8500\tLR: 2.009463\n",
      "Training Epoch: 21 [4992/50000]\tLoss: 4.0629\tLR: 2.009719\n",
      "Training Epoch: 21 [5120/50000]\tLoss: 4.1477\tLR: 2.009974\n",
      "Training Epoch: 21 [5248/50000]\tLoss: 4.0299\tLR: 2.010230\n",
      "Training Epoch: 21 [5376/50000]\tLoss: 4.1364\tLR: 2.010486\n",
      "Training Epoch: 21 [5504/50000]\tLoss: 3.9267\tLR: 2.010742\n",
      "Training Epoch: 21 [5632/50000]\tLoss: 3.8148\tLR: 2.010997\n",
      "Training Epoch: 21 [5760/50000]\tLoss: 4.1012\tLR: 2.011253\n",
      "Training Epoch: 21 [5888/50000]\tLoss: 3.9411\tLR: 2.011509\n",
      "Training Epoch: 21 [6016/50000]\tLoss: 4.0653\tLR: 2.011765\n",
      "Training Epoch: 21 [6144/50000]\tLoss: 4.1990\tLR: 2.012020\n",
      "Training Epoch: 21 [6272/50000]\tLoss: 4.0636\tLR: 2.012276\n",
      "Training Epoch: 21 [6400/50000]\tLoss: 3.8702\tLR: 2.012532\n",
      "Training Epoch: 21 [6528/50000]\tLoss: 4.0033\tLR: 2.012788\n",
      "Training Epoch: 21 [6656/50000]\tLoss: 3.9361\tLR: 2.013043\n",
      "Training Epoch: 21 [6784/50000]\tLoss: 4.1762\tLR: 2.013299\n",
      "Training Epoch: 21 [6912/50000]\tLoss: 4.2183\tLR: 2.013555\n",
      "Training Epoch: 21 [7040/50000]\tLoss: 4.0683\tLR: 2.013811\n",
      "Training Epoch: 21 [7168/50000]\tLoss: 4.2579\tLR: 2.014066\n",
      "Training Epoch: 21 [7296/50000]\tLoss: 4.0815\tLR: 2.014322\n",
      "Training Epoch: 21 [7424/50000]\tLoss: 4.0267\tLR: 2.014578\n",
      "Training Epoch: 21 [7552/50000]\tLoss: 4.0642\tLR: 2.014834\n",
      "Training Epoch: 21 [7680/50000]\tLoss: 3.9014\tLR: 2.015090\n",
      "Training Epoch: 21 [7808/50000]\tLoss: 4.0432\tLR: 2.015345\n",
      "Training Epoch: 21 [7936/50000]\tLoss: 3.9256\tLR: 2.015601\n",
      "Training Epoch: 21 [8064/50000]\tLoss: 3.8984\tLR: 2.015857\n",
      "Training Epoch: 21 [8192/50000]\tLoss: 4.0627\tLR: 2.016113\n",
      "Training Epoch: 21 [8320/50000]\tLoss: 3.8769\tLR: 2.016368\n",
      "Training Epoch: 21 [8448/50000]\tLoss: 4.0826\tLR: 2.016624\n",
      "Training Epoch: 21 [8576/50000]\tLoss: 4.0127\tLR: 2.016880\n",
      "Training Epoch: 21 [8704/50000]\tLoss: 3.7783\tLR: 2.017136\n",
      "Training Epoch: 21 [8832/50000]\tLoss: 3.9020\tLR: 2.017391\n",
      "Training Epoch: 21 [8960/50000]\tLoss: 4.0882\tLR: 2.017647\n",
      "Training Epoch: 21 [9088/50000]\tLoss: 3.7604\tLR: 2.017903\n",
      "Training Epoch: 21 [9216/50000]\tLoss: 4.0795\tLR: 2.018159\n",
      "Training Epoch: 21 [9344/50000]\tLoss: 3.8760\tLR: 2.018414\n",
      "Training Epoch: 21 [9472/50000]\tLoss: 3.9694\tLR: 2.018670\n",
      "Training Epoch: 21 [9600/50000]\tLoss: 4.0831\tLR: 2.018926\n",
      "Training Epoch: 21 [9728/50000]\tLoss: 4.0522\tLR: 2.019182\n",
      "Training Epoch: 21 [9856/50000]\tLoss: 3.8525\tLR: 2.019437\n",
      "Training Epoch: 21 [9984/50000]\tLoss: 4.0514\tLR: 2.019693\n",
      "Training Epoch: 21 [10112/50000]\tLoss: 3.9719\tLR: 2.019949\n",
      "Training Epoch: 21 [10240/50000]\tLoss: 4.0711\tLR: 2.020205\n",
      "Training Epoch: 21 [10368/50000]\tLoss: 3.8782\tLR: 2.020460\n",
      "Training Epoch: 21 [10496/50000]\tLoss: 3.9984\tLR: 2.020716\n",
      "Training Epoch: 21 [10624/50000]\tLoss: 3.9552\tLR: 2.020972\n",
      "Training Epoch: 21 [10752/50000]\tLoss: 3.9618\tLR: 2.021228\n",
      "Training Epoch: 21 [10880/50000]\tLoss: 4.0199\tLR: 2.021483\n",
      "Training Epoch: 21 [11008/50000]\tLoss: 3.8814\tLR: 2.021739\n",
      "Training Epoch: 21 [11136/50000]\tLoss: 3.9077\tLR: 2.021995\n",
      "Training Epoch: 21 [11264/50000]\tLoss: 3.9103\tLR: 2.022251\n",
      "Training Epoch: 21 [11392/50000]\tLoss: 4.0354\tLR: 2.022506\n",
      "Training Epoch: 21 [11520/50000]\tLoss: 3.8788\tLR: 2.022762\n",
      "Training Epoch: 21 [11648/50000]\tLoss: 4.1582\tLR: 2.023018\n",
      "Training Epoch: 21 [11776/50000]\tLoss: 4.0889\tLR: 2.023274\n",
      "Training Epoch: 21 [11904/50000]\tLoss: 4.0486\tLR: 2.023529\n",
      "Training Epoch: 21 [12032/50000]\tLoss: 3.7660\tLR: 2.023785\n",
      "Training Epoch: 21 [12160/50000]\tLoss: 3.8067\tLR: 2.024041\n",
      "Training Epoch: 21 [12288/50000]\tLoss: 4.1578\tLR: 2.024297\n",
      "Training Epoch: 21 [12416/50000]\tLoss: 3.8420\tLR: 2.024552\n",
      "Training Epoch: 21 [12544/50000]\tLoss: 4.1184\tLR: 2.024808\n",
      "Training Epoch: 21 [12672/50000]\tLoss: 3.8874\tLR: 2.025064\n",
      "Training Epoch: 21 [12800/50000]\tLoss: 3.9320\tLR: 2.025320\n",
      "Training Epoch: 21 [12928/50000]\tLoss: 3.9459\tLR: 2.025575\n",
      "Training Epoch: 21 [13056/50000]\tLoss: 4.0707\tLR: 2.025831\n",
      "Training Epoch: 21 [13184/50000]\tLoss: 3.9030\tLR: 2.026087\n",
      "Training Epoch: 21 [13312/50000]\tLoss: 4.0003\tLR: 2.026343\n",
      "Training Epoch: 21 [13440/50000]\tLoss: 3.7538\tLR: 2.026598\n",
      "Training Epoch: 21 [13568/50000]\tLoss: 4.1295\tLR: 2.026854\n",
      "Training Epoch: 21 [13696/50000]\tLoss: 4.1583\tLR: 2.027110\n",
      "Training Epoch: 21 [13824/50000]\tLoss: 4.0774\tLR: 2.027366\n",
      "Training Epoch: 21 [13952/50000]\tLoss: 3.9916\tLR: 2.027621\n",
      "Training Epoch: 21 [14080/50000]\tLoss: 4.0348\tLR: 2.027877\n",
      "Training Epoch: 21 [14208/50000]\tLoss: 3.8608\tLR: 2.028133\n",
      "Training Epoch: 21 [14336/50000]\tLoss: 3.9719\tLR: 2.028389\n",
      "Training Epoch: 21 [14464/50000]\tLoss: 3.8338\tLR: 2.028645\n",
      "Training Epoch: 21 [14592/50000]\tLoss: 3.9913\tLR: 2.028900\n",
      "Training Epoch: 21 [14720/50000]\tLoss: 4.0553\tLR: 2.029156\n",
      "Training Epoch: 21 [14848/50000]\tLoss: 4.1876\tLR: 2.029412\n",
      "Training Epoch: 21 [14976/50000]\tLoss: 4.0193\tLR: 2.029668\n",
      "Training Epoch: 21 [15104/50000]\tLoss: 3.9769\tLR: 2.029923\n",
      "Training Epoch: 21 [15232/50000]\tLoss: 3.8616\tLR: 2.030179\n",
      "Training Epoch: 21 [15360/50000]\tLoss: 4.1742\tLR: 2.030435\n",
      "Training Epoch: 21 [15488/50000]\tLoss: 4.1559\tLR: 2.030691\n",
      "Training Epoch: 21 [15616/50000]\tLoss: 4.0109\tLR: 2.030946\n",
      "Training Epoch: 21 [15744/50000]\tLoss: 4.0901\tLR: 2.031202\n",
      "Training Epoch: 21 [15872/50000]\tLoss: 3.8667\tLR: 2.031458\n",
      "Training Epoch: 21 [16000/50000]\tLoss: 3.9979\tLR: 2.031714\n",
      "Training Epoch: 21 [16128/50000]\tLoss: 4.1701\tLR: 2.031969\n",
      "Training Epoch: 21 [16256/50000]\tLoss: 4.0936\tLR: 2.032225\n",
      "Training Epoch: 21 [16384/50000]\tLoss: 4.1309\tLR: 2.032481\n",
      "Training Epoch: 21 [16512/50000]\tLoss: 4.1727\tLR: 2.032737\n",
      "Training Epoch: 21 [16640/50000]\tLoss: 4.1613\tLR: 2.032992\n",
      "Training Epoch: 21 [16768/50000]\tLoss: 4.1685\tLR: 2.033248\n",
      "Training Epoch: 21 [16896/50000]\tLoss: 4.1119\tLR: 2.033504\n",
      "Training Epoch: 21 [17024/50000]\tLoss: 4.3881\tLR: 2.033760\n",
      "Training Epoch: 21 [17152/50000]\tLoss: 4.1793\tLR: 2.034015\n",
      "Training Epoch: 21 [17280/50000]\tLoss: 3.9406\tLR: 2.034271\n",
      "Training Epoch: 21 [17408/50000]\tLoss: 4.0826\tLR: 2.034527\n",
      "Training Epoch: 21 [17536/50000]\tLoss: 4.2212\tLR: 2.034783\n",
      "Training Epoch: 21 [17664/50000]\tLoss: 4.0331\tLR: 2.035038\n",
      "Training Epoch: 21 [17792/50000]\tLoss: 4.1921\tLR: 2.035294\n",
      "Training Epoch: 21 [17920/50000]\tLoss: 4.2382\tLR: 2.035550\n",
      "Training Epoch: 21 [18048/50000]\tLoss: 4.0955\tLR: 2.035806\n",
      "Training Epoch: 21 [18176/50000]\tLoss: 4.1241\tLR: 2.036061\n",
      "Training Epoch: 21 [18304/50000]\tLoss: 4.1590\tLR: 2.036317\n",
      "Training Epoch: 21 [18432/50000]\tLoss: 3.7282\tLR: 2.036573\n",
      "Training Epoch: 21 [18560/50000]\tLoss: 4.0115\tLR: 2.036829\n",
      "Training Epoch: 21 [18688/50000]\tLoss: 4.0365\tLR: 2.037084\n",
      "Training Epoch: 21 [18816/50000]\tLoss: 4.0246\tLR: 2.037340\n",
      "Training Epoch: 21 [18944/50000]\tLoss: 3.9180\tLR: 2.037596\n",
      "Training Epoch: 21 [19072/50000]\tLoss: 4.0126\tLR: 2.037852\n",
      "Training Epoch: 21 [19200/50000]\tLoss: 3.8682\tLR: 2.038107\n",
      "Training Epoch: 21 [19328/50000]\tLoss: 3.8142\tLR: 2.038363\n",
      "Training Epoch: 21 [19456/50000]\tLoss: 4.1217\tLR: 2.038619\n",
      "Training Epoch: 21 [19584/50000]\tLoss: 4.0371\tLR: 2.038875\n",
      "Training Epoch: 21 [19712/50000]\tLoss: 4.0638\tLR: 2.039130\n",
      "Training Epoch: 21 [19840/50000]\tLoss: 4.0806\tLR: 2.039386\n",
      "Training Epoch: 21 [19968/50000]\tLoss: 4.1160\tLR: 2.039642\n",
      "Training Epoch: 21 [20096/50000]\tLoss: 3.7474\tLR: 2.039898\n",
      "Training Epoch: 21 [20224/50000]\tLoss: 4.0721\tLR: 2.040153\n",
      "Training Epoch: 21 [20352/50000]\tLoss: 3.8547\tLR: 2.040409\n",
      "Training Epoch: 21 [20480/50000]\tLoss: 3.8798\tLR: 2.040665\n",
      "Training Epoch: 21 [20608/50000]\tLoss: 4.1243\tLR: 2.040921\n",
      "Training Epoch: 21 [20736/50000]\tLoss: 3.8184\tLR: 2.041176\n",
      "Training Epoch: 21 [20864/50000]\tLoss: 4.1082\tLR: 2.041432\n",
      "Training Epoch: 21 [20992/50000]\tLoss: 4.1325\tLR: 2.041688\n",
      "Training Epoch: 21 [21120/50000]\tLoss: 4.0412\tLR: 2.041944\n",
      "Training Epoch: 21 [21248/50000]\tLoss: 3.9192\tLR: 2.042199\n",
      "Training Epoch: 21 [21376/50000]\tLoss: 4.1071\tLR: 2.042455\n",
      "Training Epoch: 21 [21504/50000]\tLoss: 3.9720\tLR: 2.042711\n",
      "Training Epoch: 21 [21632/50000]\tLoss: 3.8862\tLR: 2.042967\n",
      "Training Epoch: 21 [21760/50000]\tLoss: 3.8616\tLR: 2.043223\n",
      "Training Epoch: 21 [21888/50000]\tLoss: 3.9264\tLR: 2.043478\n",
      "Training Epoch: 21 [22016/50000]\tLoss: 3.7676\tLR: 2.043734\n",
      "Training Epoch: 21 [22144/50000]\tLoss: 3.7614\tLR: 2.043990\n",
      "Training Epoch: 21 [22272/50000]\tLoss: 3.8027\tLR: 2.044246\n",
      "Training Epoch: 21 [22400/50000]\tLoss: 4.0827\tLR: 2.044501\n",
      "Training Epoch: 21 [22528/50000]\tLoss: 3.9405\tLR: 2.044757\n",
      "Training Epoch: 21 [22656/50000]\tLoss: 4.1175\tLR: 2.045013\n",
      "Training Epoch: 21 [22784/50000]\tLoss: 3.8974\tLR: 2.045269\n",
      "Training Epoch: 21 [22912/50000]\tLoss: 3.9604\tLR: 2.045524\n",
      "Training Epoch: 21 [23040/50000]\tLoss: 3.9129\tLR: 2.045780\n",
      "Training Epoch: 21 [23168/50000]\tLoss: 4.1926\tLR: 2.046036\n",
      "Training Epoch: 21 [23296/50000]\tLoss: 4.0266\tLR: 2.046292\n",
      "Training Epoch: 21 [23424/50000]\tLoss: 4.1294\tLR: 2.046547\n",
      "Training Epoch: 21 [23552/50000]\tLoss: 4.0457\tLR: 2.046803\n",
      "Training Epoch: 21 [23680/50000]\tLoss: 4.0859\tLR: 2.047059\n",
      "Training Epoch: 21 [23808/50000]\tLoss: 4.1989\tLR: 2.047315\n",
      "Training Epoch: 21 [23936/50000]\tLoss: 4.0541\tLR: 2.047570\n",
      "Training Epoch: 21 [24064/50000]\tLoss: 3.9680\tLR: 2.047826\n",
      "Training Epoch: 21 [24192/50000]\tLoss: 4.1079\tLR: 2.048082\n",
      "Training Epoch: 21 [24320/50000]\tLoss: 3.9037\tLR: 2.048338\n",
      "Training Epoch: 21 [24448/50000]\tLoss: 3.9726\tLR: 2.048593\n",
      "Training Epoch: 21 [24576/50000]\tLoss: 3.8444\tLR: 2.048849\n",
      "Training Epoch: 21 [24704/50000]\tLoss: 3.9287\tLR: 2.049105\n",
      "Training Epoch: 21 [24832/50000]\tLoss: 3.9288\tLR: 2.049361\n",
      "Training Epoch: 21 [24960/50000]\tLoss: 4.2411\tLR: 2.049616\n",
      "Training Epoch: 21 [25088/50000]\tLoss: 3.8704\tLR: 2.049872\n",
      "Training Epoch: 21 [25216/50000]\tLoss: 3.7621\tLR: 2.050128\n",
      "Training Epoch: 21 [25344/50000]\tLoss: 3.9946\tLR: 2.050384\n",
      "Training Epoch: 21 [25472/50000]\tLoss: 4.1406\tLR: 2.050639\n",
      "Training Epoch: 21 [25600/50000]\tLoss: 4.0852\tLR: 2.050895\n",
      "Training Epoch: 21 [25728/50000]\tLoss: 4.1048\tLR: 2.051151\n",
      "Training Epoch: 21 [25856/50000]\tLoss: 4.0754\tLR: 2.051407\n",
      "Training Epoch: 21 [25984/50000]\tLoss: 4.1276\tLR: 2.051662\n",
      "Training Epoch: 21 [26112/50000]\tLoss: 3.9652\tLR: 2.051918\n",
      "Training Epoch: 21 [26240/50000]\tLoss: 3.9634\tLR: 2.052174\n",
      "Training Epoch: 21 [26368/50000]\tLoss: 4.0391\tLR: 2.052430\n",
      "Training Epoch: 21 [26496/50000]\tLoss: 3.9284\tLR: 2.052685\n",
      "Training Epoch: 21 [26624/50000]\tLoss: 3.9641\tLR: 2.052941\n",
      "Training Epoch: 21 [26752/50000]\tLoss: 3.9817\tLR: 2.053197\n",
      "Training Epoch: 21 [26880/50000]\tLoss: 3.8659\tLR: 2.053453\n",
      "Training Epoch: 21 [27008/50000]\tLoss: 3.9762\tLR: 2.053708\n",
      "Training Epoch: 21 [27136/50000]\tLoss: 3.8818\tLR: 2.053964\n",
      "Training Epoch: 21 [27264/50000]\tLoss: 4.0124\tLR: 2.054220\n",
      "Training Epoch: 21 [27392/50000]\tLoss: 3.9312\tLR: 2.054476\n",
      "Training Epoch: 21 [27520/50000]\tLoss: 4.0188\tLR: 2.054731\n",
      "Training Epoch: 21 [27648/50000]\tLoss: 3.9553\tLR: 2.054987\n",
      "Training Epoch: 21 [27776/50000]\tLoss: 4.0103\tLR: 2.055243\n",
      "Training Epoch: 21 [27904/50000]\tLoss: 3.9792\tLR: 2.055499\n",
      "Training Epoch: 21 [28032/50000]\tLoss: 3.8585\tLR: 2.055754\n",
      "Training Epoch: 21 [28160/50000]\tLoss: 4.2893\tLR: 2.056010\n",
      "Training Epoch: 21 [28288/50000]\tLoss: 3.9977\tLR: 2.056266\n",
      "Training Epoch: 21 [28416/50000]\tLoss: 4.0244\tLR: 2.056522\n",
      "Training Epoch: 21 [28544/50000]\tLoss: 4.2757\tLR: 2.056777\n",
      "Training Epoch: 21 [28672/50000]\tLoss: 3.9361\tLR: 2.057033\n",
      "Training Epoch: 21 [28800/50000]\tLoss: 4.0308\tLR: 2.057289\n",
      "Training Epoch: 21 [28928/50000]\tLoss: 4.0387\tLR: 2.057545\n",
      "Training Epoch: 21 [29056/50000]\tLoss: 4.0954\tLR: 2.057801\n",
      "Training Epoch: 21 [29184/50000]\tLoss: 4.0867\tLR: 2.058056\n",
      "Training Epoch: 21 [29312/50000]\tLoss: 4.1242\tLR: 2.058312\n",
      "Training Epoch: 21 [29440/50000]\tLoss: 4.0158\tLR: 2.058568\n",
      "Training Epoch: 21 [29568/50000]\tLoss: 3.9685\tLR: 2.058824\n",
      "Training Epoch: 21 [29696/50000]\tLoss: 4.1001\tLR: 2.059079\n",
      "Training Epoch: 21 [29824/50000]\tLoss: 4.0665\tLR: 2.059335\n",
      "Training Epoch: 21 [29952/50000]\tLoss: 4.0652\tLR: 2.059591\n",
      "Training Epoch: 21 [30080/50000]\tLoss: 4.1238\tLR: 2.059847\n",
      "Training Epoch: 21 [30208/50000]\tLoss: 3.9214\tLR: 2.060102\n",
      "Training Epoch: 21 [30336/50000]\tLoss: 4.0198\tLR: 2.060358\n",
      "Training Epoch: 21 [30464/50000]\tLoss: 4.0673\tLR: 2.060614\n",
      "Training Epoch: 21 [30592/50000]\tLoss: 3.8506\tLR: 2.060870\n",
      "Training Epoch: 21 [30720/50000]\tLoss: 4.0067\tLR: 2.061125\n",
      "Training Epoch: 21 [30848/50000]\tLoss: 4.1594\tLR: 2.061381\n",
      "Training Epoch: 21 [30976/50000]\tLoss: 3.8869\tLR: 2.061637\n",
      "Training Epoch: 21 [31104/50000]\tLoss: 4.2438\tLR: 2.061893\n",
      "Training Epoch: 21 [31232/50000]\tLoss: 4.0606\tLR: 2.062148\n",
      "Training Epoch: 21 [31360/50000]\tLoss: 4.0967\tLR: 2.062404\n",
      "Training Epoch: 21 [31488/50000]\tLoss: 4.1335\tLR: 2.062660\n",
      "Training Epoch: 21 [31616/50000]\tLoss: 4.0526\tLR: 2.062916\n",
      "Training Epoch: 21 [31744/50000]\tLoss: 4.1143\tLR: 2.063171\n",
      "Training Epoch: 21 [31872/50000]\tLoss: 3.9270\tLR: 2.063427\n",
      "Training Epoch: 21 [32000/50000]\tLoss: 3.9517\tLR: 2.063683\n",
      "Training Epoch: 21 [32128/50000]\tLoss: 4.0057\tLR: 2.063939\n",
      "Training Epoch: 21 [32256/50000]\tLoss: 4.0147\tLR: 2.064194\n",
      "Training Epoch: 21 [32384/50000]\tLoss: 3.8255\tLR: 2.064450\n",
      "Training Epoch: 21 [32512/50000]\tLoss: 4.0628\tLR: 2.064706\n",
      "Training Epoch: 21 [32640/50000]\tLoss: 4.1256\tLR: 2.064962\n",
      "Training Epoch: 21 [32768/50000]\tLoss: 3.9249\tLR: 2.065217\n",
      "Training Epoch: 21 [32896/50000]\tLoss: 4.0546\tLR: 2.065473\n",
      "Training Epoch: 21 [33024/50000]\tLoss: 4.0113\tLR: 2.065729\n",
      "Training Epoch: 21 [33152/50000]\tLoss: 4.0916\tLR: 2.065985\n",
      "Training Epoch: 21 [33280/50000]\tLoss: 4.1583\tLR: 2.066240\n",
      "Training Epoch: 21 [33408/50000]\tLoss: 4.0271\tLR: 2.066496\n",
      "Training Epoch: 21 [33536/50000]\tLoss: 4.2517\tLR: 2.066752\n",
      "Training Epoch: 21 [33664/50000]\tLoss: 4.1207\tLR: 2.067008\n",
      "Training Epoch: 21 [33792/50000]\tLoss: 4.1238\tLR: 2.067263\n",
      "Training Epoch: 21 [33920/50000]\tLoss: 3.9816\tLR: 2.067519\n",
      "Training Epoch: 21 [34048/50000]\tLoss: 4.1210\tLR: 2.067775\n",
      "Training Epoch: 21 [34176/50000]\tLoss: 3.8009\tLR: 2.068031\n",
      "Training Epoch: 21 [34304/50000]\tLoss: 4.0169\tLR: 2.068286\n",
      "Training Epoch: 21 [34432/50000]\tLoss: 3.8939\tLR: 2.068542\n",
      "Training Epoch: 21 [34560/50000]\tLoss: 4.1853\tLR: 2.068798\n",
      "Training Epoch: 21 [34688/50000]\tLoss: 4.2926\tLR: 2.069054\n",
      "Training Epoch: 21 [34816/50000]\tLoss: 4.1220\tLR: 2.069309\n",
      "Training Epoch: 21 [34944/50000]\tLoss: 4.0396\tLR: 2.069565\n",
      "Training Epoch: 21 [35072/50000]\tLoss: 4.0739\tLR: 2.069821\n",
      "Training Epoch: 21 [35200/50000]\tLoss: 4.2380\tLR: 2.070077\n",
      "Training Epoch: 21 [35328/50000]\tLoss: 4.1768\tLR: 2.070332\n",
      "Training Epoch: 21 [35456/50000]\tLoss: 4.1371\tLR: 2.070588\n",
      "Training Epoch: 21 [35584/50000]\tLoss: 4.0767\tLR: 2.070844\n",
      "Training Epoch: 21 [35712/50000]\tLoss: 4.1840\tLR: 2.071100\n",
      "Training Epoch: 21 [35840/50000]\tLoss: 4.2109\tLR: 2.071355\n",
      "Training Epoch: 21 [35968/50000]\tLoss: 4.1911\tLR: 2.071611\n",
      "Training Epoch: 21 [36096/50000]\tLoss: 4.1334\tLR: 2.071867\n",
      "Training Epoch: 21 [36224/50000]\tLoss: 4.0304\tLR: 2.072123\n",
      "Training Epoch: 21 [36352/50000]\tLoss: 3.9429\tLR: 2.072379\n",
      "Training Epoch: 21 [36480/50000]\tLoss: 3.9090\tLR: 2.072634\n",
      "Training Epoch: 21 [36608/50000]\tLoss: 4.0265\tLR: 2.072890\n",
      "Training Epoch: 21 [36736/50000]\tLoss: 4.2027\tLR: 2.073146\n",
      "Training Epoch: 21 [36864/50000]\tLoss: 4.1169\tLR: 2.073402\n",
      "Training Epoch: 21 [36992/50000]\tLoss: 3.9031\tLR: 2.073657\n",
      "Training Epoch: 21 [37120/50000]\tLoss: 4.1277\tLR: 2.073913\n",
      "Training Epoch: 21 [37248/50000]\tLoss: 4.1854\tLR: 2.074169\n",
      "Training Epoch: 21 [37376/50000]\tLoss: 4.0085\tLR: 2.074425\n",
      "Training Epoch: 21 [37504/50000]\tLoss: 4.1058\tLR: 2.074680\n",
      "Training Epoch: 21 [37632/50000]\tLoss: 4.1734\tLR: 2.074936\n",
      "Training Epoch: 21 [37760/50000]\tLoss: 3.8956\tLR: 2.075192\n",
      "Training Epoch: 21 [37888/50000]\tLoss: 4.0345\tLR: 2.075448\n",
      "Training Epoch: 21 [38016/50000]\tLoss: 4.1960\tLR: 2.075703\n",
      "Training Epoch: 21 [38144/50000]\tLoss: 4.2368\tLR: 2.075959\n",
      "Training Epoch: 21 [38272/50000]\tLoss: 4.0660\tLR: 2.076215\n",
      "Training Epoch: 21 [38400/50000]\tLoss: 4.1259\tLR: 2.076471\n",
      "Training Epoch: 21 [38528/50000]\tLoss: 4.0337\tLR: 2.076726\n",
      "Training Epoch: 21 [38656/50000]\tLoss: 3.9087\tLR: 2.076982\n",
      "Training Epoch: 21 [38784/50000]\tLoss: 3.9883\tLR: 2.077238\n",
      "Training Epoch: 21 [38912/50000]\tLoss: 3.9853\tLR: 2.077494\n",
      "Training Epoch: 21 [39040/50000]\tLoss: 3.9665\tLR: 2.077749\n",
      "Training Epoch: 21 [39168/50000]\tLoss: 3.9263\tLR: 2.078005\n",
      "Training Epoch: 21 [39296/50000]\tLoss: 4.1288\tLR: 2.078261\n",
      "Training Epoch: 21 [39424/50000]\tLoss: 4.0603\tLR: 2.078517\n",
      "Training Epoch: 21 [39552/50000]\tLoss: 4.1158\tLR: 2.078772\n",
      "Training Epoch: 21 [39680/50000]\tLoss: 3.8155\tLR: 2.079028\n",
      "Training Epoch: 21 [39808/50000]\tLoss: 4.0707\tLR: 2.079284\n",
      "Training Epoch: 21 [39936/50000]\tLoss: 4.1582\tLR: 2.079540\n",
      "Training Epoch: 21 [40064/50000]\tLoss: 3.8883\tLR: 2.079795\n",
      "Training Epoch: 21 [40192/50000]\tLoss: 3.9184\tLR: 2.080051\n",
      "Training Epoch: 21 [40320/50000]\tLoss: 3.8909\tLR: 2.080307\n",
      "Training Epoch: 21 [40448/50000]\tLoss: 4.0389\tLR: 2.080563\n",
      "Training Epoch: 21 [40576/50000]\tLoss: 3.8829\tLR: 2.080818\n",
      "Training Epoch: 21 [40704/50000]\tLoss: 4.1713\tLR: 2.081074\n",
      "Training Epoch: 21 [40832/50000]\tLoss: 3.8203\tLR: 2.081330\n",
      "Training Epoch: 21 [40960/50000]\tLoss: 4.0297\tLR: 2.081586\n",
      "Training Epoch: 21 [41088/50000]\tLoss: 4.1186\tLR: 2.081841\n",
      "Training Epoch: 21 [41216/50000]\tLoss: 4.1015\tLR: 2.082097\n",
      "Training Epoch: 21 [41344/50000]\tLoss: 4.1138\tLR: 2.082353\n",
      "Training Epoch: 21 [41472/50000]\tLoss: 4.1747\tLR: 2.082609\n",
      "Training Epoch: 21 [41600/50000]\tLoss: 4.1263\tLR: 2.082864\n",
      "Training Epoch: 21 [41728/50000]\tLoss: 3.9795\tLR: 2.083120\n",
      "Training Epoch: 21 [41856/50000]\tLoss: 4.1238\tLR: 2.083376\n",
      "Training Epoch: 21 [41984/50000]\tLoss: 3.9890\tLR: 2.083632\n",
      "Training Epoch: 21 [42112/50000]\tLoss: 4.0796\tLR: 2.083887\n",
      "Training Epoch: 21 [42240/50000]\tLoss: 4.1295\tLR: 2.084143\n",
      "Training Epoch: 21 [42368/50000]\tLoss: 4.0289\tLR: 2.084399\n",
      "Training Epoch: 21 [42496/50000]\tLoss: 4.1057\tLR: 2.084655\n",
      "Training Epoch: 21 [42624/50000]\tLoss: 4.2083\tLR: 2.084910\n",
      "Training Epoch: 21 [42752/50000]\tLoss: 4.0385\tLR: 2.085166\n",
      "Training Epoch: 21 [42880/50000]\tLoss: 3.9555\tLR: 2.085422\n",
      "Training Epoch: 21 [43008/50000]\tLoss: 4.0297\tLR: 2.085678\n",
      "Training Epoch: 21 [43136/50000]\tLoss: 4.0805\tLR: 2.085934\n",
      "Training Epoch: 21 [43264/50000]\tLoss: 4.1098\tLR: 2.086189\n",
      "Training Epoch: 21 [43392/50000]\tLoss: 4.1029\tLR: 2.086445\n",
      "Training Epoch: 21 [43520/50000]\tLoss: 4.2028\tLR: 2.086701\n",
      "Training Epoch: 21 [43648/50000]\tLoss: 4.2243\tLR: 2.086957\n",
      "Training Epoch: 21 [43776/50000]\tLoss: 4.0383\tLR: 2.087212\n",
      "Training Epoch: 21 [43904/50000]\tLoss: 3.9903\tLR: 2.087468\n",
      "Training Epoch: 21 [44032/50000]\tLoss: 4.0725\tLR: 2.087724\n",
      "Training Epoch: 21 [44160/50000]\tLoss: 4.0230\tLR: 2.087980\n",
      "Training Epoch: 21 [44288/50000]\tLoss: 4.1011\tLR: 2.088235\n",
      "Training Epoch: 21 [44416/50000]\tLoss: 4.0536\tLR: 2.088491\n",
      "Training Epoch: 21 [44544/50000]\tLoss: 4.0075\tLR: 2.088747\n",
      "Training Epoch: 21 [44672/50000]\tLoss: 4.0564\tLR: 2.089003\n",
      "Training Epoch: 21 [44800/50000]\tLoss: 4.0420\tLR: 2.089258\n",
      "Training Epoch: 21 [44928/50000]\tLoss: 4.0617\tLR: 2.089514\n",
      "Training Epoch: 21 [45056/50000]\tLoss: 4.1360\tLR: 2.089770\n",
      "Training Epoch: 21 [45184/50000]\tLoss: 4.1185\tLR: 2.090026\n",
      "Training Epoch: 21 [45312/50000]\tLoss: 4.0938\tLR: 2.090281\n",
      "Training Epoch: 21 [45440/50000]\tLoss: 4.1468\tLR: 2.090537\n",
      "Training Epoch: 21 [45568/50000]\tLoss: 4.0128\tLR: 2.090793\n",
      "Training Epoch: 21 [45696/50000]\tLoss: 4.1741\tLR: 2.091049\n",
      "Training Epoch: 21 [45824/50000]\tLoss: 4.1434\tLR: 2.091304\n",
      "Training Epoch: 21 [45952/50000]\tLoss: 4.2606\tLR: 2.091560\n",
      "Training Epoch: 21 [46080/50000]\tLoss: 4.1305\tLR: 2.091816\n",
      "Training Epoch: 21 [46208/50000]\tLoss: 4.0476\tLR: 2.092072\n",
      "Training Epoch: 21 [46336/50000]\tLoss: 4.1313\tLR: 2.092327\n",
      "Training Epoch: 21 [46464/50000]\tLoss: 4.1776\tLR: 2.092583\n",
      "Training Epoch: 21 [46592/50000]\tLoss: 4.2600\tLR: 2.092839\n",
      "Training Epoch: 21 [46720/50000]\tLoss: 4.2451\tLR: 2.093095\n",
      "Training Epoch: 21 [46848/50000]\tLoss: 4.1864\tLR: 2.093350\n",
      "Training Epoch: 21 [46976/50000]\tLoss: 4.1151\tLR: 2.093606\n",
      "Training Epoch: 21 [47104/50000]\tLoss: 4.0653\tLR: 2.093862\n",
      "Training Epoch: 21 [47232/50000]\tLoss: 4.0537\tLR: 2.094118\n",
      "Training Epoch: 21 [47360/50000]\tLoss: 4.1788\tLR: 2.094373\n",
      "Training Epoch: 21 [47488/50000]\tLoss: 3.9865\tLR: 2.094629\n",
      "Training Epoch: 21 [47616/50000]\tLoss: 3.8813\tLR: 2.094885\n",
      "Training Epoch: 21 [47744/50000]\tLoss: 4.3114\tLR: 2.095141\n",
      "Training Epoch: 21 [47872/50000]\tLoss: 3.8507\tLR: 2.095396\n",
      "Training Epoch: 21 [48000/50000]\tLoss: 4.1054\tLR: 2.095652\n",
      "Training Epoch: 21 [48128/50000]\tLoss: 4.1310\tLR: 2.095908\n",
      "Training Epoch: 21 [48256/50000]\tLoss: 3.8154\tLR: 2.096164\n",
      "Training Epoch: 21 [48384/50000]\tLoss: 3.8886\tLR: 2.096419\n",
      "Training Epoch: 21 [48512/50000]\tLoss: 3.9920\tLR: 2.096675\n",
      "Training Epoch: 21 [48640/50000]\tLoss: 3.8960\tLR: 2.096931\n",
      "Training Epoch: 21 [48768/50000]\tLoss: 4.0530\tLR: 2.097187\n",
      "Training Epoch: 21 [48896/50000]\tLoss: 3.9888\tLR: 2.097442\n",
      "Training Epoch: 21 [49024/50000]\tLoss: 4.0841\tLR: 2.097698\n",
      "Training Epoch: 21 [49152/50000]\tLoss: 3.8238\tLR: 2.097954\n",
      "Training Epoch: 21 [49280/50000]\tLoss: 4.1185\tLR: 2.098210\n",
      "Training Epoch: 21 [49408/50000]\tLoss: 3.9734\tLR: 2.098465\n",
      "Training Epoch: 21 [49536/50000]\tLoss: 4.1855\tLR: 2.098721\n",
      "Training Epoch: 21 [49664/50000]\tLoss: 4.0403\tLR: 2.098977\n",
      "Training Epoch: 21 [49792/50000]\tLoss: 3.9686\tLR: 2.099233\n",
      "Training Epoch: 21 [49920/50000]\tLoss: 4.0634\tLR: 2.099488\n",
      "Training Epoch: 21 [50000/50000]\tLoss: 4.1709\tLR: 2.099744\n",
      "epoch 21 training time consumed: 488.80s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   29441 GB |   29441 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   29351 GB |   29351 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      90 GB |      90 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   29441 GB |   29441 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   29351 GB |   29351 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      90 GB |      90 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   29025 GB |   29025 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   28934 GB |   28934 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |      90 GB |      90 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    3122 K  |    3121 K  |\n",
      "|       from large pool |      24    |      65    |    1330 K  |    1330 K  |\n",
      "|       from small pool |     231    |     274    |    1791 K  |    1791 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    3122 K  |    3121 K  |\n",
      "|       from large pool |      24    |      65    |    1330 K  |    1330 K  |\n",
      "|       from small pool |     231    |     274    |    1791 K  |    1791 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      46    |    1808 K  |    1808 K  |\n",
      "|       from large pool |      10    |      23    |     639 K  |     639 K  |\n",
      "|       from small pool |      27    |      35    |    1169 K  |    1169 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 21, Average loss: 0.0587, Accuracy: 0.0269, Time consumed:30.96s\n",
      "\n",
      "Training Epoch: 22 [128/50000]\tLoss: 4.1349\tLR: 0.100000\n",
      "Training Epoch: 22 [256/50000]\tLoss: 4.0789\tLR: 2.100256\n",
      "Training Epoch: 22 [384/50000]\tLoss: 3.8436\tLR: 2.100512\n",
      "Training Epoch: 22 [512/50000]\tLoss: 4.1873\tLR: 2.100767\n",
      "Training Epoch: 22 [640/50000]\tLoss: 4.1350\tLR: 2.101023\n",
      "Training Epoch: 22 [768/50000]\tLoss: 4.0846\tLR: 2.101279\n",
      "Training Epoch: 22 [896/50000]\tLoss: 3.9995\tLR: 2.101535\n",
      "Training Epoch: 22 [1024/50000]\tLoss: 4.1527\tLR: 2.101790\n",
      "Training Epoch: 22 [1152/50000]\tLoss: 3.9910\tLR: 2.102046\n",
      "Training Epoch: 22 [1280/50000]\tLoss: 4.1536\tLR: 2.102302\n",
      "Training Epoch: 22 [1408/50000]\tLoss: 4.1235\tLR: 2.102558\n",
      "Training Epoch: 22 [1536/50000]\tLoss: 4.1306\tLR: 2.102813\n",
      "Training Epoch: 22 [1664/50000]\tLoss: 4.2796\tLR: 2.103069\n",
      "Training Epoch: 22 [1792/50000]\tLoss: 4.1460\tLR: 2.103325\n",
      "Training Epoch: 22 [1920/50000]\tLoss: 3.9845\tLR: 2.103581\n",
      "Training Epoch: 22 [2048/50000]\tLoss: 4.2191\tLR: 2.103836\n",
      "Training Epoch: 22 [2176/50000]\tLoss: 4.0231\tLR: 2.104092\n",
      "Training Epoch: 22 [2304/50000]\tLoss: 4.1428\tLR: 2.104348\n",
      "Training Epoch: 22 [2432/50000]\tLoss: 4.0376\tLR: 2.104604\n",
      "Training Epoch: 22 [2560/50000]\tLoss: 4.1197\tLR: 2.104859\n",
      "Training Epoch: 22 [2688/50000]\tLoss: 3.9988\tLR: 2.105115\n",
      "Training Epoch: 22 [2816/50000]\tLoss: 4.1618\tLR: 2.105371\n",
      "Training Epoch: 22 [2944/50000]\tLoss: 4.2144\tLR: 2.105627\n",
      "Training Epoch: 22 [3072/50000]\tLoss: 3.9230\tLR: 2.105882\n",
      "Training Epoch: 22 [3200/50000]\tLoss: 4.0341\tLR: 2.106138\n",
      "Training Epoch: 22 [3328/50000]\tLoss: 4.1804\tLR: 2.106394\n",
      "Training Epoch: 22 [3456/50000]\tLoss: 4.0836\tLR: 2.106650\n",
      "Training Epoch: 22 [3584/50000]\tLoss: 4.0795\tLR: 2.106905\n",
      "Training Epoch: 22 [3712/50000]\tLoss: 4.0498\tLR: 2.107161\n",
      "Training Epoch: 22 [3840/50000]\tLoss: 4.1566\tLR: 2.107417\n",
      "Training Epoch: 22 [3968/50000]\tLoss: 4.0248\tLR: 2.107673\n",
      "Training Epoch: 22 [4096/50000]\tLoss: 4.0809\tLR: 2.107928\n",
      "Training Epoch: 22 [4224/50000]\tLoss: 4.1616\tLR: 2.108184\n",
      "Training Epoch: 22 [4352/50000]\tLoss: 4.0583\tLR: 2.108440\n",
      "Training Epoch: 22 [4480/50000]\tLoss: 4.0672\tLR: 2.108696\n",
      "Training Epoch: 22 [4608/50000]\tLoss: 4.0560\tLR: 2.108951\n",
      "Training Epoch: 22 [4736/50000]\tLoss: 4.0818\tLR: 2.109207\n",
      "Training Epoch: 22 [4864/50000]\tLoss: 4.0861\tLR: 2.109463\n",
      "Training Epoch: 22 [4992/50000]\tLoss: 4.0129\tLR: 2.109719\n",
      "Training Epoch: 22 [5120/50000]\tLoss: 4.2115\tLR: 2.109974\n",
      "Training Epoch: 22 [5248/50000]\tLoss: 4.1437\tLR: 2.110230\n",
      "Training Epoch: 22 [5376/50000]\tLoss: 4.0893\tLR: 2.110486\n",
      "Training Epoch: 22 [5504/50000]\tLoss: 3.8772\tLR: 2.110742\n",
      "Training Epoch: 22 [5632/50000]\tLoss: 3.9736\tLR: 2.110997\n",
      "Training Epoch: 22 [5760/50000]\tLoss: 4.1692\tLR: 2.111253\n",
      "Training Epoch: 22 [5888/50000]\tLoss: 4.0021\tLR: 2.111509\n",
      "Training Epoch: 22 [6016/50000]\tLoss: 3.9640\tLR: 2.111765\n",
      "Training Epoch: 22 [6144/50000]\tLoss: 4.0420\tLR: 2.112020\n",
      "Training Epoch: 22 [6272/50000]\tLoss: 3.9740\tLR: 2.112276\n",
      "Training Epoch: 22 [6400/50000]\tLoss: 3.8122\tLR: 2.112532\n",
      "Training Epoch: 22 [6528/50000]\tLoss: 4.1405\tLR: 2.112788\n",
      "Training Epoch: 22 [6656/50000]\tLoss: 4.0830\tLR: 2.113043\n",
      "Training Epoch: 22 [6784/50000]\tLoss: 3.9976\tLR: 2.113299\n",
      "Training Epoch: 22 [6912/50000]\tLoss: 3.8375\tLR: 2.113555\n",
      "Training Epoch: 22 [7040/50000]\tLoss: 4.0121\tLR: 2.113811\n",
      "Training Epoch: 22 [7168/50000]\tLoss: 4.1508\tLR: 2.114066\n",
      "Training Epoch: 22 [7296/50000]\tLoss: 4.0925\tLR: 2.114322\n",
      "Training Epoch: 22 [7424/50000]\tLoss: 3.8642\tLR: 2.114578\n",
      "Training Epoch: 22 [7552/50000]\tLoss: 4.0261\tLR: 2.114834\n",
      "Training Epoch: 22 [7680/50000]\tLoss: 4.0526\tLR: 2.115090\n",
      "Training Epoch: 22 [7808/50000]\tLoss: 4.1737\tLR: 2.115345\n",
      "Training Epoch: 22 [7936/50000]\tLoss: 4.1213\tLR: 2.115601\n",
      "Training Epoch: 22 [8064/50000]\tLoss: 3.9216\tLR: 2.115857\n",
      "Training Epoch: 22 [8192/50000]\tLoss: 3.9513\tLR: 2.116113\n",
      "Training Epoch: 22 [8320/50000]\tLoss: 4.1006\tLR: 2.116368\n",
      "Training Epoch: 22 [8448/50000]\tLoss: 4.1696\tLR: 2.116624\n",
      "Training Epoch: 22 [8576/50000]\tLoss: 4.0176\tLR: 2.116880\n",
      "Training Epoch: 22 [8704/50000]\tLoss: 4.1813\tLR: 2.117136\n",
      "Training Epoch: 22 [8832/50000]\tLoss: 4.0482\tLR: 2.117391\n",
      "Training Epoch: 22 [8960/50000]\tLoss: 4.0638\tLR: 2.117647\n",
      "Training Epoch: 22 [9088/50000]\tLoss: 4.1738\tLR: 2.117903\n",
      "Training Epoch: 22 [9216/50000]\tLoss: 4.1711\tLR: 2.118159\n",
      "Training Epoch: 22 [9344/50000]\tLoss: 4.1676\tLR: 2.118414\n",
      "Training Epoch: 22 [9472/50000]\tLoss: 4.1796\tLR: 2.118670\n",
      "Training Epoch: 22 [9600/50000]\tLoss: 4.1622\tLR: 2.118926\n",
      "Training Epoch: 22 [9728/50000]\tLoss: 4.0912\tLR: 2.119182\n",
      "Training Epoch: 22 [9856/50000]\tLoss: 3.9752\tLR: 2.119437\n",
      "Training Epoch: 22 [9984/50000]\tLoss: 4.0772\tLR: 2.119693\n",
      "Training Epoch: 22 [10112/50000]\tLoss: 3.8261\tLR: 2.119949\n",
      "Training Epoch: 22 [10240/50000]\tLoss: 3.9522\tLR: 2.120205\n",
      "Training Epoch: 22 [10368/50000]\tLoss: 4.1463\tLR: 2.120460\n",
      "Training Epoch: 22 [10496/50000]\tLoss: 4.0119\tLR: 2.120716\n",
      "Training Epoch: 22 [10624/50000]\tLoss: 4.1177\tLR: 2.120972\n",
      "Training Epoch: 22 [10752/50000]\tLoss: 4.1554\tLR: 2.121228\n",
      "Training Epoch: 22 [10880/50000]\tLoss: 4.3683\tLR: 2.121483\n",
      "Training Epoch: 22 [11008/50000]\tLoss: 4.2073\tLR: 2.121739\n",
      "Training Epoch: 22 [11136/50000]\tLoss: 4.1053\tLR: 2.121995\n",
      "Training Epoch: 22 [11264/50000]\tLoss: 4.0154\tLR: 2.122251\n",
      "Training Epoch: 22 [11392/50000]\tLoss: 4.0059\tLR: 2.122506\n",
      "Training Epoch: 22 [11520/50000]\tLoss: 4.1968\tLR: 2.122762\n",
      "Training Epoch: 22 [11648/50000]\tLoss: 4.1809\tLR: 2.123018\n",
      "Training Epoch: 22 [11776/50000]\tLoss: 4.2109\tLR: 2.123274\n",
      "Training Epoch: 22 [11904/50000]\tLoss: 3.9080\tLR: 2.123529\n",
      "Training Epoch: 22 [12032/50000]\tLoss: 3.8889\tLR: 2.123785\n",
      "Training Epoch: 22 [12160/50000]\tLoss: 3.9208\tLR: 2.124041\n",
      "Training Epoch: 22 [12288/50000]\tLoss: 3.9917\tLR: 2.124297\n",
      "Training Epoch: 22 [12416/50000]\tLoss: 4.2708\tLR: 2.124552\n",
      "Training Epoch: 22 [12544/50000]\tLoss: 3.9148\tLR: 2.124808\n",
      "Training Epoch: 22 [12672/50000]\tLoss: 4.1912\tLR: 2.125064\n",
      "Training Epoch: 22 [12800/50000]\tLoss: 4.0703\tLR: 2.125320\n",
      "Training Epoch: 22 [12928/50000]\tLoss: 3.9634\tLR: 2.125575\n",
      "Training Epoch: 22 [13056/50000]\tLoss: 4.2759\tLR: 2.125831\n",
      "Training Epoch: 22 [13184/50000]\tLoss: 4.1881\tLR: 2.126087\n",
      "Training Epoch: 22 [13312/50000]\tLoss: 4.0912\tLR: 2.126343\n",
      "Training Epoch: 22 [13440/50000]\tLoss: 4.1606\tLR: 2.126598\n",
      "Training Epoch: 22 [13568/50000]\tLoss: 4.1763\tLR: 2.126854\n",
      "Training Epoch: 22 [13696/50000]\tLoss: 4.1071\tLR: 2.127110\n",
      "Training Epoch: 22 [13824/50000]\tLoss: 4.1284\tLR: 2.127366\n",
      "Training Epoch: 22 [13952/50000]\tLoss: 4.1624\tLR: 2.127621\n",
      "Training Epoch: 22 [14080/50000]\tLoss: 3.9758\tLR: 2.127877\n",
      "Training Epoch: 22 [14208/50000]\tLoss: 3.8461\tLR: 2.128133\n",
      "Training Epoch: 22 [14336/50000]\tLoss: 3.9769\tLR: 2.128389\n",
      "Training Epoch: 22 [14464/50000]\tLoss: 4.2515\tLR: 2.128645\n",
      "Training Epoch: 22 [14592/50000]\tLoss: 3.8390\tLR: 2.128900\n",
      "Training Epoch: 22 [14720/50000]\tLoss: 4.0786\tLR: 2.129156\n",
      "Training Epoch: 22 [14848/50000]\tLoss: 4.1285\tLR: 2.129412\n",
      "Training Epoch: 22 [14976/50000]\tLoss: 3.9701\tLR: 2.129668\n",
      "Training Epoch: 22 [15104/50000]\tLoss: 4.1051\tLR: 2.129923\n",
      "Training Epoch: 22 [15232/50000]\tLoss: 4.1315\tLR: 2.130179\n",
      "Training Epoch: 22 [15360/50000]\tLoss: 4.0933\tLR: 2.130435\n",
      "Training Epoch: 22 [15488/50000]\tLoss: 4.0110\tLR: 2.130691\n",
      "Training Epoch: 22 [15616/50000]\tLoss: 4.0343\tLR: 2.130946\n",
      "Training Epoch: 22 [15744/50000]\tLoss: 4.1153\tLR: 2.131202\n",
      "Training Epoch: 22 [15872/50000]\tLoss: 3.8725\tLR: 2.131458\n",
      "Training Epoch: 22 [16000/50000]\tLoss: 4.2776\tLR: 2.131714\n",
      "Training Epoch: 22 [16128/50000]\tLoss: 3.9510\tLR: 2.131969\n",
      "Training Epoch: 22 [16256/50000]\tLoss: 3.8979\tLR: 2.132225\n",
      "Training Epoch: 22 [16384/50000]\tLoss: 4.2772\tLR: 2.132481\n",
      "Training Epoch: 22 [16512/50000]\tLoss: 3.8429\tLR: 2.132737\n",
      "Training Epoch: 22 [16640/50000]\tLoss: 4.0925\tLR: 2.132992\n",
      "Training Epoch: 22 [16768/50000]\tLoss: 4.0796\tLR: 2.133248\n",
      "Training Epoch: 22 [16896/50000]\tLoss: 3.8259\tLR: 2.133504\n",
      "Training Epoch: 22 [17024/50000]\tLoss: 3.9818\tLR: 2.133760\n",
      "Training Epoch: 22 [17152/50000]\tLoss: 4.3270\tLR: 2.134015\n",
      "Training Epoch: 22 [17280/50000]\tLoss: 3.8228\tLR: 2.134271\n",
      "Training Epoch: 22 [17408/50000]\tLoss: 3.9837\tLR: 2.134527\n",
      "Training Epoch: 22 [17536/50000]\tLoss: 3.8939\tLR: 2.134783\n",
      "Training Epoch: 22 [17664/50000]\tLoss: 4.3473\tLR: 2.135038\n",
      "Training Epoch: 22 [17792/50000]\tLoss: 4.1487\tLR: 2.135294\n",
      "Training Epoch: 22 [17920/50000]\tLoss: 3.8941\tLR: 2.135550\n",
      "Training Epoch: 22 [18048/50000]\tLoss: 4.0990\tLR: 2.135806\n",
      "Training Epoch: 22 [18176/50000]\tLoss: 3.9746\tLR: 2.136061\n",
      "Training Epoch: 22 [18304/50000]\tLoss: 3.9326\tLR: 2.136317\n",
      "Training Epoch: 22 [18432/50000]\tLoss: 4.1912\tLR: 2.136573\n",
      "Training Epoch: 22 [18560/50000]\tLoss: 4.3520\tLR: 2.136829\n",
      "Training Epoch: 22 [18688/50000]\tLoss: 4.1199\tLR: 2.137084\n",
      "Training Epoch: 22 [18816/50000]\tLoss: 4.1104\tLR: 2.137340\n",
      "Training Epoch: 22 [18944/50000]\tLoss: 4.1404\tLR: 2.137596\n",
      "Training Epoch: 22 [19072/50000]\tLoss: 4.1278\tLR: 2.137852\n",
      "Training Epoch: 22 [19200/50000]\tLoss: 4.1633\tLR: 2.138107\n",
      "Training Epoch: 22 [19328/50000]\tLoss: 4.2769\tLR: 2.138363\n",
      "Training Epoch: 22 [19456/50000]\tLoss: 4.2589\tLR: 2.138619\n",
      "Training Epoch: 22 [19584/50000]\tLoss: 4.2536\tLR: 2.138875\n",
      "Training Epoch: 22 [19712/50000]\tLoss: 4.2136\tLR: 2.139130\n",
      "Training Epoch: 22 [19840/50000]\tLoss: 4.0897\tLR: 2.139386\n",
      "Training Epoch: 22 [19968/50000]\tLoss: 4.1940\tLR: 2.139642\n",
      "Training Epoch: 22 [20096/50000]\tLoss: 4.0660\tLR: 2.139898\n",
      "Training Epoch: 22 [20224/50000]\tLoss: 3.9496\tLR: 2.140153\n",
      "Training Epoch: 22 [20352/50000]\tLoss: 4.2069\tLR: 2.140409\n",
      "Training Epoch: 22 [20480/50000]\tLoss: 3.9879\tLR: 2.140665\n",
      "Training Epoch: 22 [20608/50000]\tLoss: 4.0463\tLR: 2.140921\n",
      "Training Epoch: 22 [20736/50000]\tLoss: 4.0076\tLR: 2.141176\n",
      "Training Epoch: 22 [20864/50000]\tLoss: 4.2299\tLR: 2.141432\n",
      "Training Epoch: 22 [20992/50000]\tLoss: 3.8479\tLR: 2.141688\n",
      "Training Epoch: 22 [21120/50000]\tLoss: 3.9768\tLR: 2.141944\n",
      "Training Epoch: 22 [21248/50000]\tLoss: 4.0013\tLR: 2.142199\n",
      "Training Epoch: 22 [21376/50000]\tLoss: 3.9607\tLR: 2.142455\n",
      "Training Epoch: 22 [21504/50000]\tLoss: 3.9391\tLR: 2.142711\n",
      "Training Epoch: 22 [21632/50000]\tLoss: 3.9565\tLR: 2.142967\n",
      "Training Epoch: 22 [21760/50000]\tLoss: 4.1055\tLR: 2.143223\n",
      "Training Epoch: 22 [21888/50000]\tLoss: 3.9727\tLR: 2.143478\n",
      "Training Epoch: 22 [22016/50000]\tLoss: 3.9845\tLR: 2.143734\n",
      "Training Epoch: 22 [22144/50000]\tLoss: 3.9982\tLR: 2.143990\n",
      "Training Epoch: 22 [22272/50000]\tLoss: 4.0891\tLR: 2.144246\n",
      "Training Epoch: 22 [22400/50000]\tLoss: 3.8141\tLR: 2.144501\n",
      "Training Epoch: 22 [22528/50000]\tLoss: 4.1056\tLR: 2.144757\n",
      "Training Epoch: 22 [22656/50000]\tLoss: 4.1456\tLR: 2.145013\n",
      "Training Epoch: 22 [22784/50000]\tLoss: 4.4189\tLR: 2.145269\n",
      "Training Epoch: 22 [22912/50000]\tLoss: 4.1522\tLR: 2.145524\n",
      "Training Epoch: 22 [23040/50000]\tLoss: 4.0929\tLR: 2.145780\n",
      "Training Epoch: 22 [23168/50000]\tLoss: 4.2040\tLR: 2.146036\n",
      "Training Epoch: 22 [23296/50000]\tLoss: 4.1964\tLR: 2.146292\n",
      "Training Epoch: 22 [23424/50000]\tLoss: 4.1983\tLR: 2.146547\n",
      "Training Epoch: 22 [23552/50000]\tLoss: 4.1450\tLR: 2.146803\n",
      "Training Epoch: 22 [23680/50000]\tLoss: 4.1673\tLR: 2.147059\n",
      "Training Epoch: 22 [23808/50000]\tLoss: 4.2652\tLR: 2.147315\n",
      "Training Epoch: 22 [23936/50000]\tLoss: 4.2931\tLR: 2.147570\n",
      "Training Epoch: 22 [24064/50000]\tLoss: 4.1784\tLR: 2.147826\n",
      "Training Epoch: 22 [24192/50000]\tLoss: 4.0733\tLR: 2.148082\n",
      "Training Epoch: 22 [24320/50000]\tLoss: 4.1090\tLR: 2.148338\n",
      "Training Epoch: 22 [24448/50000]\tLoss: 4.0596\tLR: 2.148593\n",
      "Training Epoch: 22 [24576/50000]\tLoss: 4.1942\tLR: 2.148849\n",
      "Training Epoch: 22 [24704/50000]\tLoss: 4.1094\tLR: 2.149105\n",
      "Training Epoch: 22 [24832/50000]\tLoss: 4.0474\tLR: 2.149361\n",
      "Training Epoch: 22 [24960/50000]\tLoss: 4.1231\tLR: 2.149616\n",
      "Training Epoch: 22 [25088/50000]\tLoss: 4.0701\tLR: 2.149872\n",
      "Training Epoch: 22 [25216/50000]\tLoss: 4.2833\tLR: 2.150128\n",
      "Training Epoch: 22 [25344/50000]\tLoss: 4.0696\tLR: 2.150384\n",
      "Training Epoch: 22 [25472/50000]\tLoss: 3.9864\tLR: 2.150639\n",
      "Training Epoch: 22 [25600/50000]\tLoss: 4.1484\tLR: 2.150895\n",
      "Training Epoch: 22 [25728/50000]\tLoss: 3.9364\tLR: 2.151151\n",
      "Training Epoch: 22 [25856/50000]\tLoss: 3.8973\tLR: 2.151407\n",
      "Training Epoch: 22 [25984/50000]\tLoss: 4.3159\tLR: 2.151662\n",
      "Training Epoch: 22 [26112/50000]\tLoss: 4.0170\tLR: 2.151918\n",
      "Training Epoch: 22 [26240/50000]\tLoss: 4.1316\tLR: 2.152174\n",
      "Training Epoch: 22 [26368/50000]\tLoss: 4.0440\tLR: 2.152430\n",
      "Training Epoch: 22 [26496/50000]\tLoss: 4.0655\tLR: 2.152685\n",
      "Training Epoch: 22 [26624/50000]\tLoss: 4.1157\tLR: 2.152941\n",
      "Training Epoch: 22 [26752/50000]\tLoss: 3.9941\tLR: 2.153197\n",
      "Training Epoch: 22 [26880/50000]\tLoss: 4.1474\tLR: 2.153453\n",
      "Training Epoch: 22 [27008/50000]\tLoss: 4.0773\tLR: 2.153708\n",
      "Training Epoch: 22 [27136/50000]\tLoss: 3.9553\tLR: 2.153964\n",
      "Training Epoch: 22 [27264/50000]\tLoss: 4.2984\tLR: 2.154220\n",
      "Training Epoch: 22 [27392/50000]\tLoss: 4.0450\tLR: 2.154476\n",
      "Training Epoch: 22 [27520/50000]\tLoss: 4.0909\tLR: 2.154731\n",
      "Training Epoch: 22 [27648/50000]\tLoss: 4.1263\tLR: 2.154987\n",
      "Training Epoch: 22 [27776/50000]\tLoss: 3.9501\tLR: 2.155243\n",
      "Training Epoch: 22 [27904/50000]\tLoss: 4.1174\tLR: 2.155499\n",
      "Training Epoch: 22 [28032/50000]\tLoss: 4.2262\tLR: 2.155754\n",
      "Training Epoch: 22 [28160/50000]\tLoss: 4.0810\tLR: 2.156010\n",
      "Training Epoch: 22 [28288/50000]\tLoss: 4.1177\tLR: 2.156266\n",
      "Training Epoch: 22 [28416/50000]\tLoss: 4.1137\tLR: 2.156522\n",
      "Training Epoch: 22 [28544/50000]\tLoss: 4.3392\tLR: 2.156777\n",
      "Training Epoch: 22 [28672/50000]\tLoss: 3.9955\tLR: 2.157033\n",
      "Training Epoch: 22 [28800/50000]\tLoss: 4.0292\tLR: 2.157289\n",
      "Training Epoch: 22 [28928/50000]\tLoss: 4.2120\tLR: 2.157545\n",
      "Training Epoch: 22 [29056/50000]\tLoss: 4.0239\tLR: 2.157801\n",
      "Training Epoch: 22 [29184/50000]\tLoss: 4.0174\tLR: 2.158056\n",
      "Training Epoch: 22 [29312/50000]\tLoss: 4.1995\tLR: 2.158312\n",
      "Training Epoch: 22 [29440/50000]\tLoss: 4.2518\tLR: 2.158568\n",
      "Training Epoch: 22 [29568/50000]\tLoss: 4.3358\tLR: 2.158824\n",
      "Training Epoch: 22 [29696/50000]\tLoss: 3.9821\tLR: 2.159079\n",
      "Training Epoch: 22 [29824/50000]\tLoss: 4.2816\tLR: 2.159335\n",
      "Training Epoch: 22 [29952/50000]\tLoss: 4.1823\tLR: 2.159591\n",
      "Training Epoch: 22 [30080/50000]\tLoss: 4.2819\tLR: 2.159847\n",
      "Training Epoch: 22 [30208/50000]\tLoss: 4.2717\tLR: 2.160102\n",
      "Training Epoch: 22 [30336/50000]\tLoss: 4.2060\tLR: 2.160358\n",
      "Training Epoch: 22 [30464/50000]\tLoss: 4.1938\tLR: 2.160614\n",
      "Training Epoch: 22 [30592/50000]\tLoss: 4.2289\tLR: 2.160870\n",
      "Training Epoch: 22 [30720/50000]\tLoss: 4.2211\tLR: 2.161125\n",
      "Training Epoch: 22 [30848/50000]\tLoss: 4.1233\tLR: 2.161381\n",
      "Training Epoch: 22 [30976/50000]\tLoss: 4.2772\tLR: 2.161637\n",
      "Training Epoch: 22 [31104/50000]\tLoss: 4.1369\tLR: 2.161893\n",
      "Training Epoch: 22 [31232/50000]\tLoss: 3.9889\tLR: 2.162148\n",
      "Training Epoch: 22 [31360/50000]\tLoss: 4.1899\tLR: 2.162404\n",
      "Training Epoch: 22 [31488/50000]\tLoss: 4.0696\tLR: 2.162660\n",
      "Training Epoch: 22 [31616/50000]\tLoss: 4.0277\tLR: 2.162916\n",
      "Training Epoch: 22 [31744/50000]\tLoss: 4.0339\tLR: 2.163171\n",
      "Training Epoch: 22 [31872/50000]\tLoss: 4.0330\tLR: 2.163427\n",
      "Training Epoch: 22 [32000/50000]\tLoss: 3.9063\tLR: 2.163683\n",
      "Training Epoch: 22 [32128/50000]\tLoss: 4.2577\tLR: 2.163939\n",
      "Training Epoch: 22 [32256/50000]\tLoss: 3.9983\tLR: 2.164194\n",
      "Training Epoch: 22 [32384/50000]\tLoss: 3.9657\tLR: 2.164450\n",
      "Training Epoch: 22 [32512/50000]\tLoss: 4.0974\tLR: 2.164706\n",
      "Training Epoch: 22 [32640/50000]\tLoss: 4.0157\tLR: 2.164962\n",
      "Training Epoch: 22 [32768/50000]\tLoss: 4.0146\tLR: 2.165217\n",
      "Training Epoch: 22 [32896/50000]\tLoss: 4.0319\tLR: 2.165473\n",
      "Training Epoch: 22 [33024/50000]\tLoss: 3.9731\tLR: 2.165729\n",
      "Training Epoch: 22 [33152/50000]\tLoss: 3.9056\tLR: 2.165985\n",
      "Training Epoch: 22 [33280/50000]\tLoss: 4.0885\tLR: 2.166240\n",
      "Training Epoch: 22 [33408/50000]\tLoss: 4.0547\tLR: 2.166496\n",
      "Training Epoch: 22 [33536/50000]\tLoss: 3.9869\tLR: 2.166752\n",
      "Training Epoch: 22 [33664/50000]\tLoss: 4.0185\tLR: 2.167008\n",
      "Training Epoch: 22 [33792/50000]\tLoss: 4.2056\tLR: 2.167263\n",
      "Training Epoch: 22 [33920/50000]\tLoss: 4.2209\tLR: 2.167519\n",
      "Training Epoch: 22 [34048/50000]\tLoss: 4.0395\tLR: 2.167775\n",
      "Training Epoch: 22 [34176/50000]\tLoss: 4.1298\tLR: 2.168031\n",
      "Training Epoch: 22 [34304/50000]\tLoss: 4.1348\tLR: 2.168286\n",
      "Training Epoch: 22 [34432/50000]\tLoss: 4.1332\tLR: 2.168542\n",
      "Training Epoch: 22 [34560/50000]\tLoss: 4.1640\tLR: 2.168798\n",
      "Training Epoch: 22 [34688/50000]\tLoss: 4.1170\tLR: 2.169054\n",
      "Training Epoch: 22 [34816/50000]\tLoss: 4.0947\tLR: 2.169309\n",
      "Training Epoch: 22 [34944/50000]\tLoss: 4.0838\tLR: 2.169565\n",
      "Training Epoch: 22 [35072/50000]\tLoss: 3.9609\tLR: 2.169821\n",
      "Training Epoch: 22 [35200/50000]\tLoss: 4.2748\tLR: 2.170077\n",
      "Training Epoch: 22 [35328/50000]\tLoss: 4.0509\tLR: 2.170332\n",
      "Training Epoch: 22 [35456/50000]\tLoss: 4.2008\tLR: 2.170588\n",
      "Training Epoch: 22 [35584/50000]\tLoss: 4.2155\tLR: 2.170844\n",
      "Training Epoch: 22 [35712/50000]\tLoss: 4.0232\tLR: 2.171100\n",
      "Training Epoch: 22 [35840/50000]\tLoss: 4.1923\tLR: 2.171355\n",
      "Training Epoch: 22 [35968/50000]\tLoss: 4.1651\tLR: 2.171611\n",
      "Training Epoch: 22 [36096/50000]\tLoss: 4.1879\tLR: 2.171867\n",
      "Training Epoch: 22 [36224/50000]\tLoss: 4.1723\tLR: 2.172123\n",
      "Training Epoch: 22 [36352/50000]\tLoss: 4.1514\tLR: 2.172379\n",
      "Training Epoch: 22 [36480/50000]\tLoss: 4.0136\tLR: 2.172634\n",
      "Training Epoch: 22 [36608/50000]\tLoss: 4.1526\tLR: 2.172890\n",
      "Training Epoch: 22 [36736/50000]\tLoss: 4.1647\tLR: 2.173146\n",
      "Training Epoch: 22 [36864/50000]\tLoss: 4.1022\tLR: 2.173402\n",
      "Training Epoch: 22 [36992/50000]\tLoss: 4.2894\tLR: 2.173657\n",
      "Training Epoch: 22 [37120/50000]\tLoss: 4.0499\tLR: 2.173913\n",
      "Training Epoch: 22 [37248/50000]\tLoss: 4.1667\tLR: 2.174169\n",
      "Training Epoch: 22 [37376/50000]\tLoss: 4.1696\tLR: 2.174425\n",
      "Training Epoch: 22 [37504/50000]\tLoss: 4.0000\tLR: 2.174680\n",
      "Training Epoch: 22 [37632/50000]\tLoss: 3.9847\tLR: 2.174936\n",
      "Training Epoch: 22 [37760/50000]\tLoss: 4.1548\tLR: 2.175192\n",
      "Training Epoch: 22 [37888/50000]\tLoss: 4.2128\tLR: 2.175448\n",
      "Training Epoch: 22 [38016/50000]\tLoss: 4.0480\tLR: 2.175703\n",
      "Training Epoch: 22 [38144/50000]\tLoss: 4.3115\tLR: 2.175959\n",
      "Training Epoch: 22 [38272/50000]\tLoss: 4.2580\tLR: 2.176215\n",
      "Training Epoch: 22 [38400/50000]\tLoss: 4.2058\tLR: 2.176471\n",
      "Training Epoch: 22 [38528/50000]\tLoss: 4.1941\tLR: 2.176726\n",
      "Training Epoch: 22 [38656/50000]\tLoss: 4.2865\tLR: 2.176982\n",
      "Training Epoch: 22 [38784/50000]\tLoss: 4.2542\tLR: 2.177238\n",
      "Training Epoch: 22 [38912/50000]\tLoss: 4.2106\tLR: 2.177494\n",
      "Training Epoch: 22 [39040/50000]\tLoss: 4.3009\tLR: 2.177749\n",
      "Training Epoch: 22 [39168/50000]\tLoss: 4.2620\tLR: 2.178005\n",
      "Training Epoch: 22 [39296/50000]\tLoss: 4.3737\tLR: 2.178261\n",
      "Training Epoch: 22 [39424/50000]\tLoss: 4.2796\tLR: 2.178517\n",
      "Training Epoch: 22 [39552/50000]\tLoss: 3.8940\tLR: 2.178772\n",
      "Training Epoch: 22 [39680/50000]\tLoss: 4.3195\tLR: 2.179028\n",
      "Training Epoch: 22 [39808/50000]\tLoss: 4.0785\tLR: 2.179284\n",
      "Training Epoch: 22 [39936/50000]\tLoss: 4.2752\tLR: 2.179540\n",
      "Training Epoch: 22 [40064/50000]\tLoss: 4.1989\tLR: 2.179795\n",
      "Training Epoch: 22 [40192/50000]\tLoss: 4.1357\tLR: 2.180051\n",
      "Training Epoch: 22 [40320/50000]\tLoss: 4.0058\tLR: 2.180307\n",
      "Training Epoch: 22 [40448/50000]\tLoss: 4.2610\tLR: 2.180563\n",
      "Training Epoch: 22 [40576/50000]\tLoss: 4.1812\tLR: 2.180818\n",
      "Training Epoch: 22 [40704/50000]\tLoss: 4.4045\tLR: 2.181074\n",
      "Training Epoch: 22 [40832/50000]\tLoss: 4.0532\tLR: 2.181330\n",
      "Training Epoch: 22 [40960/50000]\tLoss: 4.1663\tLR: 2.181586\n",
      "Training Epoch: 22 [41088/50000]\tLoss: 4.3368\tLR: 2.181841\n",
      "Training Epoch: 22 [41216/50000]\tLoss: 4.1831\tLR: 2.182097\n",
      "Training Epoch: 22 [41344/50000]\tLoss: 4.1512\tLR: 2.182353\n",
      "Training Epoch: 22 [41472/50000]\tLoss: 4.2716\tLR: 2.182609\n",
      "Training Epoch: 22 [41600/50000]\tLoss: 4.1839\tLR: 2.182864\n",
      "Training Epoch: 22 [41728/50000]\tLoss: 4.0194\tLR: 2.183120\n",
      "Training Epoch: 22 [41856/50000]\tLoss: 4.1639\tLR: 2.183376\n",
      "Training Epoch: 22 [41984/50000]\tLoss: 4.1189\tLR: 2.183632\n",
      "Training Epoch: 22 [42112/50000]\tLoss: 4.0416\tLR: 2.183887\n",
      "Training Epoch: 22 [42240/50000]\tLoss: 4.0742\tLR: 2.184143\n",
      "Training Epoch: 22 [42368/50000]\tLoss: 4.1129\tLR: 2.184399\n",
      "Training Epoch: 22 [42496/50000]\tLoss: 4.1902\tLR: 2.184655\n",
      "Training Epoch: 22 [42624/50000]\tLoss: 4.2313\tLR: 2.184910\n",
      "Training Epoch: 22 [42752/50000]\tLoss: 4.2994\tLR: 2.185166\n",
      "Training Epoch: 22 [42880/50000]\tLoss: 4.0419\tLR: 2.185422\n",
      "Training Epoch: 22 [43008/50000]\tLoss: 4.1624\tLR: 2.185678\n",
      "Training Epoch: 22 [43136/50000]\tLoss: 4.3545\tLR: 2.185934\n",
      "Training Epoch: 22 [43264/50000]\tLoss: 4.1112\tLR: 2.186189\n",
      "Training Epoch: 22 [43392/50000]\tLoss: 4.2887\tLR: 2.186445\n",
      "Training Epoch: 22 [43520/50000]\tLoss: 4.2316\tLR: 2.186701\n",
      "Training Epoch: 22 [43648/50000]\tLoss: 4.1922\tLR: 2.186957\n",
      "Training Epoch: 22 [43776/50000]\tLoss: 4.1851\tLR: 2.187212\n",
      "Training Epoch: 22 [43904/50000]\tLoss: 4.1707\tLR: 2.187468\n",
      "Training Epoch: 22 [44032/50000]\tLoss: 4.2202\tLR: 2.187724\n",
      "Training Epoch: 22 [44160/50000]\tLoss: 4.0787\tLR: 2.187980\n",
      "Training Epoch: 22 [44288/50000]\tLoss: 4.1230\tLR: 2.188235\n",
      "Training Epoch: 22 [44416/50000]\tLoss: 4.1258\tLR: 2.188491\n",
      "Training Epoch: 22 [44544/50000]\tLoss: 4.0742\tLR: 2.188747\n",
      "Training Epoch: 22 [44672/50000]\tLoss: 4.1612\tLR: 2.189003\n",
      "Training Epoch: 22 [44800/50000]\tLoss: 4.0977\tLR: 2.189258\n",
      "Training Epoch: 22 [44928/50000]\tLoss: 4.0295\tLR: 2.189514\n",
      "Training Epoch: 22 [45056/50000]\tLoss: 4.1049\tLR: 2.189770\n",
      "Training Epoch: 22 [45184/50000]\tLoss: 4.1526\tLR: 2.190026\n",
      "Training Epoch: 22 [45312/50000]\tLoss: 4.1669\tLR: 2.190281\n",
      "Training Epoch: 22 [45440/50000]\tLoss: 4.0458\tLR: 2.190537\n",
      "Training Epoch: 22 [45568/50000]\tLoss: 4.1550\tLR: 2.190793\n",
      "Training Epoch: 22 [45696/50000]\tLoss: 4.2316\tLR: 2.191049\n",
      "Training Epoch: 22 [45824/50000]\tLoss: 4.1797\tLR: 2.191304\n",
      "Training Epoch: 22 [45952/50000]\tLoss: 4.1735\tLR: 2.191560\n",
      "Training Epoch: 22 [46080/50000]\tLoss: 4.2208\tLR: 2.191816\n",
      "Training Epoch: 22 [46208/50000]\tLoss: 4.0608\tLR: 2.192072\n",
      "Training Epoch: 22 [46336/50000]\tLoss: 4.2417\tLR: 2.192327\n",
      "Training Epoch: 22 [46464/50000]\tLoss: 4.3256\tLR: 2.192583\n",
      "Training Epoch: 22 [46592/50000]\tLoss: 4.2144\tLR: 2.192839\n",
      "Training Epoch: 22 [46720/50000]\tLoss: 4.1665\tLR: 2.193095\n",
      "Training Epoch: 22 [46848/50000]\tLoss: 4.1389\tLR: 2.193350\n",
      "Training Epoch: 22 [46976/50000]\tLoss: 3.9379\tLR: 2.193606\n",
      "Training Epoch: 22 [47104/50000]\tLoss: 4.2512\tLR: 2.193862\n",
      "Training Epoch: 22 [47232/50000]\tLoss: 4.0925\tLR: 2.194118\n",
      "Training Epoch: 22 [47360/50000]\tLoss: 4.2617\tLR: 2.194373\n",
      "Training Epoch: 22 [47488/50000]\tLoss: 4.1263\tLR: 2.194629\n",
      "Training Epoch: 22 [47616/50000]\tLoss: 4.1497\tLR: 2.194885\n",
      "Training Epoch: 22 [47744/50000]\tLoss: 4.2142\tLR: 2.195141\n",
      "Training Epoch: 22 [47872/50000]\tLoss: 4.1724\tLR: 2.195396\n",
      "Training Epoch: 22 [48000/50000]\tLoss: 4.0618\tLR: 2.195652\n",
      "Training Epoch: 22 [48128/50000]\tLoss: 4.1013\tLR: 2.195908\n",
      "Training Epoch: 22 [48256/50000]\tLoss: 4.2496\tLR: 2.196164\n",
      "Training Epoch: 22 [48384/50000]\tLoss: 4.2916\tLR: 2.196419\n",
      "Training Epoch: 22 [48512/50000]\tLoss: 4.1588\tLR: 2.196675\n",
      "Training Epoch: 22 [48640/50000]\tLoss: 4.0588\tLR: 2.196931\n",
      "Training Epoch: 22 [48768/50000]\tLoss: 3.9955\tLR: 2.197187\n",
      "Training Epoch: 22 [48896/50000]\tLoss: 4.0507\tLR: 2.197442\n",
      "Training Epoch: 22 [49024/50000]\tLoss: 3.9542\tLR: 2.197698\n",
      "Training Epoch: 22 [49152/50000]\tLoss: 3.8269\tLR: 2.197954\n",
      "Training Epoch: 22 [49280/50000]\tLoss: 4.1050\tLR: 2.198210\n",
      "Training Epoch: 22 [49408/50000]\tLoss: 4.0719\tLR: 2.198465\n",
      "Training Epoch: 22 [49536/50000]\tLoss: 4.0799\tLR: 2.198721\n",
      "Training Epoch: 22 [49664/50000]\tLoss: 4.1568\tLR: 2.198977\n",
      "Training Epoch: 22 [49792/50000]\tLoss: 4.1723\tLR: 2.199233\n",
      "Training Epoch: 22 [49920/50000]\tLoss: 4.2077\tLR: 2.199488\n",
      "Training Epoch: 22 [50000/50000]\tLoss: 4.0083\tLR: 2.199744\n",
      "epoch 22 training time consumed: 488.95s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   30843 GB |   30843 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   30749 GB |   30748 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      94 GB |      94 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   30843 GB |   30843 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   30749 GB |   30748 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      94 GB |      94 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   30407 GB |   30407 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   30312 GB |   30312 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |      94 GB |      94 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    3270 K  |    3270 K  |\n",
      "|       from large pool |      24    |      65    |    1394 K  |    1394 K  |\n",
      "|       from small pool |     231    |     274    |    1876 K  |    1876 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    3270 K  |    3270 K  |\n",
      "|       from large pool |      24    |      65    |    1394 K  |    1394 K  |\n",
      "|       from small pool |     231    |     274    |    1876 K  |    1876 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      46    |    1894 K  |    1894 K  |\n",
      "|       from large pool |      10    |      23    |     670 K  |     670 K  |\n",
      "|       from small pool |      25    |      35    |    1224 K  |    1224 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 22, Average loss: 0.0357, Accuracy: 0.0381, Time consumed:31.27s\n",
      "\n",
      "Training Epoch: 23 [128/50000]\tLoss: 4.0734\tLR: 0.100000\n",
      "Training Epoch: 23 [256/50000]\tLoss: 4.1687\tLR: 2.200256\n",
      "Training Epoch: 23 [384/50000]\tLoss: 3.9778\tLR: 2.200512\n",
      "Training Epoch: 23 [512/50000]\tLoss: 4.1071\tLR: 2.200767\n",
      "Training Epoch: 23 [640/50000]\tLoss: 3.9859\tLR: 2.201023\n",
      "Training Epoch: 23 [768/50000]\tLoss: 4.1334\tLR: 2.201279\n",
      "Training Epoch: 23 [896/50000]\tLoss: 4.0201\tLR: 2.201535\n",
      "Training Epoch: 23 [1024/50000]\tLoss: 4.2753\tLR: 2.201790\n",
      "Training Epoch: 23 [1152/50000]\tLoss: 4.1668\tLR: 2.202046\n",
      "Training Epoch: 23 [1280/50000]\tLoss: 4.0483\tLR: 2.202302\n",
      "Training Epoch: 23 [1408/50000]\tLoss: 4.1187\tLR: 2.202558\n",
      "Training Epoch: 23 [1536/50000]\tLoss: 4.1502\tLR: 2.202813\n",
      "Training Epoch: 23 [1664/50000]\tLoss: 4.0022\tLR: 2.203069\n",
      "Training Epoch: 23 [1792/50000]\tLoss: 4.2747\tLR: 2.203325\n",
      "Training Epoch: 23 [1920/50000]\tLoss: 4.1889\tLR: 2.203581\n",
      "Training Epoch: 23 [2048/50000]\tLoss: 4.0196\tLR: 2.203836\n",
      "Training Epoch: 23 [2176/50000]\tLoss: 4.3047\tLR: 2.204092\n",
      "Training Epoch: 23 [2304/50000]\tLoss: 4.1991\tLR: 2.204348\n",
      "Training Epoch: 23 [2432/50000]\tLoss: 4.2208\tLR: 2.204604\n",
      "Training Epoch: 23 [2560/50000]\tLoss: 4.1579\tLR: 2.204859\n",
      "Training Epoch: 23 [2688/50000]\tLoss: 4.1583\tLR: 2.205115\n",
      "Training Epoch: 23 [2816/50000]\tLoss: 4.0479\tLR: 2.205371\n",
      "Training Epoch: 23 [2944/50000]\tLoss: 4.0794\tLR: 2.205627\n",
      "Training Epoch: 23 [3072/50000]\tLoss: 4.0171\tLR: 2.205882\n",
      "Training Epoch: 23 [3200/50000]\tLoss: 4.1739\tLR: 2.206138\n",
      "Training Epoch: 23 [3328/50000]\tLoss: 3.9151\tLR: 2.206394\n",
      "Training Epoch: 23 [3456/50000]\tLoss: 4.0776\tLR: 2.206650\n",
      "Training Epoch: 23 [3584/50000]\tLoss: 4.0887\tLR: 2.206905\n",
      "Training Epoch: 23 [3712/50000]\tLoss: 4.0108\tLR: 2.207161\n",
      "Training Epoch: 23 [3840/50000]\tLoss: 4.1100\tLR: 2.207417\n",
      "Training Epoch: 23 [3968/50000]\tLoss: 4.0639\tLR: 2.207673\n",
      "Training Epoch: 23 [4096/50000]\tLoss: 4.0807\tLR: 2.207928\n",
      "Training Epoch: 23 [4224/50000]\tLoss: 4.1168\tLR: 2.208184\n",
      "Training Epoch: 23 [4352/50000]\tLoss: 4.0775\tLR: 2.208440\n",
      "Training Epoch: 23 [4480/50000]\tLoss: 4.0868\tLR: 2.208696\n",
      "Training Epoch: 23 [4608/50000]\tLoss: 3.9676\tLR: 2.208951\n",
      "Training Epoch: 23 [4736/50000]\tLoss: 4.0166\tLR: 2.209207\n",
      "Training Epoch: 23 [4864/50000]\tLoss: 4.2224\tLR: 2.209463\n",
      "Training Epoch: 23 [4992/50000]\tLoss: 4.0051\tLR: 2.209719\n",
      "Training Epoch: 23 [5120/50000]\tLoss: 4.1984\tLR: 2.209974\n",
      "Training Epoch: 23 [5248/50000]\tLoss: 4.3613\tLR: 2.210230\n",
      "Training Epoch: 23 [5376/50000]\tLoss: 4.2154\tLR: 2.210486\n",
      "Training Epoch: 23 [5504/50000]\tLoss: 4.2694\tLR: 2.210742\n",
      "Training Epoch: 23 [5632/50000]\tLoss: 4.1802\tLR: 2.210997\n",
      "Training Epoch: 23 [5760/50000]\tLoss: 4.1398\tLR: 2.211253\n",
      "Training Epoch: 23 [5888/50000]\tLoss: 4.2676\tLR: 2.211509\n",
      "Training Epoch: 23 [6016/50000]\tLoss: 4.0598\tLR: 2.211765\n",
      "Training Epoch: 23 [6144/50000]\tLoss: 4.1496\tLR: 2.212020\n",
      "Training Epoch: 23 [6272/50000]\tLoss: 4.1473\tLR: 2.212276\n",
      "Training Epoch: 23 [6400/50000]\tLoss: 4.1565\tLR: 2.212532\n",
      "Training Epoch: 23 [6528/50000]\tLoss: 4.1676\tLR: 2.212788\n",
      "Training Epoch: 23 [6656/50000]\tLoss: 4.2156\tLR: 2.213043\n",
      "Training Epoch: 23 [6784/50000]\tLoss: 3.9953\tLR: 2.213299\n",
      "Training Epoch: 23 [6912/50000]\tLoss: 4.0795\tLR: 2.213555\n",
      "Training Epoch: 23 [7040/50000]\tLoss: 4.0897\tLR: 2.213811\n",
      "Training Epoch: 23 [7168/50000]\tLoss: 3.9867\tLR: 2.214066\n",
      "Training Epoch: 23 [7296/50000]\tLoss: 4.0848\tLR: 2.214322\n",
      "Training Epoch: 23 [7424/50000]\tLoss: 4.0232\tLR: 2.214578\n",
      "Training Epoch: 23 [7552/50000]\tLoss: 4.1999\tLR: 2.214834\n",
      "Training Epoch: 23 [7680/50000]\tLoss: 3.9769\tLR: 2.215090\n",
      "Training Epoch: 23 [7808/50000]\tLoss: 3.9337\tLR: 2.215345\n",
      "Training Epoch: 23 [7936/50000]\tLoss: 4.0591\tLR: 2.215601\n",
      "Training Epoch: 23 [8064/50000]\tLoss: 4.1996\tLR: 2.215857\n",
      "Training Epoch: 23 [8192/50000]\tLoss: 4.1200\tLR: 2.216113\n",
      "Training Epoch: 23 [8320/50000]\tLoss: 4.1751\tLR: 2.216368\n",
      "Training Epoch: 23 [8448/50000]\tLoss: 4.1859\tLR: 2.216624\n",
      "Training Epoch: 23 [8576/50000]\tLoss: 4.2477\tLR: 2.216880\n",
      "Training Epoch: 23 [8704/50000]\tLoss: 4.0755\tLR: 2.217136\n",
      "Training Epoch: 23 [8832/50000]\tLoss: 4.4219\tLR: 2.217391\n",
      "Training Epoch: 23 [8960/50000]\tLoss: 4.1581\tLR: 2.217647\n",
      "Training Epoch: 23 [9088/50000]\tLoss: 4.3022\tLR: 2.217903\n",
      "Training Epoch: 23 [9216/50000]\tLoss: 4.1787\tLR: 2.218159\n",
      "Training Epoch: 23 [9344/50000]\tLoss: 4.1424\tLR: 2.218414\n",
      "Training Epoch: 23 [9472/50000]\tLoss: 4.0854\tLR: 2.218670\n",
      "Training Epoch: 23 [9600/50000]\tLoss: 4.2524\tLR: 2.218926\n",
      "Training Epoch: 23 [9728/50000]\tLoss: 4.1174\tLR: 2.219182\n",
      "Training Epoch: 23 [9856/50000]\tLoss: 4.0059\tLR: 2.219437\n",
      "Training Epoch: 23 [9984/50000]\tLoss: 4.3676\tLR: 2.219693\n",
      "Training Epoch: 23 [10112/50000]\tLoss: 4.1959\tLR: 2.219949\n",
      "Training Epoch: 23 [10240/50000]\tLoss: 4.1233\tLR: 2.220205\n",
      "Training Epoch: 23 [10368/50000]\tLoss: 4.2859\tLR: 2.220460\n",
      "Training Epoch: 23 [10496/50000]\tLoss: 4.3171\tLR: 2.220716\n",
      "Training Epoch: 23 [10624/50000]\tLoss: 4.0905\tLR: 2.220972\n",
      "Training Epoch: 23 [10752/50000]\tLoss: 4.2452\tLR: 2.221228\n",
      "Training Epoch: 23 [10880/50000]\tLoss: 4.2072\tLR: 2.221483\n",
      "Training Epoch: 23 [11008/50000]\tLoss: 4.3013\tLR: 2.221739\n",
      "Training Epoch: 23 [11136/50000]\tLoss: 4.1541\tLR: 2.221995\n",
      "Training Epoch: 23 [11264/50000]\tLoss: 4.2284\tLR: 2.222251\n",
      "Training Epoch: 23 [11392/50000]\tLoss: 4.2533\tLR: 2.222506\n",
      "Training Epoch: 23 [11520/50000]\tLoss: 4.2257\tLR: 2.222762\n",
      "Training Epoch: 23 [11648/50000]\tLoss: 4.3827\tLR: 2.223018\n",
      "Training Epoch: 23 [11776/50000]\tLoss: 4.3315\tLR: 2.223274\n",
      "Training Epoch: 23 [11904/50000]\tLoss: 4.2945\tLR: 2.223529\n",
      "Training Epoch: 23 [12032/50000]\tLoss: 4.2629\tLR: 2.223785\n",
      "Training Epoch: 23 [12160/50000]\tLoss: 4.2137\tLR: 2.224041\n",
      "Training Epoch: 23 [12288/50000]\tLoss: 4.2662\tLR: 2.224297\n",
      "Training Epoch: 23 [12416/50000]\tLoss: 4.1144\tLR: 2.224552\n",
      "Training Epoch: 23 [12544/50000]\tLoss: 4.1343\tLR: 2.224808\n",
      "Training Epoch: 23 [12672/50000]\tLoss: 4.3964\tLR: 2.225064\n",
      "Training Epoch: 23 [12800/50000]\tLoss: 4.3350\tLR: 2.225320\n",
      "Training Epoch: 23 [12928/50000]\tLoss: 4.3337\tLR: 2.225575\n",
      "Training Epoch: 23 [13056/50000]\tLoss: 4.0184\tLR: 2.225831\n",
      "Training Epoch: 23 [13184/50000]\tLoss: 4.2153\tLR: 2.226087\n",
      "Training Epoch: 23 [13312/50000]\tLoss: 4.2552\tLR: 2.226343\n",
      "Training Epoch: 23 [13440/50000]\tLoss: 4.3133\tLR: 2.226598\n",
      "Training Epoch: 23 [13568/50000]\tLoss: 4.1121\tLR: 2.226854\n",
      "Training Epoch: 23 [13696/50000]\tLoss: 4.2632\tLR: 2.227110\n",
      "Training Epoch: 23 [13824/50000]\tLoss: 4.1590\tLR: 2.227366\n",
      "Training Epoch: 23 [13952/50000]\tLoss: 4.2864\tLR: 2.227621\n",
      "Training Epoch: 23 [14080/50000]\tLoss: 4.1270\tLR: 2.227877\n",
      "Training Epoch: 23 [14208/50000]\tLoss: 4.3858\tLR: 2.228133\n",
      "Training Epoch: 23 [14336/50000]\tLoss: 4.3295\tLR: 2.228389\n",
      "Training Epoch: 23 [14464/50000]\tLoss: 4.1535\tLR: 2.228645\n",
      "Training Epoch: 23 [14592/50000]\tLoss: 4.2658\tLR: 2.228900\n",
      "Training Epoch: 23 [14720/50000]\tLoss: 4.1949\tLR: 2.229156\n",
      "Training Epoch: 23 [14848/50000]\tLoss: 4.2160\tLR: 2.229412\n",
      "Training Epoch: 23 [14976/50000]\tLoss: 4.2741\tLR: 2.229668\n",
      "Training Epoch: 23 [15104/50000]\tLoss: 4.2545\tLR: 2.229923\n",
      "Training Epoch: 23 [15232/50000]\tLoss: 3.9957\tLR: 2.230179\n",
      "Training Epoch: 23 [15360/50000]\tLoss: 4.0115\tLR: 2.230435\n",
      "Training Epoch: 23 [15488/50000]\tLoss: 4.3116\tLR: 2.230691\n",
      "Training Epoch: 23 [15616/50000]\tLoss: 4.3494\tLR: 2.230946\n",
      "Training Epoch: 23 [15744/50000]\tLoss: 4.3415\tLR: 2.231202\n",
      "Training Epoch: 23 [15872/50000]\tLoss: 4.2188\tLR: 2.231458\n",
      "Training Epoch: 23 [16000/50000]\tLoss: 4.0350\tLR: 2.231714\n",
      "Training Epoch: 23 [16128/50000]\tLoss: 4.1487\tLR: 2.231969\n",
      "Training Epoch: 23 [16256/50000]\tLoss: 4.1901\tLR: 2.232225\n",
      "Training Epoch: 23 [16384/50000]\tLoss: 4.2116\tLR: 2.232481\n",
      "Training Epoch: 23 [16512/50000]\tLoss: 4.2037\tLR: 2.232737\n",
      "Training Epoch: 23 [16640/50000]\tLoss: 4.1110\tLR: 2.232992\n",
      "Training Epoch: 23 [16768/50000]\tLoss: 4.0332\tLR: 2.233248\n",
      "Training Epoch: 23 [16896/50000]\tLoss: 4.1254\tLR: 2.233504\n",
      "Training Epoch: 23 [17024/50000]\tLoss: 4.0697\tLR: 2.233760\n",
      "Training Epoch: 23 [17152/50000]\tLoss: 4.4468\tLR: 2.234015\n",
      "Training Epoch: 23 [17280/50000]\tLoss: 4.2390\tLR: 2.234271\n",
      "Training Epoch: 23 [17408/50000]\tLoss: 4.1448\tLR: 2.234527\n",
      "Training Epoch: 23 [17536/50000]\tLoss: 4.0753\tLR: 2.234783\n",
      "Training Epoch: 23 [17664/50000]\tLoss: 4.3806\tLR: 2.235038\n",
      "Training Epoch: 23 [17792/50000]\tLoss: 4.1835\tLR: 2.235294\n",
      "Training Epoch: 23 [17920/50000]\tLoss: 4.2125\tLR: 2.235550\n",
      "Training Epoch: 23 [18048/50000]\tLoss: 4.0432\tLR: 2.235806\n",
      "Training Epoch: 23 [18176/50000]\tLoss: 4.1998\tLR: 2.236061\n",
      "Training Epoch: 23 [18304/50000]\tLoss: 4.2718\tLR: 2.236317\n",
      "Training Epoch: 23 [18432/50000]\tLoss: 4.1678\tLR: 2.236573\n",
      "Training Epoch: 23 [18560/50000]\tLoss: 4.0826\tLR: 2.236829\n",
      "Training Epoch: 23 [18688/50000]\tLoss: 4.2066\tLR: 2.237084\n",
      "Training Epoch: 23 [18816/50000]\tLoss: 4.1646\tLR: 2.237340\n",
      "Training Epoch: 23 [18944/50000]\tLoss: 4.1212\tLR: 2.237596\n",
      "Training Epoch: 23 [19072/50000]\tLoss: 4.1268\tLR: 2.237852\n",
      "Training Epoch: 23 [19200/50000]\tLoss: 4.1915\tLR: 2.238107\n",
      "Training Epoch: 23 [19328/50000]\tLoss: 4.1966\tLR: 2.238363\n",
      "Training Epoch: 23 [19456/50000]\tLoss: 4.2130\tLR: 2.238619\n",
      "Training Epoch: 23 [19584/50000]\tLoss: 4.1861\tLR: 2.238875\n",
      "Training Epoch: 23 [19712/50000]\tLoss: 4.1563\tLR: 2.239130\n",
      "Training Epoch: 23 [19840/50000]\tLoss: 4.2296\tLR: 2.239386\n",
      "Training Epoch: 23 [19968/50000]\tLoss: 4.2679\tLR: 2.239642\n",
      "Training Epoch: 23 [20096/50000]\tLoss: 4.1837\tLR: 2.239898\n",
      "Training Epoch: 23 [20224/50000]\tLoss: 4.1320\tLR: 2.240153\n",
      "Training Epoch: 23 [20352/50000]\tLoss: 4.3709\tLR: 2.240409\n",
      "Training Epoch: 23 [20480/50000]\tLoss: 4.1125\tLR: 2.240665\n",
      "Training Epoch: 23 [20608/50000]\tLoss: 4.1256\tLR: 2.240921\n",
      "Training Epoch: 23 [20736/50000]\tLoss: 4.0150\tLR: 2.241176\n",
      "Training Epoch: 23 [20864/50000]\tLoss: 4.2639\tLR: 2.241432\n",
      "Training Epoch: 23 [20992/50000]\tLoss: 4.2191\tLR: 2.241688\n",
      "Training Epoch: 23 [21120/50000]\tLoss: 4.3337\tLR: 2.241944\n",
      "Training Epoch: 23 [21248/50000]\tLoss: 4.1311\tLR: 2.242199\n",
      "Training Epoch: 23 [21376/50000]\tLoss: 4.2420\tLR: 2.242455\n",
      "Training Epoch: 23 [21504/50000]\tLoss: 4.1330\tLR: 2.242711\n",
      "Training Epoch: 23 [21632/50000]\tLoss: 4.2063\tLR: 2.242967\n",
      "Training Epoch: 23 [21760/50000]\tLoss: 4.0821\tLR: 2.243223\n",
      "Training Epoch: 23 [21888/50000]\tLoss: 4.1999\tLR: 2.243478\n",
      "Training Epoch: 23 [22016/50000]\tLoss: 4.3168\tLR: 2.243734\n",
      "Training Epoch: 23 [22144/50000]\tLoss: 4.2021\tLR: 2.243990\n",
      "Training Epoch: 23 [22272/50000]\tLoss: 4.1045\tLR: 2.244246\n",
      "Training Epoch: 23 [22400/50000]\tLoss: 4.2980\tLR: 2.244501\n",
      "Training Epoch: 23 [22528/50000]\tLoss: 4.2675\tLR: 2.244757\n",
      "Training Epoch: 23 [22656/50000]\tLoss: 4.1125\tLR: 2.245013\n",
      "Training Epoch: 23 [22784/50000]\tLoss: 4.1022\tLR: 2.245269\n",
      "Training Epoch: 23 [22912/50000]\tLoss: 4.1815\tLR: 2.245524\n",
      "Training Epoch: 23 [23040/50000]\tLoss: 4.3749\tLR: 2.245780\n",
      "Training Epoch: 23 [23168/50000]\tLoss: 4.2819\tLR: 2.246036\n",
      "Training Epoch: 23 [23296/50000]\tLoss: 4.1232\tLR: 2.246292\n",
      "Training Epoch: 23 [23424/50000]\tLoss: 4.1485\tLR: 2.246547\n",
      "Training Epoch: 23 [23552/50000]\tLoss: 4.0954\tLR: 2.246803\n",
      "Training Epoch: 23 [23680/50000]\tLoss: 4.2337\tLR: 2.247059\n",
      "Training Epoch: 23 [23808/50000]\tLoss: 4.1098\tLR: 2.247315\n",
      "Training Epoch: 23 [23936/50000]\tLoss: 4.0103\tLR: 2.247570\n",
      "Training Epoch: 23 [24064/50000]\tLoss: 4.1204\tLR: 2.247826\n",
      "Training Epoch: 23 [24192/50000]\tLoss: 4.4421\tLR: 2.248082\n",
      "Training Epoch: 23 [24320/50000]\tLoss: 4.2274\tLR: 2.248338\n",
      "Training Epoch: 23 [24448/50000]\tLoss: 4.1342\tLR: 2.248593\n",
      "Training Epoch: 23 [24576/50000]\tLoss: 4.2579\tLR: 2.248849\n",
      "Training Epoch: 23 [24704/50000]\tLoss: 4.1928\tLR: 2.249105\n",
      "Training Epoch: 23 [24832/50000]\tLoss: 4.2558\tLR: 2.249361\n",
      "Training Epoch: 23 [24960/50000]\tLoss: 4.0255\tLR: 2.249616\n",
      "Training Epoch: 23 [25088/50000]\tLoss: 4.1956\tLR: 2.249872\n",
      "Training Epoch: 23 [25216/50000]\tLoss: 4.0766\tLR: 2.250128\n",
      "Training Epoch: 23 [25344/50000]\tLoss: 4.3794\tLR: 2.250384\n",
      "Training Epoch: 23 [25472/50000]\tLoss: 4.1855\tLR: 2.250639\n",
      "Training Epoch: 23 [25600/50000]\tLoss: 4.2631\tLR: 2.250895\n",
      "Training Epoch: 23 [25728/50000]\tLoss: 3.9929\tLR: 2.251151\n",
      "Training Epoch: 23 [25856/50000]\tLoss: 4.2785\tLR: 2.251407\n",
      "Training Epoch: 23 [25984/50000]\tLoss: 4.3426\tLR: 2.251662\n",
      "Training Epoch: 23 [26112/50000]\tLoss: 4.1772\tLR: 2.251918\n",
      "Training Epoch: 23 [26240/50000]\tLoss: 4.1478\tLR: 2.252174\n",
      "Training Epoch: 23 [26368/50000]\tLoss: 4.1262\tLR: 2.252430\n",
      "Training Epoch: 23 [26496/50000]\tLoss: 4.0690\tLR: 2.252685\n",
      "Training Epoch: 23 [26624/50000]\tLoss: 4.1065\tLR: 2.252941\n",
      "Training Epoch: 23 [26752/50000]\tLoss: 4.2319\tLR: 2.253197\n",
      "Training Epoch: 23 [26880/50000]\tLoss: 4.1865\tLR: 2.253453\n",
      "Training Epoch: 23 [27008/50000]\tLoss: 4.1822\tLR: 2.253708\n",
      "Training Epoch: 23 [27136/50000]\tLoss: 3.9397\tLR: 2.253964\n",
      "Training Epoch: 23 [27264/50000]\tLoss: 4.2612\tLR: 2.254220\n",
      "Training Epoch: 23 [27392/50000]\tLoss: 4.1958\tLR: 2.254476\n",
      "Training Epoch: 23 [27520/50000]\tLoss: 4.1887\tLR: 2.254731\n",
      "Training Epoch: 23 [27648/50000]\tLoss: 4.2355\tLR: 2.254987\n",
      "Training Epoch: 23 [27776/50000]\tLoss: 4.4344\tLR: 2.255243\n",
      "Training Epoch: 23 [27904/50000]\tLoss: 4.1968\tLR: 2.255499\n",
      "Training Epoch: 23 [28032/50000]\tLoss: 4.1678\tLR: 2.255754\n",
      "Training Epoch: 23 [28160/50000]\tLoss: 4.2785\tLR: 2.256010\n",
      "Training Epoch: 23 [28288/50000]\tLoss: 4.2023\tLR: 2.256266\n",
      "Training Epoch: 23 [28416/50000]\tLoss: 4.2404\tLR: 2.256522\n",
      "Training Epoch: 23 [28544/50000]\tLoss: 4.1008\tLR: 2.256777\n",
      "Training Epoch: 23 [28672/50000]\tLoss: 4.1890\tLR: 2.257033\n",
      "Training Epoch: 23 [28800/50000]\tLoss: 4.2695\tLR: 2.257289\n",
      "Training Epoch: 23 [28928/50000]\tLoss: 4.3225\tLR: 2.257545\n",
      "Training Epoch: 23 [29056/50000]\tLoss: 4.1378\tLR: 2.257801\n",
      "Training Epoch: 23 [29184/50000]\tLoss: 3.9320\tLR: 2.258056\n",
      "Training Epoch: 23 [29312/50000]\tLoss: 4.2795\tLR: 2.258312\n",
      "Training Epoch: 23 [29440/50000]\tLoss: 4.2001\tLR: 2.258568\n",
      "Training Epoch: 23 [29568/50000]\tLoss: 4.2484\tLR: 2.258824\n",
      "Training Epoch: 23 [29696/50000]\tLoss: 4.2097\tLR: 2.259079\n",
      "Training Epoch: 23 [29824/50000]\tLoss: 4.0455\tLR: 2.259335\n",
      "Training Epoch: 23 [29952/50000]\tLoss: 4.0368\tLR: 2.259591\n",
      "Training Epoch: 23 [30080/50000]\tLoss: 4.1854\tLR: 2.259847\n",
      "Training Epoch: 23 [30208/50000]\tLoss: 4.2256\tLR: 2.260102\n",
      "Training Epoch: 23 [30336/50000]\tLoss: 4.1017\tLR: 2.260358\n",
      "Training Epoch: 23 [30464/50000]\tLoss: 4.2283\tLR: 2.260614\n",
      "Training Epoch: 23 [30592/50000]\tLoss: 4.2016\tLR: 2.260870\n",
      "Training Epoch: 23 [30720/50000]\tLoss: 4.1677\tLR: 2.261125\n",
      "Training Epoch: 23 [30848/50000]\tLoss: 4.2002\tLR: 2.261381\n",
      "Training Epoch: 23 [30976/50000]\tLoss: 4.2179\tLR: 2.261637\n",
      "Training Epoch: 23 [31104/50000]\tLoss: 4.2451\tLR: 2.261893\n",
      "Training Epoch: 23 [31232/50000]\tLoss: 4.1726\tLR: 2.262148\n",
      "Training Epoch: 23 [31360/50000]\tLoss: 4.2050\tLR: 2.262404\n",
      "Training Epoch: 23 [31488/50000]\tLoss: 4.1832\tLR: 2.262660\n",
      "Training Epoch: 23 [31616/50000]\tLoss: 4.1925\tLR: 2.262916\n",
      "Training Epoch: 23 [31744/50000]\tLoss: 4.1445\tLR: 2.263171\n",
      "Training Epoch: 23 [31872/50000]\tLoss: 4.0604\tLR: 2.263427\n",
      "Training Epoch: 23 [32000/50000]\tLoss: 4.2992\tLR: 2.263683\n",
      "Training Epoch: 23 [32128/50000]\tLoss: 4.2322\tLR: 2.263939\n",
      "Training Epoch: 23 [32256/50000]\tLoss: 4.3013\tLR: 2.264194\n",
      "Training Epoch: 23 [32384/50000]\tLoss: 4.2278\tLR: 2.264450\n",
      "Training Epoch: 23 [32512/50000]\tLoss: 4.1083\tLR: 2.264706\n",
      "Training Epoch: 23 [32640/50000]\tLoss: 4.0510\tLR: 2.264962\n",
      "Training Epoch: 23 [32768/50000]\tLoss: 4.1735\tLR: 2.265217\n",
      "Training Epoch: 23 [32896/50000]\tLoss: 4.2971\tLR: 2.265473\n",
      "Training Epoch: 23 [33024/50000]\tLoss: 4.0107\tLR: 2.265729\n",
      "Training Epoch: 23 [33152/50000]\tLoss: 4.0469\tLR: 2.265985\n",
      "Training Epoch: 23 [33280/50000]\tLoss: 4.1725\tLR: 2.266240\n",
      "Training Epoch: 23 [33408/50000]\tLoss: 4.0069\tLR: 2.266496\n",
      "Training Epoch: 23 [33536/50000]\tLoss: 4.1184\tLR: 2.266752\n",
      "Training Epoch: 23 [33664/50000]\tLoss: 4.0654\tLR: 2.267008\n",
      "Training Epoch: 23 [33792/50000]\tLoss: 4.1720\tLR: 2.267263\n",
      "Training Epoch: 23 [33920/50000]\tLoss: 4.2078\tLR: 2.267519\n",
      "Training Epoch: 23 [34048/50000]\tLoss: 4.3347\tLR: 2.267775\n",
      "Training Epoch: 23 [34176/50000]\tLoss: 4.0392\tLR: 2.268031\n",
      "Training Epoch: 23 [34304/50000]\tLoss: 4.0904\tLR: 2.268286\n",
      "Training Epoch: 23 [34432/50000]\tLoss: 4.1645\tLR: 2.268542\n",
      "Training Epoch: 23 [34560/50000]\tLoss: 4.2185\tLR: 2.268798\n",
      "Training Epoch: 23 [34688/50000]\tLoss: 4.1433\tLR: 2.269054\n",
      "Training Epoch: 23 [34816/50000]\tLoss: 4.1989\tLR: 2.269309\n",
      "Training Epoch: 23 [34944/50000]\tLoss: 4.2578\tLR: 2.269565\n",
      "Training Epoch: 23 [35072/50000]\tLoss: 4.1511\tLR: 2.269821\n",
      "Training Epoch: 23 [35200/50000]\tLoss: 4.3281\tLR: 2.270077\n",
      "Training Epoch: 23 [35328/50000]\tLoss: 4.0329\tLR: 2.270332\n",
      "Training Epoch: 23 [35456/50000]\tLoss: 4.1904\tLR: 2.270588\n",
      "Training Epoch: 23 [35584/50000]\tLoss: 4.0940\tLR: 2.270844\n",
      "Training Epoch: 23 [35712/50000]\tLoss: 4.1633\tLR: 2.271100\n",
      "Training Epoch: 23 [35840/50000]\tLoss: 4.3029\tLR: 2.271355\n",
      "Training Epoch: 23 [35968/50000]\tLoss: 4.1977\tLR: 2.271611\n",
      "Training Epoch: 23 [36096/50000]\tLoss: 4.1894\tLR: 2.271867\n",
      "Training Epoch: 23 [36224/50000]\tLoss: 4.0768\tLR: 2.272123\n",
      "Training Epoch: 23 [36352/50000]\tLoss: 4.1962\tLR: 2.272379\n",
      "Training Epoch: 23 [36480/50000]\tLoss: 4.2555\tLR: 2.272634\n",
      "Training Epoch: 23 [36608/50000]\tLoss: 4.3523\tLR: 2.272890\n",
      "Training Epoch: 23 [36736/50000]\tLoss: 4.1936\tLR: 2.273146\n",
      "Training Epoch: 23 [36864/50000]\tLoss: 4.3161\tLR: 2.273402\n",
      "Training Epoch: 23 [36992/50000]\tLoss: 4.2705\tLR: 2.273657\n",
      "Training Epoch: 23 [37120/50000]\tLoss: 4.1964\tLR: 2.273913\n",
      "Training Epoch: 23 [37248/50000]\tLoss: 4.0148\tLR: 2.274169\n",
      "Training Epoch: 23 [37376/50000]\tLoss: 4.2986\tLR: 2.274425\n",
      "Training Epoch: 23 [37504/50000]\tLoss: 4.2200\tLR: 2.274680\n",
      "Training Epoch: 23 [37632/50000]\tLoss: 4.0636\tLR: 2.274936\n",
      "Training Epoch: 23 [37760/50000]\tLoss: 4.3937\tLR: 2.275192\n",
      "Training Epoch: 23 [37888/50000]\tLoss: 4.3285\tLR: 2.275448\n",
      "Training Epoch: 23 [38016/50000]\tLoss: 4.3296\tLR: 2.275703\n",
      "Training Epoch: 23 [38144/50000]\tLoss: 4.1899\tLR: 2.275959\n",
      "Training Epoch: 23 [38272/50000]\tLoss: 4.1693\tLR: 2.276215\n",
      "Training Epoch: 23 [38400/50000]\tLoss: 4.1841\tLR: 2.276471\n",
      "Training Epoch: 23 [38528/50000]\tLoss: 4.1448\tLR: 2.276726\n",
      "Training Epoch: 23 [38656/50000]\tLoss: 4.3255\tLR: 2.276982\n",
      "Training Epoch: 23 [38784/50000]\tLoss: 4.2846\tLR: 2.277238\n",
      "Training Epoch: 23 [38912/50000]\tLoss: 4.1709\tLR: 2.277494\n",
      "Training Epoch: 23 [39040/50000]\tLoss: 4.1235\tLR: 2.277749\n",
      "Training Epoch: 23 [39168/50000]\tLoss: 4.0960\tLR: 2.278005\n",
      "Training Epoch: 23 [39296/50000]\tLoss: 4.4778\tLR: 2.278261\n",
      "Training Epoch: 23 [39424/50000]\tLoss: 4.2968\tLR: 2.278517\n",
      "Training Epoch: 23 [39552/50000]\tLoss: 4.2455\tLR: 2.278772\n",
      "Training Epoch: 23 [39680/50000]\tLoss: 4.2532\tLR: 2.279028\n",
      "Training Epoch: 23 [39808/50000]\tLoss: 4.2266\tLR: 2.279284\n",
      "Training Epoch: 23 [39936/50000]\tLoss: 4.1755\tLR: 2.279540\n",
      "Training Epoch: 23 [40064/50000]\tLoss: 4.3170\tLR: 2.279795\n",
      "Training Epoch: 23 [40192/50000]\tLoss: 4.1357\tLR: 2.280051\n",
      "Training Epoch: 23 [40320/50000]\tLoss: 4.2838\tLR: 2.280307\n",
      "Training Epoch: 23 [40448/50000]\tLoss: 4.1472\tLR: 2.280563\n",
      "Training Epoch: 23 [40576/50000]\tLoss: 4.2926\tLR: 2.280818\n",
      "Training Epoch: 23 [40704/50000]\tLoss: 4.3033\tLR: 2.281074\n",
      "Training Epoch: 23 [40832/50000]\tLoss: 4.1454\tLR: 2.281330\n",
      "Training Epoch: 23 [40960/50000]\tLoss: 4.3551\tLR: 2.281586\n",
      "Training Epoch: 23 [41088/50000]\tLoss: 4.2665\tLR: 2.281841\n",
      "Training Epoch: 23 [41216/50000]\tLoss: 4.1952\tLR: 2.282097\n",
      "Training Epoch: 23 [41344/50000]\tLoss: 4.1697\tLR: 2.282353\n",
      "Training Epoch: 23 [41472/50000]\tLoss: 4.1102\tLR: 2.282609\n",
      "Training Epoch: 23 [41600/50000]\tLoss: 4.3732\tLR: 2.282864\n",
      "Training Epoch: 23 [41728/50000]\tLoss: 4.2458\tLR: 2.283120\n",
      "Training Epoch: 23 [41856/50000]\tLoss: 4.2307\tLR: 2.283376\n",
      "Training Epoch: 23 [41984/50000]\tLoss: 4.2976\tLR: 2.283632\n",
      "Training Epoch: 23 [42112/50000]\tLoss: 4.3210\tLR: 2.283887\n",
      "Training Epoch: 23 [42240/50000]\tLoss: 4.1069\tLR: 2.284143\n",
      "Training Epoch: 23 [42368/50000]\tLoss: 4.2836\tLR: 2.284399\n",
      "Training Epoch: 23 [42496/50000]\tLoss: 4.1523\tLR: 2.284655\n",
      "Training Epoch: 23 [42624/50000]\tLoss: 4.1896\tLR: 2.284910\n",
      "Training Epoch: 23 [42752/50000]\tLoss: 4.2202\tLR: 2.285166\n",
      "Training Epoch: 23 [42880/50000]\tLoss: 4.2008\tLR: 2.285422\n",
      "Training Epoch: 23 [43008/50000]\tLoss: 4.1374\tLR: 2.285678\n",
      "Training Epoch: 23 [43136/50000]\tLoss: 4.4977\tLR: 2.285934\n",
      "Training Epoch: 23 [43264/50000]\tLoss: 4.3250\tLR: 2.286189\n",
      "Training Epoch: 23 [43392/50000]\tLoss: 4.1368\tLR: 2.286445\n",
      "Training Epoch: 23 [43520/50000]\tLoss: 4.1938\tLR: 2.286701\n",
      "Training Epoch: 23 [43648/50000]\tLoss: 4.3257\tLR: 2.286957\n",
      "Training Epoch: 23 [43776/50000]\tLoss: 4.3841\tLR: 2.287212\n",
      "Training Epoch: 23 [43904/50000]\tLoss: 4.3089\tLR: 2.287468\n",
      "Training Epoch: 23 [44032/50000]\tLoss: 4.1763\tLR: 2.287724\n",
      "Training Epoch: 23 [44160/50000]\tLoss: 4.1119\tLR: 2.287980\n",
      "Training Epoch: 23 [44288/50000]\tLoss: 4.2700\tLR: 2.288235\n",
      "Training Epoch: 23 [44416/50000]\tLoss: 4.1583\tLR: 2.288491\n",
      "Training Epoch: 23 [44544/50000]\tLoss: 4.2880\tLR: 2.288747\n",
      "Training Epoch: 23 [44672/50000]\tLoss: 4.3473\tLR: 2.289003\n",
      "Training Epoch: 23 [44800/50000]\tLoss: 4.2320\tLR: 2.289258\n",
      "Training Epoch: 23 [44928/50000]\tLoss: 4.2616\tLR: 2.289514\n",
      "Training Epoch: 23 [45056/50000]\tLoss: 4.3243\tLR: 2.289770\n",
      "Training Epoch: 23 [45184/50000]\tLoss: 4.3667\tLR: 2.290026\n",
      "Training Epoch: 23 [45312/50000]\tLoss: 4.2135\tLR: 2.290281\n",
      "Training Epoch: 23 [45440/50000]\tLoss: 4.2933\tLR: 2.290537\n",
      "Training Epoch: 23 [45568/50000]\tLoss: 4.2823\tLR: 2.290793\n",
      "Training Epoch: 23 [45696/50000]\tLoss: 4.2660\tLR: 2.291049\n",
      "Training Epoch: 23 [45824/50000]\tLoss: 4.2676\tLR: 2.291304\n",
      "Training Epoch: 23 [45952/50000]\tLoss: 4.2188\tLR: 2.291560\n",
      "Training Epoch: 23 [46080/50000]\tLoss: 4.1733\tLR: 2.291816\n",
      "Training Epoch: 23 [46208/50000]\tLoss: 4.3901\tLR: 2.292072\n",
      "Training Epoch: 23 [46336/50000]\tLoss: 4.2327\tLR: 2.292327\n",
      "Training Epoch: 23 [46464/50000]\tLoss: 4.2916\tLR: 2.292583\n",
      "Training Epoch: 23 [46592/50000]\tLoss: 4.0952\tLR: 2.292839\n",
      "Training Epoch: 23 [46720/50000]\tLoss: 4.2765\tLR: 2.293095\n",
      "Training Epoch: 23 [46848/50000]\tLoss: 4.1610\tLR: 2.293350\n",
      "Training Epoch: 23 [46976/50000]\tLoss: 4.2557\tLR: 2.293606\n",
      "Training Epoch: 23 [47104/50000]\tLoss: 4.3192\tLR: 2.293862\n",
      "Training Epoch: 23 [47232/50000]\tLoss: 4.3546\tLR: 2.294118\n",
      "Training Epoch: 23 [47360/50000]\tLoss: 4.4043\tLR: 2.294373\n",
      "Training Epoch: 23 [47488/50000]\tLoss: 4.2616\tLR: 2.294629\n",
      "Training Epoch: 23 [47616/50000]\tLoss: 4.2940\tLR: 2.294885\n",
      "Training Epoch: 23 [47744/50000]\tLoss: 4.3563\tLR: 2.295141\n",
      "Training Epoch: 23 [47872/50000]\tLoss: 4.1501\tLR: 2.295396\n",
      "Training Epoch: 23 [48000/50000]\tLoss: 4.4029\tLR: 2.295652\n",
      "Training Epoch: 23 [48128/50000]\tLoss: 4.2457\tLR: 2.295908\n",
      "Training Epoch: 23 [48256/50000]\tLoss: 4.1425\tLR: 2.296164\n",
      "Training Epoch: 23 [48384/50000]\tLoss: 4.3702\tLR: 2.296419\n",
      "Training Epoch: 23 [48512/50000]\tLoss: 4.3608\tLR: 2.296675\n",
      "Training Epoch: 23 [48640/50000]\tLoss: 4.2319\tLR: 2.296931\n",
      "Training Epoch: 23 [48768/50000]\tLoss: 4.2677\tLR: 2.297187\n",
      "Training Epoch: 23 [48896/50000]\tLoss: 4.1851\tLR: 2.297442\n",
      "Training Epoch: 23 [49024/50000]\tLoss: 4.4017\tLR: 2.297698\n",
      "Training Epoch: 23 [49152/50000]\tLoss: 4.2686\tLR: 2.297954\n",
      "Training Epoch: 23 [49280/50000]\tLoss: 4.2076\tLR: 2.298210\n",
      "Training Epoch: 23 [49408/50000]\tLoss: 4.1888\tLR: 2.298465\n",
      "Training Epoch: 23 [49536/50000]\tLoss: 4.2351\tLR: 2.298721\n",
      "Training Epoch: 23 [49664/50000]\tLoss: 4.1944\tLR: 2.298977\n",
      "Training Epoch: 23 [49792/50000]\tLoss: 4.3021\tLR: 2.299233\n",
      "Training Epoch: 23 [49920/50000]\tLoss: 4.1992\tLR: 2.299488\n",
      "Training Epoch: 23 [50000/50000]\tLoss: 4.3152\tLR: 2.299744\n",
      "epoch 23 training time consumed: 488.98s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   32245 GB |   32245 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   32146 GB |   32146 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      99 GB |      99 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   32245 GB |   32245 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   32146 GB |   32146 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |      99 GB |      99 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   31789 GB |   31789 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   31690 GB |   31690 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |      99 GB |      99 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    3419 K  |    3419 K  |\n",
      "|       from large pool |      24    |      65    |    1457 K  |    1457 K  |\n",
      "|       from small pool |     231    |     274    |    1961 K  |    1961 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    3419 K  |    3419 K  |\n",
      "|       from large pool |      24    |      65    |    1457 K  |    1457 K  |\n",
      "|       from small pool |     231    |     274    |    1961 K  |    1961 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      46    |    1981 K  |    1981 K  |\n",
      "|       from large pool |      10    |      23    |     700 K  |     700 K  |\n",
      "|       from small pool |      26    |      35    |    1280 K  |    1280 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 23, Average loss: 0.0356, Accuracy: 0.0497, Time consumed:31.03s\n",
      "\n",
      "Training Epoch: 24 [128/50000]\tLoss: 4.3528\tLR: 0.100000\n",
      "Training Epoch: 24 [256/50000]\tLoss: 4.3023\tLR: 2.300256\n",
      "Training Epoch: 24 [384/50000]\tLoss: 4.2707\tLR: 2.300512\n",
      "Training Epoch: 24 [512/50000]\tLoss: 4.2283\tLR: 2.300767\n",
      "Training Epoch: 24 [640/50000]\tLoss: 4.2722\tLR: 2.301023\n",
      "Training Epoch: 24 [768/50000]\tLoss: 4.1861\tLR: 2.301279\n",
      "Training Epoch: 24 [896/50000]\tLoss: 4.2132\tLR: 2.301535\n",
      "Training Epoch: 24 [1024/50000]\tLoss: 4.1582\tLR: 2.301790\n",
      "Training Epoch: 24 [1152/50000]\tLoss: 4.1855\tLR: 2.302046\n",
      "Training Epoch: 24 [1280/50000]\tLoss: 4.2120\tLR: 2.302302\n",
      "Training Epoch: 24 [1408/50000]\tLoss: 4.3696\tLR: 2.302558\n",
      "Training Epoch: 24 [1536/50000]\tLoss: 4.3467\tLR: 2.302813\n",
      "Training Epoch: 24 [1664/50000]\tLoss: 4.2152\tLR: 2.303069\n",
      "Training Epoch: 24 [1792/50000]\tLoss: 4.2407\tLR: 2.303325\n",
      "Training Epoch: 24 [1920/50000]\tLoss: 4.2747\tLR: 2.303581\n",
      "Training Epoch: 24 [2048/50000]\tLoss: 4.2222\tLR: 2.303836\n",
      "Training Epoch: 24 [2176/50000]\tLoss: 4.2429\tLR: 2.304092\n",
      "Training Epoch: 24 [2304/50000]\tLoss: 4.2160\tLR: 2.304348\n",
      "Training Epoch: 24 [2432/50000]\tLoss: 4.2706\tLR: 2.304604\n",
      "Training Epoch: 24 [2560/50000]\tLoss: 4.4453\tLR: 2.304859\n",
      "Training Epoch: 24 [2688/50000]\tLoss: 4.4509\tLR: 2.305115\n",
      "Training Epoch: 24 [2816/50000]\tLoss: 4.2944\tLR: 2.305371\n",
      "Training Epoch: 24 [2944/50000]\tLoss: 4.2745\tLR: 2.305627\n",
      "Training Epoch: 24 [3072/50000]\tLoss: 4.2542\tLR: 2.305882\n",
      "Training Epoch: 24 [3200/50000]\tLoss: 4.2068\tLR: 2.306138\n",
      "Training Epoch: 24 [3328/50000]\tLoss: 4.4779\tLR: 2.306394\n",
      "Training Epoch: 24 [3456/50000]\tLoss: 4.3405\tLR: 2.306650\n",
      "Training Epoch: 24 [3584/50000]\tLoss: 4.4342\tLR: 2.306905\n",
      "Training Epoch: 24 [3712/50000]\tLoss: 4.3133\tLR: 2.307161\n",
      "Training Epoch: 24 [3840/50000]\tLoss: 4.3726\tLR: 2.307417\n",
      "Training Epoch: 24 [3968/50000]\tLoss: 4.3283\tLR: 2.307673\n",
      "Training Epoch: 24 [4096/50000]\tLoss: 4.3692\tLR: 2.307928\n",
      "Training Epoch: 24 [4224/50000]\tLoss: 4.4761\tLR: 2.308184\n",
      "Training Epoch: 24 [4352/50000]\tLoss: 4.1518\tLR: 2.308440\n",
      "Training Epoch: 24 [4480/50000]\tLoss: 4.3944\tLR: 2.308696\n",
      "Training Epoch: 24 [4608/50000]\tLoss: 4.2094\tLR: 2.308951\n",
      "Training Epoch: 24 [4736/50000]\tLoss: 4.2087\tLR: 2.309207\n",
      "Training Epoch: 24 [4864/50000]\tLoss: 4.4066\tLR: 2.309463\n",
      "Training Epoch: 24 [4992/50000]\tLoss: 4.2468\tLR: 2.309719\n",
      "Training Epoch: 24 [5120/50000]\tLoss: 4.2805\tLR: 2.309974\n",
      "Training Epoch: 24 [5248/50000]\tLoss: 4.1569\tLR: 2.310230\n",
      "Training Epoch: 24 [5376/50000]\tLoss: 4.3272\tLR: 2.310486\n",
      "Training Epoch: 24 [5504/50000]\tLoss: 4.2747\tLR: 2.310742\n",
      "Training Epoch: 24 [5632/50000]\tLoss: 4.1554\tLR: 2.310997\n",
      "Training Epoch: 24 [5760/50000]\tLoss: 4.3179\tLR: 2.311253\n",
      "Training Epoch: 24 [5888/50000]\tLoss: 4.1783\tLR: 2.311509\n",
      "Training Epoch: 24 [6016/50000]\tLoss: 4.2783\tLR: 2.311765\n",
      "Training Epoch: 24 [6144/50000]\tLoss: 4.2037\tLR: 2.312020\n",
      "Training Epoch: 24 [6272/50000]\tLoss: 4.2299\tLR: 2.312276\n",
      "Training Epoch: 24 [6400/50000]\tLoss: 4.2915\tLR: 2.312532\n",
      "Training Epoch: 24 [6528/50000]\tLoss: 4.2041\tLR: 2.312788\n",
      "Training Epoch: 24 [6656/50000]\tLoss: 4.3556\tLR: 2.313043\n",
      "Training Epoch: 24 [6784/50000]\tLoss: 4.2889\tLR: 2.313299\n",
      "Training Epoch: 24 [6912/50000]\tLoss: 4.3330\tLR: 2.313555\n",
      "Training Epoch: 24 [7040/50000]\tLoss: 4.2389\tLR: 2.313811\n",
      "Training Epoch: 24 [7168/50000]\tLoss: 4.2440\tLR: 2.314066\n",
      "Training Epoch: 24 [7296/50000]\tLoss: 4.3100\tLR: 2.314322\n",
      "Training Epoch: 24 [7424/50000]\tLoss: 4.4555\tLR: 2.314578\n",
      "Training Epoch: 24 [7552/50000]\tLoss: 4.3025\tLR: 2.314834\n",
      "Training Epoch: 24 [7680/50000]\tLoss: 4.1813\tLR: 2.315090\n",
      "Training Epoch: 24 [7808/50000]\tLoss: 4.2770\tLR: 2.315345\n",
      "Training Epoch: 24 [7936/50000]\tLoss: 4.1734\tLR: 2.315601\n",
      "Training Epoch: 24 [8064/50000]\tLoss: 4.3389\tLR: 2.315857\n",
      "Training Epoch: 24 [8192/50000]\tLoss: 4.2661\tLR: 2.316113\n",
      "Training Epoch: 24 [8320/50000]\tLoss: 4.2486\tLR: 2.316368\n",
      "Training Epoch: 24 [8448/50000]\tLoss: 4.3229\tLR: 2.316624\n",
      "Training Epoch: 24 [8576/50000]\tLoss: 4.2292\tLR: 2.316880\n",
      "Training Epoch: 24 [8704/50000]\tLoss: 4.2910\tLR: 2.317136\n",
      "Training Epoch: 24 [8832/50000]\tLoss: 4.2071\tLR: 2.317391\n",
      "Training Epoch: 24 [8960/50000]\tLoss: 4.3085\tLR: 2.317647\n",
      "Training Epoch: 24 [9088/50000]\tLoss: 4.1958\tLR: 2.317903\n",
      "Training Epoch: 24 [9216/50000]\tLoss: 4.4747\tLR: 2.318159\n",
      "Training Epoch: 24 [9344/50000]\tLoss: 4.1704\tLR: 2.318414\n",
      "Training Epoch: 24 [9472/50000]\tLoss: 4.2241\tLR: 2.318670\n",
      "Training Epoch: 24 [9600/50000]\tLoss: 4.2782\tLR: 2.318926\n",
      "Training Epoch: 24 [9728/50000]\tLoss: 4.4958\tLR: 2.319182\n",
      "Training Epoch: 24 [9856/50000]\tLoss: 4.3015\tLR: 2.319437\n",
      "Training Epoch: 24 [9984/50000]\tLoss: 4.3645\tLR: 2.319693\n",
      "Training Epoch: 24 [10112/50000]\tLoss: 4.2405\tLR: 2.319949\n",
      "Training Epoch: 24 [10240/50000]\tLoss: 4.4553\tLR: 2.320205\n",
      "Training Epoch: 24 [10368/50000]\tLoss: 4.3148\tLR: 2.320460\n",
      "Training Epoch: 24 [10496/50000]\tLoss: 4.2308\tLR: 2.320716\n",
      "Training Epoch: 24 [10624/50000]\tLoss: 4.2013\tLR: 2.320972\n",
      "Training Epoch: 24 [10752/50000]\tLoss: 4.2569\tLR: 2.321228\n",
      "Training Epoch: 24 [10880/50000]\tLoss: 4.2132\tLR: 2.321483\n",
      "Training Epoch: 24 [11008/50000]\tLoss: 4.3248\tLR: 2.321739\n",
      "Training Epoch: 24 [11136/50000]\tLoss: 4.3180\tLR: 2.321995\n",
      "Training Epoch: 24 [11264/50000]\tLoss: 4.2526\tLR: 2.322251\n",
      "Training Epoch: 24 [11392/50000]\tLoss: 4.2283\tLR: 2.322506\n",
      "Training Epoch: 24 [11520/50000]\tLoss: 4.2542\tLR: 2.322762\n",
      "Training Epoch: 24 [11648/50000]\tLoss: 4.2004\tLR: 2.323018\n",
      "Training Epoch: 24 [11776/50000]\tLoss: 4.3635\tLR: 2.323274\n",
      "Training Epoch: 24 [11904/50000]\tLoss: 4.2540\tLR: 2.323529\n",
      "Training Epoch: 24 [12032/50000]\tLoss: 4.2497\tLR: 2.323785\n",
      "Training Epoch: 24 [12160/50000]\tLoss: 4.2486\tLR: 2.324041\n",
      "Training Epoch: 24 [12288/50000]\tLoss: 4.2126\tLR: 2.324297\n",
      "Training Epoch: 24 [12416/50000]\tLoss: 4.2120\tLR: 2.324552\n",
      "Training Epoch: 24 [12544/50000]\tLoss: 4.2382\tLR: 2.324808\n",
      "Training Epoch: 24 [12672/50000]\tLoss: 4.2772\tLR: 2.325064\n",
      "Training Epoch: 24 [12800/50000]\tLoss: 4.3792\tLR: 2.325320\n",
      "Training Epoch: 24 [12928/50000]\tLoss: 4.2470\tLR: 2.325575\n",
      "Training Epoch: 24 [13056/50000]\tLoss: 4.2609\tLR: 2.325831\n",
      "Training Epoch: 24 [13184/50000]\tLoss: 4.2941\tLR: 2.326087\n",
      "Training Epoch: 24 [13312/50000]\tLoss: 4.1918\tLR: 2.326343\n",
      "Training Epoch: 24 [13440/50000]\tLoss: 4.2420\tLR: 2.326598\n",
      "Training Epoch: 24 [13568/50000]\tLoss: 4.0641\tLR: 2.326854\n",
      "Training Epoch: 24 [13696/50000]\tLoss: 4.0635\tLR: 2.327110\n",
      "Training Epoch: 24 [13824/50000]\tLoss: 4.2494\tLR: 2.327366\n",
      "Training Epoch: 24 [13952/50000]\tLoss: 4.1155\tLR: 2.327621\n",
      "Training Epoch: 24 [14080/50000]\tLoss: 4.0949\tLR: 2.327877\n",
      "Training Epoch: 24 [14208/50000]\tLoss: 4.3203\tLR: 2.328133\n",
      "Training Epoch: 24 [14336/50000]\tLoss: 4.2778\tLR: 2.328389\n",
      "Training Epoch: 24 [14464/50000]\tLoss: 4.3229\tLR: 2.328645\n",
      "Training Epoch: 24 [14592/50000]\tLoss: 4.2973\tLR: 2.328900\n",
      "Training Epoch: 24 [14720/50000]\tLoss: 4.4284\tLR: 2.329156\n",
      "Training Epoch: 24 [14848/50000]\tLoss: 4.3316\tLR: 2.329412\n",
      "Training Epoch: 24 [14976/50000]\tLoss: 4.5165\tLR: 2.329668\n",
      "Training Epoch: 24 [15104/50000]\tLoss: 4.4650\tLR: 2.329923\n",
      "Training Epoch: 24 [15232/50000]\tLoss: 4.3020\tLR: 2.330179\n",
      "Training Epoch: 24 [15360/50000]\tLoss: 4.4135\tLR: 2.330435\n",
      "Training Epoch: 24 [15488/50000]\tLoss: 4.4668\tLR: 2.330691\n",
      "Training Epoch: 24 [15616/50000]\tLoss: 4.3406\tLR: 2.330946\n",
      "Training Epoch: 24 [15744/50000]\tLoss: 4.2027\tLR: 2.331202\n",
      "Training Epoch: 24 [15872/50000]\tLoss: 4.2978\tLR: 2.331458\n",
      "Training Epoch: 24 [16000/50000]\tLoss: 4.3777\tLR: 2.331714\n",
      "Training Epoch: 24 [16128/50000]\tLoss: 4.2221\tLR: 2.331969\n",
      "Training Epoch: 24 [16256/50000]\tLoss: 4.2955\tLR: 2.332225\n",
      "Training Epoch: 24 [16384/50000]\tLoss: 4.4231\tLR: 2.332481\n",
      "Training Epoch: 24 [16512/50000]\tLoss: 4.1897\tLR: 2.332737\n",
      "Training Epoch: 24 [16640/50000]\tLoss: 4.3373\tLR: 2.332992\n",
      "Training Epoch: 24 [16768/50000]\tLoss: 4.3424\tLR: 2.333248\n",
      "Training Epoch: 24 [16896/50000]\tLoss: 4.2352\tLR: 2.333504\n",
      "Training Epoch: 24 [17024/50000]\tLoss: 4.1384\tLR: 2.333760\n",
      "Training Epoch: 24 [17152/50000]\tLoss: 4.3326\tLR: 2.334015\n",
      "Training Epoch: 24 [17280/50000]\tLoss: 4.3515\tLR: 2.334271\n",
      "Training Epoch: 24 [17408/50000]\tLoss: 4.2193\tLR: 2.334527\n",
      "Training Epoch: 24 [17536/50000]\tLoss: 4.2001\tLR: 2.334783\n",
      "Training Epoch: 24 [17664/50000]\tLoss: 4.1556\tLR: 2.335038\n",
      "Training Epoch: 24 [17792/50000]\tLoss: 4.2193\tLR: 2.335294\n",
      "Training Epoch: 24 [17920/50000]\tLoss: 4.3002\tLR: 2.335550\n",
      "Training Epoch: 24 [18048/50000]\tLoss: 4.1800\tLR: 2.335806\n",
      "Training Epoch: 24 [18176/50000]\tLoss: 4.3651\tLR: 2.336061\n",
      "Training Epoch: 24 [18304/50000]\tLoss: 4.3202\tLR: 2.336317\n",
      "Training Epoch: 24 [18432/50000]\tLoss: 4.1381\tLR: 2.336573\n",
      "Training Epoch: 24 [18560/50000]\tLoss: 4.2776\tLR: 2.336829\n",
      "Training Epoch: 24 [18688/50000]\tLoss: 4.1761\tLR: 2.337084\n",
      "Training Epoch: 24 [18816/50000]\tLoss: 4.1954\tLR: 2.337340\n",
      "Training Epoch: 24 [18944/50000]\tLoss: 4.2164\tLR: 2.337596\n",
      "Training Epoch: 24 [19072/50000]\tLoss: 4.4176\tLR: 2.337852\n",
      "Training Epoch: 24 [19200/50000]\tLoss: 4.3565\tLR: 2.338107\n",
      "Training Epoch: 24 [19328/50000]\tLoss: 4.2457\tLR: 2.338363\n",
      "Training Epoch: 24 [19456/50000]\tLoss: 4.2887\tLR: 2.338619\n",
      "Training Epoch: 24 [19584/50000]\tLoss: 4.2140\tLR: 2.338875\n",
      "Training Epoch: 24 [19712/50000]\tLoss: 4.3928\tLR: 2.339130\n",
      "Training Epoch: 24 [19840/50000]\tLoss: 4.2545\tLR: 2.339386\n",
      "Training Epoch: 24 [19968/50000]\tLoss: 4.2363\tLR: 2.339642\n",
      "Training Epoch: 24 [20096/50000]\tLoss: 4.4437\tLR: 2.339898\n",
      "Training Epoch: 24 [20224/50000]\tLoss: 4.2312\tLR: 2.340153\n",
      "Training Epoch: 24 [20352/50000]\tLoss: 4.3960\tLR: 2.340409\n",
      "Training Epoch: 24 [20480/50000]\tLoss: 4.2011\tLR: 2.340665\n",
      "Training Epoch: 24 [20608/50000]\tLoss: 4.3156\tLR: 2.340921\n",
      "Training Epoch: 24 [20736/50000]\tLoss: 4.2359\tLR: 2.341176\n",
      "Training Epoch: 24 [20864/50000]\tLoss: 4.1768\tLR: 2.341432\n",
      "Training Epoch: 24 [20992/50000]\tLoss: 4.1576\tLR: 2.341688\n",
      "Training Epoch: 24 [21120/50000]\tLoss: 4.2165\tLR: 2.341944\n",
      "Training Epoch: 24 [21248/50000]\tLoss: 4.3214\tLR: 2.342199\n",
      "Training Epoch: 24 [21376/50000]\tLoss: 4.2402\tLR: 2.342455\n",
      "Training Epoch: 24 [21504/50000]\tLoss: 4.2327\tLR: 2.342711\n",
      "Training Epoch: 24 [21632/50000]\tLoss: 4.3210\tLR: 2.342967\n",
      "Training Epoch: 24 [21760/50000]\tLoss: 4.2774\tLR: 2.343223\n",
      "Training Epoch: 24 [21888/50000]\tLoss: 4.3257\tLR: 2.343478\n",
      "Training Epoch: 24 [22016/50000]\tLoss: 4.1792\tLR: 2.343734\n",
      "Training Epoch: 24 [22144/50000]\tLoss: 4.3719\tLR: 2.343990\n",
      "Training Epoch: 24 [22272/50000]\tLoss: 4.3191\tLR: 2.344246\n",
      "Training Epoch: 24 [22400/50000]\tLoss: 4.2571\tLR: 2.344501\n",
      "Training Epoch: 24 [22528/50000]\tLoss: 4.2377\tLR: 2.344757\n",
      "Training Epoch: 24 [22656/50000]\tLoss: 4.2694\tLR: 2.345013\n",
      "Training Epoch: 24 [22784/50000]\tLoss: 4.2746\tLR: 2.345269\n",
      "Training Epoch: 24 [22912/50000]\tLoss: 4.3893\tLR: 2.345524\n",
      "Training Epoch: 24 [23040/50000]\tLoss: 4.3245\tLR: 2.345780\n",
      "Training Epoch: 24 [23168/50000]\tLoss: 4.1907\tLR: 2.346036\n",
      "Training Epoch: 24 [23296/50000]\tLoss: 4.2660\tLR: 2.346292\n",
      "Training Epoch: 24 [23424/50000]\tLoss: 4.2872\tLR: 2.346547\n",
      "Training Epoch: 24 [23552/50000]\tLoss: 4.3159\tLR: 2.346803\n",
      "Training Epoch: 24 [23680/50000]\tLoss: 4.3115\tLR: 2.347059\n",
      "Training Epoch: 24 [23808/50000]\tLoss: 4.3565\tLR: 2.347315\n",
      "Training Epoch: 24 [23936/50000]\tLoss: 4.3890\tLR: 2.347570\n",
      "Training Epoch: 24 [24064/50000]\tLoss: 4.2942\tLR: 2.347826\n",
      "Training Epoch: 24 [24192/50000]\tLoss: 4.3173\tLR: 2.348082\n",
      "Training Epoch: 24 [24320/50000]\tLoss: 4.3600\tLR: 2.348338\n",
      "Training Epoch: 24 [24448/50000]\tLoss: 4.2777\tLR: 2.348593\n",
      "Training Epoch: 24 [24576/50000]\tLoss: 4.2283\tLR: 2.348849\n",
      "Training Epoch: 24 [24704/50000]\tLoss: 4.2158\tLR: 2.349105\n",
      "Training Epoch: 24 [24832/50000]\tLoss: 4.4693\tLR: 2.349361\n",
      "Training Epoch: 24 [24960/50000]\tLoss: 4.3642\tLR: 2.349616\n",
      "Training Epoch: 24 [25088/50000]\tLoss: 4.1771\tLR: 2.349872\n",
      "Training Epoch: 24 [25216/50000]\tLoss: 4.3259\tLR: 2.350128\n",
      "Training Epoch: 24 [25344/50000]\tLoss: 4.2474\tLR: 2.350384\n",
      "Training Epoch: 24 [25472/50000]\tLoss: 4.0958\tLR: 2.350639\n",
      "Training Epoch: 24 [25600/50000]\tLoss: 4.1275\tLR: 2.350895\n",
      "Training Epoch: 24 [25728/50000]\tLoss: 4.1996\tLR: 2.351151\n",
      "Training Epoch: 24 [25856/50000]\tLoss: 4.2587\tLR: 2.351407\n",
      "Training Epoch: 24 [25984/50000]\tLoss: 4.1866\tLR: 2.351662\n",
      "Training Epoch: 24 [26112/50000]\tLoss: 4.3161\tLR: 2.351918\n",
      "Training Epoch: 24 [26240/50000]\tLoss: 4.3687\tLR: 2.352174\n",
      "Training Epoch: 24 [26368/50000]\tLoss: 4.2917\tLR: 2.352430\n",
      "Training Epoch: 24 [26496/50000]\tLoss: 4.2338\tLR: 2.352685\n",
      "Training Epoch: 24 [26624/50000]\tLoss: 4.4184\tLR: 2.352941\n",
      "Training Epoch: 24 [26752/50000]\tLoss: 4.1892\tLR: 2.353197\n",
      "Training Epoch: 24 [26880/50000]\tLoss: 4.2625\tLR: 2.353453\n",
      "Training Epoch: 24 [27008/50000]\tLoss: 4.0927\tLR: 2.353708\n",
      "Training Epoch: 24 [27136/50000]\tLoss: 4.3064\tLR: 2.353964\n",
      "Training Epoch: 24 [27264/50000]\tLoss: 4.2180\tLR: 2.354220\n",
      "Training Epoch: 24 [27392/50000]\tLoss: 4.3405\tLR: 2.354476\n",
      "Training Epoch: 24 [27520/50000]\tLoss: 4.2337\tLR: 2.354731\n",
      "Training Epoch: 24 [27648/50000]\tLoss: 4.3844\tLR: 2.354987\n",
      "Training Epoch: 24 [27776/50000]\tLoss: 4.2244\tLR: 2.355243\n",
      "Training Epoch: 24 [27904/50000]\tLoss: 4.2605\tLR: 2.355499\n",
      "Training Epoch: 24 [28032/50000]\tLoss: 4.2684\tLR: 2.355754\n",
      "Training Epoch: 24 [28160/50000]\tLoss: 4.2595\tLR: 2.356010\n",
      "Training Epoch: 24 [28288/50000]\tLoss: 4.4622\tLR: 2.356266\n",
      "Training Epoch: 24 [28416/50000]\tLoss: 4.3992\tLR: 2.356522\n",
      "Training Epoch: 24 [28544/50000]\tLoss: 4.1560\tLR: 2.356777\n",
      "Training Epoch: 24 [28672/50000]\tLoss: 4.1991\tLR: 2.357033\n",
      "Training Epoch: 24 [28800/50000]\tLoss: 4.3173\tLR: 2.357289\n",
      "Training Epoch: 24 [28928/50000]\tLoss: 4.2359\tLR: 2.357545\n",
      "Training Epoch: 24 [29056/50000]\tLoss: 4.3954\tLR: 2.357801\n",
      "Training Epoch: 24 [29184/50000]\tLoss: 4.3729\tLR: 2.358056\n",
      "Training Epoch: 24 [29312/50000]\tLoss: 4.3476\tLR: 2.358312\n",
      "Training Epoch: 24 [29440/50000]\tLoss: 4.3341\tLR: 2.358568\n",
      "Training Epoch: 24 [29568/50000]\tLoss: 4.3308\tLR: 2.358824\n",
      "Training Epoch: 24 [29696/50000]\tLoss: 4.2080\tLR: 2.359079\n",
      "Training Epoch: 24 [29824/50000]\tLoss: 4.2227\tLR: 2.359335\n",
      "Training Epoch: 24 [29952/50000]\tLoss: 4.3635\tLR: 2.359591\n",
      "Training Epoch: 24 [30080/50000]\tLoss: 4.3655\tLR: 2.359847\n",
      "Training Epoch: 24 [30208/50000]\tLoss: 4.1493\tLR: 2.360102\n",
      "Training Epoch: 24 [30336/50000]\tLoss: 4.3118\tLR: 2.360358\n",
      "Training Epoch: 24 [30464/50000]\tLoss: 4.3560\tLR: 2.360614\n",
      "Training Epoch: 24 [30592/50000]\tLoss: 4.1296\tLR: 2.360870\n",
      "Training Epoch: 24 [30720/50000]\tLoss: 4.2418\tLR: 2.361125\n",
      "Training Epoch: 24 [30848/50000]\tLoss: 4.2902\tLR: 2.361381\n",
      "Training Epoch: 24 [30976/50000]\tLoss: 4.3342\tLR: 2.361637\n",
      "Training Epoch: 24 [31104/50000]\tLoss: 4.2420\tLR: 2.361893\n",
      "Training Epoch: 24 [31232/50000]\tLoss: 4.4176\tLR: 2.362148\n",
      "Training Epoch: 24 [31360/50000]\tLoss: 4.1999\tLR: 2.362404\n",
      "Training Epoch: 24 [31488/50000]\tLoss: 4.3612\tLR: 2.362660\n",
      "Training Epoch: 24 [31616/50000]\tLoss: 4.4000\tLR: 2.362916\n",
      "Training Epoch: 24 [31744/50000]\tLoss: 4.4150\tLR: 2.363171\n",
      "Training Epoch: 24 [31872/50000]\tLoss: 4.4029\tLR: 2.363427\n",
      "Training Epoch: 24 [32000/50000]\tLoss: 4.4166\tLR: 2.363683\n",
      "Training Epoch: 24 [32128/50000]\tLoss: 4.2669\tLR: 2.363939\n",
      "Training Epoch: 24 [32256/50000]\tLoss: 4.1792\tLR: 2.364194\n",
      "Training Epoch: 24 [32384/50000]\tLoss: 4.3002\tLR: 2.364450\n",
      "Training Epoch: 24 [32512/50000]\tLoss: 4.4094\tLR: 2.364706\n",
      "Training Epoch: 24 [32640/50000]\tLoss: 4.1943\tLR: 2.364962\n",
      "Training Epoch: 24 [32768/50000]\tLoss: 4.2526\tLR: 2.365217\n",
      "Training Epoch: 24 [32896/50000]\tLoss: 4.3720\tLR: 2.365473\n",
      "Training Epoch: 24 [33024/50000]\tLoss: 4.2158\tLR: 2.365729\n",
      "Training Epoch: 24 [33152/50000]\tLoss: 4.0663\tLR: 2.365985\n",
      "Training Epoch: 24 [33280/50000]\tLoss: 4.3071\tLR: 2.366240\n",
      "Training Epoch: 24 [33408/50000]\tLoss: 4.3620\tLR: 2.366496\n",
      "Training Epoch: 24 [33536/50000]\tLoss: 4.4004\tLR: 2.366752\n",
      "Training Epoch: 24 [33664/50000]\tLoss: 4.3469\tLR: 2.367008\n",
      "Training Epoch: 24 [33792/50000]\tLoss: 4.3910\tLR: 2.367263\n",
      "Training Epoch: 24 [33920/50000]\tLoss: 4.2793\tLR: 2.367519\n",
      "Training Epoch: 24 [34048/50000]\tLoss: 4.3109\tLR: 2.367775\n",
      "Training Epoch: 24 [34176/50000]\tLoss: 4.3064\tLR: 2.368031\n",
      "Training Epoch: 24 [34304/50000]\tLoss: 4.4450\tLR: 2.368286\n",
      "Training Epoch: 24 [34432/50000]\tLoss: 4.2932\tLR: 2.368542\n",
      "Training Epoch: 24 [34560/50000]\tLoss: 4.2601\tLR: 2.368798\n",
      "Training Epoch: 24 [34688/50000]\tLoss: 4.3366\tLR: 2.369054\n",
      "Training Epoch: 24 [34816/50000]\tLoss: 4.1777\tLR: 2.369309\n",
      "Training Epoch: 24 [34944/50000]\tLoss: 4.2726\tLR: 2.369565\n",
      "Training Epoch: 24 [35072/50000]\tLoss: 4.6019\tLR: 2.369821\n",
      "Training Epoch: 24 [35200/50000]\tLoss: 4.2543\tLR: 2.370077\n",
      "Training Epoch: 24 [35328/50000]\tLoss: 4.3580\tLR: 2.370332\n",
      "Training Epoch: 24 [35456/50000]\tLoss: 4.3605\tLR: 2.370588\n",
      "Training Epoch: 24 [35584/50000]\tLoss: 4.3159\tLR: 2.370844\n",
      "Training Epoch: 24 [35712/50000]\tLoss: 4.4676\tLR: 2.371100\n",
      "Training Epoch: 24 [35840/50000]\tLoss: 4.4321\tLR: 2.371355\n",
      "Training Epoch: 24 [35968/50000]\tLoss: 4.3591\tLR: 2.371611\n",
      "Training Epoch: 24 [36096/50000]\tLoss: 4.2699\tLR: 2.371867\n",
      "Training Epoch: 24 [36224/50000]\tLoss: 4.4441\tLR: 2.372123\n",
      "Training Epoch: 24 [36352/50000]\tLoss: 4.3525\tLR: 2.372379\n",
      "Training Epoch: 24 [36480/50000]\tLoss: 4.4469\tLR: 2.372634\n",
      "Training Epoch: 24 [36608/50000]\tLoss: 4.2362\tLR: 2.372890\n",
      "Training Epoch: 24 [36736/50000]\tLoss: 4.2988\tLR: 2.373146\n",
      "Training Epoch: 24 [36864/50000]\tLoss: 4.4219\tLR: 2.373402\n",
      "Training Epoch: 24 [36992/50000]\tLoss: 4.1853\tLR: 2.373657\n",
      "Training Epoch: 24 [37120/50000]\tLoss: 4.2347\tLR: 2.373913\n",
      "Training Epoch: 24 [37248/50000]\tLoss: 4.2667\tLR: 2.374169\n",
      "Training Epoch: 24 [37376/50000]\tLoss: 4.2948\tLR: 2.374425\n",
      "Training Epoch: 24 [37504/50000]\tLoss: 4.4284\tLR: 2.374680\n",
      "Training Epoch: 24 [37632/50000]\tLoss: 4.3866\tLR: 2.374936\n",
      "Training Epoch: 24 [37760/50000]\tLoss: 4.4987\tLR: 2.375192\n",
      "Training Epoch: 24 [37888/50000]\tLoss: 4.4358\tLR: 2.375448\n",
      "Training Epoch: 24 [38016/50000]\tLoss: 4.4166\tLR: 2.375703\n",
      "Training Epoch: 24 [38144/50000]\tLoss: 4.1704\tLR: 2.375959\n",
      "Training Epoch: 24 [38272/50000]\tLoss: 4.3262\tLR: 2.376215\n",
      "Training Epoch: 24 [38400/50000]\tLoss: 4.4011\tLR: 2.376471\n",
      "Training Epoch: 24 [38528/50000]\tLoss: 4.1473\tLR: 2.376726\n",
      "Training Epoch: 24 [38656/50000]\tLoss: 4.3413\tLR: 2.376982\n",
      "Training Epoch: 24 [38784/50000]\tLoss: 4.4265\tLR: 2.377238\n",
      "Training Epoch: 24 [38912/50000]\tLoss: 4.2009\tLR: 2.377494\n",
      "Training Epoch: 24 [39040/50000]\tLoss: 4.3656\tLR: 2.377749\n",
      "Training Epoch: 24 [39168/50000]\tLoss: 4.2615\tLR: 2.378005\n",
      "Training Epoch: 24 [39296/50000]\tLoss: 4.2448\tLR: 2.378261\n",
      "Training Epoch: 24 [39424/50000]\tLoss: 4.1758\tLR: 2.378517\n",
      "Training Epoch: 24 [39552/50000]\tLoss: 4.3036\tLR: 2.378772\n",
      "Training Epoch: 24 [39680/50000]\tLoss: 4.3891\tLR: 2.379028\n",
      "Training Epoch: 24 [39808/50000]\tLoss: 4.1836\tLR: 2.379284\n",
      "Training Epoch: 24 [39936/50000]\tLoss: 4.2306\tLR: 2.379540\n",
      "Training Epoch: 24 [40064/50000]\tLoss: 4.2206\tLR: 2.379795\n",
      "Training Epoch: 24 [40192/50000]\tLoss: 4.2713\tLR: 2.380051\n",
      "Training Epoch: 24 [40320/50000]\tLoss: 4.1916\tLR: 2.380307\n",
      "Training Epoch: 24 [40448/50000]\tLoss: 4.4382\tLR: 2.380563\n",
      "Training Epoch: 24 [40576/50000]\tLoss: 4.1383\tLR: 2.380818\n",
      "Training Epoch: 24 [40704/50000]\tLoss: 4.4574\tLR: 2.381074\n",
      "Training Epoch: 24 [40832/50000]\tLoss: 4.3105\tLR: 2.381330\n",
      "Training Epoch: 24 [40960/50000]\tLoss: 4.2763\tLR: 2.381586\n",
      "Training Epoch: 24 [41088/50000]\tLoss: 4.2802\tLR: 2.381841\n",
      "Training Epoch: 24 [41216/50000]\tLoss: 4.2014\tLR: 2.382097\n",
      "Training Epoch: 24 [41344/50000]\tLoss: 4.3846\tLR: 2.382353\n",
      "Training Epoch: 24 [41472/50000]\tLoss: 4.2721\tLR: 2.382609\n",
      "Training Epoch: 24 [41600/50000]\tLoss: 4.1377\tLR: 2.382864\n",
      "Training Epoch: 24 [41728/50000]\tLoss: 4.2553\tLR: 2.383120\n",
      "Training Epoch: 24 [41856/50000]\tLoss: 4.2707\tLR: 2.383376\n",
      "Training Epoch: 24 [41984/50000]\tLoss: 4.2162\tLR: 2.383632\n",
      "Training Epoch: 24 [42112/50000]\tLoss: 4.2988\tLR: 2.383887\n",
      "Training Epoch: 24 [42240/50000]\tLoss: 4.0757\tLR: 2.384143\n",
      "Training Epoch: 24 [42368/50000]\tLoss: 4.1437\tLR: 2.384399\n",
      "Training Epoch: 24 [42496/50000]\tLoss: 4.1054\tLR: 2.384655\n",
      "Training Epoch: 24 [42624/50000]\tLoss: 4.2878\tLR: 2.384910\n",
      "Training Epoch: 24 [42752/50000]\tLoss: 4.1667\tLR: 2.385166\n",
      "Training Epoch: 24 [42880/50000]\tLoss: 4.2146\tLR: 2.385422\n",
      "Training Epoch: 24 [43008/50000]\tLoss: 4.1830\tLR: 2.385678\n",
      "Training Epoch: 24 [43136/50000]\tLoss: 4.3745\tLR: 2.385934\n",
      "Training Epoch: 24 [43264/50000]\tLoss: 4.1975\tLR: 2.386189\n",
      "Training Epoch: 24 [43392/50000]\tLoss: 4.3366\tLR: 2.386445\n",
      "Training Epoch: 24 [43520/50000]\tLoss: 4.2401\tLR: 2.386701\n",
      "Training Epoch: 24 [43648/50000]\tLoss: 4.2892\tLR: 2.386957\n",
      "Training Epoch: 24 [43776/50000]\tLoss: 4.2320\tLR: 2.387212\n",
      "Training Epoch: 24 [43904/50000]\tLoss: 4.3790\tLR: 2.387468\n",
      "Training Epoch: 24 [44032/50000]\tLoss: 4.1789\tLR: 2.387724\n",
      "Training Epoch: 24 [44160/50000]\tLoss: 4.2256\tLR: 2.387980\n",
      "Training Epoch: 24 [44288/50000]\tLoss: 4.3622\tLR: 2.388235\n",
      "Training Epoch: 24 [44416/50000]\tLoss: 4.0646\tLR: 2.388491\n",
      "Training Epoch: 24 [44544/50000]\tLoss: 4.1790\tLR: 2.388747\n",
      "Training Epoch: 24 [44672/50000]\tLoss: 4.2574\tLR: 2.389003\n",
      "Training Epoch: 24 [44800/50000]\tLoss: 4.2339\tLR: 2.389258\n",
      "Training Epoch: 24 [44928/50000]\tLoss: 4.2269\tLR: 2.389514\n",
      "Training Epoch: 24 [45056/50000]\tLoss: 4.1263\tLR: 2.389770\n",
      "Training Epoch: 24 [45184/50000]\tLoss: 4.3952\tLR: 2.390026\n",
      "Training Epoch: 24 [45312/50000]\tLoss: 4.2661\tLR: 2.390281\n",
      "Training Epoch: 24 [45440/50000]\tLoss: 4.3177\tLR: 2.390537\n",
      "Training Epoch: 24 [45568/50000]\tLoss: 4.2326\tLR: 2.390793\n",
      "Training Epoch: 24 [45696/50000]\tLoss: 4.2665\tLR: 2.391049\n",
      "Training Epoch: 24 [45824/50000]\tLoss: 4.2840\tLR: 2.391304\n",
      "Training Epoch: 24 [45952/50000]\tLoss: 4.1495\tLR: 2.391560\n",
      "Training Epoch: 24 [46080/50000]\tLoss: 4.3811\tLR: 2.391816\n",
      "Training Epoch: 24 [46208/50000]\tLoss: 4.2873\tLR: 2.392072\n",
      "Training Epoch: 24 [46336/50000]\tLoss: 4.1091\tLR: 2.392327\n",
      "Training Epoch: 24 [46464/50000]\tLoss: 4.3306\tLR: 2.392583\n",
      "Training Epoch: 24 [46592/50000]\tLoss: 4.3046\tLR: 2.392839\n",
      "Training Epoch: 24 [46720/50000]\tLoss: 4.2309\tLR: 2.393095\n",
      "Training Epoch: 24 [46848/50000]\tLoss: 4.2630\tLR: 2.393350\n",
      "Training Epoch: 24 [46976/50000]\tLoss: 4.4068\tLR: 2.393606\n",
      "Training Epoch: 24 [47104/50000]\tLoss: 4.4103\tLR: 2.393862\n",
      "Training Epoch: 24 [47232/50000]\tLoss: 4.2895\tLR: 2.394118\n",
      "Training Epoch: 24 [47360/50000]\tLoss: 4.3083\tLR: 2.394373\n",
      "Training Epoch: 24 [47488/50000]\tLoss: 4.3483\tLR: 2.394629\n",
      "Training Epoch: 24 [47616/50000]\tLoss: 4.3048\tLR: 2.394885\n",
      "Training Epoch: 24 [47744/50000]\tLoss: 4.2524\tLR: 2.395141\n",
      "Training Epoch: 24 [47872/50000]\tLoss: 4.3448\tLR: 2.395396\n",
      "Training Epoch: 24 [48000/50000]\tLoss: 4.3439\tLR: 2.395652\n",
      "Training Epoch: 24 [48128/50000]\tLoss: 4.2940\tLR: 2.395908\n",
      "Training Epoch: 24 [48256/50000]\tLoss: 4.2915\tLR: 2.396164\n",
      "Training Epoch: 24 [48384/50000]\tLoss: 4.3763\tLR: 2.396419\n",
      "Training Epoch: 24 [48512/50000]\tLoss: 4.1476\tLR: 2.396675\n",
      "Training Epoch: 24 [48640/50000]\tLoss: 4.2598\tLR: 2.396931\n",
      "Training Epoch: 24 [48768/50000]\tLoss: 4.4267\tLR: 2.397187\n",
      "Training Epoch: 24 [48896/50000]\tLoss: 4.3049\tLR: 2.397442\n",
      "Training Epoch: 24 [49024/50000]\tLoss: 4.3456\tLR: 2.397698\n",
      "Training Epoch: 24 [49152/50000]\tLoss: 4.3385\tLR: 2.397954\n",
      "Training Epoch: 24 [49280/50000]\tLoss: 4.2675\tLR: 2.398210\n",
      "Training Epoch: 24 [49408/50000]\tLoss: 4.1848\tLR: 2.398465\n",
      "Training Epoch: 24 [49536/50000]\tLoss: 4.2312\tLR: 2.398721\n",
      "Training Epoch: 24 [49664/50000]\tLoss: 4.1734\tLR: 2.398977\n",
      "Training Epoch: 24 [49792/50000]\tLoss: 4.2460\tLR: 2.399233\n",
      "Training Epoch: 24 [49920/50000]\tLoss: 4.3628\tLR: 2.399488\n",
      "Training Epoch: 24 [50000/50000]\tLoss: 4.2093\tLR: 2.399744\n",
      "epoch 24 training time consumed: 489.02s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   33647 GB |   33647 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   33544 GB |   33544 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     103 GB |     103 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   33647 GB |   33647 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   33544 GB |   33544 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     103 GB |     103 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   33172 GB |   33172 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   33068 GB |   33068 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     103 GB |     103 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    3568 K  |    3567 K  |\n",
      "|       from large pool |      24    |      65    |    1520 K  |    1520 K  |\n",
      "|       from small pool |     231    |     274    |    2047 K  |    2046 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    3568 K  |    3567 K  |\n",
      "|       from large pool |      24    |      65    |    1520 K  |    1520 K  |\n",
      "|       from small pool |     231    |     274    |    2047 K  |    2046 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      46    |    2067 K  |    2067 K  |\n",
      "|       from large pool |      10    |      23    |     731 K  |     731 K  |\n",
      "|       from small pool |      27    |      35    |    1336 K  |    1336 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 24, Average loss: 0.0340, Accuracy: 0.0511, Time consumed:31.04s\n",
      "\n",
      "Training Epoch: 25 [128/50000]\tLoss: 4.2789\tLR: 0.100000\n",
      "Training Epoch: 25 [256/50000]\tLoss: 4.3774\tLR: 2.400256\n",
      "Training Epoch: 25 [384/50000]\tLoss: 4.2495\tLR: 2.400512\n",
      "Training Epoch: 25 [512/50000]\tLoss: 4.3581\tLR: 2.400767\n",
      "Training Epoch: 25 [640/50000]\tLoss: 4.3104\tLR: 2.401023\n",
      "Training Epoch: 25 [768/50000]\tLoss: 4.3276\tLR: 2.401279\n",
      "Training Epoch: 25 [896/50000]\tLoss: 4.3002\tLR: 2.401535\n",
      "Training Epoch: 25 [1024/50000]\tLoss: 4.2982\tLR: 2.401790\n",
      "Training Epoch: 25 [1152/50000]\tLoss: 4.3481\tLR: 2.402046\n",
      "Training Epoch: 25 [1280/50000]\tLoss: 4.2025\tLR: 2.402302\n",
      "Training Epoch: 25 [1408/50000]\tLoss: 4.3466\tLR: 2.402558\n",
      "Training Epoch: 25 [1536/50000]\tLoss: 4.2091\tLR: 2.402813\n",
      "Training Epoch: 25 [1664/50000]\tLoss: 4.2008\tLR: 2.403069\n",
      "Training Epoch: 25 [1792/50000]\tLoss: 4.4204\tLR: 2.403325\n",
      "Training Epoch: 25 [1920/50000]\tLoss: 4.2152\tLR: 2.403581\n",
      "Training Epoch: 25 [2048/50000]\tLoss: 4.1419\tLR: 2.403836\n",
      "Training Epoch: 25 [2176/50000]\tLoss: 4.2060\tLR: 2.404092\n",
      "Training Epoch: 25 [2304/50000]\tLoss: 4.1634\tLR: 2.404348\n",
      "Training Epoch: 25 [2432/50000]\tLoss: 4.2242\tLR: 2.404604\n",
      "Training Epoch: 25 [2560/50000]\tLoss: 4.3891\tLR: 2.404859\n",
      "Training Epoch: 25 [2688/50000]\tLoss: 4.1870\tLR: 2.405115\n",
      "Training Epoch: 25 [2816/50000]\tLoss: 4.2704\tLR: 2.405371\n",
      "Training Epoch: 25 [2944/50000]\tLoss: 4.2802\tLR: 2.405627\n",
      "Training Epoch: 25 [3072/50000]\tLoss: 4.2667\tLR: 2.405882\n",
      "Training Epoch: 25 [3200/50000]\tLoss: 4.3790\tLR: 2.406138\n",
      "Training Epoch: 25 [3328/50000]\tLoss: 4.2795\tLR: 2.406394\n",
      "Training Epoch: 25 [3456/50000]\tLoss: 4.1704\tLR: 2.406650\n",
      "Training Epoch: 25 [3584/50000]\tLoss: 4.2807\tLR: 2.406905\n",
      "Training Epoch: 25 [3712/50000]\tLoss: 4.3344\tLR: 2.407161\n",
      "Training Epoch: 25 [3840/50000]\tLoss: 4.3147\tLR: 2.407417\n",
      "Training Epoch: 25 [3968/50000]\tLoss: 4.4061\tLR: 2.407673\n",
      "Training Epoch: 25 [4096/50000]\tLoss: 4.2970\tLR: 2.407928\n",
      "Training Epoch: 25 [4224/50000]\tLoss: 4.3317\tLR: 2.408184\n",
      "Training Epoch: 25 [4352/50000]\tLoss: 4.2705\tLR: 2.408440\n",
      "Training Epoch: 25 [4480/50000]\tLoss: 4.3151\tLR: 2.408696\n",
      "Training Epoch: 25 [4608/50000]\tLoss: 4.1481\tLR: 2.408951\n",
      "Training Epoch: 25 [4736/50000]\tLoss: 4.3130\tLR: 2.409207\n",
      "Training Epoch: 25 [4864/50000]\tLoss: 4.2307\tLR: 2.409463\n",
      "Training Epoch: 25 [4992/50000]\tLoss: 4.1925\tLR: 2.409719\n",
      "Training Epoch: 25 [5120/50000]\tLoss: 4.1456\tLR: 2.409974\n",
      "Training Epoch: 25 [5248/50000]\tLoss: 4.2324\tLR: 2.410230\n",
      "Training Epoch: 25 [5376/50000]\tLoss: 4.6026\tLR: 2.410486\n",
      "Training Epoch: 25 [5504/50000]\tLoss: 4.2589\tLR: 2.410742\n",
      "Training Epoch: 25 [5632/50000]\tLoss: 4.2110\tLR: 2.410997\n",
      "Training Epoch: 25 [5760/50000]\tLoss: 4.3470\tLR: 2.411253\n",
      "Training Epoch: 25 [5888/50000]\tLoss: 4.3880\tLR: 2.411509\n",
      "Training Epoch: 25 [6016/50000]\tLoss: 4.3399\tLR: 2.411765\n",
      "Training Epoch: 25 [6144/50000]\tLoss: 4.3259\tLR: 2.412020\n",
      "Training Epoch: 25 [6272/50000]\tLoss: 4.2450\tLR: 2.412276\n",
      "Training Epoch: 25 [6400/50000]\tLoss: 4.2797\tLR: 2.412532\n",
      "Training Epoch: 25 [6528/50000]\tLoss: 4.4272\tLR: 2.412788\n",
      "Training Epoch: 25 [6656/50000]\tLoss: 4.2620\tLR: 2.413043\n",
      "Training Epoch: 25 [6784/50000]\tLoss: 4.1741\tLR: 2.413299\n",
      "Training Epoch: 25 [6912/50000]\tLoss: 4.3515\tLR: 2.413555\n",
      "Training Epoch: 25 [7040/50000]\tLoss: 4.2497\tLR: 2.413811\n",
      "Training Epoch: 25 [7168/50000]\tLoss: 4.4386\tLR: 2.414066\n",
      "Training Epoch: 25 [7296/50000]\tLoss: 4.2251\tLR: 2.414322\n",
      "Training Epoch: 25 [7424/50000]\tLoss: 4.2864\tLR: 2.414578\n",
      "Training Epoch: 25 [7552/50000]\tLoss: 4.4312\tLR: 2.414834\n",
      "Training Epoch: 25 [7680/50000]\tLoss: 4.2998\tLR: 2.415090\n",
      "Training Epoch: 25 [7808/50000]\tLoss: 4.3717\tLR: 2.415345\n",
      "Training Epoch: 25 [7936/50000]\tLoss: 4.2295\tLR: 2.415601\n",
      "Training Epoch: 25 [8064/50000]\tLoss: 4.2960\tLR: 2.415857\n",
      "Training Epoch: 25 [8192/50000]\tLoss: 4.3186\tLR: 2.416113\n",
      "Training Epoch: 25 [8320/50000]\tLoss: 4.3375\tLR: 2.416368\n",
      "Training Epoch: 25 [8448/50000]\tLoss: 4.3048\tLR: 2.416624\n",
      "Training Epoch: 25 [8576/50000]\tLoss: 4.3750\tLR: 2.416880\n",
      "Training Epoch: 25 [8704/50000]\tLoss: 4.1437\tLR: 2.417136\n",
      "Training Epoch: 25 [8832/50000]\tLoss: 4.2344\tLR: 2.417391\n",
      "Training Epoch: 25 [8960/50000]\tLoss: 4.1365\tLR: 2.417647\n",
      "Training Epoch: 25 [9088/50000]\tLoss: 4.3900\tLR: 2.417903\n",
      "Training Epoch: 25 [9216/50000]\tLoss: 4.1678\tLR: 2.418159\n",
      "Training Epoch: 25 [9344/50000]\tLoss: 4.0714\tLR: 2.418414\n",
      "Training Epoch: 25 [9472/50000]\tLoss: 4.2314\tLR: 2.418670\n",
      "Training Epoch: 25 [9600/50000]\tLoss: 4.3469\tLR: 2.418926\n",
      "Training Epoch: 25 [9728/50000]\tLoss: 4.3685\tLR: 2.419182\n",
      "Training Epoch: 25 [9856/50000]\tLoss: 4.3682\tLR: 2.419437\n",
      "Training Epoch: 25 [9984/50000]\tLoss: 4.3116\tLR: 2.419693\n",
      "Training Epoch: 25 [10112/50000]\tLoss: 4.3174\tLR: 2.419949\n",
      "Training Epoch: 25 [10240/50000]\tLoss: 4.3478\tLR: 2.420205\n",
      "Training Epoch: 25 [10368/50000]\tLoss: 4.3632\tLR: 2.420460\n",
      "Training Epoch: 25 [10496/50000]\tLoss: 4.1277\tLR: 2.420716\n",
      "Training Epoch: 25 [10624/50000]\tLoss: 4.3779\tLR: 2.420972\n",
      "Training Epoch: 25 [10752/50000]\tLoss: 4.4545\tLR: 2.421228\n",
      "Training Epoch: 25 [10880/50000]\tLoss: 4.3164\tLR: 2.421483\n",
      "Training Epoch: 25 [11008/50000]\tLoss: 4.4791\tLR: 2.421739\n",
      "Training Epoch: 25 [11136/50000]\tLoss: 4.3058\tLR: 2.421995\n",
      "Training Epoch: 25 [11264/50000]\tLoss: 4.2495\tLR: 2.422251\n",
      "Training Epoch: 25 [11392/50000]\tLoss: 4.3185\tLR: 2.422506\n",
      "Training Epoch: 25 [11520/50000]\tLoss: 4.2982\tLR: 2.422762\n",
      "Training Epoch: 25 [11648/50000]\tLoss: 4.3879\tLR: 2.423018\n",
      "Training Epoch: 25 [11776/50000]\tLoss: 4.2525\tLR: 2.423274\n",
      "Training Epoch: 25 [11904/50000]\tLoss: 4.1808\tLR: 2.423529\n",
      "Training Epoch: 25 [12032/50000]\tLoss: 4.2314\tLR: 2.423785\n",
      "Training Epoch: 25 [12160/50000]\tLoss: 4.4349\tLR: 2.424041\n",
      "Training Epoch: 25 [12288/50000]\tLoss: 4.3347\tLR: 2.424297\n",
      "Training Epoch: 25 [12416/50000]\tLoss: 4.2229\tLR: 2.424552\n",
      "Training Epoch: 25 [12544/50000]\tLoss: 4.2842\tLR: 2.424808\n",
      "Training Epoch: 25 [12672/50000]\tLoss: 4.4256\tLR: 2.425064\n",
      "Training Epoch: 25 [12800/50000]\tLoss: 4.1519\tLR: 2.425320\n",
      "Training Epoch: 25 [12928/50000]\tLoss: 4.3243\tLR: 2.425575\n",
      "Training Epoch: 25 [13056/50000]\tLoss: 4.3691\tLR: 2.425831\n",
      "Training Epoch: 25 [13184/50000]\tLoss: 4.3928\tLR: 2.426087\n",
      "Training Epoch: 25 [13312/50000]\tLoss: 4.3732\tLR: 2.426343\n",
      "Training Epoch: 25 [13440/50000]\tLoss: 4.4237\tLR: 2.426598\n",
      "Training Epoch: 25 [13568/50000]\tLoss: 4.4129\tLR: 2.426854\n",
      "Training Epoch: 25 [13696/50000]\tLoss: 4.4161\tLR: 2.427110\n",
      "Training Epoch: 25 [13824/50000]\tLoss: 4.2885\tLR: 2.427366\n",
      "Training Epoch: 25 [13952/50000]\tLoss: 4.3157\tLR: 2.427621\n",
      "Training Epoch: 25 [14080/50000]\tLoss: 4.1363\tLR: 2.427877\n",
      "Training Epoch: 25 [14208/50000]\tLoss: 4.2921\tLR: 2.428133\n",
      "Training Epoch: 25 [14336/50000]\tLoss: 4.1820\tLR: 2.428389\n",
      "Training Epoch: 25 [14464/50000]\tLoss: 4.3091\tLR: 2.428645\n",
      "Training Epoch: 25 [14592/50000]\tLoss: 4.2805\tLR: 2.428900\n",
      "Training Epoch: 25 [14720/50000]\tLoss: 4.4129\tLR: 2.429156\n",
      "Training Epoch: 25 [14848/50000]\tLoss: 4.3319\tLR: 2.429412\n",
      "Training Epoch: 25 [14976/50000]\tLoss: 4.3526\tLR: 2.429668\n",
      "Training Epoch: 25 [15104/50000]\tLoss: 4.2667\tLR: 2.429923\n",
      "Training Epoch: 25 [15232/50000]\tLoss: 4.2597\tLR: 2.430179\n",
      "Training Epoch: 25 [15360/50000]\tLoss: 4.2411\tLR: 2.430435\n",
      "Training Epoch: 25 [15488/50000]\tLoss: 4.2708\tLR: 2.430691\n",
      "Training Epoch: 25 [15616/50000]\tLoss: 4.1471\tLR: 2.430946\n",
      "Training Epoch: 25 [15744/50000]\tLoss: 4.3108\tLR: 2.431202\n",
      "Training Epoch: 25 [15872/50000]\tLoss: 4.4812\tLR: 2.431458\n",
      "Training Epoch: 25 [16000/50000]\tLoss: 4.2448\tLR: 2.431714\n",
      "Training Epoch: 25 [16128/50000]\tLoss: 4.3263\tLR: 2.431969\n",
      "Training Epoch: 25 [16256/50000]\tLoss: 4.3506\tLR: 2.432225\n",
      "Training Epoch: 25 [16384/50000]\tLoss: 4.1588\tLR: 2.432481\n",
      "Training Epoch: 25 [16512/50000]\tLoss: 4.2113\tLR: 2.432737\n",
      "Training Epoch: 25 [16640/50000]\tLoss: 4.3166\tLR: 2.432992\n",
      "Training Epoch: 25 [16768/50000]\tLoss: 4.2244\tLR: 2.433248\n",
      "Training Epoch: 25 [16896/50000]\tLoss: 4.3201\tLR: 2.433504\n",
      "Training Epoch: 25 [17024/50000]\tLoss: 4.3689\tLR: 2.433760\n",
      "Training Epoch: 25 [17152/50000]\tLoss: 4.1725\tLR: 2.434015\n",
      "Training Epoch: 25 [17280/50000]\tLoss: 4.2062\tLR: 2.434271\n",
      "Training Epoch: 25 [17408/50000]\tLoss: 4.3616\tLR: 2.434527\n",
      "Training Epoch: 25 [17536/50000]\tLoss: 4.3853\tLR: 2.434783\n",
      "Training Epoch: 25 [17664/50000]\tLoss: 4.3924\tLR: 2.435038\n",
      "Training Epoch: 25 [17792/50000]\tLoss: 4.3046\tLR: 2.435294\n",
      "Training Epoch: 25 [17920/50000]\tLoss: 4.2622\tLR: 2.435550\n",
      "Training Epoch: 25 [18048/50000]\tLoss: 4.2285\tLR: 2.435806\n",
      "Training Epoch: 25 [18176/50000]\tLoss: 4.3009\tLR: 2.436061\n",
      "Training Epoch: 25 [18304/50000]\tLoss: 4.4115\tLR: 2.436317\n",
      "Training Epoch: 25 [18432/50000]\tLoss: 4.3905\tLR: 2.436573\n",
      "Training Epoch: 25 [18560/50000]\tLoss: 4.3882\tLR: 2.436829\n",
      "Training Epoch: 25 [18688/50000]\tLoss: 4.2337\tLR: 2.437084\n",
      "Training Epoch: 25 [18816/50000]\tLoss: 4.3559\tLR: 2.437340\n",
      "Training Epoch: 25 [18944/50000]\tLoss: 4.3301\tLR: 2.437596\n",
      "Training Epoch: 25 [19072/50000]\tLoss: 4.5023\tLR: 2.437852\n",
      "Training Epoch: 25 [19200/50000]\tLoss: 4.3740\tLR: 2.438107\n",
      "Training Epoch: 25 [19328/50000]\tLoss: 4.3071\tLR: 2.438363\n",
      "Training Epoch: 25 [19456/50000]\tLoss: 4.2692\tLR: 2.438619\n",
      "Training Epoch: 25 [19584/50000]\tLoss: 4.2324\tLR: 2.438875\n",
      "Training Epoch: 25 [19712/50000]\tLoss: 4.3397\tLR: 2.439130\n",
      "Training Epoch: 25 [19840/50000]\tLoss: 4.2742\tLR: 2.439386\n",
      "Training Epoch: 25 [19968/50000]\tLoss: 4.3592\tLR: 2.439642\n",
      "Training Epoch: 25 [20096/50000]\tLoss: 4.2670\tLR: 2.439898\n",
      "Training Epoch: 25 [20224/50000]\tLoss: 4.3414\tLR: 2.440153\n",
      "Training Epoch: 25 [20352/50000]\tLoss: 4.3220\tLR: 2.440409\n",
      "Training Epoch: 25 [20480/50000]\tLoss: 4.2686\tLR: 2.440665\n",
      "Training Epoch: 25 [20608/50000]\tLoss: 4.1975\tLR: 2.440921\n",
      "Training Epoch: 25 [20736/50000]\tLoss: 4.4225\tLR: 2.441176\n",
      "Training Epoch: 25 [20864/50000]\tLoss: 4.1889\tLR: 2.441432\n",
      "Training Epoch: 25 [20992/50000]\tLoss: 4.0795\tLR: 2.441688\n",
      "Training Epoch: 25 [21120/50000]\tLoss: 4.3048\tLR: 2.441944\n",
      "Training Epoch: 25 [21248/50000]\tLoss: 4.2190\tLR: 2.442199\n",
      "Training Epoch: 25 [21376/50000]\tLoss: 4.4520\tLR: 2.442455\n",
      "Training Epoch: 25 [21504/50000]\tLoss: 4.2779\tLR: 2.442711\n",
      "Training Epoch: 25 [21632/50000]\tLoss: 4.2408\tLR: 2.442967\n",
      "Training Epoch: 25 [21760/50000]\tLoss: 4.2592\tLR: 2.443223\n",
      "Training Epoch: 25 [21888/50000]\tLoss: 4.2980\tLR: 2.443478\n",
      "Training Epoch: 25 [22016/50000]\tLoss: 4.2641\tLR: 2.443734\n",
      "Training Epoch: 25 [22144/50000]\tLoss: 4.2275\tLR: 2.443990\n",
      "Training Epoch: 25 [22272/50000]\tLoss: 4.4050\tLR: 2.444246\n",
      "Training Epoch: 25 [22400/50000]\tLoss: 4.2434\tLR: 2.444501\n",
      "Training Epoch: 25 [22528/50000]\tLoss: 4.1642\tLR: 2.444757\n",
      "Training Epoch: 25 [22656/50000]\tLoss: 4.3523\tLR: 2.445013\n",
      "Training Epoch: 25 [22784/50000]\tLoss: 4.2952\tLR: 2.445269\n",
      "Training Epoch: 25 [22912/50000]\tLoss: 4.5896\tLR: 2.445524\n",
      "Training Epoch: 25 [23040/50000]\tLoss: 4.4029\tLR: 2.445780\n",
      "Training Epoch: 25 [23168/50000]\tLoss: 4.3710\tLR: 2.446036\n",
      "Training Epoch: 25 [23296/50000]\tLoss: 4.3571\tLR: 2.446292\n",
      "Training Epoch: 25 [23424/50000]\tLoss: 4.3347\tLR: 2.446547\n",
      "Training Epoch: 25 [23552/50000]\tLoss: 4.4388\tLR: 2.446803\n",
      "Training Epoch: 25 [23680/50000]\tLoss: 4.3315\tLR: 2.447059\n",
      "Training Epoch: 25 [23808/50000]\tLoss: 4.3499\tLR: 2.447315\n",
      "Training Epoch: 25 [23936/50000]\tLoss: 4.3163\tLR: 2.447570\n",
      "Training Epoch: 25 [24064/50000]\tLoss: 4.2600\tLR: 2.447826\n",
      "Training Epoch: 25 [24192/50000]\tLoss: 4.2345\tLR: 2.448082\n",
      "Training Epoch: 25 [24320/50000]\tLoss: 4.3619\tLR: 2.448338\n",
      "Training Epoch: 25 [24448/50000]\tLoss: 4.4517\tLR: 2.448593\n",
      "Training Epoch: 25 [24576/50000]\tLoss: 4.1134\tLR: 2.448849\n",
      "Training Epoch: 25 [24704/50000]\tLoss: 4.5697\tLR: 2.449105\n",
      "Training Epoch: 25 [24832/50000]\tLoss: 4.4233\tLR: 2.449361\n",
      "Training Epoch: 25 [24960/50000]\tLoss: 4.3916\tLR: 2.449616\n",
      "Training Epoch: 25 [25088/50000]\tLoss: 4.3231\tLR: 2.449872\n",
      "Training Epoch: 25 [25216/50000]\tLoss: 4.4064\tLR: 2.450128\n",
      "Training Epoch: 25 [25344/50000]\tLoss: 4.4459\tLR: 2.450384\n",
      "Training Epoch: 25 [25472/50000]\tLoss: 4.3574\tLR: 2.450639\n",
      "Training Epoch: 25 [25600/50000]\tLoss: 4.4586\tLR: 2.450895\n",
      "Training Epoch: 25 [25728/50000]\tLoss: 4.1252\tLR: 2.451151\n",
      "Training Epoch: 25 [25856/50000]\tLoss: 4.5956\tLR: 2.451407\n",
      "Training Epoch: 25 [25984/50000]\tLoss: 4.3626\tLR: 2.451662\n",
      "Training Epoch: 25 [26112/50000]\tLoss: 4.4436\tLR: 2.451918\n",
      "Training Epoch: 25 [26240/50000]\tLoss: 4.3862\tLR: 2.452174\n",
      "Training Epoch: 25 [26368/50000]\tLoss: 4.4753\tLR: 2.452430\n",
      "Training Epoch: 25 [26496/50000]\tLoss: 4.4244\tLR: 2.452685\n",
      "Training Epoch: 25 [26624/50000]\tLoss: 4.3787\tLR: 2.452941\n",
      "Training Epoch: 25 [26752/50000]\tLoss: 4.4396\tLR: 2.453197\n",
      "Training Epoch: 25 [26880/50000]\tLoss: 4.2667\tLR: 2.453453\n",
      "Training Epoch: 25 [27008/50000]\tLoss: 4.2006\tLR: 2.453708\n",
      "Training Epoch: 25 [27136/50000]\tLoss: 4.3877\tLR: 2.453964\n",
      "Training Epoch: 25 [27264/50000]\tLoss: 4.2864\tLR: 2.454220\n",
      "Training Epoch: 25 [27392/50000]\tLoss: 4.4233\tLR: 2.454476\n",
      "Training Epoch: 25 [27520/50000]\tLoss: 4.4867\tLR: 2.454731\n",
      "Training Epoch: 25 [27648/50000]\tLoss: 4.4056\tLR: 2.454987\n",
      "Training Epoch: 25 [27776/50000]\tLoss: 4.2333\tLR: 2.455243\n",
      "Training Epoch: 25 [27904/50000]\tLoss: 4.3325\tLR: 2.455499\n",
      "Training Epoch: 25 [28032/50000]\tLoss: 4.3339\tLR: 2.455754\n",
      "Training Epoch: 25 [28160/50000]\tLoss: 4.4249\tLR: 2.456010\n",
      "Training Epoch: 25 [28288/50000]\tLoss: 4.3685\tLR: 2.456266\n",
      "Training Epoch: 25 [28416/50000]\tLoss: 4.3198\tLR: 2.456522\n",
      "Training Epoch: 25 [28544/50000]\tLoss: 4.4292\tLR: 2.456777\n",
      "Training Epoch: 25 [28672/50000]\tLoss: 4.4198\tLR: 2.457033\n",
      "Training Epoch: 25 [28800/50000]\tLoss: 4.2619\tLR: 2.457289\n",
      "Training Epoch: 25 [28928/50000]\tLoss: 4.5495\tLR: 2.457545\n",
      "Training Epoch: 25 [29056/50000]\tLoss: 4.2435\tLR: 2.457801\n",
      "Training Epoch: 25 [29184/50000]\tLoss: 4.1647\tLR: 2.458056\n",
      "Training Epoch: 25 [29312/50000]\tLoss: 4.1715\tLR: 2.458312\n",
      "Training Epoch: 25 [29440/50000]\tLoss: 4.3016\tLR: 2.458568\n",
      "Training Epoch: 25 [29568/50000]\tLoss: 4.3489\tLR: 2.458824\n",
      "Training Epoch: 25 [29696/50000]\tLoss: 4.3644\tLR: 2.459079\n",
      "Training Epoch: 25 [29824/50000]\tLoss: 4.3259\tLR: 2.459335\n",
      "Training Epoch: 25 [29952/50000]\tLoss: 4.2420\tLR: 2.459591\n",
      "Training Epoch: 25 [30080/50000]\tLoss: 4.3997\tLR: 2.459847\n",
      "Training Epoch: 25 [30208/50000]\tLoss: 4.4746\tLR: 2.460102\n",
      "Training Epoch: 25 [30336/50000]\tLoss: 4.1529\tLR: 2.460358\n",
      "Training Epoch: 25 [30464/50000]\tLoss: 4.2765\tLR: 2.460614\n",
      "Training Epoch: 25 [30592/50000]\tLoss: 4.3827\tLR: 2.460870\n",
      "Training Epoch: 25 [30720/50000]\tLoss: 4.3763\tLR: 2.461125\n",
      "Training Epoch: 25 [30848/50000]\tLoss: 4.3505\tLR: 2.461381\n",
      "Training Epoch: 25 [30976/50000]\tLoss: 4.3840\tLR: 2.461637\n",
      "Training Epoch: 25 [31104/50000]\tLoss: 4.2626\tLR: 2.461893\n",
      "Training Epoch: 25 [31232/50000]\tLoss: 4.3582\tLR: 2.462148\n",
      "Training Epoch: 25 [31360/50000]\tLoss: 4.2694\tLR: 2.462404\n",
      "Training Epoch: 25 [31488/50000]\tLoss: 4.4197\tLR: 2.462660\n",
      "Training Epoch: 25 [31616/50000]\tLoss: 4.3267\tLR: 2.462916\n",
      "Training Epoch: 25 [31744/50000]\tLoss: 4.2228\tLR: 2.463171\n",
      "Training Epoch: 25 [31872/50000]\tLoss: 4.2664\tLR: 2.463427\n",
      "Training Epoch: 25 [32000/50000]\tLoss: 4.3378\tLR: 2.463683\n",
      "Training Epoch: 25 [32128/50000]\tLoss: 4.1994\tLR: 2.463939\n",
      "Training Epoch: 25 [32256/50000]\tLoss: 4.2980\tLR: 2.464194\n",
      "Training Epoch: 25 [32384/50000]\tLoss: 4.2366\tLR: 2.464450\n",
      "Training Epoch: 25 [32512/50000]\tLoss: 4.4419\tLR: 2.464706\n",
      "Training Epoch: 25 [32640/50000]\tLoss: 4.2747\tLR: 2.464962\n",
      "Training Epoch: 25 [32768/50000]\tLoss: 4.2265\tLR: 2.465217\n",
      "Training Epoch: 25 [32896/50000]\tLoss: 4.3006\tLR: 2.465473\n",
      "Training Epoch: 25 [33024/50000]\tLoss: 4.2733\tLR: 2.465729\n",
      "Training Epoch: 25 [33152/50000]\tLoss: 4.2510\tLR: 2.465985\n",
      "Training Epoch: 25 [33280/50000]\tLoss: 4.4378\tLR: 2.466240\n",
      "Training Epoch: 25 [33408/50000]\tLoss: 4.2940\tLR: 2.466496\n",
      "Training Epoch: 25 [33536/50000]\tLoss: 4.3080\tLR: 2.466752\n",
      "Training Epoch: 25 [33664/50000]\tLoss: 4.1788\tLR: 2.467008\n",
      "Training Epoch: 25 [33792/50000]\tLoss: 4.3084\tLR: 2.467263\n",
      "Training Epoch: 25 [33920/50000]\tLoss: 4.2477\tLR: 2.467519\n",
      "Training Epoch: 25 [34048/50000]\tLoss: 4.1713\tLR: 2.467775\n",
      "Training Epoch: 25 [34176/50000]\tLoss: 4.2590\tLR: 2.468031\n",
      "Training Epoch: 25 [34304/50000]\tLoss: 4.3896\tLR: 2.468286\n",
      "Training Epoch: 25 [34432/50000]\tLoss: 4.3066\tLR: 2.468542\n",
      "Training Epoch: 25 [34560/50000]\tLoss: 4.2042\tLR: 2.468798\n",
      "Training Epoch: 25 [34688/50000]\tLoss: 4.2835\tLR: 2.469054\n",
      "Training Epoch: 25 [34816/50000]\tLoss: 4.2872\tLR: 2.469309\n",
      "Training Epoch: 25 [34944/50000]\tLoss: 4.2166\tLR: 2.469565\n",
      "Training Epoch: 25 [35072/50000]\tLoss: 4.4904\tLR: 2.469821\n",
      "Training Epoch: 25 [35200/50000]\tLoss: 4.4093\tLR: 2.470077\n",
      "Training Epoch: 25 [35328/50000]\tLoss: 4.1493\tLR: 2.470332\n",
      "Training Epoch: 25 [35456/50000]\tLoss: 4.4479\tLR: 2.470588\n",
      "Training Epoch: 25 [35584/50000]\tLoss: 4.2576\tLR: 2.470844\n",
      "Training Epoch: 25 [35712/50000]\tLoss: 4.3155\tLR: 2.471100\n",
      "Training Epoch: 25 [35840/50000]\tLoss: 4.3593\tLR: 2.471355\n",
      "Training Epoch: 25 [35968/50000]\tLoss: 4.2922\tLR: 2.471611\n",
      "Training Epoch: 25 [36096/50000]\tLoss: 4.2788\tLR: 2.471867\n",
      "Training Epoch: 25 [36224/50000]\tLoss: 4.2746\tLR: 2.472123\n",
      "Training Epoch: 25 [36352/50000]\tLoss: 4.5343\tLR: 2.472379\n",
      "Training Epoch: 25 [36480/50000]\tLoss: 4.4437\tLR: 2.472634\n",
      "Training Epoch: 25 [36608/50000]\tLoss: 4.3331\tLR: 2.472890\n",
      "Training Epoch: 25 [36736/50000]\tLoss: 4.2873\tLR: 2.473146\n",
      "Training Epoch: 25 [36864/50000]\tLoss: 4.4237\tLR: 2.473402\n",
      "Training Epoch: 25 [36992/50000]\tLoss: 4.2655\tLR: 2.473657\n",
      "Training Epoch: 25 [37120/50000]\tLoss: 4.3214\tLR: 2.473913\n",
      "Training Epoch: 25 [37248/50000]\tLoss: 4.1367\tLR: 2.474169\n",
      "Training Epoch: 25 [37376/50000]\tLoss: 4.1651\tLR: 2.474425\n",
      "Training Epoch: 25 [37504/50000]\tLoss: 4.2451\tLR: 2.474680\n",
      "Training Epoch: 25 [37632/50000]\tLoss: 4.2715\tLR: 2.474936\n",
      "Training Epoch: 25 [37760/50000]\tLoss: 4.1194\tLR: 2.475192\n",
      "Training Epoch: 25 [37888/50000]\tLoss: 4.5005\tLR: 2.475448\n",
      "Training Epoch: 25 [38016/50000]\tLoss: 4.1697\tLR: 2.475703\n",
      "Training Epoch: 25 [38144/50000]\tLoss: 4.2096\tLR: 2.475959\n",
      "Training Epoch: 25 [38272/50000]\tLoss: 4.2652\tLR: 2.476215\n",
      "Training Epoch: 25 [38400/50000]\tLoss: 4.3126\tLR: 2.476471\n",
      "Training Epoch: 25 [38528/50000]\tLoss: 4.4328\tLR: 2.476726\n",
      "Training Epoch: 25 [38656/50000]\tLoss: 4.3470\tLR: 2.476982\n",
      "Training Epoch: 25 [38784/50000]\tLoss: 4.3350\tLR: 2.477238\n",
      "Training Epoch: 25 [38912/50000]\tLoss: 4.3753\tLR: 2.477494\n",
      "Training Epoch: 25 [39040/50000]\tLoss: 4.4123\tLR: 2.477749\n",
      "Training Epoch: 25 [39168/50000]\tLoss: 4.1728\tLR: 2.478005\n",
      "Training Epoch: 25 [39296/50000]\tLoss: 4.1763\tLR: 2.478261\n",
      "Training Epoch: 25 [39424/50000]\tLoss: 4.3276\tLR: 2.478517\n",
      "Training Epoch: 25 [39552/50000]\tLoss: 4.3909\tLR: 2.478772\n",
      "Training Epoch: 25 [39680/50000]\tLoss: 4.4859\tLR: 2.479028\n",
      "Training Epoch: 25 [39808/50000]\tLoss: 4.2292\tLR: 2.479284\n",
      "Training Epoch: 25 [39936/50000]\tLoss: 4.3100\tLR: 2.479540\n",
      "Training Epoch: 25 [40064/50000]\tLoss: 4.4850\tLR: 2.479795\n",
      "Training Epoch: 25 [40192/50000]\tLoss: 4.4344\tLR: 2.480051\n",
      "Training Epoch: 25 [40320/50000]\tLoss: 4.3026\tLR: 2.480307\n",
      "Training Epoch: 25 [40448/50000]\tLoss: 4.3920\tLR: 2.480563\n",
      "Training Epoch: 25 [40576/50000]\tLoss: 4.3495\tLR: 2.480818\n",
      "Training Epoch: 25 [40704/50000]\tLoss: 4.3929\tLR: 2.481074\n",
      "Training Epoch: 25 [40832/50000]\tLoss: 4.3985\tLR: 2.481330\n",
      "Training Epoch: 25 [40960/50000]\tLoss: 4.1818\tLR: 2.481586\n",
      "Training Epoch: 25 [41088/50000]\tLoss: 4.2551\tLR: 2.481841\n",
      "Training Epoch: 25 [41216/50000]\tLoss: 4.4104\tLR: 2.482097\n",
      "Training Epoch: 25 [41344/50000]\tLoss: 4.3505\tLR: 2.482353\n",
      "Training Epoch: 25 [41472/50000]\tLoss: 4.2004\tLR: 2.482609\n",
      "Training Epoch: 25 [41600/50000]\tLoss: 4.3838\tLR: 2.482864\n",
      "Training Epoch: 25 [41728/50000]\tLoss: 4.2752\tLR: 2.483120\n",
      "Training Epoch: 25 [41856/50000]\tLoss: 4.2697\tLR: 2.483376\n",
      "Training Epoch: 25 [41984/50000]\tLoss: 4.3059\tLR: 2.483632\n",
      "Training Epoch: 25 [42112/50000]\tLoss: 4.1975\tLR: 2.483887\n",
      "Training Epoch: 25 [42240/50000]\tLoss: 4.3278\tLR: 2.484143\n",
      "Training Epoch: 25 [42368/50000]\tLoss: 4.2028\tLR: 2.484399\n",
      "Training Epoch: 25 [42496/50000]\tLoss: 4.3126\tLR: 2.484655\n",
      "Training Epoch: 25 [42624/50000]\tLoss: 4.2897\tLR: 2.484910\n",
      "Training Epoch: 25 [42752/50000]\tLoss: 4.3778\tLR: 2.485166\n",
      "Training Epoch: 25 [42880/50000]\tLoss: 4.3232\tLR: 2.485422\n",
      "Training Epoch: 25 [43008/50000]\tLoss: 4.2413\tLR: 2.485678\n",
      "Training Epoch: 25 [43136/50000]\tLoss: 4.4939\tLR: 2.485934\n",
      "Training Epoch: 25 [43264/50000]\tLoss: 4.2536\tLR: 2.486189\n",
      "Training Epoch: 25 [43392/50000]\tLoss: 4.3199\tLR: 2.486445\n",
      "Training Epoch: 25 [43520/50000]\tLoss: 4.3468\tLR: 2.486701\n",
      "Training Epoch: 25 [43648/50000]\tLoss: 4.2509\tLR: 2.486957\n",
      "Training Epoch: 25 [43776/50000]\tLoss: 4.4113\tLR: 2.487212\n",
      "Training Epoch: 25 [43904/50000]\tLoss: 4.1625\tLR: 2.487468\n",
      "Training Epoch: 25 [44032/50000]\tLoss: 4.3008\tLR: 2.487724\n",
      "Training Epoch: 25 [44160/50000]\tLoss: 4.1677\tLR: 2.487980\n",
      "Training Epoch: 25 [44288/50000]\tLoss: 4.1633\tLR: 2.488235\n",
      "Training Epoch: 25 [44416/50000]\tLoss: 4.3229\tLR: 2.488491\n",
      "Training Epoch: 25 [44544/50000]\tLoss: 4.4236\tLR: 2.488747\n",
      "Training Epoch: 25 [44672/50000]\tLoss: 4.2902\tLR: 2.489003\n",
      "Training Epoch: 25 [44800/50000]\tLoss: 4.3679\tLR: 2.489258\n",
      "Training Epoch: 25 [44928/50000]\tLoss: 4.4364\tLR: 2.489514\n",
      "Training Epoch: 25 [45056/50000]\tLoss: 4.4731\tLR: 2.489770\n",
      "Training Epoch: 25 [45184/50000]\tLoss: 4.4429\tLR: 2.490026\n",
      "Training Epoch: 25 [45312/50000]\tLoss: 4.3210\tLR: 2.490281\n",
      "Training Epoch: 25 [45440/50000]\tLoss: 4.2169\tLR: 2.490537\n",
      "Training Epoch: 25 [45568/50000]\tLoss: 4.0926\tLR: 2.490793\n",
      "Training Epoch: 25 [45696/50000]\tLoss: 4.4421\tLR: 2.491049\n",
      "Training Epoch: 25 [45824/50000]\tLoss: 4.2366\tLR: 2.491304\n",
      "Training Epoch: 25 [45952/50000]\tLoss: 4.1930\tLR: 2.491560\n",
      "Training Epoch: 25 [46080/50000]\tLoss: 4.3261\tLR: 2.491816\n",
      "Training Epoch: 25 [46208/50000]\tLoss: 4.1975\tLR: 2.492072\n",
      "Training Epoch: 25 [46336/50000]\tLoss: 4.1942\tLR: 2.492327\n",
      "Training Epoch: 25 [46464/50000]\tLoss: 4.3553\tLR: 2.492583\n",
      "Training Epoch: 25 [46592/50000]\tLoss: 4.4356\tLR: 2.492839\n",
      "Training Epoch: 25 [46720/50000]\tLoss: 4.3712\tLR: 2.493095\n",
      "Training Epoch: 25 [46848/50000]\tLoss: 4.3954\tLR: 2.493350\n",
      "Training Epoch: 25 [46976/50000]\tLoss: 4.3671\tLR: 2.493606\n",
      "Training Epoch: 25 [47104/50000]\tLoss: 4.3927\tLR: 2.493862\n",
      "Training Epoch: 25 [47232/50000]\tLoss: 4.3481\tLR: 2.494118\n",
      "Training Epoch: 25 [47360/50000]\tLoss: 4.4469\tLR: 2.494373\n",
      "Training Epoch: 25 [47488/50000]\tLoss: 4.1943\tLR: 2.494629\n",
      "Training Epoch: 25 [47616/50000]\tLoss: 4.1576\tLR: 2.494885\n",
      "Training Epoch: 25 [47744/50000]\tLoss: 4.1943\tLR: 2.495141\n",
      "Training Epoch: 25 [47872/50000]\tLoss: 4.2521\tLR: 2.495396\n",
      "Training Epoch: 25 [48000/50000]\tLoss: 4.2284\tLR: 2.495652\n",
      "Training Epoch: 25 [48128/50000]\tLoss: 4.2221\tLR: 2.495908\n",
      "Training Epoch: 25 [48256/50000]\tLoss: 4.3845\tLR: 2.496164\n",
      "Training Epoch: 25 [48384/50000]\tLoss: 4.3104\tLR: 2.496419\n",
      "Training Epoch: 25 [48512/50000]\tLoss: 4.2910\tLR: 2.496675\n",
      "Training Epoch: 25 [48640/50000]\tLoss: 4.2411\tLR: 2.496931\n",
      "Training Epoch: 25 [48768/50000]\tLoss: 4.3425\tLR: 2.497187\n",
      "Training Epoch: 25 [48896/50000]\tLoss: 4.3709\tLR: 2.497442\n",
      "Training Epoch: 25 [49024/50000]\tLoss: 4.3413\tLR: 2.497698\n",
      "Training Epoch: 25 [49152/50000]\tLoss: 4.4184\tLR: 2.497954\n",
      "Training Epoch: 25 [49280/50000]\tLoss: 4.3443\tLR: 2.498210\n",
      "Training Epoch: 25 [49408/50000]\tLoss: 4.3375\tLR: 2.498465\n",
      "Training Epoch: 25 [49536/50000]\tLoss: 4.4564\tLR: 2.498721\n",
      "Training Epoch: 25 [49664/50000]\tLoss: 4.3198\tLR: 2.498977\n",
      "Training Epoch: 25 [49792/50000]\tLoss: 4.2804\tLR: 2.499233\n",
      "Training Epoch: 25 [49920/50000]\tLoss: 4.3370\tLR: 2.499488\n",
      "Training Epoch: 25 [50000/50000]\tLoss: 4.6301\tLR: 2.499744\n",
      "epoch 25 training time consumed: 488.71s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   35049 GB |   35049 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   34941 GB |   34941 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     107 GB |     107 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   35049 GB |   35049 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   34941 GB |   34941 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     107 GB |     107 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   34554 GB |   34554 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   34446 GB |   34446 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     107 GB |     107 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    3716 K  |    3716 K  |\n",
      "|       from large pool |      24    |      65    |    1584 K  |    1584 K  |\n",
      "|       from small pool |     231    |     274    |    2132 K  |    2132 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    3716 K  |    3716 K  |\n",
      "|       from large pool |      24    |      65    |    1584 K  |    1584 K  |\n",
      "|       from small pool |     231    |     274    |    2132 K  |    2132 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      38    |      46    |    2153 K  |    2153 K  |\n",
      "|       from large pool |      10    |      23    |     761 K  |     761 K  |\n",
      "|       from small pool |      28    |      35    |    1391 K  |    1391 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 25, Average loss: 0.0347, Accuracy: 0.0284, Time consumed:31.07s\n",
      "\n",
      "Training Epoch: 26 [128/50000]\tLoss: 4.2954\tLR: 0.100000\n",
      "Training Epoch: 26 [256/50000]\tLoss: 4.3465\tLR: 2.500256\n",
      "Training Epoch: 26 [384/50000]\tLoss: 4.4201\tLR: 2.500512\n",
      "Training Epoch: 26 [512/50000]\tLoss: 4.3422\tLR: 2.500767\n",
      "Training Epoch: 26 [640/50000]\tLoss: 4.2829\tLR: 2.501023\n",
      "Training Epoch: 26 [768/50000]\tLoss: 4.1710\tLR: 2.501279\n",
      "Training Epoch: 26 [896/50000]\tLoss: 4.4327\tLR: 2.501535\n",
      "Training Epoch: 26 [1024/50000]\tLoss: 4.3901\tLR: 2.501790\n",
      "Training Epoch: 26 [1152/50000]\tLoss: 4.3703\tLR: 2.502046\n",
      "Training Epoch: 26 [1280/50000]\tLoss: 4.3538\tLR: 2.502302\n",
      "Training Epoch: 26 [1408/50000]\tLoss: 4.2502\tLR: 2.502558\n",
      "Training Epoch: 26 [1536/50000]\tLoss: 4.3442\tLR: 2.502813\n",
      "Training Epoch: 26 [1664/50000]\tLoss: 4.2663\tLR: 2.503069\n",
      "Training Epoch: 26 [1792/50000]\tLoss: 4.3138\tLR: 2.503325\n",
      "Training Epoch: 26 [1920/50000]\tLoss: 4.3382\tLR: 2.503581\n",
      "Training Epoch: 26 [2048/50000]\tLoss: 4.2291\tLR: 2.503836\n",
      "Training Epoch: 26 [2176/50000]\tLoss: 4.3210\tLR: 2.504092\n",
      "Training Epoch: 26 [2304/50000]\tLoss: 4.1359\tLR: 2.504348\n",
      "Training Epoch: 26 [2432/50000]\tLoss: 4.3714\tLR: 2.504604\n",
      "Training Epoch: 26 [2560/50000]\tLoss: 4.3665\tLR: 2.504859\n",
      "Training Epoch: 26 [2688/50000]\tLoss: 4.2843\tLR: 2.505115\n",
      "Training Epoch: 26 [2816/50000]\tLoss: 4.1931\tLR: 2.505371\n",
      "Training Epoch: 26 [2944/50000]\tLoss: 4.2693\tLR: 2.505627\n",
      "Training Epoch: 26 [3072/50000]\tLoss: 4.2279\tLR: 2.505882\n",
      "Training Epoch: 26 [3200/50000]\tLoss: 4.2706\tLR: 2.506138\n",
      "Training Epoch: 26 [3328/50000]\tLoss: 4.2700\tLR: 2.506394\n",
      "Training Epoch: 26 [3456/50000]\tLoss: 4.4472\tLR: 2.506650\n",
      "Training Epoch: 26 [3584/50000]\tLoss: 4.4862\tLR: 2.506905\n",
      "Training Epoch: 26 [3712/50000]\tLoss: 4.2740\tLR: 2.507161\n",
      "Training Epoch: 26 [3840/50000]\tLoss: 4.3618\tLR: 2.507417\n",
      "Training Epoch: 26 [3968/50000]\tLoss: 4.4902\tLR: 2.507673\n",
      "Training Epoch: 26 [4096/50000]\tLoss: 4.3867\tLR: 2.507928\n",
      "Training Epoch: 26 [4224/50000]\tLoss: 4.1396\tLR: 2.508184\n",
      "Training Epoch: 26 [4352/50000]\tLoss: 4.4110\tLR: 2.508440\n",
      "Training Epoch: 26 [4480/50000]\tLoss: 4.2034\tLR: 2.508696\n",
      "Training Epoch: 26 [4608/50000]\tLoss: 4.3279\tLR: 2.508951\n",
      "Training Epoch: 26 [4736/50000]\tLoss: 4.3594\tLR: 2.509207\n",
      "Training Epoch: 26 [4864/50000]\tLoss: 4.2148\tLR: 2.509463\n",
      "Training Epoch: 26 [4992/50000]\tLoss: 4.4437\tLR: 2.509719\n",
      "Training Epoch: 26 [5120/50000]\tLoss: 4.4263\tLR: 2.509974\n",
      "Training Epoch: 26 [5248/50000]\tLoss: 4.2705\tLR: 2.510230\n",
      "Training Epoch: 26 [5376/50000]\tLoss: 4.3265\tLR: 2.510486\n",
      "Training Epoch: 26 [5504/50000]\tLoss: 4.3959\tLR: 2.510742\n",
      "Training Epoch: 26 [5632/50000]\tLoss: 4.4179\tLR: 2.510997\n",
      "Training Epoch: 26 [5760/50000]\tLoss: 4.3037\tLR: 2.511253\n",
      "Training Epoch: 26 [5888/50000]\tLoss: 4.3556\tLR: 2.511509\n",
      "Training Epoch: 26 [6016/50000]\tLoss: 4.4107\tLR: 2.511765\n",
      "Training Epoch: 26 [6144/50000]\tLoss: 4.2978\tLR: 2.512020\n",
      "Training Epoch: 26 [6272/50000]\tLoss: 4.1854\tLR: 2.512276\n",
      "Training Epoch: 26 [6400/50000]\tLoss: 4.2340\tLR: 2.512532\n",
      "Training Epoch: 26 [6528/50000]\tLoss: 4.2274\tLR: 2.512788\n",
      "Training Epoch: 26 [6656/50000]\tLoss: 4.4296\tLR: 2.513043\n",
      "Training Epoch: 26 [6784/50000]\tLoss: 4.3573\tLR: 2.513299\n",
      "Training Epoch: 26 [6912/50000]\tLoss: 4.2275\tLR: 2.513555\n",
      "Training Epoch: 26 [7040/50000]\tLoss: 4.2592\tLR: 2.513811\n",
      "Training Epoch: 26 [7168/50000]\tLoss: 4.4255\tLR: 2.514066\n",
      "Training Epoch: 26 [7296/50000]\tLoss: 4.3573\tLR: 2.514322\n",
      "Training Epoch: 26 [7424/50000]\tLoss: 4.4372\tLR: 2.514578\n",
      "Training Epoch: 26 [7552/50000]\tLoss: 4.3113\tLR: 2.514834\n",
      "Training Epoch: 26 [7680/50000]\tLoss: 4.3908\tLR: 2.515090\n",
      "Training Epoch: 26 [7808/50000]\tLoss: 4.4450\tLR: 2.515345\n",
      "Training Epoch: 26 [7936/50000]\tLoss: 4.1528\tLR: 2.515601\n",
      "Training Epoch: 26 [8064/50000]\tLoss: 4.2381\tLR: 2.515857\n",
      "Training Epoch: 26 [8192/50000]\tLoss: 4.3411\tLR: 2.516113\n",
      "Training Epoch: 26 [8320/50000]\tLoss: 4.2943\tLR: 2.516368\n",
      "Training Epoch: 26 [8448/50000]\tLoss: 4.2705\tLR: 2.516624\n",
      "Training Epoch: 26 [8576/50000]\tLoss: 4.2317\tLR: 2.516880\n",
      "Training Epoch: 26 [8704/50000]\tLoss: 4.3247\tLR: 2.517136\n",
      "Training Epoch: 26 [8832/50000]\tLoss: 4.3925\tLR: 2.517391\n",
      "Training Epoch: 26 [8960/50000]\tLoss: 4.2871\tLR: 2.517647\n",
      "Training Epoch: 26 [9088/50000]\tLoss: 4.1941\tLR: 2.517903\n",
      "Training Epoch: 26 [9216/50000]\tLoss: 4.2967\tLR: 2.518159\n",
      "Training Epoch: 26 [9344/50000]\tLoss: 4.3692\tLR: 2.518414\n",
      "Training Epoch: 26 [9472/50000]\tLoss: 4.2873\tLR: 2.518670\n",
      "Training Epoch: 26 [9600/50000]\tLoss: 4.1415\tLR: 2.518926\n",
      "Training Epoch: 26 [9728/50000]\tLoss: 4.2805\tLR: 2.519182\n",
      "Training Epoch: 26 [9856/50000]\tLoss: 4.2214\tLR: 2.519437\n",
      "Training Epoch: 26 [9984/50000]\tLoss: 4.2824\tLR: 2.519693\n",
      "Training Epoch: 26 [10112/50000]\tLoss: 4.4080\tLR: 2.519949\n",
      "Training Epoch: 26 [10240/50000]\tLoss: 4.2590\tLR: 2.520205\n",
      "Training Epoch: 26 [10368/50000]\tLoss: 4.2763\tLR: 2.520460\n",
      "Training Epoch: 26 [10496/50000]\tLoss: 4.4228\tLR: 2.520716\n",
      "Training Epoch: 26 [10624/50000]\tLoss: 4.2288\tLR: 2.520972\n",
      "Training Epoch: 26 [10752/50000]\tLoss: 4.3003\tLR: 2.521228\n",
      "Training Epoch: 26 [10880/50000]\tLoss: 4.1841\tLR: 2.521483\n",
      "Training Epoch: 26 [11008/50000]\tLoss: 4.2601\tLR: 2.521739\n",
      "Training Epoch: 26 [11136/50000]\tLoss: 4.3900\tLR: 2.521995\n",
      "Training Epoch: 26 [11264/50000]\tLoss: 4.2651\tLR: 2.522251\n",
      "Training Epoch: 26 [11392/50000]\tLoss: 4.4252\tLR: 2.522506\n",
      "Training Epoch: 26 [11520/50000]\tLoss: 4.3352\tLR: 2.522762\n",
      "Training Epoch: 26 [11648/50000]\tLoss: 4.2284\tLR: 2.523018\n",
      "Training Epoch: 26 [11776/50000]\tLoss: 4.3702\tLR: 2.523274\n",
      "Training Epoch: 26 [11904/50000]\tLoss: 4.3564\tLR: 2.523529\n",
      "Training Epoch: 26 [12032/50000]\tLoss: 4.2174\tLR: 2.523785\n",
      "Training Epoch: 26 [12160/50000]\tLoss: 4.4779\tLR: 2.524041\n",
      "Training Epoch: 26 [12288/50000]\tLoss: 4.3470\tLR: 2.524297\n",
      "Training Epoch: 26 [12416/50000]\tLoss: 4.3405\tLR: 2.524552\n",
      "Training Epoch: 26 [12544/50000]\tLoss: 4.2720\tLR: 2.524808\n",
      "Training Epoch: 26 [12672/50000]\tLoss: 4.3091\tLR: 2.525064\n",
      "Training Epoch: 26 [12800/50000]\tLoss: 4.2753\tLR: 2.525320\n",
      "Training Epoch: 26 [12928/50000]\tLoss: 4.1724\tLR: 2.525575\n",
      "Training Epoch: 26 [13056/50000]\tLoss: 4.2986\tLR: 2.525831\n",
      "Training Epoch: 26 [13184/50000]\tLoss: 4.3695\tLR: 2.526087\n",
      "Training Epoch: 26 [13312/50000]\tLoss: 4.1368\tLR: 2.526343\n",
      "Training Epoch: 26 [13440/50000]\tLoss: 4.1639\tLR: 2.526598\n",
      "Training Epoch: 26 [13568/50000]\tLoss: 4.2373\tLR: 2.526854\n",
      "Training Epoch: 26 [13696/50000]\tLoss: 4.1171\tLR: 2.527110\n",
      "Training Epoch: 26 [13824/50000]\tLoss: 4.3637\tLR: 2.527366\n",
      "Training Epoch: 26 [13952/50000]\tLoss: 4.3726\tLR: 2.527621\n",
      "Training Epoch: 26 [14080/50000]\tLoss: 4.2769\tLR: 2.527877\n",
      "Training Epoch: 26 [14208/50000]\tLoss: 4.3111\tLR: 2.528133\n",
      "Training Epoch: 26 [14336/50000]\tLoss: 4.2828\tLR: 2.528389\n",
      "Training Epoch: 26 [14464/50000]\tLoss: 4.3027\tLR: 2.528645\n",
      "Training Epoch: 26 [14592/50000]\tLoss: 4.2945\tLR: 2.528900\n",
      "Training Epoch: 26 [14720/50000]\tLoss: 4.3242\tLR: 2.529156\n",
      "Training Epoch: 26 [14848/50000]\tLoss: 4.3161\tLR: 2.529412\n",
      "Training Epoch: 26 [14976/50000]\tLoss: 4.2950\tLR: 2.529668\n",
      "Training Epoch: 26 [15104/50000]\tLoss: 4.2597\tLR: 2.529923\n",
      "Training Epoch: 26 [15232/50000]\tLoss: 4.3416\tLR: 2.530179\n",
      "Training Epoch: 26 [15360/50000]\tLoss: 4.2460\tLR: 2.530435\n",
      "Training Epoch: 26 [15488/50000]\tLoss: 4.2864\tLR: 2.530691\n",
      "Training Epoch: 26 [15616/50000]\tLoss: 4.1631\tLR: 2.530946\n",
      "Training Epoch: 26 [15744/50000]\tLoss: 4.3325\tLR: 2.531202\n",
      "Training Epoch: 26 [15872/50000]\tLoss: 4.3701\tLR: 2.531458\n",
      "Training Epoch: 26 [16000/50000]\tLoss: 4.1740\tLR: 2.531714\n",
      "Training Epoch: 26 [16128/50000]\tLoss: 4.3105\tLR: 2.531969\n",
      "Training Epoch: 26 [16256/50000]\tLoss: 4.4252\tLR: 2.532225\n",
      "Training Epoch: 26 [16384/50000]\tLoss: 4.2668\tLR: 2.532481\n",
      "Training Epoch: 26 [16512/50000]\tLoss: 4.4099\tLR: 2.532737\n",
      "Training Epoch: 26 [16640/50000]\tLoss: 4.3099\tLR: 2.532992\n",
      "Training Epoch: 26 [16768/50000]\tLoss: 4.3844\tLR: 2.533248\n",
      "Training Epoch: 26 [16896/50000]\tLoss: 4.2053\tLR: 2.533504\n",
      "Training Epoch: 26 [17024/50000]\tLoss: 4.3932\tLR: 2.533760\n",
      "Training Epoch: 26 [17152/50000]\tLoss: 4.4872\tLR: 2.534015\n",
      "Training Epoch: 26 [17280/50000]\tLoss: 4.3552\tLR: 2.534271\n",
      "Training Epoch: 26 [17408/50000]\tLoss: 4.3318\tLR: 2.534527\n",
      "Training Epoch: 26 [17536/50000]\tLoss: 4.3207\tLR: 2.534783\n",
      "Training Epoch: 26 [17664/50000]\tLoss: 4.3732\tLR: 2.535038\n",
      "Training Epoch: 26 [17792/50000]\tLoss: 4.3468\tLR: 2.535294\n",
      "Training Epoch: 26 [17920/50000]\tLoss: 4.3128\tLR: 2.535550\n",
      "Training Epoch: 26 [18048/50000]\tLoss: 4.1415\tLR: 2.535806\n",
      "Training Epoch: 26 [18176/50000]\tLoss: 4.3668\tLR: 2.536061\n",
      "Training Epoch: 26 [18304/50000]\tLoss: 4.2787\tLR: 2.536317\n",
      "Training Epoch: 26 [18432/50000]\tLoss: 4.2795\tLR: 2.536573\n",
      "Training Epoch: 26 [18560/50000]\tLoss: 4.3218\tLR: 2.536829\n",
      "Training Epoch: 26 [18688/50000]\tLoss: 4.2311\tLR: 2.537084\n",
      "Training Epoch: 26 [18816/50000]\tLoss: 4.1873\tLR: 2.537340\n",
      "Training Epoch: 26 [18944/50000]\tLoss: 4.1708\tLR: 2.537596\n",
      "Training Epoch: 26 [19072/50000]\tLoss: 4.5876\tLR: 2.537852\n",
      "Training Epoch: 26 [19200/50000]\tLoss: 4.1934\tLR: 2.538107\n",
      "Training Epoch: 26 [19328/50000]\tLoss: 4.2595\tLR: 2.538363\n",
      "Training Epoch: 26 [19456/50000]\tLoss: 4.2880\tLR: 2.538619\n",
      "Training Epoch: 26 [19584/50000]\tLoss: 4.2624\tLR: 2.538875\n",
      "Training Epoch: 26 [19712/50000]\tLoss: 4.6008\tLR: 2.539130\n",
      "Training Epoch: 26 [19840/50000]\tLoss: 4.4613\tLR: 2.539386\n",
      "Training Epoch: 26 [19968/50000]\tLoss: 4.4026\tLR: 2.539642\n",
      "Training Epoch: 26 [20096/50000]\tLoss: 4.3650\tLR: 2.539898\n",
      "Training Epoch: 26 [20224/50000]\tLoss: 4.3744\tLR: 2.540153\n",
      "Training Epoch: 26 [20352/50000]\tLoss: 4.3169\tLR: 2.540409\n",
      "Training Epoch: 26 [20480/50000]\tLoss: 4.3279\tLR: 2.540665\n",
      "Training Epoch: 26 [20608/50000]\tLoss: 4.3970\tLR: 2.540921\n",
      "Training Epoch: 26 [20736/50000]\tLoss: 4.2810\tLR: 2.541176\n",
      "Training Epoch: 26 [20864/50000]\tLoss: 4.2838\tLR: 2.541432\n",
      "Training Epoch: 26 [20992/50000]\tLoss: 4.3717\tLR: 2.541688\n",
      "Training Epoch: 26 [21120/50000]\tLoss: 4.4150\tLR: 2.541944\n",
      "Training Epoch: 26 [21248/50000]\tLoss: 4.3515\tLR: 2.542199\n",
      "Training Epoch: 26 [21376/50000]\tLoss: 4.2947\tLR: 2.542455\n",
      "Training Epoch: 26 [21504/50000]\tLoss: 4.3261\tLR: 2.542711\n",
      "Training Epoch: 26 [21632/50000]\tLoss: 4.3776\tLR: 2.542967\n",
      "Training Epoch: 26 [21760/50000]\tLoss: 4.3036\tLR: 2.543223\n",
      "Training Epoch: 26 [21888/50000]\tLoss: 4.3029\tLR: 2.543478\n",
      "Training Epoch: 26 [22016/50000]\tLoss: 4.4161\tLR: 2.543734\n",
      "Training Epoch: 26 [22144/50000]\tLoss: 4.3508\tLR: 2.543990\n",
      "Training Epoch: 26 [22272/50000]\tLoss: 4.2490\tLR: 2.544246\n",
      "Training Epoch: 26 [22400/50000]\tLoss: 4.3160\tLR: 2.544501\n",
      "Training Epoch: 26 [22528/50000]\tLoss: 4.2571\tLR: 2.544757\n",
      "Training Epoch: 26 [22656/50000]\tLoss: 4.2737\tLR: 2.545013\n",
      "Training Epoch: 26 [22784/50000]\tLoss: 4.2478\tLR: 2.545269\n",
      "Training Epoch: 26 [22912/50000]\tLoss: 4.3169\tLR: 2.545524\n",
      "Training Epoch: 26 [23040/50000]\tLoss: 4.1538\tLR: 2.545780\n",
      "Training Epoch: 26 [23168/50000]\tLoss: 4.4084\tLR: 2.546036\n",
      "Training Epoch: 26 [23296/50000]\tLoss: 4.4260\tLR: 2.546292\n",
      "Training Epoch: 26 [23424/50000]\tLoss: 4.3351\tLR: 2.546547\n",
      "Training Epoch: 26 [23552/50000]\tLoss: 4.2827\tLR: 2.546803\n",
      "Training Epoch: 26 [23680/50000]\tLoss: 4.3365\tLR: 2.547059\n",
      "Training Epoch: 26 [23808/50000]\tLoss: 4.3152\tLR: 2.547315\n",
      "Training Epoch: 26 [23936/50000]\tLoss: 4.2281\tLR: 2.547570\n",
      "Training Epoch: 26 [24064/50000]\tLoss: 4.2952\tLR: 2.547826\n",
      "Training Epoch: 26 [24192/50000]\tLoss: 4.2973\tLR: 2.548082\n",
      "Training Epoch: 26 [24320/50000]\tLoss: 4.3658\tLR: 2.548338\n",
      "Training Epoch: 26 [24448/50000]\tLoss: 4.2717\tLR: 2.548593\n",
      "Training Epoch: 26 [24576/50000]\tLoss: 4.2655\tLR: 2.548849\n",
      "Training Epoch: 26 [24704/50000]\tLoss: 4.3713\tLR: 2.549105\n",
      "Training Epoch: 26 [24832/50000]\tLoss: 4.3245\tLR: 2.549361\n",
      "Training Epoch: 26 [24960/50000]\tLoss: 4.3796\tLR: 2.549616\n",
      "Training Epoch: 26 [25088/50000]\tLoss: 4.4003\tLR: 2.549872\n",
      "Training Epoch: 26 [25216/50000]\tLoss: 4.2919\tLR: 2.550128\n",
      "Training Epoch: 26 [25344/50000]\tLoss: 4.2300\tLR: 2.550384\n",
      "Training Epoch: 26 [25472/50000]\tLoss: 4.3189\tLR: 2.550639\n",
      "Training Epoch: 26 [25600/50000]\tLoss: 4.4718\tLR: 2.550895\n",
      "Training Epoch: 26 [25728/50000]\tLoss: 4.2820\tLR: 2.551151\n",
      "Training Epoch: 26 [25856/50000]\tLoss: 4.3633\tLR: 2.551407\n",
      "Training Epoch: 26 [25984/50000]\tLoss: 4.4062\tLR: 2.551662\n",
      "Training Epoch: 26 [26112/50000]\tLoss: 4.2950\tLR: 2.551918\n",
      "Training Epoch: 26 [26240/50000]\tLoss: 4.3892\tLR: 2.552174\n",
      "Training Epoch: 26 [26368/50000]\tLoss: 4.4848\tLR: 2.552430\n",
      "Training Epoch: 26 [26496/50000]\tLoss: 4.3133\tLR: 2.552685\n",
      "Training Epoch: 26 [26624/50000]\tLoss: 4.3582\tLR: 2.552941\n",
      "Training Epoch: 26 [26752/50000]\tLoss: 4.3090\tLR: 2.553197\n",
      "Training Epoch: 26 [26880/50000]\tLoss: 4.4060\tLR: 2.553453\n",
      "Training Epoch: 26 [27008/50000]\tLoss: 4.2942\tLR: 2.553708\n",
      "Training Epoch: 26 [27136/50000]\tLoss: 4.3503\tLR: 2.553964\n",
      "Training Epoch: 26 [27264/50000]\tLoss: 4.3805\tLR: 2.554220\n",
      "Training Epoch: 26 [27392/50000]\tLoss: 4.4215\tLR: 2.554476\n",
      "Training Epoch: 26 [27520/50000]\tLoss: 4.3455\tLR: 2.554731\n",
      "Training Epoch: 26 [27648/50000]\tLoss: 4.3380\tLR: 2.554987\n",
      "Training Epoch: 26 [27776/50000]\tLoss: 4.4594\tLR: 2.555243\n",
      "Training Epoch: 26 [27904/50000]\tLoss: 4.3281\tLR: 2.555499\n",
      "Training Epoch: 26 [28032/50000]\tLoss: 4.3867\tLR: 2.555754\n",
      "Training Epoch: 26 [28160/50000]\tLoss: 4.3329\tLR: 2.556010\n",
      "Training Epoch: 26 [28288/50000]\tLoss: 4.3366\tLR: 2.556266\n",
      "Training Epoch: 26 [28416/50000]\tLoss: 4.3519\tLR: 2.556522\n",
      "Training Epoch: 26 [28544/50000]\tLoss: 4.2365\tLR: 2.556777\n",
      "Training Epoch: 26 [28672/50000]\tLoss: 4.2064\tLR: 2.557033\n",
      "Training Epoch: 26 [28800/50000]\tLoss: 4.4087\tLR: 2.557289\n",
      "Training Epoch: 26 [28928/50000]\tLoss: 4.3519\tLR: 2.557545\n",
      "Training Epoch: 26 [29056/50000]\tLoss: 4.3239\tLR: 2.557801\n",
      "Training Epoch: 26 [29184/50000]\tLoss: 4.4482\tLR: 2.558056\n",
      "Training Epoch: 26 [29312/50000]\tLoss: 4.4283\tLR: 2.558312\n",
      "Training Epoch: 26 [29440/50000]\tLoss: 4.4511\tLR: 2.558568\n",
      "Training Epoch: 26 [29568/50000]\tLoss: 4.3131\tLR: 2.558824\n",
      "Training Epoch: 26 [29696/50000]\tLoss: 4.3380\tLR: 2.559079\n",
      "Training Epoch: 26 [29824/50000]\tLoss: 4.3634\tLR: 2.559335\n",
      "Training Epoch: 26 [29952/50000]\tLoss: 4.3343\tLR: 2.559591\n",
      "Training Epoch: 26 [30080/50000]\tLoss: 4.4058\tLR: 2.559847\n",
      "Training Epoch: 26 [30208/50000]\tLoss: 4.3215\tLR: 2.560102\n",
      "Training Epoch: 26 [30336/50000]\tLoss: 4.2843\tLR: 2.560358\n",
      "Training Epoch: 26 [30464/50000]\tLoss: 4.2527\tLR: 2.560614\n",
      "Training Epoch: 26 [30592/50000]\tLoss: 4.2369\tLR: 2.560870\n",
      "Training Epoch: 26 [30720/50000]\tLoss: 4.5543\tLR: 2.561125\n",
      "Training Epoch: 26 [30848/50000]\tLoss: 4.3979\tLR: 2.561381\n",
      "Training Epoch: 26 [30976/50000]\tLoss: 4.4166\tLR: 2.561637\n",
      "Training Epoch: 26 [31104/50000]\tLoss: 4.6288\tLR: 2.561893\n",
      "Training Epoch: 26 [31232/50000]\tLoss: 4.4045\tLR: 2.562148\n",
      "Training Epoch: 26 [31360/50000]\tLoss: 4.4162\tLR: 2.562404\n",
      "Training Epoch: 26 [31488/50000]\tLoss: 4.4560\tLR: 2.562660\n",
      "Training Epoch: 26 [31616/50000]\tLoss: 4.3346\tLR: 2.562916\n",
      "Training Epoch: 26 [31744/50000]\tLoss: 4.4661\tLR: 2.563171\n",
      "Training Epoch: 26 [31872/50000]\tLoss: 4.3207\tLR: 2.563427\n",
      "Training Epoch: 26 [32000/50000]\tLoss: 4.4123\tLR: 2.563683\n",
      "Training Epoch: 26 [32128/50000]\tLoss: 4.3190\tLR: 2.563939\n",
      "Training Epoch: 26 [32256/50000]\tLoss: 4.3467\tLR: 2.564194\n",
      "Training Epoch: 26 [32384/50000]\tLoss: 4.4260\tLR: 2.564450\n",
      "Training Epoch: 26 [32512/50000]\tLoss: 4.3409\tLR: 2.564706\n",
      "Training Epoch: 26 [32640/50000]\tLoss: 4.3369\tLR: 2.564962\n",
      "Training Epoch: 26 [32768/50000]\tLoss: 4.4555\tLR: 2.565217\n",
      "Training Epoch: 26 [32896/50000]\tLoss: 4.4842\tLR: 2.565473\n",
      "Training Epoch: 26 [33024/50000]\tLoss: 4.3088\tLR: 2.565729\n",
      "Training Epoch: 26 [33152/50000]\tLoss: 4.5094\tLR: 2.565985\n",
      "Training Epoch: 26 [33280/50000]\tLoss: 4.5933\tLR: 2.566240\n",
      "Training Epoch: 26 [33408/50000]\tLoss: 4.3155\tLR: 2.566496\n",
      "Training Epoch: 26 [33536/50000]\tLoss: 4.3026\tLR: 2.566752\n",
      "Training Epoch: 26 [33664/50000]\tLoss: 4.2633\tLR: 2.567008\n",
      "Training Epoch: 26 [33792/50000]\tLoss: 4.3163\tLR: 2.567263\n",
      "Training Epoch: 26 [33920/50000]\tLoss: 4.2462\tLR: 2.567519\n",
      "Training Epoch: 26 [34048/50000]\tLoss: 4.3879\tLR: 2.567775\n",
      "Training Epoch: 26 [34176/50000]\tLoss: 4.2933\tLR: 2.568031\n",
      "Training Epoch: 26 [34304/50000]\tLoss: 4.3511\tLR: 2.568286\n",
      "Training Epoch: 26 [34432/50000]\tLoss: 4.5092\tLR: 2.568542\n",
      "Training Epoch: 26 [34560/50000]\tLoss: 4.2364\tLR: 2.568798\n",
      "Training Epoch: 26 [34688/50000]\tLoss: 4.4245\tLR: 2.569054\n",
      "Training Epoch: 26 [34816/50000]\tLoss: 4.4828\tLR: 2.569309\n",
      "Training Epoch: 26 [34944/50000]\tLoss: 4.4631\tLR: 2.569565\n",
      "Training Epoch: 26 [35072/50000]\tLoss: 4.3365\tLR: 2.569821\n",
      "Training Epoch: 26 [35200/50000]\tLoss: 4.3700\tLR: 2.570077\n",
      "Training Epoch: 26 [35328/50000]\tLoss: 4.3994\tLR: 2.570332\n",
      "Training Epoch: 26 [35456/50000]\tLoss: 4.2929\tLR: 2.570588\n",
      "Training Epoch: 26 [35584/50000]\tLoss: 4.2515\tLR: 2.570844\n",
      "Training Epoch: 26 [35712/50000]\tLoss: 4.3016\tLR: 2.571100\n",
      "Training Epoch: 26 [35840/50000]\tLoss: 4.3519\tLR: 2.571355\n",
      "Training Epoch: 26 [35968/50000]\tLoss: 4.3753\tLR: 2.571611\n",
      "Training Epoch: 26 [36096/50000]\tLoss: 4.2830\tLR: 2.571867\n",
      "Training Epoch: 26 [36224/50000]\tLoss: 4.3690\tLR: 2.572123\n",
      "Training Epoch: 26 [36352/50000]\tLoss: 4.1809\tLR: 2.572379\n",
      "Training Epoch: 26 [36480/50000]\tLoss: 4.4558\tLR: 2.572634\n",
      "Training Epoch: 26 [36608/50000]\tLoss: 4.2826\tLR: 2.572890\n",
      "Training Epoch: 26 [36736/50000]\tLoss: 4.2881\tLR: 2.573146\n",
      "Training Epoch: 26 [36864/50000]\tLoss: 4.3181\tLR: 2.573402\n",
      "Training Epoch: 26 [36992/50000]\tLoss: 4.3013\tLR: 2.573657\n",
      "Training Epoch: 26 [37120/50000]\tLoss: 4.1971\tLR: 2.573913\n",
      "Training Epoch: 26 [37248/50000]\tLoss: 4.4421\tLR: 2.574169\n",
      "Training Epoch: 26 [37376/50000]\tLoss: 4.3968\tLR: 2.574425\n",
      "Training Epoch: 26 [37504/50000]\tLoss: 4.2818\tLR: 2.574680\n",
      "Training Epoch: 26 [37632/50000]\tLoss: 4.2561\tLR: 2.574936\n",
      "Training Epoch: 26 [37760/50000]\tLoss: 4.3148\tLR: 2.575192\n",
      "Training Epoch: 26 [37888/50000]\tLoss: 4.1558\tLR: 2.575448\n",
      "Training Epoch: 26 [38016/50000]\tLoss: 4.4418\tLR: 2.575703\n",
      "Training Epoch: 26 [38144/50000]\tLoss: 4.2826\tLR: 2.575959\n",
      "Training Epoch: 26 [38272/50000]\tLoss: 4.4037\tLR: 2.576215\n",
      "Training Epoch: 26 [38400/50000]\tLoss: 4.3091\tLR: 2.576471\n",
      "Training Epoch: 26 [38528/50000]\tLoss: 4.2061\tLR: 2.576726\n",
      "Training Epoch: 26 [38656/50000]\tLoss: 4.3376\tLR: 2.576982\n",
      "Training Epoch: 26 [38784/50000]\tLoss: 4.3573\tLR: 2.577238\n",
      "Training Epoch: 26 [38912/50000]\tLoss: 4.2408\tLR: 2.577494\n",
      "Training Epoch: 26 [39040/50000]\tLoss: 4.4284\tLR: 2.577749\n",
      "Training Epoch: 26 [39168/50000]\tLoss: 4.2579\tLR: 2.578005\n",
      "Training Epoch: 26 [39296/50000]\tLoss: 4.2741\tLR: 2.578261\n",
      "Training Epoch: 26 [39424/50000]\tLoss: 4.3930\tLR: 2.578517\n",
      "Training Epoch: 26 [39552/50000]\tLoss: 4.2741\tLR: 2.578772\n",
      "Training Epoch: 26 [39680/50000]\tLoss: 4.3189\tLR: 2.579028\n",
      "Training Epoch: 26 [39808/50000]\tLoss: 4.2815\tLR: 2.579284\n",
      "Training Epoch: 26 [39936/50000]\tLoss: 4.1524\tLR: 2.579540\n",
      "Training Epoch: 26 [40064/50000]\tLoss: 4.4425\tLR: 2.579795\n",
      "Training Epoch: 26 [40192/50000]\tLoss: 4.2034\tLR: 2.580051\n",
      "Training Epoch: 26 [40320/50000]\tLoss: 4.3760\tLR: 2.580307\n",
      "Training Epoch: 26 [40448/50000]\tLoss: 4.1856\tLR: 2.580563\n",
      "Training Epoch: 26 [40576/50000]\tLoss: 4.1729\tLR: 2.580818\n",
      "Training Epoch: 26 [40704/50000]\tLoss: 4.2660\tLR: 2.581074\n",
      "Training Epoch: 26 [40832/50000]\tLoss: 4.2661\tLR: 2.581330\n",
      "Training Epoch: 26 [40960/50000]\tLoss: 4.4419\tLR: 2.581586\n",
      "Training Epoch: 26 [41088/50000]\tLoss: 4.4639\tLR: 2.581841\n",
      "Training Epoch: 26 [41216/50000]\tLoss: 4.3086\tLR: 2.582097\n",
      "Training Epoch: 26 [41344/50000]\tLoss: 4.1774\tLR: 2.582353\n",
      "Training Epoch: 26 [41472/50000]\tLoss: 4.2974\tLR: 2.582609\n",
      "Training Epoch: 26 [41600/50000]\tLoss: 4.2867\tLR: 2.582864\n",
      "Training Epoch: 26 [41728/50000]\tLoss: 4.3098\tLR: 2.583120\n",
      "Training Epoch: 26 [41856/50000]\tLoss: 4.2804\tLR: 2.583376\n",
      "Training Epoch: 26 [41984/50000]\tLoss: 4.3066\tLR: 2.583632\n",
      "Training Epoch: 26 [42112/50000]\tLoss: 4.4248\tLR: 2.583887\n",
      "Training Epoch: 26 [42240/50000]\tLoss: 4.3106\tLR: 2.584143\n",
      "Training Epoch: 26 [42368/50000]\tLoss: 4.2757\tLR: 2.584399\n",
      "Training Epoch: 26 [42496/50000]\tLoss: 4.2882\tLR: 2.584655\n",
      "Training Epoch: 26 [42624/50000]\tLoss: 4.3523\tLR: 2.584910\n",
      "Training Epoch: 26 [42752/50000]\tLoss: 4.4286\tLR: 2.585166\n",
      "Training Epoch: 26 [42880/50000]\tLoss: 4.3605\tLR: 2.585422\n",
      "Training Epoch: 26 [43008/50000]\tLoss: 4.3981\tLR: 2.585678\n",
      "Training Epoch: 26 [43136/50000]\tLoss: 4.4083\tLR: 2.585934\n",
      "Training Epoch: 26 [43264/50000]\tLoss: 4.3349\tLR: 2.586189\n",
      "Training Epoch: 26 [43392/50000]\tLoss: 4.2738\tLR: 2.586445\n",
      "Training Epoch: 26 [43520/50000]\tLoss: 4.3041\tLR: 2.586701\n",
      "Training Epoch: 26 [43648/50000]\tLoss: 4.4123\tLR: 2.586957\n",
      "Training Epoch: 26 [43776/50000]\tLoss: 4.5264\tLR: 2.587212\n",
      "Training Epoch: 26 [43904/50000]\tLoss: 4.3046\tLR: 2.587468\n",
      "Training Epoch: 26 [44032/50000]\tLoss: 4.3102\tLR: 2.587724\n",
      "Training Epoch: 26 [44160/50000]\tLoss: 4.2201\tLR: 2.587980\n",
      "Training Epoch: 26 [44288/50000]\tLoss: 4.3345\tLR: 2.588235\n",
      "Training Epoch: 26 [44416/50000]\tLoss: 4.3042\tLR: 2.588491\n",
      "Training Epoch: 26 [44544/50000]\tLoss: 4.3206\tLR: 2.588747\n",
      "Training Epoch: 26 [44672/50000]\tLoss: 4.2634\tLR: 2.589003\n",
      "Training Epoch: 26 [44800/50000]\tLoss: 4.4299\tLR: 2.589258\n",
      "Training Epoch: 26 [44928/50000]\tLoss: 4.3071\tLR: 2.589514\n",
      "Training Epoch: 26 [45056/50000]\tLoss: 4.3557\tLR: 2.589770\n",
      "Training Epoch: 26 [45184/50000]\tLoss: 4.3912\tLR: 2.590026\n",
      "Training Epoch: 26 [45312/50000]\tLoss: 4.3349\tLR: 2.590281\n",
      "Training Epoch: 26 [45440/50000]\tLoss: 4.3363\tLR: 2.590537\n",
      "Training Epoch: 26 [45568/50000]\tLoss: 4.2607\tLR: 2.590793\n",
      "Training Epoch: 26 [45696/50000]\tLoss: 4.3328\tLR: 2.591049\n",
      "Training Epoch: 26 [45824/50000]\tLoss: 4.2890\tLR: 2.591304\n",
      "Training Epoch: 26 [45952/50000]\tLoss: 4.3270\tLR: 2.591560\n",
      "Training Epoch: 26 [46080/50000]\tLoss: 4.3910\tLR: 2.591816\n",
      "Training Epoch: 26 [46208/50000]\tLoss: 4.3885\tLR: 2.592072\n",
      "Training Epoch: 26 [46336/50000]\tLoss: 4.3616\tLR: 2.592327\n",
      "Training Epoch: 26 [46464/50000]\tLoss: 4.2529\tLR: 2.592583\n",
      "Training Epoch: 26 [46592/50000]\tLoss: 4.1793\tLR: 2.592839\n",
      "Training Epoch: 26 [46720/50000]\tLoss: 4.2942\tLR: 2.593095\n",
      "Training Epoch: 26 [46848/50000]\tLoss: 4.3092\tLR: 2.593350\n",
      "Training Epoch: 26 [46976/50000]\tLoss: 4.2335\tLR: 2.593606\n",
      "Training Epoch: 26 [47104/50000]\tLoss: 4.2852\tLR: 2.593862\n",
      "Training Epoch: 26 [47232/50000]\tLoss: 4.4834\tLR: 2.594118\n",
      "Training Epoch: 26 [47360/50000]\tLoss: 4.4079\tLR: 2.594373\n",
      "Training Epoch: 26 [47488/50000]\tLoss: 4.3432\tLR: 2.594629\n",
      "Training Epoch: 26 [47616/50000]\tLoss: 4.3812\tLR: 2.594885\n",
      "Training Epoch: 26 [47744/50000]\tLoss: 4.3409\tLR: 2.595141\n",
      "Training Epoch: 26 [47872/50000]\tLoss: 4.2724\tLR: 2.595396\n",
      "Training Epoch: 26 [48000/50000]\tLoss: 4.2738\tLR: 2.595652\n",
      "Training Epoch: 26 [48128/50000]\tLoss: 4.2424\tLR: 2.595908\n",
      "Training Epoch: 26 [48256/50000]\tLoss: 4.3506\tLR: 2.596164\n",
      "Training Epoch: 26 [48384/50000]\tLoss: 4.0760\tLR: 2.596419\n",
      "Training Epoch: 26 [48512/50000]\tLoss: 4.2207\tLR: 2.596675\n",
      "Training Epoch: 26 [48640/50000]\tLoss: 4.2496\tLR: 2.596931\n",
      "Training Epoch: 26 [48768/50000]\tLoss: 4.1837\tLR: 2.597187\n",
      "Training Epoch: 26 [48896/50000]\tLoss: 4.3890\tLR: 2.597442\n",
      "Training Epoch: 26 [49024/50000]\tLoss: 4.3220\tLR: 2.597698\n",
      "Training Epoch: 26 [49152/50000]\tLoss: 4.4270\tLR: 2.597954\n",
      "Training Epoch: 26 [49280/50000]\tLoss: 4.4691\tLR: 2.598210\n",
      "Training Epoch: 26 [49408/50000]\tLoss: 4.4080\tLR: 2.598465\n",
      "Training Epoch: 26 [49536/50000]\tLoss: 4.2636\tLR: 2.598721\n",
      "Training Epoch: 26 [49664/50000]\tLoss: 4.2884\tLR: 2.598977\n",
      "Training Epoch: 26 [49792/50000]\tLoss: 4.3936\tLR: 2.599233\n",
      "Training Epoch: 26 [49920/50000]\tLoss: 4.3360\tLR: 2.599488\n",
      "Training Epoch: 26 [50000/50000]\tLoss: 4.3049\tLR: 2.599744\n",
      "epoch 26 training time consumed: 489.06s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   36451 GB |   36451 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   36339 GB |   36339 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     111 GB |     111 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   36451 GB |   36451 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   36339 GB |   36339 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     111 GB |     111 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   35936 GB |   35936 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   35824 GB |   35824 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     111 GB |     111 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    3865 K  |    3865 K  |\n",
      "|       from large pool |      24    |      65    |    1647 K  |    1647 K  |\n",
      "|       from small pool |     231    |     274    |    2217 K  |    2217 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    3865 K  |    3865 K  |\n",
      "|       from large pool |      24    |      65    |    1647 K  |    1647 K  |\n",
      "|       from small pool |     231    |     274    |    2217 K  |    2217 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      46    |    2239 K  |    2239 K  |\n",
      "|       from large pool |      10    |      23    |     791 K  |     791 K  |\n",
      "|       from small pool |      27    |      35    |    1447 K  |    1447 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 26, Average loss: 0.0341, Accuracy: 0.0395, Time consumed:31.11s\n",
      "\n",
      "Training Epoch: 27 [128/50000]\tLoss: 4.2184\tLR: 0.100000\n",
      "Training Epoch: 27 [256/50000]\tLoss: 4.3135\tLR: 2.600256\n",
      "Training Epoch: 27 [384/50000]\tLoss: 4.2834\tLR: 2.600512\n",
      "Training Epoch: 27 [512/50000]\tLoss: 4.2557\tLR: 2.600767\n",
      "Training Epoch: 27 [640/50000]\tLoss: 4.3110\tLR: 2.601023\n",
      "Training Epoch: 27 [768/50000]\tLoss: 4.2827\tLR: 2.601279\n",
      "Training Epoch: 27 [896/50000]\tLoss: 4.3167\tLR: 2.601535\n",
      "Training Epoch: 27 [1024/50000]\tLoss: 4.3682\tLR: 2.601790\n",
      "Training Epoch: 27 [1152/50000]\tLoss: 4.2994\tLR: 2.602046\n",
      "Training Epoch: 27 [1280/50000]\tLoss: 4.2823\tLR: 2.602302\n",
      "Training Epoch: 27 [1408/50000]\tLoss: 4.3507\tLR: 2.602558\n",
      "Training Epoch: 27 [1536/50000]\tLoss: 4.2300\tLR: 2.602813\n",
      "Training Epoch: 27 [1664/50000]\tLoss: 4.2591\tLR: 2.603069\n",
      "Training Epoch: 27 [1792/50000]\tLoss: 4.2892\tLR: 2.603325\n",
      "Training Epoch: 27 [1920/50000]\tLoss: 4.2573\tLR: 2.603581\n",
      "Training Epoch: 27 [2048/50000]\tLoss: 4.4300\tLR: 2.603836\n",
      "Training Epoch: 27 [2176/50000]\tLoss: 4.4020\tLR: 2.604092\n",
      "Training Epoch: 27 [2304/50000]\tLoss: 4.4693\tLR: 2.604348\n",
      "Training Epoch: 27 [2432/50000]\tLoss: 4.4052\tLR: 2.604604\n",
      "Training Epoch: 27 [2560/50000]\tLoss: 4.3918\tLR: 2.604859\n",
      "Training Epoch: 27 [2688/50000]\tLoss: 4.2728\tLR: 2.605115\n",
      "Training Epoch: 27 [2816/50000]\tLoss: 4.3082\tLR: 2.605371\n",
      "Training Epoch: 27 [2944/50000]\tLoss: 4.4052\tLR: 2.605627\n",
      "Training Epoch: 27 [3072/50000]\tLoss: 4.3621\tLR: 2.605882\n",
      "Training Epoch: 27 [3200/50000]\tLoss: 4.2518\tLR: 2.606138\n",
      "Training Epoch: 27 [3328/50000]\tLoss: 4.3875\tLR: 2.606394\n",
      "Training Epoch: 27 [3456/50000]\tLoss: 4.2938\tLR: 2.606650\n",
      "Training Epoch: 27 [3584/50000]\tLoss: 4.2644\tLR: 2.606905\n",
      "Training Epoch: 27 [3712/50000]\tLoss: 4.3098\tLR: 2.607161\n",
      "Training Epoch: 27 [3840/50000]\tLoss: 4.3724\tLR: 2.607417\n",
      "Training Epoch: 27 [3968/50000]\tLoss: 4.2656\tLR: 2.607673\n",
      "Training Epoch: 27 [4096/50000]\tLoss: 4.2899\tLR: 2.607928\n",
      "Training Epoch: 27 [4224/50000]\tLoss: 4.2168\tLR: 2.608184\n",
      "Training Epoch: 27 [4352/50000]\tLoss: 4.4052\tLR: 2.608440\n",
      "Training Epoch: 27 [4480/50000]\tLoss: 4.2958\tLR: 2.608696\n",
      "Training Epoch: 27 [4608/50000]\tLoss: 4.3717\tLR: 2.608951\n",
      "Training Epoch: 27 [4736/50000]\tLoss: 4.2545\tLR: 2.609207\n",
      "Training Epoch: 27 [4864/50000]\tLoss: 4.3259\tLR: 2.609463\n",
      "Training Epoch: 27 [4992/50000]\tLoss: 4.3435\tLR: 2.609719\n",
      "Training Epoch: 27 [5120/50000]\tLoss: 4.4451\tLR: 2.609974\n",
      "Training Epoch: 27 [5248/50000]\tLoss: 4.3383\tLR: 2.610230\n",
      "Training Epoch: 27 [5376/50000]\tLoss: 4.2664\tLR: 2.610486\n",
      "Training Epoch: 27 [5504/50000]\tLoss: 4.4022\tLR: 2.610742\n",
      "Training Epoch: 27 [5632/50000]\tLoss: 4.4350\tLR: 2.610997\n",
      "Training Epoch: 27 [5760/50000]\tLoss: 4.4057\tLR: 2.611253\n",
      "Training Epoch: 27 [5888/50000]\tLoss: 4.3902\tLR: 2.611509\n",
      "Training Epoch: 27 [6016/50000]\tLoss: 4.1895\tLR: 2.611765\n",
      "Training Epoch: 27 [6144/50000]\tLoss: 4.3062\tLR: 2.612020\n",
      "Training Epoch: 27 [6272/50000]\tLoss: 4.5843\tLR: 2.612276\n",
      "Training Epoch: 27 [6400/50000]\tLoss: 4.3265\tLR: 2.612532\n",
      "Training Epoch: 27 [6528/50000]\tLoss: 4.2610\tLR: 2.612788\n",
      "Training Epoch: 27 [6656/50000]\tLoss: 4.3160\tLR: 2.613043\n",
      "Training Epoch: 27 [6784/50000]\tLoss: 4.4026\tLR: 2.613299\n",
      "Training Epoch: 27 [6912/50000]\tLoss: 4.4121\tLR: 2.613555\n",
      "Training Epoch: 27 [7040/50000]\tLoss: 4.4001\tLR: 2.613811\n",
      "Training Epoch: 27 [7168/50000]\tLoss: 4.3445\tLR: 2.614066\n",
      "Training Epoch: 27 [7296/50000]\tLoss: 4.3542\tLR: 2.614322\n",
      "Training Epoch: 27 [7424/50000]\tLoss: 4.4291\tLR: 2.614578\n",
      "Training Epoch: 27 [7552/50000]\tLoss: 4.3853\tLR: 2.614834\n",
      "Training Epoch: 27 [7680/50000]\tLoss: 4.3786\tLR: 2.615090\n",
      "Training Epoch: 27 [7808/50000]\tLoss: 4.3950\tLR: 2.615345\n",
      "Training Epoch: 27 [7936/50000]\tLoss: 4.3363\tLR: 2.615601\n",
      "Training Epoch: 27 [8064/50000]\tLoss: 4.4121\tLR: 2.615857\n",
      "Training Epoch: 27 [8192/50000]\tLoss: 4.4151\tLR: 2.616113\n",
      "Training Epoch: 27 [8320/50000]\tLoss: 4.3513\tLR: 2.616368\n",
      "Training Epoch: 27 [8448/50000]\tLoss: 4.3313\tLR: 2.616624\n",
      "Training Epoch: 27 [8576/50000]\tLoss: 4.3652\tLR: 2.616880\n",
      "Training Epoch: 27 [8704/50000]\tLoss: 4.2249\tLR: 2.617136\n",
      "Training Epoch: 27 [8832/50000]\tLoss: 4.2365\tLR: 2.617391\n",
      "Training Epoch: 27 [8960/50000]\tLoss: 4.4060\tLR: 2.617647\n",
      "Training Epoch: 27 [9088/50000]\tLoss: 4.2727\tLR: 2.617903\n",
      "Training Epoch: 27 [9216/50000]\tLoss: 4.2482\tLR: 2.618159\n",
      "Training Epoch: 27 [9344/50000]\tLoss: 4.2990\tLR: 2.618414\n",
      "Training Epoch: 27 [9472/50000]\tLoss: 4.3596\tLR: 2.618670\n",
      "Training Epoch: 27 [9600/50000]\tLoss: 4.2960\tLR: 2.618926\n",
      "Training Epoch: 27 [9728/50000]\tLoss: 4.2503\tLR: 2.619182\n",
      "Training Epoch: 27 [9856/50000]\tLoss: 4.3546\tLR: 2.619437\n",
      "Training Epoch: 27 [9984/50000]\tLoss: 4.2599\tLR: 2.619693\n",
      "Training Epoch: 27 [10112/50000]\tLoss: 4.3515\tLR: 2.619949\n",
      "Training Epoch: 27 [10240/50000]\tLoss: 4.0888\tLR: 2.620205\n",
      "Training Epoch: 27 [10368/50000]\tLoss: 4.1808\tLR: 2.620460\n",
      "Training Epoch: 27 [10496/50000]\tLoss: 4.2367\tLR: 2.620716\n",
      "Training Epoch: 27 [10624/50000]\tLoss: 4.3081\tLR: 2.620972\n",
      "Training Epoch: 27 [10752/50000]\tLoss: 4.3886\tLR: 2.621228\n",
      "Training Epoch: 27 [10880/50000]\tLoss: 4.4618\tLR: 2.621483\n",
      "Training Epoch: 27 [11008/50000]\tLoss: 4.3660\tLR: 2.621739\n",
      "Training Epoch: 27 [11136/50000]\tLoss: 4.3169\tLR: 2.621995\n",
      "Training Epoch: 27 [11264/50000]\tLoss: 4.3015\tLR: 2.622251\n",
      "Training Epoch: 27 [11392/50000]\tLoss: 4.3164\tLR: 2.622506\n",
      "Training Epoch: 27 [11520/50000]\tLoss: 4.4457\tLR: 2.622762\n",
      "Training Epoch: 27 [11648/50000]\tLoss: 4.1302\tLR: 2.623018\n",
      "Training Epoch: 27 [11776/50000]\tLoss: 4.5135\tLR: 2.623274\n",
      "Training Epoch: 27 [11904/50000]\tLoss: 4.3467\tLR: 2.623529\n",
      "Training Epoch: 27 [12032/50000]\tLoss: 4.2363\tLR: 2.623785\n",
      "Training Epoch: 27 [12160/50000]\tLoss: 4.4307\tLR: 2.624041\n",
      "Training Epoch: 27 [12288/50000]\tLoss: 4.3197\tLR: 2.624297\n",
      "Training Epoch: 27 [12416/50000]\tLoss: 4.3693\tLR: 2.624552\n",
      "Training Epoch: 27 [12544/50000]\tLoss: 4.4626\tLR: 2.624808\n",
      "Training Epoch: 27 [12672/50000]\tLoss: 4.3294\tLR: 2.625064\n",
      "Training Epoch: 27 [12800/50000]\tLoss: 4.2832\tLR: 2.625320\n",
      "Training Epoch: 27 [12928/50000]\tLoss: 4.3948\tLR: 2.625575\n",
      "Training Epoch: 27 [13056/50000]\tLoss: 4.2513\tLR: 2.625831\n",
      "Training Epoch: 27 [13184/50000]\tLoss: 4.2828\tLR: 2.626087\n",
      "Training Epoch: 27 [13312/50000]\tLoss: 4.2150\tLR: 2.626343\n",
      "Training Epoch: 27 [13440/50000]\tLoss: 4.4154\tLR: 2.626598\n",
      "Training Epoch: 27 [13568/50000]\tLoss: 4.1395\tLR: 2.626854\n",
      "Training Epoch: 27 [13696/50000]\tLoss: 4.2113\tLR: 2.627110\n",
      "Training Epoch: 27 [13824/50000]\tLoss: 4.4033\tLR: 2.627366\n",
      "Training Epoch: 27 [13952/50000]\tLoss: 4.2588\tLR: 2.627621\n",
      "Training Epoch: 27 [14080/50000]\tLoss: 4.4191\tLR: 2.627877\n",
      "Training Epoch: 27 [14208/50000]\tLoss: 4.2788\tLR: 2.628133\n",
      "Training Epoch: 27 [14336/50000]\tLoss: 4.4398\tLR: 2.628389\n",
      "Training Epoch: 27 [14464/50000]\tLoss: 4.3676\tLR: 2.628645\n",
      "Training Epoch: 27 [14592/50000]\tLoss: 4.3602\tLR: 2.628900\n",
      "Training Epoch: 27 [14720/50000]\tLoss: 4.2945\tLR: 2.629156\n",
      "Training Epoch: 27 [14848/50000]\tLoss: 4.3711\tLR: 2.629412\n",
      "Training Epoch: 27 [14976/50000]\tLoss: 4.3543\tLR: 2.629668\n",
      "Training Epoch: 27 [15104/50000]\tLoss: 4.3562\tLR: 2.629923\n",
      "Training Epoch: 27 [15232/50000]\tLoss: 4.2999\tLR: 2.630179\n",
      "Training Epoch: 27 [15360/50000]\tLoss: 4.3489\tLR: 2.630435\n",
      "Training Epoch: 27 [15488/50000]\tLoss: 4.3342\tLR: 2.630691\n",
      "Training Epoch: 27 [15616/50000]\tLoss: 4.2208\tLR: 2.630946\n",
      "Training Epoch: 27 [15744/50000]\tLoss: 4.4991\tLR: 2.631202\n",
      "Training Epoch: 27 [15872/50000]\tLoss: 4.3374\tLR: 2.631458\n",
      "Training Epoch: 27 [16000/50000]\tLoss: 4.4009\tLR: 2.631714\n",
      "Training Epoch: 27 [16128/50000]\tLoss: 4.3188\tLR: 2.631969\n",
      "Training Epoch: 27 [16256/50000]\tLoss: 4.2896\tLR: 2.632225\n",
      "Training Epoch: 27 [16384/50000]\tLoss: 4.4178\tLR: 2.632481\n",
      "Training Epoch: 27 [16512/50000]\tLoss: 4.2298\tLR: 2.632737\n",
      "Training Epoch: 27 [16640/50000]\tLoss: 4.2526\tLR: 2.632992\n",
      "Training Epoch: 27 [16768/50000]\tLoss: 4.3422\tLR: 2.633248\n",
      "Training Epoch: 27 [16896/50000]\tLoss: 4.3456\tLR: 2.633504\n",
      "Training Epoch: 27 [17024/50000]\tLoss: 4.3748\tLR: 2.633760\n",
      "Training Epoch: 27 [17152/50000]\tLoss: 4.2614\tLR: 2.634015\n",
      "Training Epoch: 27 [17280/50000]\tLoss: 4.3440\tLR: 2.634271\n",
      "Training Epoch: 27 [17408/50000]\tLoss: 4.2811\tLR: 2.634527\n",
      "Training Epoch: 27 [17536/50000]\tLoss: 4.1871\tLR: 2.634783\n",
      "Training Epoch: 27 [17664/50000]\tLoss: 4.4546\tLR: 2.635038\n",
      "Training Epoch: 27 [17792/50000]\tLoss: 4.3524\tLR: 2.635294\n",
      "Training Epoch: 27 [17920/50000]\tLoss: 4.3231\tLR: 2.635550\n",
      "Training Epoch: 27 [18048/50000]\tLoss: 4.2943\tLR: 2.635806\n",
      "Training Epoch: 27 [18176/50000]\tLoss: 4.2585\tLR: 2.636061\n",
      "Training Epoch: 27 [18304/50000]\tLoss: 4.4108\tLR: 2.636317\n",
      "Training Epoch: 27 [18432/50000]\tLoss: 4.4003\tLR: 2.636573\n",
      "Training Epoch: 27 [18560/50000]\tLoss: 4.3209\tLR: 2.636829\n",
      "Training Epoch: 27 [18688/50000]\tLoss: 4.3470\tLR: 2.637084\n",
      "Training Epoch: 27 [18816/50000]\tLoss: 4.2670\tLR: 2.637340\n",
      "Training Epoch: 27 [18944/50000]\tLoss: 4.3525\tLR: 2.637596\n",
      "Training Epoch: 27 [19072/50000]\tLoss: 4.2270\tLR: 2.637852\n",
      "Training Epoch: 27 [19200/50000]\tLoss: 4.3427\tLR: 2.638107\n",
      "Training Epoch: 27 [19328/50000]\tLoss: 4.4854\tLR: 2.638363\n",
      "Training Epoch: 27 [19456/50000]\tLoss: 4.3730\tLR: 2.638619\n",
      "Training Epoch: 27 [19584/50000]\tLoss: 4.3743\tLR: 2.638875\n",
      "Training Epoch: 27 [19712/50000]\tLoss: 4.4319\tLR: 2.639130\n",
      "Training Epoch: 27 [19840/50000]\tLoss: 4.5730\tLR: 2.639386\n",
      "Training Epoch: 27 [19968/50000]\tLoss: 4.4325\tLR: 2.639642\n",
      "Training Epoch: 27 [20096/50000]\tLoss: 4.3601\tLR: 2.639898\n",
      "Training Epoch: 27 [20224/50000]\tLoss: 4.3761\tLR: 2.640153\n",
      "Training Epoch: 27 [20352/50000]\tLoss: 4.3289\tLR: 2.640409\n",
      "Training Epoch: 27 [20480/50000]\tLoss: 4.3741\tLR: 2.640665\n",
      "Training Epoch: 27 [20608/50000]\tLoss: 4.2964\tLR: 2.640921\n",
      "Training Epoch: 27 [20736/50000]\tLoss: 4.2923\tLR: 2.641176\n",
      "Training Epoch: 27 [20864/50000]\tLoss: 4.3624\tLR: 2.641432\n",
      "Training Epoch: 27 [20992/50000]\tLoss: 4.4803\tLR: 2.641688\n",
      "Training Epoch: 27 [21120/50000]\tLoss: 4.4276\tLR: 2.641944\n",
      "Training Epoch: 27 [21248/50000]\tLoss: 4.3658\tLR: 2.642199\n",
      "Training Epoch: 27 [21376/50000]\tLoss: 4.3134\tLR: 2.642455\n",
      "Training Epoch: 27 [21504/50000]\tLoss: 4.3314\tLR: 2.642711\n",
      "Training Epoch: 27 [21632/50000]\tLoss: 4.2617\tLR: 2.642967\n",
      "Training Epoch: 27 [21760/50000]\tLoss: 4.2694\tLR: 2.643223\n",
      "Training Epoch: 27 [21888/50000]\tLoss: 4.4130\tLR: 2.643478\n",
      "Training Epoch: 27 [22016/50000]\tLoss: 4.1959\tLR: 2.643734\n",
      "Training Epoch: 27 [22144/50000]\tLoss: 4.4541\tLR: 2.643990\n",
      "Training Epoch: 27 [22272/50000]\tLoss: 4.5733\tLR: 2.644246\n",
      "Training Epoch: 27 [22400/50000]\tLoss: 4.3839\tLR: 2.644501\n",
      "Training Epoch: 27 [22528/50000]\tLoss: 4.5166\tLR: 2.644757\n",
      "Training Epoch: 27 [22656/50000]\tLoss: 4.4082\tLR: 2.645013\n",
      "Training Epoch: 27 [22784/50000]\tLoss: 4.5723\tLR: 2.645269\n",
      "Training Epoch: 27 [22912/50000]\tLoss: 4.4584\tLR: 2.645524\n",
      "Training Epoch: 27 [23040/50000]\tLoss: 4.3421\tLR: 2.645780\n",
      "Training Epoch: 27 [23168/50000]\tLoss: 4.3342\tLR: 2.646036\n",
      "Training Epoch: 27 [23296/50000]\tLoss: 4.4861\tLR: 2.646292\n",
      "Training Epoch: 27 [23424/50000]\tLoss: 4.2914\tLR: 2.646547\n",
      "Training Epoch: 27 [23552/50000]\tLoss: 4.3379\tLR: 2.646803\n",
      "Training Epoch: 27 [23680/50000]\tLoss: 4.1750\tLR: 2.647059\n",
      "Training Epoch: 27 [23808/50000]\tLoss: 4.4097\tLR: 2.647315\n",
      "Training Epoch: 27 [23936/50000]\tLoss: 4.1448\tLR: 2.647570\n",
      "Training Epoch: 27 [24064/50000]\tLoss: 4.3792\tLR: 2.647826\n",
      "Training Epoch: 27 [24192/50000]\tLoss: 4.3061\tLR: 2.648082\n",
      "Training Epoch: 27 [24320/50000]\tLoss: 4.3818\tLR: 2.648338\n",
      "Training Epoch: 27 [24448/50000]\tLoss: 4.3030\tLR: 2.648593\n",
      "Training Epoch: 27 [24576/50000]\tLoss: 4.2419\tLR: 2.648849\n",
      "Training Epoch: 27 [24704/50000]\tLoss: 4.2721\tLR: 2.649105\n",
      "Training Epoch: 27 [24832/50000]\tLoss: 4.3713\tLR: 2.649361\n",
      "Training Epoch: 27 [24960/50000]\tLoss: 4.3313\tLR: 2.649616\n",
      "Training Epoch: 27 [25088/50000]\tLoss: 4.3600\tLR: 2.649872\n",
      "Training Epoch: 27 [25216/50000]\tLoss: 4.3219\tLR: 2.650128\n",
      "Training Epoch: 27 [25344/50000]\tLoss: 4.3200\tLR: 2.650384\n",
      "Training Epoch: 27 [25472/50000]\tLoss: 4.3544\tLR: 2.650639\n",
      "Training Epoch: 27 [25600/50000]\tLoss: 4.2639\tLR: 2.650895\n",
      "Training Epoch: 27 [25728/50000]\tLoss: 4.4498\tLR: 2.651151\n",
      "Training Epoch: 27 [25856/50000]\tLoss: 4.2458\tLR: 2.651407\n",
      "Training Epoch: 27 [25984/50000]\tLoss: 4.4284\tLR: 2.651662\n",
      "Training Epoch: 27 [26112/50000]\tLoss: 4.4054\tLR: 2.651918\n",
      "Training Epoch: 27 [26240/50000]\tLoss: 4.3599\tLR: 2.652174\n",
      "Training Epoch: 27 [26368/50000]\tLoss: 4.3989\tLR: 2.652430\n",
      "Training Epoch: 27 [26496/50000]\tLoss: 4.2974\tLR: 2.652685\n",
      "Training Epoch: 27 [26624/50000]\tLoss: 4.2734\tLR: 2.652941\n",
      "Training Epoch: 27 [26752/50000]\tLoss: 4.2970\tLR: 2.653197\n",
      "Training Epoch: 27 [26880/50000]\tLoss: 4.3027\tLR: 2.653453\n",
      "Training Epoch: 27 [27008/50000]\tLoss: 4.2737\tLR: 2.653708\n",
      "Training Epoch: 27 [27136/50000]\tLoss: 4.2505\tLR: 2.653964\n",
      "Training Epoch: 27 [27264/50000]\tLoss: 4.2545\tLR: 2.654220\n",
      "Training Epoch: 27 [27392/50000]\tLoss: 4.3244\tLR: 2.654476\n",
      "Training Epoch: 27 [27520/50000]\tLoss: 4.2784\tLR: 2.654731\n",
      "Training Epoch: 27 [27648/50000]\tLoss: 4.3083\tLR: 2.654987\n",
      "Training Epoch: 27 [27776/50000]\tLoss: 4.3500\tLR: 2.655243\n",
      "Training Epoch: 27 [27904/50000]\tLoss: 4.4231\tLR: 2.655499\n",
      "Training Epoch: 27 [28032/50000]\tLoss: 4.3313\tLR: 2.655754\n",
      "Training Epoch: 27 [28160/50000]\tLoss: 4.3311\tLR: 2.656010\n",
      "Training Epoch: 27 [28288/50000]\tLoss: 4.4603\tLR: 2.656266\n",
      "Training Epoch: 27 [28416/50000]\tLoss: 4.5531\tLR: 2.656522\n",
      "Training Epoch: 27 [28544/50000]\tLoss: 4.3416\tLR: 2.656777\n",
      "Training Epoch: 27 [28672/50000]\tLoss: 4.3488\tLR: 2.657033\n",
      "Training Epoch: 27 [28800/50000]\tLoss: 4.3838\tLR: 2.657289\n",
      "Training Epoch: 27 [28928/50000]\tLoss: 4.3642\tLR: 2.657545\n",
      "Training Epoch: 27 [29056/50000]\tLoss: 4.2608\tLR: 2.657801\n",
      "Training Epoch: 27 [29184/50000]\tLoss: 4.4132\tLR: 2.658056\n",
      "Training Epoch: 27 [29312/50000]\tLoss: 4.2344\tLR: 2.658312\n",
      "Training Epoch: 27 [29440/50000]\tLoss: 4.1482\tLR: 2.658568\n",
      "Training Epoch: 27 [29568/50000]\tLoss: 4.3564\tLR: 2.658824\n",
      "Training Epoch: 27 [29696/50000]\tLoss: 4.2909\tLR: 2.659079\n",
      "Training Epoch: 27 [29824/50000]\tLoss: 4.3742\tLR: 2.659335\n",
      "Training Epoch: 27 [29952/50000]\tLoss: 4.3308\tLR: 2.659591\n",
      "Training Epoch: 27 [30080/50000]\tLoss: 4.2979\tLR: 2.659847\n",
      "Training Epoch: 27 [30208/50000]\tLoss: 4.3213\tLR: 2.660102\n",
      "Training Epoch: 27 [30336/50000]\tLoss: 4.2681\tLR: 2.660358\n",
      "Training Epoch: 27 [30464/50000]\tLoss: 4.2782\tLR: 2.660614\n",
      "Training Epoch: 27 [30592/50000]\tLoss: 4.2984\tLR: 2.660870\n",
      "Training Epoch: 27 [30720/50000]\tLoss: 4.4175\tLR: 2.661125\n",
      "Training Epoch: 27 [30848/50000]\tLoss: 4.1959\tLR: 2.661381\n",
      "Training Epoch: 27 [30976/50000]\tLoss: 4.2068\tLR: 2.661637\n",
      "Training Epoch: 27 [31104/50000]\tLoss: 4.3531\tLR: 2.661893\n",
      "Training Epoch: 27 [31232/50000]\tLoss: 4.4503\tLR: 2.662148\n",
      "Training Epoch: 27 [31360/50000]\tLoss: 4.3242\tLR: 2.662404\n",
      "Training Epoch: 27 [31488/50000]\tLoss: 4.4686\tLR: 2.662660\n",
      "Training Epoch: 27 [31616/50000]\tLoss: 4.3345\tLR: 2.662916\n",
      "Training Epoch: 27 [31744/50000]\tLoss: 4.1858\tLR: 2.663171\n",
      "Training Epoch: 27 [31872/50000]\tLoss: 4.2438\tLR: 2.663427\n",
      "Training Epoch: 27 [32000/50000]\tLoss: 4.3086\tLR: 2.663683\n",
      "Training Epoch: 27 [32128/50000]\tLoss: 4.2983\tLR: 2.663939\n",
      "Training Epoch: 27 [32256/50000]\tLoss: 4.1738\tLR: 2.664194\n",
      "Training Epoch: 27 [32384/50000]\tLoss: 4.3563\tLR: 2.664450\n",
      "Training Epoch: 27 [32512/50000]\tLoss: 4.3522\tLR: 2.664706\n",
      "Training Epoch: 27 [32640/50000]\tLoss: 4.2132\tLR: 2.664962\n",
      "Training Epoch: 27 [32768/50000]\tLoss: 4.3685\tLR: 2.665217\n",
      "Training Epoch: 27 [32896/50000]\tLoss: 4.3702\tLR: 2.665473\n",
      "Training Epoch: 27 [33024/50000]\tLoss: 4.3092\tLR: 2.665729\n",
      "Training Epoch: 27 [33152/50000]\tLoss: 4.2744\tLR: 2.665985\n",
      "Training Epoch: 27 [33280/50000]\tLoss: 4.2338\tLR: 2.666240\n",
      "Training Epoch: 27 [33408/50000]\tLoss: 4.3278\tLR: 2.666496\n",
      "Training Epoch: 27 [33536/50000]\tLoss: 4.3420\tLR: 2.666752\n",
      "Training Epoch: 27 [33664/50000]\tLoss: 4.2728\tLR: 2.667008\n",
      "Training Epoch: 27 [33792/50000]\tLoss: 4.4360\tLR: 2.667263\n",
      "Training Epoch: 27 [33920/50000]\tLoss: 4.3744\tLR: 2.667519\n",
      "Training Epoch: 27 [34048/50000]\tLoss: 4.3540\tLR: 2.667775\n",
      "Training Epoch: 27 [34176/50000]\tLoss: 4.2961\tLR: 2.668031\n",
      "Training Epoch: 27 [34304/50000]\tLoss: 4.3211\tLR: 2.668286\n",
      "Training Epoch: 27 [34432/50000]\tLoss: 4.2633\tLR: 2.668542\n",
      "Training Epoch: 27 [34560/50000]\tLoss: 4.3839\tLR: 2.668798\n",
      "Training Epoch: 27 [34688/50000]\tLoss: 4.4922\tLR: 2.669054\n",
      "Training Epoch: 27 [34816/50000]\tLoss: 4.3767\tLR: 2.669309\n",
      "Training Epoch: 27 [34944/50000]\tLoss: 4.2520\tLR: 2.669565\n",
      "Training Epoch: 27 [35072/50000]\tLoss: 4.3359\tLR: 2.669821\n",
      "Training Epoch: 27 [35200/50000]\tLoss: 4.4504\tLR: 2.670077\n",
      "Training Epoch: 27 [35328/50000]\tLoss: 4.3578\tLR: 2.670332\n",
      "Training Epoch: 27 [35456/50000]\tLoss: 4.4643\tLR: 2.670588\n",
      "Training Epoch: 27 [35584/50000]\tLoss: 4.4323\tLR: 2.670844\n",
      "Training Epoch: 27 [35712/50000]\tLoss: 4.4259\tLR: 2.671100\n",
      "Training Epoch: 27 [35840/50000]\tLoss: 4.3813\tLR: 2.671355\n",
      "Training Epoch: 27 [35968/50000]\tLoss: 4.3288\tLR: 2.671611\n",
      "Training Epoch: 27 [36096/50000]\tLoss: 4.2849\tLR: 2.671867\n",
      "Training Epoch: 27 [36224/50000]\tLoss: 4.1817\tLR: 2.672123\n",
      "Training Epoch: 27 [36352/50000]\tLoss: 4.4315\tLR: 2.672379\n",
      "Training Epoch: 27 [36480/50000]\tLoss: 4.1539\tLR: 2.672634\n",
      "Training Epoch: 27 [36608/50000]\tLoss: 4.5227\tLR: 2.672890\n",
      "Training Epoch: 27 [36736/50000]\tLoss: 4.4428\tLR: 2.673146\n",
      "Training Epoch: 27 [36864/50000]\tLoss: 4.4495\tLR: 2.673402\n",
      "Training Epoch: 27 [36992/50000]\tLoss: 4.3831\tLR: 2.673657\n",
      "Training Epoch: 27 [37120/50000]\tLoss: 4.4188\tLR: 2.673913\n",
      "Training Epoch: 27 [37248/50000]\tLoss: 4.3710\tLR: 2.674169\n",
      "Training Epoch: 27 [37376/50000]\tLoss: 4.3638\tLR: 2.674425\n",
      "Training Epoch: 27 [37504/50000]\tLoss: 4.3018\tLR: 2.674680\n",
      "Training Epoch: 27 [37632/50000]\tLoss: 4.4110\tLR: 2.674936\n",
      "Training Epoch: 27 [37760/50000]\tLoss: 4.1711\tLR: 2.675192\n",
      "Training Epoch: 27 [37888/50000]\tLoss: 4.2808\tLR: 2.675448\n",
      "Training Epoch: 27 [38016/50000]\tLoss: 4.4112\tLR: 2.675703\n",
      "Training Epoch: 27 [38144/50000]\tLoss: 4.4809\tLR: 2.675959\n",
      "Training Epoch: 27 [38272/50000]\tLoss: 4.3530\tLR: 2.676215\n",
      "Training Epoch: 27 [38400/50000]\tLoss: 4.3493\tLR: 2.676471\n",
      "Training Epoch: 27 [38528/50000]\tLoss: 4.3260\tLR: 2.676726\n",
      "Training Epoch: 27 [38656/50000]\tLoss: 4.4677\tLR: 2.676982\n",
      "Training Epoch: 27 [38784/50000]\tLoss: 4.3464\tLR: 2.677238\n",
      "Training Epoch: 27 [38912/50000]\tLoss: 4.3096\tLR: 2.677494\n",
      "Training Epoch: 27 [39040/50000]\tLoss: 4.4248\tLR: 2.677749\n",
      "Training Epoch: 27 [39168/50000]\tLoss: 4.3370\tLR: 2.678005\n",
      "Training Epoch: 27 [39296/50000]\tLoss: 4.5054\tLR: 2.678261\n",
      "Training Epoch: 27 [39424/50000]\tLoss: 4.4138\tLR: 2.678517\n",
      "Training Epoch: 27 [39552/50000]\tLoss: 4.3752\tLR: 2.678772\n",
      "Training Epoch: 27 [39680/50000]\tLoss: 4.2258\tLR: 2.679028\n",
      "Training Epoch: 27 [39808/50000]\tLoss: 4.2296\tLR: 2.679284\n",
      "Training Epoch: 27 [39936/50000]\tLoss: 4.3119\tLR: 2.679540\n",
      "Training Epoch: 27 [40064/50000]\tLoss: 4.3692\tLR: 2.679795\n",
      "Training Epoch: 27 [40192/50000]\tLoss: 4.2889\tLR: 2.680051\n",
      "Training Epoch: 27 [40320/50000]\tLoss: 4.2472\tLR: 2.680307\n",
      "Training Epoch: 27 [40448/50000]\tLoss: 4.3690\tLR: 2.680563\n",
      "Training Epoch: 27 [40576/50000]\tLoss: 4.4623\tLR: 2.680818\n",
      "Training Epoch: 27 [40704/50000]\tLoss: 4.5093\tLR: 2.681074\n",
      "Training Epoch: 27 [40832/50000]\tLoss: 4.4208\tLR: 2.681330\n",
      "Training Epoch: 27 [40960/50000]\tLoss: 4.3898\tLR: 2.681586\n",
      "Training Epoch: 27 [41088/50000]\tLoss: 4.4455\tLR: 2.681841\n",
      "Training Epoch: 27 [41216/50000]\tLoss: 4.2499\tLR: 2.682097\n",
      "Training Epoch: 27 [41344/50000]\tLoss: 4.3425\tLR: 2.682353\n",
      "Training Epoch: 27 [41472/50000]\tLoss: 4.3899\tLR: 2.682609\n",
      "Training Epoch: 27 [41600/50000]\tLoss: 4.1225\tLR: 2.682864\n",
      "Training Epoch: 27 [41728/50000]\tLoss: 4.2662\tLR: 2.683120\n",
      "Training Epoch: 27 [41856/50000]\tLoss: 4.3168\tLR: 2.683376\n",
      "Training Epoch: 27 [41984/50000]\tLoss: 4.2372\tLR: 2.683632\n",
      "Training Epoch: 27 [42112/50000]\tLoss: 4.2720\tLR: 2.683887\n",
      "Training Epoch: 27 [42240/50000]\tLoss: 4.2488\tLR: 2.684143\n",
      "Training Epoch: 27 [42368/50000]\tLoss: 4.3030\tLR: 2.684399\n",
      "Training Epoch: 27 [42496/50000]\tLoss: 4.4085\tLR: 2.684655\n",
      "Training Epoch: 27 [42624/50000]\tLoss: 4.4453\tLR: 2.684910\n",
      "Training Epoch: 27 [42752/50000]\tLoss: 4.3660\tLR: 2.685166\n",
      "Training Epoch: 27 [42880/50000]\tLoss: 4.4448\tLR: 2.685422\n",
      "Training Epoch: 27 [43008/50000]\tLoss: 4.4573\tLR: 2.685678\n",
      "Training Epoch: 27 [43136/50000]\tLoss: 4.3647\tLR: 2.685934\n",
      "Training Epoch: 27 [43264/50000]\tLoss: 4.4629\tLR: 2.686189\n",
      "Training Epoch: 27 [43392/50000]\tLoss: 4.3267\tLR: 2.686445\n",
      "Training Epoch: 27 [43520/50000]\tLoss: 4.3766\tLR: 2.686701\n",
      "Training Epoch: 27 [43648/50000]\tLoss: 4.2507\tLR: 2.686957\n",
      "Training Epoch: 27 [43776/50000]\tLoss: 4.2281\tLR: 2.687212\n",
      "Training Epoch: 27 [43904/50000]\tLoss: 4.2661\tLR: 2.687468\n",
      "Training Epoch: 27 [44032/50000]\tLoss: 4.3476\tLR: 2.687724\n",
      "Training Epoch: 27 [44160/50000]\tLoss: 4.2436\tLR: 2.687980\n",
      "Training Epoch: 27 [44288/50000]\tLoss: 4.4420\tLR: 2.688235\n",
      "Training Epoch: 27 [44416/50000]\tLoss: 4.2215\tLR: 2.688491\n",
      "Training Epoch: 27 [44544/50000]\tLoss: 4.3701\tLR: 2.688747\n",
      "Training Epoch: 27 [44672/50000]\tLoss: 4.3500\tLR: 2.689003\n",
      "Training Epoch: 27 [44800/50000]\tLoss: 4.4150\tLR: 2.689258\n",
      "Training Epoch: 27 [44928/50000]\tLoss: 4.3286\tLR: 2.689514\n",
      "Training Epoch: 27 [45056/50000]\tLoss: 4.3826\tLR: 2.689770\n",
      "Training Epoch: 27 [45184/50000]\tLoss: 4.2815\tLR: 2.690026\n",
      "Training Epoch: 27 [45312/50000]\tLoss: 4.4372\tLR: 2.690281\n",
      "Training Epoch: 27 [45440/50000]\tLoss: 4.3967\tLR: 2.690537\n",
      "Training Epoch: 27 [45568/50000]\tLoss: 4.2528\tLR: 2.690793\n",
      "Training Epoch: 27 [45696/50000]\tLoss: 4.1040\tLR: 2.691049\n",
      "Training Epoch: 27 [45824/50000]\tLoss: 4.2033\tLR: 2.691304\n",
      "Training Epoch: 27 [45952/50000]\tLoss: 4.3055\tLR: 2.691560\n",
      "Training Epoch: 27 [46080/50000]\tLoss: 4.3675\tLR: 2.691816\n",
      "Training Epoch: 27 [46208/50000]\tLoss: 4.2938\tLR: 2.692072\n",
      "Training Epoch: 27 [46336/50000]\tLoss: 4.2675\tLR: 2.692327\n",
      "Training Epoch: 27 [46464/50000]\tLoss: 4.4795\tLR: 2.692583\n",
      "Training Epoch: 27 [46592/50000]\tLoss: 4.3968\tLR: 2.692839\n",
      "Training Epoch: 27 [46720/50000]\tLoss: 4.0516\tLR: 2.693095\n",
      "Training Epoch: 27 [46848/50000]\tLoss: 4.5168\tLR: 2.693350\n",
      "Training Epoch: 27 [46976/50000]\tLoss: 4.3634\tLR: 2.693606\n",
      "Training Epoch: 27 [47104/50000]\tLoss: 4.3589\tLR: 2.693862\n",
      "Training Epoch: 27 [47232/50000]\tLoss: 4.3381\tLR: 2.694118\n",
      "Training Epoch: 27 [47360/50000]\tLoss: 4.2147\tLR: 2.694373\n",
      "Training Epoch: 27 [47488/50000]\tLoss: 4.2096\tLR: 2.694629\n",
      "Training Epoch: 27 [47616/50000]\tLoss: 4.3972\tLR: 2.694885\n",
      "Training Epoch: 27 [47744/50000]\tLoss: 4.3772\tLR: 2.695141\n",
      "Training Epoch: 27 [47872/50000]\tLoss: 4.3855\tLR: 2.695396\n",
      "Training Epoch: 27 [48000/50000]\tLoss: 4.3982\tLR: 2.695652\n",
      "Training Epoch: 27 [48128/50000]\tLoss: 4.3109\tLR: 2.695908\n",
      "Training Epoch: 27 [48256/50000]\tLoss: 4.3769\tLR: 2.696164\n",
      "Training Epoch: 27 [48384/50000]\tLoss: 4.4255\tLR: 2.696419\n",
      "Training Epoch: 27 [48512/50000]\tLoss: 4.3819\tLR: 2.696675\n",
      "Training Epoch: 27 [48640/50000]\tLoss: 4.3865\tLR: 2.696931\n",
      "Training Epoch: 27 [48768/50000]\tLoss: 4.1975\tLR: 2.697187\n",
      "Training Epoch: 27 [48896/50000]\tLoss: 4.2360\tLR: 2.697442\n",
      "Training Epoch: 27 [49024/50000]\tLoss: 4.4848\tLR: 2.697698\n",
      "Training Epoch: 27 [49152/50000]\tLoss: 4.2794\tLR: 2.697954\n",
      "Training Epoch: 27 [49280/50000]\tLoss: 4.4067\tLR: 2.698210\n",
      "Training Epoch: 27 [49408/50000]\tLoss: 4.4143\tLR: 2.698465\n",
      "Training Epoch: 27 [49536/50000]\tLoss: 4.5034\tLR: 2.698721\n",
      "Training Epoch: 27 [49664/50000]\tLoss: 4.4014\tLR: 2.698977\n",
      "Training Epoch: 27 [49792/50000]\tLoss: 4.4212\tLR: 2.699233\n",
      "Training Epoch: 27 [49920/50000]\tLoss: 4.4611\tLR: 2.699488\n",
      "Training Epoch: 27 [50000/50000]\tLoss: 4.4399\tLR: 2.699744\n",
      "epoch 27 training time consumed: 488.98s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   37853 GB |   37853 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   37737 GB |   37737 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     116 GB |     116 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   37853 GB |   37853 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   37737 GB |   37737 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     116 GB |     116 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   37318 GB |   37318 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   37202 GB |   37202 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     116 GB |     116 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    4014 K  |    4013 K  |\n",
      "|       from large pool |      24    |      65    |    1711 K  |    1711 K  |\n",
      "|       from small pool |     231    |     274    |    2303 K  |    2302 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    4014 K  |    4013 K  |\n",
      "|       from large pool |      24    |      65    |    1711 K  |    1711 K  |\n",
      "|       from small pool |     231    |     274    |    2303 K  |    2302 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      46    |    2325 K  |    2325 K  |\n",
      "|       from large pool |      10    |      23    |     822 K  |     822 K  |\n",
      "|       from small pool |      25    |      35    |    1503 K  |    1503 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 27, Average loss: 0.0393, Accuracy: 0.0206, Time consumed:31.06s\n",
      "\n",
      "Training Epoch: 28 [128/50000]\tLoss: 4.4350\tLR: 0.100000\n",
      "Training Epoch: 28 [256/50000]\tLoss: 4.4025\tLR: 2.700256\n",
      "Training Epoch: 28 [384/50000]\tLoss: 4.3386\tLR: 2.700512\n",
      "Training Epoch: 28 [512/50000]\tLoss: 4.4258\tLR: 2.700767\n",
      "Training Epoch: 28 [640/50000]\tLoss: 4.4016\tLR: 2.701023\n",
      "Training Epoch: 28 [768/50000]\tLoss: 4.3358\tLR: 2.701279\n",
      "Training Epoch: 28 [896/50000]\tLoss: 4.2640\tLR: 2.701535\n",
      "Training Epoch: 28 [1024/50000]\tLoss: 4.3222\tLR: 2.701790\n",
      "Training Epoch: 28 [1152/50000]\tLoss: 4.3228\tLR: 2.702046\n",
      "Training Epoch: 28 [1280/50000]\tLoss: 4.2619\tLR: 2.702302\n",
      "Training Epoch: 28 [1408/50000]\tLoss: 4.4140\tLR: 2.702558\n",
      "Training Epoch: 28 [1536/50000]\tLoss: 4.4853\tLR: 2.702813\n",
      "Training Epoch: 28 [1664/50000]\tLoss: 4.3884\tLR: 2.703069\n",
      "Training Epoch: 28 [1792/50000]\tLoss: 4.2932\tLR: 2.703325\n",
      "Training Epoch: 28 [1920/50000]\tLoss: 4.3217\tLR: 2.703581\n",
      "Training Epoch: 28 [2048/50000]\tLoss: 4.4695\tLR: 2.703836\n",
      "Training Epoch: 28 [2176/50000]\tLoss: 4.3181\tLR: 2.704092\n",
      "Training Epoch: 28 [2304/50000]\tLoss: 4.3952\tLR: 2.704348\n",
      "Training Epoch: 28 [2432/50000]\tLoss: 4.1815\tLR: 2.704604\n",
      "Training Epoch: 28 [2560/50000]\tLoss: 4.5046\tLR: 2.704859\n",
      "Training Epoch: 28 [2688/50000]\tLoss: 4.3299\tLR: 2.705115\n",
      "Training Epoch: 28 [2816/50000]\tLoss: 4.2428\tLR: 2.705371\n",
      "Training Epoch: 28 [2944/50000]\tLoss: 4.3405\tLR: 2.705627\n",
      "Training Epoch: 28 [3072/50000]\tLoss: 4.2799\tLR: 2.705882\n",
      "Training Epoch: 28 [3200/50000]\tLoss: 4.1543\tLR: 2.706138\n",
      "Training Epoch: 28 [3328/50000]\tLoss: 4.2737\tLR: 2.706394\n",
      "Training Epoch: 28 [3456/50000]\tLoss: 4.4718\tLR: 2.706650\n",
      "Training Epoch: 28 [3584/50000]\tLoss: 4.3328\tLR: 2.706905\n",
      "Training Epoch: 28 [3712/50000]\tLoss: 4.3211\tLR: 2.707161\n",
      "Training Epoch: 28 [3840/50000]\tLoss: 4.3450\tLR: 2.707417\n",
      "Training Epoch: 28 [3968/50000]\tLoss: 4.3153\tLR: 2.707673\n",
      "Training Epoch: 28 [4096/50000]\tLoss: 4.3889\tLR: 2.707928\n",
      "Training Epoch: 28 [4224/50000]\tLoss: 4.3857\tLR: 2.708184\n",
      "Training Epoch: 28 [4352/50000]\tLoss: 4.3822\tLR: 2.708440\n",
      "Training Epoch: 28 [4480/50000]\tLoss: 4.1966\tLR: 2.708696\n",
      "Training Epoch: 28 [4608/50000]\tLoss: 4.3602\tLR: 2.708951\n",
      "Training Epoch: 28 [4736/50000]\tLoss: 4.3017\tLR: 2.709207\n",
      "Training Epoch: 28 [4864/50000]\tLoss: 4.2402\tLR: 2.709463\n",
      "Training Epoch: 28 [4992/50000]\tLoss: 4.3127\tLR: 2.709719\n",
      "Training Epoch: 28 [5120/50000]\tLoss: 4.4044\tLR: 2.709974\n",
      "Training Epoch: 28 [5248/50000]\tLoss: 4.3906\tLR: 2.710230\n",
      "Training Epoch: 28 [5376/50000]\tLoss: 4.2632\tLR: 2.710486\n",
      "Training Epoch: 28 [5504/50000]\tLoss: 4.4319\tLR: 2.710742\n",
      "Training Epoch: 28 [5632/50000]\tLoss: 4.3535\tLR: 2.710997\n",
      "Training Epoch: 28 [5760/50000]\tLoss: 4.2896\tLR: 2.711253\n",
      "Training Epoch: 28 [5888/50000]\tLoss: 4.2621\tLR: 2.711509\n",
      "Training Epoch: 28 [6016/50000]\tLoss: 4.4034\tLR: 2.711765\n",
      "Training Epoch: 28 [6144/50000]\tLoss: 4.5352\tLR: 2.712020\n",
      "Training Epoch: 28 [6272/50000]\tLoss: 4.4255\tLR: 2.712276\n",
      "Training Epoch: 28 [6400/50000]\tLoss: 4.2467\tLR: 2.712532\n",
      "Training Epoch: 28 [6528/50000]\tLoss: 4.3685\tLR: 2.712788\n",
      "Training Epoch: 28 [6656/50000]\tLoss: 4.2568\tLR: 2.713043\n",
      "Training Epoch: 28 [6784/50000]\tLoss: 4.3270\tLR: 2.713299\n",
      "Training Epoch: 28 [6912/50000]\tLoss: 4.4106\tLR: 2.713555\n",
      "Training Epoch: 28 [7040/50000]\tLoss: 4.2504\tLR: 2.713811\n",
      "Training Epoch: 28 [7168/50000]\tLoss: 4.2214\tLR: 2.714066\n",
      "Training Epoch: 28 [7296/50000]\tLoss: 4.4289\tLR: 2.714322\n",
      "Training Epoch: 28 [7424/50000]\tLoss: 4.2638\tLR: 2.714578\n",
      "Training Epoch: 28 [7552/50000]\tLoss: 4.3465\tLR: 2.714834\n",
      "Training Epoch: 28 [7680/50000]\tLoss: 4.2563\tLR: 2.715090\n",
      "Training Epoch: 28 [7808/50000]\tLoss: 4.3336\tLR: 2.715345\n",
      "Training Epoch: 28 [7936/50000]\tLoss: 4.4815\tLR: 2.715601\n",
      "Training Epoch: 28 [8064/50000]\tLoss: 4.4910\tLR: 2.715857\n",
      "Training Epoch: 28 [8192/50000]\tLoss: 4.4012\tLR: 2.716113\n",
      "Training Epoch: 28 [8320/50000]\tLoss: 4.5268\tLR: 2.716368\n",
      "Training Epoch: 28 [8448/50000]\tLoss: 4.5122\tLR: 2.716624\n",
      "Training Epoch: 28 [8576/50000]\tLoss: 4.2513\tLR: 2.716880\n",
      "Training Epoch: 28 [8704/50000]\tLoss: 4.3066\tLR: 2.717136\n",
      "Training Epoch: 28 [8832/50000]\tLoss: 4.3728\tLR: 2.717391\n",
      "Training Epoch: 28 [8960/50000]\tLoss: 4.3652\tLR: 2.717647\n",
      "Training Epoch: 28 [9088/50000]\tLoss: 4.2710\tLR: 2.717903\n",
      "Training Epoch: 28 [9216/50000]\tLoss: 4.3091\tLR: 2.718159\n",
      "Training Epoch: 28 [9344/50000]\tLoss: 4.5156\tLR: 2.718414\n",
      "Training Epoch: 28 [9472/50000]\tLoss: 4.3746\tLR: 2.718670\n",
      "Training Epoch: 28 [9600/50000]\tLoss: 4.3475\tLR: 2.718926\n",
      "Training Epoch: 28 [9728/50000]\tLoss: 4.4471\tLR: 2.719182\n",
      "Training Epoch: 28 [9856/50000]\tLoss: 4.4539\tLR: 2.719437\n",
      "Training Epoch: 28 [9984/50000]\tLoss: 4.4119\tLR: 2.719693\n",
      "Training Epoch: 28 [10112/50000]\tLoss: 4.3986\tLR: 2.719949\n",
      "Training Epoch: 28 [10240/50000]\tLoss: 4.3538\tLR: 2.720205\n",
      "Training Epoch: 28 [10368/50000]\tLoss: 4.4699\tLR: 2.720460\n",
      "Training Epoch: 28 [10496/50000]\tLoss: 4.2762\tLR: 2.720716\n",
      "Training Epoch: 28 [10624/50000]\tLoss: 4.3343\tLR: 2.720972\n",
      "Training Epoch: 28 [10752/50000]\tLoss: 4.4356\tLR: 2.721228\n",
      "Training Epoch: 28 [10880/50000]\tLoss: 4.2048\tLR: 2.721483\n",
      "Training Epoch: 28 [11008/50000]\tLoss: 4.2878\tLR: 2.721739\n",
      "Training Epoch: 28 [11136/50000]\tLoss: 4.3496\tLR: 2.721995\n",
      "Training Epoch: 28 [11264/50000]\tLoss: 4.3406\tLR: 2.722251\n",
      "Training Epoch: 28 [11392/50000]\tLoss: 4.2587\tLR: 2.722506\n",
      "Training Epoch: 28 [11520/50000]\tLoss: 4.2563\tLR: 2.722762\n",
      "Training Epoch: 28 [11648/50000]\tLoss: 4.3387\tLR: 2.723018\n",
      "Training Epoch: 28 [11776/50000]\tLoss: 4.3719\tLR: 2.723274\n",
      "Training Epoch: 28 [11904/50000]\tLoss: 4.4906\tLR: 2.723529\n",
      "Training Epoch: 28 [12032/50000]\tLoss: 4.2962\tLR: 2.723785\n",
      "Training Epoch: 28 [12160/50000]\tLoss: 4.3084\tLR: 2.724041\n",
      "Training Epoch: 28 [12288/50000]\tLoss: 4.2885\tLR: 2.724297\n",
      "Training Epoch: 28 [12416/50000]\tLoss: 4.5775\tLR: 2.724552\n",
      "Training Epoch: 28 [12544/50000]\tLoss: 4.4684\tLR: 2.724808\n",
      "Training Epoch: 28 [12672/50000]\tLoss: 4.2762\tLR: 2.725064\n",
      "Training Epoch: 28 [12800/50000]\tLoss: 4.2828\tLR: 2.725320\n",
      "Training Epoch: 28 [12928/50000]\tLoss: 4.3600\tLR: 2.725575\n",
      "Training Epoch: 28 [13056/50000]\tLoss: 4.2611\tLR: 2.725831\n",
      "Training Epoch: 28 [13184/50000]\tLoss: 4.3908\tLR: 2.726087\n",
      "Training Epoch: 28 [13312/50000]\tLoss: 4.2799\tLR: 2.726343\n",
      "Training Epoch: 28 [13440/50000]\tLoss: 4.3628\tLR: 2.726598\n",
      "Training Epoch: 28 [13568/50000]\tLoss: 4.2825\tLR: 2.726854\n",
      "Training Epoch: 28 [13696/50000]\tLoss: 4.2010\tLR: 2.727110\n",
      "Training Epoch: 28 [13824/50000]\tLoss: 4.2181\tLR: 2.727366\n",
      "Training Epoch: 28 [13952/50000]\tLoss: 4.1553\tLR: 2.727621\n",
      "Training Epoch: 28 [14080/50000]\tLoss: 4.1799\tLR: 2.727877\n",
      "Training Epoch: 28 [14208/50000]\tLoss: 4.3006\tLR: 2.728133\n",
      "Training Epoch: 28 [14336/50000]\tLoss: 4.3873\tLR: 2.728389\n",
      "Training Epoch: 28 [14464/50000]\tLoss: 4.1949\tLR: 2.728645\n",
      "Training Epoch: 28 [14592/50000]\tLoss: 4.2255\tLR: 2.728900\n",
      "Training Epoch: 28 [14720/50000]\tLoss: 4.1726\tLR: 2.729156\n",
      "Training Epoch: 28 [14848/50000]\tLoss: 4.3360\tLR: 2.729412\n",
      "Training Epoch: 28 [14976/50000]\tLoss: 4.2757\tLR: 2.729668\n",
      "Training Epoch: 28 [15104/50000]\tLoss: 4.2224\tLR: 2.729923\n",
      "Training Epoch: 28 [15232/50000]\tLoss: 4.2722\tLR: 2.730179\n",
      "Training Epoch: 28 [15360/50000]\tLoss: 4.2766\tLR: 2.730435\n",
      "Training Epoch: 28 [15488/50000]\tLoss: 4.3747\tLR: 2.730691\n",
      "Training Epoch: 28 [15616/50000]\tLoss: 4.3383\tLR: 2.730946\n",
      "Training Epoch: 28 [15744/50000]\tLoss: 4.2421\tLR: 2.731202\n",
      "Training Epoch: 28 [15872/50000]\tLoss: 4.3321\tLR: 2.731458\n",
      "Training Epoch: 28 [16000/50000]\tLoss: 4.2406\tLR: 2.731714\n",
      "Training Epoch: 28 [16128/50000]\tLoss: 4.1556\tLR: 2.731969\n",
      "Training Epoch: 28 [16256/50000]\tLoss: 4.2725\tLR: 2.732225\n",
      "Training Epoch: 28 [16384/50000]\tLoss: 4.4655\tLR: 2.732481\n",
      "Training Epoch: 28 [16512/50000]\tLoss: 4.3001\tLR: 2.732737\n",
      "Training Epoch: 28 [16640/50000]\tLoss: 4.2855\tLR: 2.732992\n",
      "Training Epoch: 28 [16768/50000]\tLoss: 4.2879\tLR: 2.733248\n",
      "Training Epoch: 28 [16896/50000]\tLoss: 4.3058\tLR: 2.733504\n",
      "Training Epoch: 28 [17024/50000]\tLoss: 4.3156\tLR: 2.733760\n",
      "Training Epoch: 28 [17152/50000]\tLoss: 4.4331\tLR: 2.734015\n",
      "Training Epoch: 28 [17280/50000]\tLoss: 4.2874\tLR: 2.734271\n",
      "Training Epoch: 28 [17408/50000]\tLoss: 4.3290\tLR: 2.734527\n",
      "Training Epoch: 28 [17536/50000]\tLoss: 4.3683\tLR: 2.734783\n",
      "Training Epoch: 28 [17664/50000]\tLoss: 4.3934\tLR: 2.735038\n",
      "Training Epoch: 28 [17792/50000]\tLoss: 4.3711\tLR: 2.735294\n",
      "Training Epoch: 28 [17920/50000]\tLoss: 4.2967\tLR: 2.735550\n",
      "Training Epoch: 28 [18048/50000]\tLoss: 4.2740\tLR: 2.735806\n",
      "Training Epoch: 28 [18176/50000]\tLoss: 4.3241\tLR: 2.736061\n",
      "Training Epoch: 28 [18304/50000]\tLoss: 4.5792\tLR: 2.736317\n",
      "Training Epoch: 28 [18432/50000]\tLoss: 4.4461\tLR: 2.736573\n",
      "Training Epoch: 28 [18560/50000]\tLoss: 4.4066\tLR: 2.736829\n",
      "Training Epoch: 28 [18688/50000]\tLoss: 4.3654\tLR: 2.737084\n",
      "Training Epoch: 28 [18816/50000]\tLoss: 4.3364\tLR: 2.737340\n",
      "Training Epoch: 28 [18944/50000]\tLoss: 4.4430\tLR: 2.737596\n",
      "Training Epoch: 28 [19072/50000]\tLoss: 4.4248\tLR: 2.737852\n",
      "Training Epoch: 28 [19200/50000]\tLoss: 4.5022\tLR: 2.738107\n",
      "Training Epoch: 28 [19328/50000]\tLoss: 4.4368\tLR: 2.738363\n",
      "Training Epoch: 28 [19456/50000]\tLoss: 4.6177\tLR: 2.738619\n",
      "Training Epoch: 28 [19584/50000]\tLoss: 4.4783\tLR: 2.738875\n",
      "Training Epoch: 28 [19712/50000]\tLoss: 4.3760\tLR: 2.739130\n",
      "Training Epoch: 28 [19840/50000]\tLoss: 4.5620\tLR: 2.739386\n",
      "Training Epoch: 28 [19968/50000]\tLoss: 4.4346\tLR: 2.739642\n",
      "Training Epoch: 28 [20096/50000]\tLoss: 4.3185\tLR: 2.739898\n",
      "Training Epoch: 28 [20224/50000]\tLoss: 4.3916\tLR: 2.740153\n",
      "Training Epoch: 28 [20352/50000]\tLoss: 4.4509\tLR: 2.740409\n",
      "Training Epoch: 28 [20480/50000]\tLoss: 4.3133\tLR: 2.740665\n",
      "Training Epoch: 28 [20608/50000]\tLoss: 4.3036\tLR: 2.740921\n",
      "Training Epoch: 28 [20736/50000]\tLoss: 4.4659\tLR: 2.741176\n",
      "Training Epoch: 28 [20864/50000]\tLoss: 4.3909\tLR: 2.741432\n",
      "Training Epoch: 28 [20992/50000]\tLoss: 4.3519\tLR: 2.741688\n",
      "Training Epoch: 28 [21120/50000]\tLoss: 4.3105\tLR: 2.741944\n",
      "Training Epoch: 28 [21248/50000]\tLoss: 4.2905\tLR: 2.742199\n",
      "Training Epoch: 28 [21376/50000]\tLoss: 4.3207\tLR: 2.742455\n",
      "Training Epoch: 28 [21504/50000]\tLoss: 4.3959\tLR: 2.742711\n",
      "Training Epoch: 28 [21632/50000]\tLoss: 4.3055\tLR: 2.742967\n",
      "Training Epoch: 28 [21760/50000]\tLoss: 4.2737\tLR: 2.743223\n",
      "Training Epoch: 28 [21888/50000]\tLoss: 4.2236\tLR: 2.743478\n",
      "Training Epoch: 28 [22016/50000]\tLoss: 4.3350\tLR: 2.743734\n",
      "Training Epoch: 28 [22144/50000]\tLoss: 4.3791\tLR: 2.743990\n",
      "Training Epoch: 28 [22272/50000]\tLoss: 4.4090\tLR: 2.744246\n",
      "Training Epoch: 28 [22400/50000]\tLoss: 4.2877\tLR: 2.744501\n",
      "Training Epoch: 28 [22528/50000]\tLoss: 4.4006\tLR: 2.744757\n",
      "Training Epoch: 28 [22656/50000]\tLoss: 4.3782\tLR: 2.745013\n",
      "Training Epoch: 28 [22784/50000]\tLoss: 4.3649\tLR: 2.745269\n",
      "Training Epoch: 28 [22912/50000]\tLoss: 4.3379\tLR: 2.745524\n",
      "Training Epoch: 28 [23040/50000]\tLoss: 4.4163\tLR: 2.745780\n",
      "Training Epoch: 28 [23168/50000]\tLoss: 4.4858\tLR: 2.746036\n",
      "Training Epoch: 28 [23296/50000]\tLoss: 4.3070\tLR: 2.746292\n",
      "Training Epoch: 28 [23424/50000]\tLoss: 4.2402\tLR: 2.746547\n",
      "Training Epoch: 28 [23552/50000]\tLoss: 4.3284\tLR: 2.746803\n",
      "Training Epoch: 28 [23680/50000]\tLoss: 4.3363\tLR: 2.747059\n",
      "Training Epoch: 28 [23808/50000]\tLoss: 4.3674\tLR: 2.747315\n",
      "Training Epoch: 28 [23936/50000]\tLoss: 4.2793\tLR: 2.747570\n",
      "Training Epoch: 28 [24064/50000]\tLoss: 4.4484\tLR: 2.747826\n",
      "Training Epoch: 28 [24192/50000]\tLoss: 4.3405\tLR: 2.748082\n",
      "Training Epoch: 28 [24320/50000]\tLoss: 4.2886\tLR: 2.748338\n",
      "Training Epoch: 28 [24448/50000]\tLoss: 4.3774\tLR: 2.748593\n",
      "Training Epoch: 28 [24576/50000]\tLoss: 4.3110\tLR: 2.748849\n",
      "Training Epoch: 28 [24704/50000]\tLoss: 4.3977\tLR: 2.749105\n",
      "Training Epoch: 28 [24832/50000]\tLoss: 4.3443\tLR: 2.749361\n",
      "Training Epoch: 28 [24960/50000]\tLoss: 4.4769\tLR: 2.749616\n",
      "Training Epoch: 28 [25088/50000]\tLoss: 4.2313\tLR: 2.749872\n",
      "Training Epoch: 28 [25216/50000]\tLoss: 4.2907\tLR: 2.750128\n",
      "Training Epoch: 28 [25344/50000]\tLoss: 4.2623\tLR: 2.750384\n",
      "Training Epoch: 28 [25472/50000]\tLoss: 4.2432\tLR: 2.750639\n",
      "Training Epoch: 28 [25600/50000]\tLoss: 4.1979\tLR: 2.750895\n",
      "Training Epoch: 28 [25728/50000]\tLoss: 4.2396\tLR: 2.751151\n",
      "Training Epoch: 28 [25856/50000]\tLoss: 4.4359\tLR: 2.751407\n",
      "Training Epoch: 28 [25984/50000]\tLoss: 4.4046\tLR: 2.751662\n",
      "Training Epoch: 28 [26112/50000]\tLoss: 4.3525\tLR: 2.751918\n",
      "Training Epoch: 28 [26240/50000]\tLoss: 4.4324\tLR: 2.752174\n",
      "Training Epoch: 28 [26368/50000]\tLoss: 4.2756\tLR: 2.752430\n",
      "Training Epoch: 28 [26496/50000]\tLoss: 4.3637\tLR: 2.752685\n",
      "Training Epoch: 28 [26624/50000]\tLoss: 4.2586\tLR: 2.752941\n",
      "Training Epoch: 28 [26752/50000]\tLoss: 4.1255\tLR: 2.753197\n",
      "Training Epoch: 28 [26880/50000]\tLoss: 4.4268\tLR: 2.753453\n",
      "Training Epoch: 28 [27008/50000]\tLoss: 4.3748\tLR: 2.753708\n",
      "Training Epoch: 28 [27136/50000]\tLoss: 4.4504\tLR: 2.753964\n",
      "Training Epoch: 28 [27264/50000]\tLoss: 4.3166\tLR: 2.754220\n",
      "Training Epoch: 28 [27392/50000]\tLoss: 4.2947\tLR: 2.754476\n",
      "Training Epoch: 28 [27520/50000]\tLoss: 4.4151\tLR: 2.754731\n",
      "Training Epoch: 28 [27648/50000]\tLoss: 4.1986\tLR: 2.754987\n",
      "Training Epoch: 28 [27776/50000]\tLoss: 4.3134\tLR: 2.755243\n",
      "Training Epoch: 28 [27904/50000]\tLoss: 4.2026\tLR: 2.755499\n",
      "Training Epoch: 28 [28032/50000]\tLoss: 4.2594\tLR: 2.755754\n",
      "Training Epoch: 28 [28160/50000]\tLoss: 4.4406\tLR: 2.756010\n",
      "Training Epoch: 28 [28288/50000]\tLoss: 4.2699\tLR: 2.756266\n",
      "Training Epoch: 28 [28416/50000]\tLoss: 4.2701\tLR: 2.756522\n",
      "Training Epoch: 28 [28544/50000]\tLoss: 4.3095\tLR: 2.756777\n",
      "Training Epoch: 28 [28672/50000]\tLoss: 4.3888\tLR: 2.757033\n",
      "Training Epoch: 28 [28800/50000]\tLoss: 4.3281\tLR: 2.757289\n",
      "Training Epoch: 28 [28928/50000]\tLoss: 4.4051\tLR: 2.757545\n",
      "Training Epoch: 28 [29056/50000]\tLoss: 4.3579\tLR: 2.757801\n",
      "Training Epoch: 28 [29184/50000]\tLoss: 4.3646\tLR: 2.758056\n",
      "Training Epoch: 28 [29312/50000]\tLoss: 4.2798\tLR: 2.758312\n",
      "Training Epoch: 28 [29440/50000]\tLoss: 4.3860\tLR: 2.758568\n",
      "Training Epoch: 28 [29568/50000]\tLoss: 4.3643\tLR: 2.758824\n",
      "Training Epoch: 28 [29696/50000]\tLoss: 4.3438\tLR: 2.759079\n",
      "Training Epoch: 28 [29824/50000]\tLoss: 4.3324\tLR: 2.759335\n",
      "Training Epoch: 28 [29952/50000]\tLoss: 4.3878\tLR: 2.759591\n",
      "Training Epoch: 28 [30080/50000]\tLoss: 4.3140\tLR: 2.759847\n",
      "Training Epoch: 28 [30208/50000]\tLoss: 4.2776\tLR: 2.760102\n",
      "Training Epoch: 28 [30336/50000]\tLoss: 4.5403\tLR: 2.760358\n",
      "Training Epoch: 28 [30464/50000]\tLoss: 4.3451\tLR: 2.760614\n",
      "Training Epoch: 28 [30592/50000]\tLoss: 4.3872\tLR: 2.760870\n",
      "Training Epoch: 28 [30720/50000]\tLoss: 4.4090\tLR: 2.761125\n",
      "Training Epoch: 28 [30848/50000]\tLoss: 4.3305\tLR: 2.761381\n",
      "Training Epoch: 28 [30976/50000]\tLoss: 4.3563\tLR: 2.761637\n",
      "Training Epoch: 28 [31104/50000]\tLoss: 4.2955\tLR: 2.761893\n",
      "Training Epoch: 28 [31232/50000]\tLoss: 4.3337\tLR: 2.762148\n",
      "Training Epoch: 28 [31360/50000]\tLoss: 4.2857\tLR: 2.762404\n",
      "Training Epoch: 28 [31488/50000]\tLoss: 4.4406\tLR: 2.762660\n",
      "Training Epoch: 28 [31616/50000]\tLoss: 4.4478\tLR: 2.762916\n",
      "Training Epoch: 28 [31744/50000]\tLoss: 4.3661\tLR: 2.763171\n",
      "Training Epoch: 28 [31872/50000]\tLoss: 4.2898\tLR: 2.763427\n",
      "Training Epoch: 28 [32000/50000]\tLoss: 4.2932\tLR: 2.763683\n",
      "Training Epoch: 28 [32128/50000]\tLoss: 4.3332\tLR: 2.763939\n",
      "Training Epoch: 28 [32256/50000]\tLoss: 4.3586\tLR: 2.764194\n",
      "Training Epoch: 28 [32384/50000]\tLoss: 4.3039\tLR: 2.764450\n",
      "Training Epoch: 28 [32512/50000]\tLoss: 4.3871\tLR: 2.764706\n",
      "Training Epoch: 28 [32640/50000]\tLoss: 4.3386\tLR: 2.764962\n",
      "Training Epoch: 28 [32768/50000]\tLoss: 4.2608\tLR: 2.765217\n",
      "Training Epoch: 28 [32896/50000]\tLoss: 4.2012\tLR: 2.765473\n",
      "Training Epoch: 28 [33024/50000]\tLoss: 4.4565\tLR: 2.765729\n",
      "Training Epoch: 28 [33152/50000]\tLoss: 4.1571\tLR: 2.765985\n",
      "Training Epoch: 28 [33280/50000]\tLoss: 4.4235\tLR: 2.766240\n",
      "Training Epoch: 28 [33408/50000]\tLoss: 4.3220\tLR: 2.766496\n",
      "Training Epoch: 28 [33536/50000]\tLoss: 4.3706\tLR: 2.766752\n",
      "Training Epoch: 28 [33664/50000]\tLoss: 4.3626\tLR: 2.767008\n",
      "Training Epoch: 28 [33792/50000]\tLoss: 4.3045\tLR: 2.767263\n",
      "Training Epoch: 28 [33920/50000]\tLoss: 4.3617\tLR: 2.767519\n",
      "Training Epoch: 28 [34048/50000]\tLoss: 4.3591\tLR: 2.767775\n",
      "Training Epoch: 28 [34176/50000]\tLoss: 4.4367\tLR: 2.768031\n",
      "Training Epoch: 28 [34304/50000]\tLoss: 4.3874\tLR: 2.768286\n",
      "Training Epoch: 28 [34432/50000]\tLoss: 4.3273\tLR: 2.768542\n",
      "Training Epoch: 28 [34560/50000]\tLoss: 4.2845\tLR: 2.768798\n",
      "Training Epoch: 28 [34688/50000]\tLoss: 4.3140\tLR: 2.769054\n",
      "Training Epoch: 28 [34816/50000]\tLoss: 4.5305\tLR: 2.769309\n",
      "Training Epoch: 28 [34944/50000]\tLoss: 4.3190\tLR: 2.769565\n",
      "Training Epoch: 28 [35072/50000]\tLoss: 4.2611\tLR: 2.769821\n",
      "Training Epoch: 28 [35200/50000]\tLoss: 4.3342\tLR: 2.770077\n",
      "Training Epoch: 28 [35328/50000]\tLoss: 4.2851\tLR: 2.770332\n",
      "Training Epoch: 28 [35456/50000]\tLoss: 4.5484\tLR: 2.770588\n",
      "Training Epoch: 28 [35584/50000]\tLoss: 4.2380\tLR: 2.770844\n",
      "Training Epoch: 28 [35712/50000]\tLoss: 4.4279\tLR: 2.771100\n",
      "Training Epoch: 28 [35840/50000]\tLoss: 4.2490\tLR: 2.771355\n",
      "Training Epoch: 28 [35968/50000]\tLoss: 4.4824\tLR: 2.771611\n",
      "Training Epoch: 28 [36096/50000]\tLoss: 4.4183\tLR: 2.771867\n",
      "Training Epoch: 28 [36224/50000]\tLoss: 4.3379\tLR: 2.772123\n",
      "Training Epoch: 28 [36352/50000]\tLoss: 4.4201\tLR: 2.772379\n",
      "Training Epoch: 28 [36480/50000]\tLoss: 4.3284\tLR: 2.772634\n",
      "Training Epoch: 28 [36608/50000]\tLoss: 4.4643\tLR: 2.772890\n",
      "Training Epoch: 28 [36736/50000]\tLoss: 4.2592\tLR: 2.773146\n",
      "Training Epoch: 28 [36864/50000]\tLoss: 4.3096\tLR: 2.773402\n",
      "Training Epoch: 28 [36992/50000]\tLoss: 4.1980\tLR: 2.773657\n",
      "Training Epoch: 28 [37120/50000]\tLoss: 4.3154\tLR: 2.773913\n",
      "Training Epoch: 28 [37248/50000]\tLoss: 4.1285\tLR: 2.774169\n",
      "Training Epoch: 28 [37376/50000]\tLoss: 4.4182\tLR: 2.774425\n",
      "Training Epoch: 28 [37504/50000]\tLoss: 4.1885\tLR: 2.774680\n",
      "Training Epoch: 28 [37632/50000]\tLoss: 4.1822\tLR: 2.774936\n",
      "Training Epoch: 28 [37760/50000]\tLoss: 4.5127\tLR: 2.775192\n",
      "Training Epoch: 28 [37888/50000]\tLoss: 4.2793\tLR: 2.775448\n",
      "Training Epoch: 28 [38016/50000]\tLoss: 4.2859\tLR: 2.775703\n",
      "Training Epoch: 28 [38144/50000]\tLoss: 4.3438\tLR: 2.775959\n",
      "Training Epoch: 28 [38272/50000]\tLoss: 4.3990\tLR: 2.776215\n",
      "Training Epoch: 28 [38400/50000]\tLoss: 4.4533\tLR: 2.776471\n",
      "Training Epoch: 28 [38528/50000]\tLoss: 4.3332\tLR: 2.776726\n",
      "Training Epoch: 28 [38656/50000]\tLoss: 4.4128\tLR: 2.776982\n",
      "Training Epoch: 28 [38784/50000]\tLoss: 4.3290\tLR: 2.777238\n",
      "Training Epoch: 28 [38912/50000]\tLoss: 4.2364\tLR: 2.777494\n",
      "Training Epoch: 28 [39040/50000]\tLoss: 4.2714\tLR: 2.777749\n",
      "Training Epoch: 28 [39168/50000]\tLoss: 4.2893\tLR: 2.778005\n",
      "Training Epoch: 28 [39296/50000]\tLoss: 4.5075\tLR: 2.778261\n",
      "Training Epoch: 28 [39424/50000]\tLoss: 4.3774\tLR: 2.778517\n",
      "Training Epoch: 28 [39552/50000]\tLoss: 4.2498\tLR: 2.778772\n",
      "Training Epoch: 28 [39680/50000]\tLoss: 4.3374\tLR: 2.779028\n",
      "Training Epoch: 28 [39808/50000]\tLoss: 4.0991\tLR: 2.779284\n",
      "Training Epoch: 28 [39936/50000]\tLoss: 4.3148\tLR: 2.779540\n",
      "Training Epoch: 28 [40064/50000]\tLoss: 4.4845\tLR: 2.779795\n",
      "Training Epoch: 28 [40192/50000]\tLoss: 4.3432\tLR: 2.780051\n",
      "Training Epoch: 28 [40320/50000]\tLoss: 4.4512\tLR: 2.780307\n",
      "Training Epoch: 28 [40448/50000]\tLoss: 4.2009\tLR: 2.780563\n",
      "Training Epoch: 28 [40576/50000]\tLoss: 4.3768\tLR: 2.780818\n",
      "Training Epoch: 28 [40704/50000]\tLoss: 4.2911\tLR: 2.781074\n",
      "Training Epoch: 28 [40832/50000]\tLoss: 4.3882\tLR: 2.781330\n",
      "Training Epoch: 28 [40960/50000]\tLoss: 4.2421\tLR: 2.781586\n",
      "Training Epoch: 28 [41088/50000]\tLoss: 4.3231\tLR: 2.781841\n",
      "Training Epoch: 28 [41216/50000]\tLoss: 4.4833\tLR: 2.782097\n",
      "Training Epoch: 28 [41344/50000]\tLoss: 4.3436\tLR: 2.782353\n",
      "Training Epoch: 28 [41472/50000]\tLoss: 4.3181\tLR: 2.782609\n",
      "Training Epoch: 28 [41600/50000]\tLoss: 4.4716\tLR: 2.782864\n",
      "Training Epoch: 28 [41728/50000]\tLoss: 4.2440\tLR: 2.783120\n",
      "Training Epoch: 28 [41856/50000]\tLoss: 4.2665\tLR: 2.783376\n",
      "Training Epoch: 28 [41984/50000]\tLoss: 4.3840\tLR: 2.783632\n",
      "Training Epoch: 28 [42112/50000]\tLoss: 4.4654\tLR: 2.783887\n",
      "Training Epoch: 28 [42240/50000]\tLoss: 4.3431\tLR: 2.784143\n",
      "Training Epoch: 28 [42368/50000]\tLoss: 4.5200\tLR: 2.784399\n",
      "Training Epoch: 28 [42496/50000]\tLoss: 4.3856\tLR: 2.784655\n",
      "Training Epoch: 28 [42624/50000]\tLoss: 4.4930\tLR: 2.784910\n",
      "Training Epoch: 28 [42752/50000]\tLoss: 4.3287\tLR: 2.785166\n",
      "Training Epoch: 28 [42880/50000]\tLoss: 4.5333\tLR: 2.785422\n",
      "Training Epoch: 28 [43008/50000]\tLoss: 4.2996\tLR: 2.785678\n",
      "Training Epoch: 28 [43136/50000]\tLoss: 4.3787\tLR: 2.785934\n",
      "Training Epoch: 28 [43264/50000]\tLoss: 4.3722\tLR: 2.786189\n",
      "Training Epoch: 28 [43392/50000]\tLoss: 4.2910\tLR: 2.786445\n",
      "Training Epoch: 28 [43520/50000]\tLoss: 4.2531\tLR: 2.786701\n",
      "Training Epoch: 28 [43648/50000]\tLoss: 4.4203\tLR: 2.786957\n",
      "Training Epoch: 28 [43776/50000]\tLoss: 4.1987\tLR: 2.787212\n",
      "Training Epoch: 28 [43904/50000]\tLoss: 4.3749\tLR: 2.787468\n",
      "Training Epoch: 28 [44032/50000]\tLoss: 4.2585\tLR: 2.787724\n",
      "Training Epoch: 28 [44160/50000]\tLoss: 4.3122\tLR: 2.787980\n",
      "Training Epoch: 28 [44288/50000]\tLoss: 4.4964\tLR: 2.788235\n",
      "Training Epoch: 28 [44416/50000]\tLoss: 4.2561\tLR: 2.788491\n",
      "Training Epoch: 28 [44544/50000]\tLoss: 4.2272\tLR: 2.788747\n",
      "Training Epoch: 28 [44672/50000]\tLoss: 4.4739\tLR: 2.789003\n",
      "Training Epoch: 28 [44800/50000]\tLoss: 4.2213\tLR: 2.789258\n",
      "Training Epoch: 28 [44928/50000]\tLoss: 4.2431\tLR: 2.789514\n",
      "Training Epoch: 28 [45056/50000]\tLoss: 4.4476\tLR: 2.789770\n",
      "Training Epoch: 28 [45184/50000]\tLoss: 4.2520\tLR: 2.790026\n",
      "Training Epoch: 28 [45312/50000]\tLoss: 4.3064\tLR: 2.790281\n",
      "Training Epoch: 28 [45440/50000]\tLoss: 4.3751\tLR: 2.790537\n",
      "Training Epoch: 28 [45568/50000]\tLoss: 4.3827\tLR: 2.790793\n",
      "Training Epoch: 28 [45696/50000]\tLoss: 4.3549\tLR: 2.791049\n",
      "Training Epoch: 28 [45824/50000]\tLoss: 4.4452\tLR: 2.791304\n",
      "Training Epoch: 28 [45952/50000]\tLoss: 4.3454\tLR: 2.791560\n",
      "Training Epoch: 28 [46080/50000]\tLoss: 4.4431\tLR: 2.791816\n",
      "Training Epoch: 28 [46208/50000]\tLoss: 4.3309\tLR: 2.792072\n",
      "Training Epoch: 28 [46336/50000]\tLoss: 4.4411\tLR: 2.792327\n",
      "Training Epoch: 28 [46464/50000]\tLoss: 4.3871\tLR: 2.792583\n",
      "Training Epoch: 28 [46592/50000]\tLoss: 4.4426\tLR: 2.792839\n",
      "Training Epoch: 28 [46720/50000]\tLoss: 4.2992\tLR: 2.793095\n",
      "Training Epoch: 28 [46848/50000]\tLoss: 4.2701\tLR: 2.793350\n",
      "Training Epoch: 28 [46976/50000]\tLoss: 4.4231\tLR: 2.793606\n",
      "Training Epoch: 28 [47104/50000]\tLoss: 4.2354\tLR: 2.793862\n",
      "Training Epoch: 28 [47232/50000]\tLoss: 4.2832\tLR: 2.794118\n",
      "Training Epoch: 28 [47360/50000]\tLoss: 4.6117\tLR: 2.794373\n",
      "Training Epoch: 28 [47488/50000]\tLoss: 4.2278\tLR: 2.794629\n",
      "Training Epoch: 28 [47616/50000]\tLoss: 4.3293\tLR: 2.794885\n",
      "Training Epoch: 28 [47744/50000]\tLoss: 4.3611\tLR: 2.795141\n",
      "Training Epoch: 28 [47872/50000]\tLoss: 4.3226\tLR: 2.795396\n",
      "Training Epoch: 28 [48000/50000]\tLoss: 4.2832\tLR: 2.795652\n",
      "Training Epoch: 28 [48128/50000]\tLoss: 4.3420\tLR: 2.795908\n",
      "Training Epoch: 28 [48256/50000]\tLoss: 4.4202\tLR: 2.796164\n",
      "Training Epoch: 28 [48384/50000]\tLoss: 4.4703\tLR: 2.796419\n",
      "Training Epoch: 28 [48512/50000]\tLoss: 4.3939\tLR: 2.796675\n",
      "Training Epoch: 28 [48640/50000]\tLoss: 4.3077\tLR: 2.796931\n",
      "Training Epoch: 28 [48768/50000]\tLoss: 4.3119\tLR: 2.797187\n",
      "Training Epoch: 28 [48896/50000]\tLoss: 4.3038\tLR: 2.797442\n",
      "Training Epoch: 28 [49024/50000]\tLoss: 4.3686\tLR: 2.797698\n",
      "Training Epoch: 28 [49152/50000]\tLoss: 4.1375\tLR: 2.797954\n",
      "Training Epoch: 28 [49280/50000]\tLoss: 4.4341\tLR: 2.798210\n",
      "Training Epoch: 28 [49408/50000]\tLoss: 4.2046\tLR: 2.798465\n",
      "Training Epoch: 28 [49536/50000]\tLoss: 4.2672\tLR: 2.798721\n",
      "Training Epoch: 28 [49664/50000]\tLoss: 4.2800\tLR: 2.798977\n",
      "Training Epoch: 28 [49792/50000]\tLoss: 4.4952\tLR: 2.799233\n",
      "Training Epoch: 28 [49920/50000]\tLoss: 4.1673\tLR: 2.799488\n",
      "Training Epoch: 28 [50000/50000]\tLoss: 4.4002\tLR: 2.799744\n",
      "epoch 28 training time consumed: 488.88s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   39255 GB |   39255 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   39134 GB |   39134 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     120 GB |     120 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   39255 GB |   39255 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   39134 GB |   39134 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     120 GB |     120 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   38701 GB |   38701 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   38580 GB |   38580 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     120 GB |     120 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    4162 K  |    4162 K  |\n",
      "|       from large pool |      24    |      65    |    1774 K  |    1774 K  |\n",
      "|       from small pool |     231    |     274    |    2388 K  |    2388 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    4162 K  |    4162 K  |\n",
      "|       from large pool |      24    |      65    |    1774 K  |    1774 K  |\n",
      "|       from small pool |     231    |     274    |    2388 K  |    2388 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      46    |    2412 K  |    2411 K  |\n",
      "|       from large pool |      10    |      23    |     852 K  |     852 K  |\n",
      "|       from small pool |      26    |      35    |    1559 K  |    1559 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 28, Average loss: 0.0360, Accuracy: 0.0370, Time consumed:31.01s\n",
      "\n",
      "Training Epoch: 29 [128/50000]\tLoss: 4.2760\tLR: 0.100000\n",
      "Training Epoch: 29 [256/50000]\tLoss: 4.3804\tLR: 2.800256\n",
      "Training Epoch: 29 [384/50000]\tLoss: 4.1545\tLR: 2.800512\n",
      "Training Epoch: 29 [512/50000]\tLoss: 4.2354\tLR: 2.800767\n",
      "Training Epoch: 29 [640/50000]\tLoss: 4.3753\tLR: 2.801023\n",
      "Training Epoch: 29 [768/50000]\tLoss: 4.4333\tLR: 2.801279\n",
      "Training Epoch: 29 [896/50000]\tLoss: 4.4314\tLR: 2.801535\n",
      "Training Epoch: 29 [1024/50000]\tLoss: 4.3850\tLR: 2.801790\n",
      "Training Epoch: 29 [1152/50000]\tLoss: 4.2889\tLR: 2.802046\n",
      "Training Epoch: 29 [1280/50000]\tLoss: 4.3036\tLR: 2.802302\n",
      "Training Epoch: 29 [1408/50000]\tLoss: 4.3340\tLR: 2.802558\n",
      "Training Epoch: 29 [1536/50000]\tLoss: 4.5368\tLR: 2.802813\n",
      "Training Epoch: 29 [1664/50000]\tLoss: 4.5240\tLR: 2.803069\n",
      "Training Epoch: 29 [1792/50000]\tLoss: 4.3186\tLR: 2.803325\n",
      "Training Epoch: 29 [1920/50000]\tLoss: 4.3737\tLR: 2.803581\n",
      "Training Epoch: 29 [2048/50000]\tLoss: 4.5206\tLR: 2.803836\n",
      "Training Epoch: 29 [2176/50000]\tLoss: 4.2874\tLR: 2.804092\n",
      "Training Epoch: 29 [2304/50000]\tLoss: 4.4427\tLR: 2.804348\n",
      "Training Epoch: 29 [2432/50000]\tLoss: 4.3243\tLR: 2.804604\n",
      "Training Epoch: 29 [2560/50000]\tLoss: 4.3096\tLR: 2.804859\n",
      "Training Epoch: 29 [2688/50000]\tLoss: 4.3446\tLR: 2.805115\n",
      "Training Epoch: 29 [2816/50000]\tLoss: 4.2084\tLR: 2.805371\n",
      "Training Epoch: 29 [2944/50000]\tLoss: 4.3583\tLR: 2.805627\n",
      "Training Epoch: 29 [3072/50000]\tLoss: 4.4045\tLR: 2.805882\n",
      "Training Epoch: 29 [3200/50000]\tLoss: 4.2875\tLR: 2.806138\n",
      "Training Epoch: 29 [3328/50000]\tLoss: 4.3410\tLR: 2.806394\n",
      "Training Epoch: 29 [3456/50000]\tLoss: 4.1753\tLR: 2.806650\n",
      "Training Epoch: 29 [3584/50000]\tLoss: 4.2610\tLR: 2.806905\n",
      "Training Epoch: 29 [3712/50000]\tLoss: 4.3925\tLR: 2.807161\n",
      "Training Epoch: 29 [3840/50000]\tLoss: 4.3492\tLR: 2.807417\n",
      "Training Epoch: 29 [3968/50000]\tLoss: 4.4147\tLR: 2.807673\n",
      "Training Epoch: 29 [4096/50000]\tLoss: 4.2681\tLR: 2.807928\n",
      "Training Epoch: 29 [4224/50000]\tLoss: 4.2548\tLR: 2.808184\n",
      "Training Epoch: 29 [4352/50000]\tLoss: 4.4882\tLR: 2.808440\n",
      "Training Epoch: 29 [4480/50000]\tLoss: 4.3501\tLR: 2.808696\n",
      "Training Epoch: 29 [4608/50000]\tLoss: 4.2805\tLR: 2.808951\n",
      "Training Epoch: 29 [4736/50000]\tLoss: 4.3179\tLR: 2.809207\n",
      "Training Epoch: 29 [4864/50000]\tLoss: 4.3978\tLR: 2.809463\n",
      "Training Epoch: 29 [4992/50000]\tLoss: 4.3558\tLR: 2.809719\n",
      "Training Epoch: 29 [5120/50000]\tLoss: 4.3297\tLR: 2.809974\n",
      "Training Epoch: 29 [5248/50000]\tLoss: 4.3964\tLR: 2.810230\n",
      "Training Epoch: 29 [5376/50000]\tLoss: 4.3903\tLR: 2.810486\n",
      "Training Epoch: 29 [5504/50000]\tLoss: 4.2795\tLR: 2.810742\n",
      "Training Epoch: 29 [5632/50000]\tLoss: 4.4433\tLR: 2.810997\n",
      "Training Epoch: 29 [5760/50000]\tLoss: 4.3196\tLR: 2.811253\n",
      "Training Epoch: 29 [5888/50000]\tLoss: 4.3916\tLR: 2.811509\n",
      "Training Epoch: 29 [6016/50000]\tLoss: 4.1961\tLR: 2.811765\n",
      "Training Epoch: 29 [6144/50000]\tLoss: 4.3054\tLR: 2.812020\n",
      "Training Epoch: 29 [6272/50000]\tLoss: 4.3702\tLR: 2.812276\n",
      "Training Epoch: 29 [6400/50000]\tLoss: 4.3219\tLR: 2.812532\n",
      "Training Epoch: 29 [6528/50000]\tLoss: 4.2514\tLR: 2.812788\n",
      "Training Epoch: 29 [6656/50000]\tLoss: 4.3904\tLR: 2.813043\n",
      "Training Epoch: 29 [6784/50000]\tLoss: 4.3433\tLR: 2.813299\n",
      "Training Epoch: 29 [6912/50000]\tLoss: 4.3847\tLR: 2.813555\n",
      "Training Epoch: 29 [7040/50000]\tLoss: 4.4145\tLR: 2.813811\n",
      "Training Epoch: 29 [7168/50000]\tLoss: 4.3655\tLR: 2.814066\n",
      "Training Epoch: 29 [7296/50000]\tLoss: 4.3088\tLR: 2.814322\n",
      "Training Epoch: 29 [7424/50000]\tLoss: 4.4849\tLR: 2.814578\n",
      "Training Epoch: 29 [7552/50000]\tLoss: 4.3030\tLR: 2.814834\n",
      "Training Epoch: 29 [7680/50000]\tLoss: 4.3423\tLR: 2.815090\n",
      "Training Epoch: 29 [7808/50000]\tLoss: 4.4219\tLR: 2.815345\n",
      "Training Epoch: 29 [7936/50000]\tLoss: 4.4099\tLR: 2.815601\n",
      "Training Epoch: 29 [8064/50000]\tLoss: 4.2761\tLR: 2.815857\n",
      "Training Epoch: 29 [8192/50000]\tLoss: 4.3795\tLR: 2.816113\n",
      "Training Epoch: 29 [8320/50000]\tLoss: 4.4518\tLR: 2.816368\n",
      "Training Epoch: 29 [8448/50000]\tLoss: 4.4052\tLR: 2.816624\n",
      "Training Epoch: 29 [8576/50000]\tLoss: 4.3716\tLR: 2.816880\n",
      "Training Epoch: 29 [8704/50000]\tLoss: 4.2380\tLR: 2.817136\n",
      "Training Epoch: 29 [8832/50000]\tLoss: 4.5150\tLR: 2.817391\n",
      "Training Epoch: 29 [8960/50000]\tLoss: 4.5211\tLR: 2.817647\n",
      "Training Epoch: 29 [9088/50000]\tLoss: 4.2899\tLR: 2.817903\n",
      "Training Epoch: 29 [9216/50000]\tLoss: 4.3793\tLR: 2.818159\n",
      "Training Epoch: 29 [9344/50000]\tLoss: 4.3057\tLR: 2.818414\n",
      "Training Epoch: 29 [9472/50000]\tLoss: 4.4225\tLR: 2.818670\n",
      "Training Epoch: 29 [9600/50000]\tLoss: 4.4813\tLR: 2.818926\n",
      "Training Epoch: 29 [9728/50000]\tLoss: 4.4065\tLR: 2.819182\n",
      "Training Epoch: 29 [9856/50000]\tLoss: 4.3330\tLR: 2.819437\n",
      "Training Epoch: 29 [9984/50000]\tLoss: 4.4038\tLR: 2.819693\n",
      "Training Epoch: 29 [10112/50000]\tLoss: 4.2948\tLR: 2.819949\n",
      "Training Epoch: 29 [10240/50000]\tLoss: 4.4085\tLR: 2.820205\n",
      "Training Epoch: 29 [10368/50000]\tLoss: 4.4099\tLR: 2.820460\n",
      "Training Epoch: 29 [10496/50000]\tLoss: 4.4426\tLR: 2.820716\n",
      "Training Epoch: 29 [10624/50000]\tLoss: 4.3857\tLR: 2.820972\n",
      "Training Epoch: 29 [10752/50000]\tLoss: 4.4166\tLR: 2.821228\n",
      "Training Epoch: 29 [10880/50000]\tLoss: 4.3114\tLR: 2.821483\n",
      "Training Epoch: 29 [11008/50000]\tLoss: 4.3814\tLR: 2.821739\n",
      "Training Epoch: 29 [11136/50000]\tLoss: 4.2735\tLR: 2.821995\n",
      "Training Epoch: 29 [11264/50000]\tLoss: 4.3326\tLR: 2.822251\n",
      "Training Epoch: 29 [11392/50000]\tLoss: 4.2089\tLR: 2.822506\n",
      "Training Epoch: 29 [11520/50000]\tLoss: 4.2869\tLR: 2.822762\n",
      "Training Epoch: 29 [11648/50000]\tLoss: 4.2816\tLR: 2.823018\n",
      "Training Epoch: 29 [11776/50000]\tLoss: 4.2711\tLR: 2.823274\n",
      "Training Epoch: 29 [11904/50000]\tLoss: 4.2812\tLR: 2.823529\n",
      "Training Epoch: 29 [12032/50000]\tLoss: 4.2869\tLR: 2.823785\n",
      "Training Epoch: 29 [12160/50000]\tLoss: 4.4774\tLR: 2.824041\n",
      "Training Epoch: 29 [12288/50000]\tLoss: 4.4104\tLR: 2.824297\n",
      "Training Epoch: 29 [12416/50000]\tLoss: 4.5118\tLR: 2.824552\n",
      "Training Epoch: 29 [12544/50000]\tLoss: 4.2702\tLR: 2.824808\n",
      "Training Epoch: 29 [12672/50000]\tLoss: 4.4152\tLR: 2.825064\n",
      "Training Epoch: 29 [12800/50000]\tLoss: 4.2713\tLR: 2.825320\n",
      "Training Epoch: 29 [12928/50000]\tLoss: 4.4127\tLR: 2.825575\n",
      "Training Epoch: 29 [13056/50000]\tLoss: 4.4564\tLR: 2.825831\n",
      "Training Epoch: 29 [13184/50000]\tLoss: 4.3621\tLR: 2.826087\n",
      "Training Epoch: 29 [13312/50000]\tLoss: 4.3584\tLR: 2.826343\n",
      "Training Epoch: 29 [13440/50000]\tLoss: 4.2892\tLR: 2.826598\n",
      "Training Epoch: 29 [13568/50000]\tLoss: 4.2714\tLR: 2.826854\n",
      "Training Epoch: 29 [13696/50000]\tLoss: 4.3744\tLR: 2.827110\n",
      "Training Epoch: 29 [13824/50000]\tLoss: 4.1991\tLR: 2.827366\n",
      "Training Epoch: 29 [13952/50000]\tLoss: 4.2574\tLR: 2.827621\n",
      "Training Epoch: 29 [14080/50000]\tLoss: 4.1330\tLR: 2.827877\n",
      "Training Epoch: 29 [14208/50000]\tLoss: 4.1382\tLR: 2.828133\n",
      "Training Epoch: 29 [14336/50000]\tLoss: 4.2177\tLR: 2.828389\n",
      "Training Epoch: 29 [14464/50000]\tLoss: 4.4184\tLR: 2.828645\n",
      "Training Epoch: 29 [14592/50000]\tLoss: 4.2101\tLR: 2.828900\n",
      "Training Epoch: 29 [14720/50000]\tLoss: 4.3659\tLR: 2.829156\n",
      "Training Epoch: 29 [14848/50000]\tLoss: 4.4742\tLR: 2.829412\n",
      "Training Epoch: 29 [14976/50000]\tLoss: 4.4573\tLR: 2.829668\n",
      "Training Epoch: 29 [15104/50000]\tLoss: 4.4197\tLR: 2.829923\n",
      "Training Epoch: 29 [15232/50000]\tLoss: 4.4104\tLR: 2.830179\n",
      "Training Epoch: 29 [15360/50000]\tLoss: 4.3189\tLR: 2.830435\n",
      "Training Epoch: 29 [15488/50000]\tLoss: 4.3143\tLR: 2.830691\n",
      "Training Epoch: 29 [15616/50000]\tLoss: 4.2105\tLR: 2.830946\n",
      "Training Epoch: 29 [15744/50000]\tLoss: 4.4359\tLR: 2.831202\n",
      "Training Epoch: 29 [15872/50000]\tLoss: 4.3236\tLR: 2.831458\n",
      "Training Epoch: 29 [16000/50000]\tLoss: 4.3016\tLR: 2.831714\n",
      "Training Epoch: 29 [16128/50000]\tLoss: 4.3198\tLR: 2.831969\n",
      "Training Epoch: 29 [16256/50000]\tLoss: 4.3504\tLR: 2.832225\n",
      "Training Epoch: 29 [16384/50000]\tLoss: 4.3472\tLR: 2.832481\n",
      "Training Epoch: 29 [16512/50000]\tLoss: 4.4019\tLR: 2.832737\n",
      "Training Epoch: 29 [16640/50000]\tLoss: 4.4304\tLR: 2.832992\n",
      "Training Epoch: 29 [16768/50000]\tLoss: 4.4733\tLR: 2.833248\n",
      "Training Epoch: 29 [16896/50000]\tLoss: 4.2741\tLR: 2.833504\n",
      "Training Epoch: 29 [17024/50000]\tLoss: 4.3382\tLR: 2.833760\n",
      "Training Epoch: 29 [17152/50000]\tLoss: 4.3935\tLR: 2.834015\n",
      "Training Epoch: 29 [17280/50000]\tLoss: 4.3120\tLR: 2.834271\n",
      "Training Epoch: 29 [17408/50000]\tLoss: 4.3892\tLR: 2.834527\n",
      "Training Epoch: 29 [17536/50000]\tLoss: 4.4187\tLR: 2.834783\n",
      "Training Epoch: 29 [17664/50000]\tLoss: 4.2357\tLR: 2.835038\n",
      "Training Epoch: 29 [17792/50000]\tLoss: 4.3499\tLR: 2.835294\n",
      "Training Epoch: 29 [17920/50000]\tLoss: 4.4024\tLR: 2.835550\n",
      "Training Epoch: 29 [18048/50000]\tLoss: 4.3238\tLR: 2.835806\n",
      "Training Epoch: 29 [18176/50000]\tLoss: 4.3466\tLR: 2.836061\n",
      "Training Epoch: 29 [18304/50000]\tLoss: 4.3206\tLR: 2.836317\n",
      "Training Epoch: 29 [18432/50000]\tLoss: 4.3136\tLR: 2.836573\n",
      "Training Epoch: 29 [18560/50000]\tLoss: 4.5470\tLR: 2.836829\n",
      "Training Epoch: 29 [18688/50000]\tLoss: 4.2406\tLR: 2.837084\n",
      "Training Epoch: 29 [18816/50000]\tLoss: 4.2704\tLR: 2.837340\n",
      "Training Epoch: 29 [18944/50000]\tLoss: 4.3131\tLR: 2.837596\n",
      "Training Epoch: 29 [19072/50000]\tLoss: 4.3997\tLR: 2.837852\n",
      "Training Epoch: 29 [19200/50000]\tLoss: 4.4089\tLR: 2.838107\n",
      "Training Epoch: 29 [19328/50000]\tLoss: 4.4708\tLR: 2.838363\n",
      "Training Epoch: 29 [19456/50000]\tLoss: 4.3317\tLR: 2.838619\n",
      "Training Epoch: 29 [19584/50000]\tLoss: 4.3897\tLR: 2.838875\n",
      "Training Epoch: 29 [19712/50000]\tLoss: 4.5288\tLR: 2.839130\n",
      "Training Epoch: 29 [19840/50000]\tLoss: 4.3775\tLR: 2.839386\n",
      "Training Epoch: 29 [19968/50000]\tLoss: 4.3230\tLR: 2.839642\n",
      "Training Epoch: 29 [20096/50000]\tLoss: 4.4925\tLR: 2.839898\n",
      "Training Epoch: 29 [20224/50000]\tLoss: 4.3845\tLR: 2.840153\n",
      "Training Epoch: 29 [20352/50000]\tLoss: 4.4510\tLR: 2.840409\n",
      "Training Epoch: 29 [20480/50000]\tLoss: 4.4057\tLR: 2.840665\n",
      "Training Epoch: 29 [20608/50000]\tLoss: 4.2347\tLR: 2.840921\n",
      "Training Epoch: 29 [20736/50000]\tLoss: 4.2672\tLR: 2.841176\n",
      "Training Epoch: 29 [20864/50000]\tLoss: 4.3575\tLR: 2.841432\n",
      "Training Epoch: 29 [20992/50000]\tLoss: 4.2280\tLR: 2.841688\n",
      "Training Epoch: 29 [21120/50000]\tLoss: 4.3865\tLR: 2.841944\n",
      "Training Epoch: 29 [21248/50000]\tLoss: 4.3088\tLR: 2.842199\n",
      "Training Epoch: 29 [21376/50000]\tLoss: 4.3737\tLR: 2.842455\n",
      "Training Epoch: 29 [21504/50000]\tLoss: 4.3627\tLR: 2.842711\n",
      "Training Epoch: 29 [21632/50000]\tLoss: 4.3487\tLR: 2.842967\n",
      "Training Epoch: 29 [21760/50000]\tLoss: 4.3592\tLR: 2.843223\n",
      "Training Epoch: 29 [21888/50000]\tLoss: 4.5493\tLR: 2.843478\n",
      "Training Epoch: 29 [22016/50000]\tLoss: 4.3753\tLR: 2.843734\n",
      "Training Epoch: 29 [22144/50000]\tLoss: 4.4020\tLR: 2.843990\n",
      "Training Epoch: 29 [22272/50000]\tLoss: 4.4130\tLR: 2.844246\n",
      "Training Epoch: 29 [22400/50000]\tLoss: 4.2387\tLR: 2.844501\n",
      "Training Epoch: 29 [22528/50000]\tLoss: 4.3827\tLR: 2.844757\n",
      "Training Epoch: 29 [22656/50000]\tLoss: 4.4522\tLR: 2.845013\n",
      "Training Epoch: 29 [22784/50000]\tLoss: 4.4657\tLR: 2.845269\n",
      "Training Epoch: 29 [22912/50000]\tLoss: 4.5151\tLR: 2.845524\n",
      "Training Epoch: 29 [23040/50000]\tLoss: 4.3615\tLR: 2.845780\n",
      "Training Epoch: 29 [23168/50000]\tLoss: 4.3766\tLR: 2.846036\n",
      "Training Epoch: 29 [23296/50000]\tLoss: 4.4287\tLR: 2.846292\n",
      "Training Epoch: 29 [23424/50000]\tLoss: 4.5089\tLR: 2.846547\n",
      "Training Epoch: 29 [23552/50000]\tLoss: 4.3644\tLR: 2.846803\n",
      "Training Epoch: 29 [23680/50000]\tLoss: 4.3082\tLR: 2.847059\n",
      "Training Epoch: 29 [23808/50000]\tLoss: 4.3498\tLR: 2.847315\n",
      "Training Epoch: 29 [23936/50000]\tLoss: 4.3190\tLR: 2.847570\n",
      "Training Epoch: 29 [24064/50000]\tLoss: 4.4519\tLR: 2.847826\n",
      "Training Epoch: 29 [24192/50000]\tLoss: 4.4333\tLR: 2.848082\n",
      "Training Epoch: 29 [24320/50000]\tLoss: 4.3122\tLR: 2.848338\n",
      "Training Epoch: 29 [24448/50000]\tLoss: 4.2950\tLR: 2.848593\n",
      "Training Epoch: 29 [24576/50000]\tLoss: 4.3963\tLR: 2.848849\n",
      "Training Epoch: 29 [24704/50000]\tLoss: 4.3516\tLR: 2.849105\n",
      "Training Epoch: 29 [24832/50000]\tLoss: 4.4190\tLR: 2.849361\n",
      "Training Epoch: 29 [24960/50000]\tLoss: 4.3832\tLR: 2.849616\n",
      "Training Epoch: 29 [25088/50000]\tLoss: 4.3022\tLR: 2.849872\n",
      "Training Epoch: 29 [25216/50000]\tLoss: 4.3421\tLR: 2.850128\n",
      "Training Epoch: 29 [25344/50000]\tLoss: 4.4208\tLR: 2.850384\n",
      "Training Epoch: 29 [25472/50000]\tLoss: 4.3292\tLR: 2.850639\n",
      "Training Epoch: 29 [25600/50000]\tLoss: 4.5950\tLR: 2.850895\n",
      "Training Epoch: 29 [25728/50000]\tLoss: 4.3575\tLR: 2.851151\n",
      "Training Epoch: 29 [25856/50000]\tLoss: 4.2646\tLR: 2.851407\n",
      "Training Epoch: 29 [25984/50000]\tLoss: 4.4085\tLR: 2.851662\n",
      "Training Epoch: 29 [26112/50000]\tLoss: 4.3786\tLR: 2.851918\n",
      "Training Epoch: 29 [26240/50000]\tLoss: 4.4110\tLR: 2.852174\n",
      "Training Epoch: 29 [26368/50000]\tLoss: 4.4828\tLR: 2.852430\n",
      "Training Epoch: 29 [26496/50000]\tLoss: 4.5363\tLR: 2.852685\n",
      "Training Epoch: 29 [26624/50000]\tLoss: 4.3962\tLR: 2.852941\n",
      "Training Epoch: 29 [26752/50000]\tLoss: 4.2978\tLR: 2.853197\n",
      "Training Epoch: 29 [26880/50000]\tLoss: 4.4688\tLR: 2.853453\n",
      "Training Epoch: 29 [27008/50000]\tLoss: 4.3940\tLR: 2.853708\n",
      "Training Epoch: 29 [27136/50000]\tLoss: 4.2444\tLR: 2.853964\n",
      "Training Epoch: 29 [27264/50000]\tLoss: 4.3181\tLR: 2.854220\n",
      "Training Epoch: 29 [27392/50000]\tLoss: 4.5395\tLR: 2.854476\n",
      "Training Epoch: 29 [27520/50000]\tLoss: 4.3561\tLR: 2.854731\n",
      "Training Epoch: 29 [27648/50000]\tLoss: 4.2680\tLR: 2.854987\n",
      "Training Epoch: 29 [27776/50000]\tLoss: 4.3761\tLR: 2.855243\n",
      "Training Epoch: 29 [27904/50000]\tLoss: 4.3996\tLR: 2.855499\n",
      "Training Epoch: 29 [28032/50000]\tLoss: 4.4092\tLR: 2.855754\n",
      "Training Epoch: 29 [28160/50000]\tLoss: 4.4186\tLR: 2.856010\n",
      "Training Epoch: 29 [28288/50000]\tLoss: 4.3320\tLR: 2.856266\n",
      "Training Epoch: 29 [28416/50000]\tLoss: 4.4734\tLR: 2.856522\n",
      "Training Epoch: 29 [28544/50000]\tLoss: 4.4691\tLR: 2.856777\n",
      "Training Epoch: 29 [28672/50000]\tLoss: 4.2910\tLR: 2.857033\n",
      "Training Epoch: 29 [28800/50000]\tLoss: 4.3985\tLR: 2.857289\n",
      "Training Epoch: 29 [28928/50000]\tLoss: 4.2713\tLR: 2.857545\n",
      "Training Epoch: 29 [29056/50000]\tLoss: 4.3414\tLR: 2.857801\n",
      "Training Epoch: 29 [29184/50000]\tLoss: 4.3547\tLR: 2.858056\n",
      "Training Epoch: 29 [29312/50000]\tLoss: 4.3413\tLR: 2.858312\n",
      "Training Epoch: 29 [29440/50000]\tLoss: 4.3493\tLR: 2.858568\n",
      "Training Epoch: 29 [29568/50000]\tLoss: 4.2533\tLR: 2.858824\n",
      "Training Epoch: 29 [29696/50000]\tLoss: 4.2537\tLR: 2.859079\n",
      "Training Epoch: 29 [29824/50000]\tLoss: 4.3391\tLR: 2.859335\n",
      "Training Epoch: 29 [29952/50000]\tLoss: 4.2599\tLR: 2.859591\n",
      "Training Epoch: 29 [30080/50000]\tLoss: 4.4587\tLR: 2.859847\n",
      "Training Epoch: 29 [30208/50000]\tLoss: 4.4729\tLR: 2.860102\n",
      "Training Epoch: 29 [30336/50000]\tLoss: 4.3406\tLR: 2.860358\n",
      "Training Epoch: 29 [30464/50000]\tLoss: 4.3358\tLR: 2.860614\n",
      "Training Epoch: 29 [30592/50000]\tLoss: 4.1874\tLR: 2.860870\n",
      "Training Epoch: 29 [30720/50000]\tLoss: 4.3509\tLR: 2.861125\n",
      "Training Epoch: 29 [30848/50000]\tLoss: 4.3190\tLR: 2.861381\n",
      "Training Epoch: 29 [30976/50000]\tLoss: 4.2874\tLR: 2.861637\n",
      "Training Epoch: 29 [31104/50000]\tLoss: 4.4550\tLR: 2.861893\n",
      "Training Epoch: 29 [31232/50000]\tLoss: 4.2380\tLR: 2.862148\n",
      "Training Epoch: 29 [31360/50000]\tLoss: 4.4915\tLR: 2.862404\n",
      "Training Epoch: 29 [31488/50000]\tLoss: 4.5301\tLR: 2.862660\n",
      "Training Epoch: 29 [31616/50000]\tLoss: 4.4036\tLR: 2.862916\n",
      "Training Epoch: 29 [31744/50000]\tLoss: 4.3329\tLR: 2.863171\n",
      "Training Epoch: 29 [31872/50000]\tLoss: 4.3368\tLR: 2.863427\n",
      "Training Epoch: 29 [32000/50000]\tLoss: 4.3450\tLR: 2.863683\n",
      "Training Epoch: 29 [32128/50000]\tLoss: 4.3274\tLR: 2.863939\n",
      "Training Epoch: 29 [32256/50000]\tLoss: 4.3914\tLR: 2.864194\n",
      "Training Epoch: 29 [32384/50000]\tLoss: 4.3372\tLR: 2.864450\n",
      "Training Epoch: 29 [32512/50000]\tLoss: 4.2543\tLR: 2.864706\n",
      "Training Epoch: 29 [32640/50000]\tLoss: 4.2275\tLR: 2.864962\n",
      "Training Epoch: 29 [32768/50000]\tLoss: 4.2940\tLR: 2.865217\n",
      "Training Epoch: 29 [32896/50000]\tLoss: 4.3347\tLR: 2.865473\n",
      "Training Epoch: 29 [33024/50000]\tLoss: 4.4023\tLR: 2.865729\n",
      "Training Epoch: 29 [33152/50000]\tLoss: 4.3978\tLR: 2.865985\n",
      "Training Epoch: 29 [33280/50000]\tLoss: 4.4012\tLR: 2.866240\n",
      "Training Epoch: 29 [33408/50000]\tLoss: 4.3709\tLR: 2.866496\n",
      "Training Epoch: 29 [33536/50000]\tLoss: 4.3645\tLR: 2.866752\n",
      "Training Epoch: 29 [33664/50000]\tLoss: 4.3463\tLR: 2.867008\n",
      "Training Epoch: 29 [33792/50000]\tLoss: 4.5464\tLR: 2.867263\n",
      "Training Epoch: 29 [33920/50000]\tLoss: 4.4055\tLR: 2.867519\n",
      "Training Epoch: 29 [34048/50000]\tLoss: 4.3696\tLR: 2.867775\n",
      "Training Epoch: 29 [34176/50000]\tLoss: 4.3557\tLR: 2.868031\n",
      "Training Epoch: 29 [34304/50000]\tLoss: 4.3834\tLR: 2.868286\n",
      "Training Epoch: 29 [34432/50000]\tLoss: 4.2126\tLR: 2.868542\n",
      "Training Epoch: 29 [34560/50000]\tLoss: 4.3568\tLR: 2.868798\n",
      "Training Epoch: 29 [34688/50000]\tLoss: 4.1125\tLR: 2.869054\n",
      "Training Epoch: 29 [34816/50000]\tLoss: 4.4311\tLR: 2.869309\n",
      "Training Epoch: 29 [34944/50000]\tLoss: 4.3828\tLR: 2.869565\n",
      "Training Epoch: 29 [35072/50000]\tLoss: 4.3061\tLR: 2.869821\n",
      "Training Epoch: 29 [35200/50000]\tLoss: 4.3395\tLR: 2.870077\n",
      "Training Epoch: 29 [35328/50000]\tLoss: 4.2715\tLR: 2.870332\n",
      "Training Epoch: 29 [35456/50000]\tLoss: 4.3965\tLR: 2.870588\n",
      "Training Epoch: 29 [35584/50000]\tLoss: 4.4651\tLR: 2.870844\n",
      "Training Epoch: 29 [35712/50000]\tLoss: 4.4569\tLR: 2.871100\n",
      "Training Epoch: 29 [35840/50000]\tLoss: 4.3264\tLR: 2.871355\n",
      "Training Epoch: 29 [35968/50000]\tLoss: 4.2784\tLR: 2.871611\n",
      "Training Epoch: 29 [36096/50000]\tLoss: 4.3801\tLR: 2.871867\n",
      "Training Epoch: 29 [36224/50000]\tLoss: 4.1558\tLR: 2.872123\n",
      "Training Epoch: 29 [36352/50000]\tLoss: 4.4569\tLR: 2.872379\n",
      "Training Epoch: 29 [36480/50000]\tLoss: 4.3147\tLR: 2.872634\n",
      "Training Epoch: 29 [36608/50000]\tLoss: 4.3242\tLR: 2.872890\n",
      "Training Epoch: 29 [36736/50000]\tLoss: 4.3619\tLR: 2.873146\n",
      "Training Epoch: 29 [36864/50000]\tLoss: 4.5485\tLR: 2.873402\n",
      "Training Epoch: 29 [36992/50000]\tLoss: 4.3521\tLR: 2.873657\n",
      "Training Epoch: 29 [37120/50000]\tLoss: 4.4888\tLR: 2.873913\n",
      "Training Epoch: 29 [37248/50000]\tLoss: 4.4889\tLR: 2.874169\n",
      "Training Epoch: 29 [37376/50000]\tLoss: 4.5196\tLR: 2.874425\n",
      "Training Epoch: 29 [37504/50000]\tLoss: 4.5042\tLR: 2.874680\n",
      "Training Epoch: 29 [37632/50000]\tLoss: 4.3657\tLR: 2.874936\n",
      "Training Epoch: 29 [37760/50000]\tLoss: 4.3774\tLR: 2.875192\n",
      "Training Epoch: 29 [37888/50000]\tLoss: 4.4491\tLR: 2.875448\n",
      "Training Epoch: 29 [38016/50000]\tLoss: 4.4181\tLR: 2.875703\n",
      "Training Epoch: 29 [38144/50000]\tLoss: 4.4542\tLR: 2.875959\n",
      "Training Epoch: 29 [38272/50000]\tLoss: 4.2792\tLR: 2.876215\n",
      "Training Epoch: 29 [38400/50000]\tLoss: 4.3814\tLR: 2.876471\n",
      "Training Epoch: 29 [38528/50000]\tLoss: 4.5501\tLR: 2.876726\n",
      "Training Epoch: 29 [38656/50000]\tLoss: 4.3392\tLR: 2.876982\n",
      "Training Epoch: 29 [38784/50000]\tLoss: 4.3832\tLR: 2.877238\n",
      "Training Epoch: 29 [38912/50000]\tLoss: 4.4752\tLR: 2.877494\n",
      "Training Epoch: 29 [39040/50000]\tLoss: 4.3632\tLR: 2.877749\n",
      "Training Epoch: 29 [39168/50000]\tLoss: 4.4820\tLR: 2.878005\n",
      "Training Epoch: 29 [39296/50000]\tLoss: 4.4184\tLR: 2.878261\n",
      "Training Epoch: 29 [39424/50000]\tLoss: 4.2376\tLR: 2.878517\n",
      "Training Epoch: 29 [39552/50000]\tLoss: 4.2900\tLR: 2.878772\n",
      "Training Epoch: 29 [39680/50000]\tLoss: 4.5369\tLR: 2.879028\n",
      "Training Epoch: 29 [39808/50000]\tLoss: 4.4225\tLR: 2.879284\n",
      "Training Epoch: 29 [39936/50000]\tLoss: 4.4797\tLR: 2.879540\n",
      "Training Epoch: 29 [40064/50000]\tLoss: 4.4527\tLR: 2.879795\n",
      "Training Epoch: 29 [40192/50000]\tLoss: 4.3489\tLR: 2.880051\n",
      "Training Epoch: 29 [40320/50000]\tLoss: 4.3755\tLR: 2.880307\n",
      "Training Epoch: 29 [40448/50000]\tLoss: 4.2859\tLR: 2.880563\n",
      "Training Epoch: 29 [40576/50000]\tLoss: 4.3815\tLR: 2.880818\n",
      "Training Epoch: 29 [40704/50000]\tLoss: 4.3568\tLR: 2.881074\n",
      "Training Epoch: 29 [40832/50000]\tLoss: 4.3260\tLR: 2.881330\n",
      "Training Epoch: 29 [40960/50000]\tLoss: 4.3427\tLR: 2.881586\n",
      "Training Epoch: 29 [41088/50000]\tLoss: 4.3757\tLR: 2.881841\n",
      "Training Epoch: 29 [41216/50000]\tLoss: 4.3663\tLR: 2.882097\n",
      "Training Epoch: 29 [41344/50000]\tLoss: 4.3431\tLR: 2.882353\n",
      "Training Epoch: 29 [41472/50000]\tLoss: 4.2209\tLR: 2.882609\n",
      "Training Epoch: 29 [41600/50000]\tLoss: 4.3386\tLR: 2.882864\n",
      "Training Epoch: 29 [41728/50000]\tLoss: 4.4794\tLR: 2.883120\n",
      "Training Epoch: 29 [41856/50000]\tLoss: 4.3691\tLR: 2.883376\n",
      "Training Epoch: 29 [41984/50000]\tLoss: 4.4366\tLR: 2.883632\n",
      "Training Epoch: 29 [42112/50000]\tLoss: 4.4879\tLR: 2.883887\n",
      "Training Epoch: 29 [42240/50000]\tLoss: 4.3822\tLR: 2.884143\n",
      "Training Epoch: 29 [42368/50000]\tLoss: 4.5305\tLR: 2.884399\n",
      "Training Epoch: 29 [42496/50000]\tLoss: 4.4995\tLR: 2.884655\n",
      "Training Epoch: 29 [42624/50000]\tLoss: 4.4794\tLR: 2.884910\n",
      "Training Epoch: 29 [42752/50000]\tLoss: 4.4195\tLR: 2.885166\n",
      "Training Epoch: 29 [42880/50000]\tLoss: 4.4798\tLR: 2.885422\n",
      "Training Epoch: 29 [43008/50000]\tLoss: 4.4824\tLR: 2.885678\n",
      "Training Epoch: 29 [43136/50000]\tLoss: 4.4056\tLR: 2.885934\n",
      "Training Epoch: 29 [43264/50000]\tLoss: 4.3971\tLR: 2.886189\n",
      "Training Epoch: 29 [43392/50000]\tLoss: 4.3158\tLR: 2.886445\n",
      "Training Epoch: 29 [43520/50000]\tLoss: 4.6599\tLR: 2.886701\n",
      "Training Epoch: 29 [43648/50000]\tLoss: 4.2779\tLR: 2.886957\n",
      "Training Epoch: 29 [43776/50000]\tLoss: 4.3828\tLR: 2.887212\n",
      "Training Epoch: 29 [43904/50000]\tLoss: 4.4560\tLR: 2.887468\n",
      "Training Epoch: 29 [44032/50000]\tLoss: 4.5556\tLR: 2.887724\n",
      "Training Epoch: 29 [44160/50000]\tLoss: 4.3731\tLR: 2.887980\n",
      "Training Epoch: 29 [44288/50000]\tLoss: 4.3760\tLR: 2.888235\n",
      "Training Epoch: 29 [44416/50000]\tLoss: 4.3770\tLR: 2.888491\n",
      "Training Epoch: 29 [44544/50000]\tLoss: 4.3169\tLR: 2.888747\n",
      "Training Epoch: 29 [44672/50000]\tLoss: 4.3926\tLR: 2.889003\n",
      "Training Epoch: 29 [44800/50000]\tLoss: 4.4838\tLR: 2.889258\n",
      "Training Epoch: 29 [44928/50000]\tLoss: 4.2411\tLR: 2.889514\n",
      "Training Epoch: 29 [45056/50000]\tLoss: 4.4276\tLR: 2.889770\n",
      "Training Epoch: 29 [45184/50000]\tLoss: 4.2975\tLR: 2.890026\n",
      "Training Epoch: 29 [45312/50000]\tLoss: 4.5715\tLR: 2.890281\n",
      "Training Epoch: 29 [45440/50000]\tLoss: 4.4236\tLR: 2.890537\n",
      "Training Epoch: 29 [45568/50000]\tLoss: 4.4013\tLR: 2.890793\n",
      "Training Epoch: 29 [45696/50000]\tLoss: 4.4885\tLR: 2.891049\n",
      "Training Epoch: 29 [45824/50000]\tLoss: 4.3242\tLR: 2.891304\n",
      "Training Epoch: 29 [45952/50000]\tLoss: 4.4166\tLR: 2.891560\n",
      "Training Epoch: 29 [46080/50000]\tLoss: 4.4954\tLR: 2.891816\n",
      "Training Epoch: 29 [46208/50000]\tLoss: 4.4007\tLR: 2.892072\n",
      "Training Epoch: 29 [46336/50000]\tLoss: 4.3456\tLR: 2.892327\n",
      "Training Epoch: 29 [46464/50000]\tLoss: 4.4753\tLR: 2.892583\n",
      "Training Epoch: 29 [46592/50000]\tLoss: 4.4409\tLR: 2.892839\n",
      "Training Epoch: 29 [46720/50000]\tLoss: 4.4112\tLR: 2.893095\n",
      "Training Epoch: 29 [46848/50000]\tLoss: 4.3124\tLR: 2.893350\n",
      "Training Epoch: 29 [46976/50000]\tLoss: 4.3679\tLR: 2.893606\n",
      "Training Epoch: 29 [47104/50000]\tLoss: 4.3588\tLR: 2.893862\n",
      "Training Epoch: 29 [47232/50000]\tLoss: 4.2139\tLR: 2.894118\n",
      "Training Epoch: 29 [47360/50000]\tLoss: 4.3434\tLR: 2.894373\n",
      "Training Epoch: 29 [47488/50000]\tLoss: 4.2586\tLR: 2.894629\n",
      "Training Epoch: 29 [47616/50000]\tLoss: 4.2426\tLR: 2.894885\n",
      "Training Epoch: 29 [47744/50000]\tLoss: 4.2116\tLR: 2.895141\n",
      "Training Epoch: 29 [47872/50000]\tLoss: 4.3514\tLR: 2.895396\n",
      "Training Epoch: 29 [48000/50000]\tLoss: 4.4549\tLR: 2.895652\n",
      "Training Epoch: 29 [48128/50000]\tLoss: 4.4529\tLR: 2.895908\n",
      "Training Epoch: 29 [48256/50000]\tLoss: 4.4071\tLR: 2.896164\n",
      "Training Epoch: 29 [48384/50000]\tLoss: 4.2640\tLR: 2.896419\n",
      "Training Epoch: 29 [48512/50000]\tLoss: 4.4910\tLR: 2.896675\n",
      "Training Epoch: 29 [48640/50000]\tLoss: 4.4295\tLR: 2.896931\n",
      "Training Epoch: 29 [48768/50000]\tLoss: 4.5469\tLR: 2.897187\n",
      "Training Epoch: 29 [48896/50000]\tLoss: 4.2306\tLR: 2.897442\n",
      "Training Epoch: 29 [49024/50000]\tLoss: 4.2680\tLR: 2.897698\n",
      "Training Epoch: 29 [49152/50000]\tLoss: 4.3129\tLR: 2.897954\n",
      "Training Epoch: 29 [49280/50000]\tLoss: 4.4153\tLR: 2.898210\n",
      "Training Epoch: 29 [49408/50000]\tLoss: 4.3477\tLR: 2.898465\n",
      "Training Epoch: 29 [49536/50000]\tLoss: 4.3504\tLR: 2.898721\n",
      "Training Epoch: 29 [49664/50000]\tLoss: 4.3699\tLR: 2.898977\n",
      "Training Epoch: 29 [49792/50000]\tLoss: 4.5010\tLR: 2.899233\n",
      "Training Epoch: 29 [49920/50000]\tLoss: 4.3466\tLR: 2.899488\n",
      "Training Epoch: 29 [50000/50000]\tLoss: 4.4610\tLR: 2.899744\n",
      "epoch 29 training time consumed: 489.10s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   40657 GB |   40657 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   40532 GB |   40532 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     124 GB |     124 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   40657 GB |   40657 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   40532 GB |   40532 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     124 GB |     124 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   40083 GB |   40083 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   39958 GB |   39958 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     124 GB |     124 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    4311 K  |    4311 K  |\n",
      "|       from large pool |      24    |      65    |    1837 K  |    1837 K  |\n",
      "|       from small pool |     231    |     274    |    2473 K  |    2473 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    4311 K  |    4311 K  |\n",
      "|       from large pool |      24    |      65    |    1837 K  |    1837 K  |\n",
      "|       from small pool |     231    |     274    |    2473 K  |    2473 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      46    |    2498 K  |    2498 K  |\n",
      "|       from large pool |      10    |      23    |     883 K  |     883 K  |\n",
      "|       from small pool |      25    |      35    |    1615 K  |    1614 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 29, Average loss: 0.0355, Accuracy: 0.0236, Time consumed:31.08s\n",
      "\n",
      "Training Epoch: 30 [128/50000]\tLoss: 4.4137\tLR: 0.100000\n",
      "Training Epoch: 30 [256/50000]\tLoss: 4.3443\tLR: 2.900256\n",
      "Training Epoch: 30 [384/50000]\tLoss: 4.3753\tLR: 2.900512\n",
      "Training Epoch: 30 [512/50000]\tLoss: 4.4496\tLR: 2.900767\n",
      "Training Epoch: 30 [640/50000]\tLoss: 4.4323\tLR: 2.901023\n",
      "Training Epoch: 30 [768/50000]\tLoss: 4.3115\tLR: 2.901279\n",
      "Training Epoch: 30 [896/50000]\tLoss: 4.3506\tLR: 2.901535\n",
      "Training Epoch: 30 [1024/50000]\tLoss: 4.4221\tLR: 2.901790\n",
      "Training Epoch: 30 [1152/50000]\tLoss: 4.3778\tLR: 2.902046\n",
      "Training Epoch: 30 [1280/50000]\tLoss: 4.1933\tLR: 2.902302\n",
      "Training Epoch: 30 [1408/50000]\tLoss: 4.3317\tLR: 2.902558\n",
      "Training Epoch: 30 [1536/50000]\tLoss: 4.3713\tLR: 2.902813\n",
      "Training Epoch: 30 [1664/50000]\tLoss: 4.3210\tLR: 2.903069\n",
      "Training Epoch: 30 [1792/50000]\tLoss: 4.3905\tLR: 2.903325\n",
      "Training Epoch: 30 [1920/50000]\tLoss: 4.3703\tLR: 2.903581\n",
      "Training Epoch: 30 [2048/50000]\tLoss: 4.4094\tLR: 2.903836\n",
      "Training Epoch: 30 [2176/50000]\tLoss: 4.3623\tLR: 2.904092\n",
      "Training Epoch: 30 [2304/50000]\tLoss: 4.3067\tLR: 2.904348\n",
      "Training Epoch: 30 [2432/50000]\tLoss: 4.2673\tLR: 2.904604\n",
      "Training Epoch: 30 [2560/50000]\tLoss: 4.1393\tLR: 2.904859\n",
      "Training Epoch: 30 [2688/50000]\tLoss: 4.3913\tLR: 2.905115\n",
      "Training Epoch: 30 [2816/50000]\tLoss: 4.2180\tLR: 2.905371\n",
      "Training Epoch: 30 [2944/50000]\tLoss: 4.2913\tLR: 2.905627\n",
      "Training Epoch: 30 [3072/50000]\tLoss: 4.3564\tLR: 2.905882\n",
      "Training Epoch: 30 [3200/50000]\tLoss: 4.3700\tLR: 2.906138\n",
      "Training Epoch: 30 [3328/50000]\tLoss: 4.3591\tLR: 2.906394\n",
      "Training Epoch: 30 [3456/50000]\tLoss: 4.3150\tLR: 2.906650\n",
      "Training Epoch: 30 [3584/50000]\tLoss: 4.3412\tLR: 2.906905\n",
      "Training Epoch: 30 [3712/50000]\tLoss: 4.3682\tLR: 2.907161\n",
      "Training Epoch: 30 [3840/50000]\tLoss: 4.2162\tLR: 2.907417\n",
      "Training Epoch: 30 [3968/50000]\tLoss: 4.2904\tLR: 2.907673\n",
      "Training Epoch: 30 [4096/50000]\tLoss: 4.2595\tLR: 2.907928\n",
      "Training Epoch: 30 [4224/50000]\tLoss: 4.3680\tLR: 2.908184\n",
      "Training Epoch: 30 [4352/50000]\tLoss: 4.2766\tLR: 2.908440\n",
      "Training Epoch: 30 [4480/50000]\tLoss: 4.3331\tLR: 2.908696\n",
      "Training Epoch: 30 [4608/50000]\tLoss: 4.1880\tLR: 2.908951\n",
      "Training Epoch: 30 [4736/50000]\tLoss: 4.4286\tLR: 2.909207\n",
      "Training Epoch: 30 [4864/50000]\tLoss: 4.2714\tLR: 2.909463\n",
      "Training Epoch: 30 [4992/50000]\tLoss: 4.4428\tLR: 2.909719\n",
      "Training Epoch: 30 [5120/50000]\tLoss: 4.5731\tLR: 2.909974\n",
      "Training Epoch: 30 [5248/50000]\tLoss: 4.4411\tLR: 2.910230\n",
      "Training Epoch: 30 [5376/50000]\tLoss: 4.4971\tLR: 2.910486\n",
      "Training Epoch: 30 [5504/50000]\tLoss: 4.4916\tLR: 2.910742\n",
      "Training Epoch: 30 [5632/50000]\tLoss: 4.3099\tLR: 2.910997\n",
      "Training Epoch: 30 [5760/50000]\tLoss: 4.3791\tLR: 2.911253\n",
      "Training Epoch: 30 [5888/50000]\tLoss: 4.4883\tLR: 2.911509\n",
      "Training Epoch: 30 [6016/50000]\tLoss: 4.3612\tLR: 2.911765\n",
      "Training Epoch: 30 [6144/50000]\tLoss: 4.3074\tLR: 2.912020\n",
      "Training Epoch: 30 [6272/50000]\tLoss: 4.3691\tLR: 2.912276\n",
      "Training Epoch: 30 [6400/50000]\tLoss: 4.4379\tLR: 2.912532\n",
      "Training Epoch: 30 [6528/50000]\tLoss: 4.3599\tLR: 2.912788\n",
      "Training Epoch: 30 [6656/50000]\tLoss: 4.3568\tLR: 2.913043\n",
      "Training Epoch: 30 [6784/50000]\tLoss: 4.3613\tLR: 2.913299\n",
      "Training Epoch: 30 [6912/50000]\tLoss: 4.2948\tLR: 2.913555\n",
      "Training Epoch: 30 [7040/50000]\tLoss: 4.3802\tLR: 2.913811\n",
      "Training Epoch: 30 [7168/50000]\tLoss: 4.3715\tLR: 2.914066\n",
      "Training Epoch: 30 [7296/50000]\tLoss: 4.3852\tLR: 2.914322\n",
      "Training Epoch: 30 [7424/50000]\tLoss: 4.4017\tLR: 2.914578\n",
      "Training Epoch: 30 [7552/50000]\tLoss: 4.2738\tLR: 2.914834\n",
      "Training Epoch: 30 [7680/50000]\tLoss: 4.4030\tLR: 2.915090\n",
      "Training Epoch: 30 [7808/50000]\tLoss: 4.2274\tLR: 2.915345\n",
      "Training Epoch: 30 [7936/50000]\tLoss: 4.3781\tLR: 2.915601\n",
      "Training Epoch: 30 [8064/50000]\tLoss: 4.3050\tLR: 2.915857\n",
      "Training Epoch: 30 [8192/50000]\tLoss: 4.3662\tLR: 2.916113\n",
      "Training Epoch: 30 [8320/50000]\tLoss: 4.1965\tLR: 2.916368\n",
      "Training Epoch: 30 [8448/50000]\tLoss: 4.4201\tLR: 2.916624\n",
      "Training Epoch: 30 [8576/50000]\tLoss: 4.5590\tLR: 2.916880\n",
      "Training Epoch: 30 [8704/50000]\tLoss: 4.4331\tLR: 2.917136\n",
      "Training Epoch: 30 [8832/50000]\tLoss: 4.3843\tLR: 2.917391\n",
      "Training Epoch: 30 [8960/50000]\tLoss: 4.3626\tLR: 2.917647\n",
      "Training Epoch: 30 [9088/50000]\tLoss: 4.3664\tLR: 2.917903\n",
      "Training Epoch: 30 [9216/50000]\tLoss: 4.4377\tLR: 2.918159\n",
      "Training Epoch: 30 [9344/50000]\tLoss: 4.2888\tLR: 2.918414\n",
      "Training Epoch: 30 [9472/50000]\tLoss: 4.2015\tLR: 2.918670\n",
      "Training Epoch: 30 [9600/50000]\tLoss: 4.3043\tLR: 2.918926\n",
      "Training Epoch: 30 [9728/50000]\tLoss: 4.2675\tLR: 2.919182\n",
      "Training Epoch: 30 [9856/50000]\tLoss: 4.1773\tLR: 2.919437\n",
      "Training Epoch: 30 [9984/50000]\tLoss: 4.1839\tLR: 2.919693\n",
      "Training Epoch: 30 [10112/50000]\tLoss: 4.3171\tLR: 2.919949\n",
      "Training Epoch: 30 [10240/50000]\tLoss: 4.2450\tLR: 2.920205\n",
      "Training Epoch: 30 [10368/50000]\tLoss: 4.4197\tLR: 2.920460\n",
      "Training Epoch: 30 [10496/50000]\tLoss: 4.3856\tLR: 2.920716\n",
      "Training Epoch: 30 [10624/50000]\tLoss: 4.3286\tLR: 2.920972\n",
      "Training Epoch: 30 [10752/50000]\tLoss: 4.3213\tLR: 2.921228\n",
      "Training Epoch: 30 [10880/50000]\tLoss: 4.3282\tLR: 2.921483\n",
      "Training Epoch: 30 [11008/50000]\tLoss: 4.2493\tLR: 2.921739\n",
      "Training Epoch: 30 [11136/50000]\tLoss: 4.3239\tLR: 2.921995\n",
      "Training Epoch: 30 [11264/50000]\tLoss: 4.3255\tLR: 2.922251\n",
      "Training Epoch: 30 [11392/50000]\tLoss: 4.4497\tLR: 2.922506\n",
      "Training Epoch: 30 [11520/50000]\tLoss: 4.2294\tLR: 2.922762\n",
      "Training Epoch: 30 [11648/50000]\tLoss: 4.3895\tLR: 2.923018\n",
      "Training Epoch: 30 [11776/50000]\tLoss: 4.4077\tLR: 2.923274\n",
      "Training Epoch: 30 [11904/50000]\tLoss: 4.3381\tLR: 2.923529\n",
      "Training Epoch: 30 [12032/50000]\tLoss: 4.3987\tLR: 2.923785\n",
      "Training Epoch: 30 [12160/50000]\tLoss: 4.4329\tLR: 2.924041\n",
      "Training Epoch: 30 [12288/50000]\tLoss: 4.3441\tLR: 2.924297\n",
      "Training Epoch: 30 [12416/50000]\tLoss: 4.3479\tLR: 2.924552\n",
      "Training Epoch: 30 [12544/50000]\tLoss: 4.4798\tLR: 2.924808\n",
      "Training Epoch: 30 [12672/50000]\tLoss: 4.4125\tLR: 2.925064\n",
      "Training Epoch: 30 [12800/50000]\tLoss: 4.3423\tLR: 2.925320\n",
      "Training Epoch: 30 [12928/50000]\tLoss: 4.2663\tLR: 2.925575\n",
      "Training Epoch: 30 [13056/50000]\tLoss: 4.3483\tLR: 2.925831\n",
      "Training Epoch: 30 [13184/50000]\tLoss: 4.2734\tLR: 2.926087\n",
      "Training Epoch: 30 [13312/50000]\tLoss: 4.3183\tLR: 2.926343\n",
      "Training Epoch: 30 [13440/50000]\tLoss: 4.3448\tLR: 2.926598\n",
      "Training Epoch: 30 [13568/50000]\tLoss: 4.3616\tLR: 2.926854\n",
      "Training Epoch: 30 [13696/50000]\tLoss: 4.3106\tLR: 2.927110\n",
      "Training Epoch: 30 [13824/50000]\tLoss: 4.2778\tLR: 2.927366\n",
      "Training Epoch: 30 [13952/50000]\tLoss: 4.3877\tLR: 2.927621\n",
      "Training Epoch: 30 [14080/50000]\tLoss: 4.2823\tLR: 2.927877\n",
      "Training Epoch: 30 [14208/50000]\tLoss: 4.2026\tLR: 2.928133\n",
      "Training Epoch: 30 [14336/50000]\tLoss: 4.2519\tLR: 2.928389\n",
      "Training Epoch: 30 [14464/50000]\tLoss: 4.2462\tLR: 2.928645\n",
      "Training Epoch: 30 [14592/50000]\tLoss: 4.3686\tLR: 2.928900\n",
      "Training Epoch: 30 [14720/50000]\tLoss: 4.1911\tLR: 2.929156\n",
      "Training Epoch: 30 [14848/50000]\tLoss: 4.4271\tLR: 2.929412\n",
      "Training Epoch: 30 [14976/50000]\tLoss: 4.3216\tLR: 2.929668\n",
      "Training Epoch: 30 [15104/50000]\tLoss: 4.2779\tLR: 2.929923\n",
      "Training Epoch: 30 [15232/50000]\tLoss: 4.4661\tLR: 2.930179\n",
      "Training Epoch: 30 [15360/50000]\tLoss: 4.3991\tLR: 2.930435\n",
      "Training Epoch: 30 [15488/50000]\tLoss: 4.2975\tLR: 2.930691\n",
      "Training Epoch: 30 [15616/50000]\tLoss: 4.4436\tLR: 2.930946\n",
      "Training Epoch: 30 [15744/50000]\tLoss: 4.4021\tLR: 2.931202\n",
      "Training Epoch: 30 [15872/50000]\tLoss: 4.3427\tLR: 2.931458\n",
      "Training Epoch: 30 [16000/50000]\tLoss: 4.3420\tLR: 2.931714\n",
      "Training Epoch: 30 [16128/50000]\tLoss: 4.3133\tLR: 2.931969\n",
      "Training Epoch: 30 [16256/50000]\tLoss: 4.4956\tLR: 2.932225\n",
      "Training Epoch: 30 [16384/50000]\tLoss: 4.4479\tLR: 2.932481\n",
      "Training Epoch: 30 [16512/50000]\tLoss: 4.5035\tLR: 2.932737\n",
      "Training Epoch: 30 [16640/50000]\tLoss: 4.4349\tLR: 2.932992\n",
      "Training Epoch: 30 [16768/50000]\tLoss: 4.4025\tLR: 2.933248\n",
      "Training Epoch: 30 [16896/50000]\tLoss: 4.3158\tLR: 2.933504\n",
      "Training Epoch: 30 [17024/50000]\tLoss: 4.4731\tLR: 2.933760\n",
      "Training Epoch: 30 [17152/50000]\tLoss: 4.4605\tLR: 2.934015\n",
      "Training Epoch: 30 [17280/50000]\tLoss: 4.3934\tLR: 2.934271\n",
      "Training Epoch: 30 [17408/50000]\tLoss: 4.5219\tLR: 2.934527\n",
      "Training Epoch: 30 [17536/50000]\tLoss: 4.4153\tLR: 2.934783\n",
      "Training Epoch: 30 [17664/50000]\tLoss: 4.2918\tLR: 2.935038\n",
      "Training Epoch: 30 [17792/50000]\tLoss: 4.3427\tLR: 2.935294\n",
      "Training Epoch: 30 [17920/50000]\tLoss: 4.3706\tLR: 2.935550\n",
      "Training Epoch: 30 [18048/50000]\tLoss: 4.3841\tLR: 2.935806\n",
      "Training Epoch: 30 [18176/50000]\tLoss: 4.4082\tLR: 2.936061\n",
      "Training Epoch: 30 [18304/50000]\tLoss: 4.4247\tLR: 2.936317\n",
      "Training Epoch: 30 [18432/50000]\tLoss: 4.4028\tLR: 2.936573\n",
      "Training Epoch: 30 [18560/50000]\tLoss: 4.4601\tLR: 2.936829\n",
      "Training Epoch: 30 [18688/50000]\tLoss: 4.4242\tLR: 2.937084\n",
      "Training Epoch: 30 [18816/50000]\tLoss: 4.3584\tLR: 2.937340\n",
      "Training Epoch: 30 [18944/50000]\tLoss: 4.1868\tLR: 2.937596\n",
      "Training Epoch: 30 [19072/50000]\tLoss: 4.4024\tLR: 2.937852\n",
      "Training Epoch: 30 [19200/50000]\tLoss: 4.4118\tLR: 2.938107\n",
      "Training Epoch: 30 [19328/50000]\tLoss: 4.4047\tLR: 2.938363\n",
      "Training Epoch: 30 [19456/50000]\tLoss: 4.3774\tLR: 2.938619\n",
      "Training Epoch: 30 [19584/50000]\tLoss: 4.3830\tLR: 2.938875\n",
      "Training Epoch: 30 [19712/50000]\tLoss: 4.4789\tLR: 2.939130\n",
      "Training Epoch: 30 [19840/50000]\tLoss: 4.4119\tLR: 2.939386\n",
      "Training Epoch: 30 [19968/50000]\tLoss: 4.2160\tLR: 2.939642\n",
      "Training Epoch: 30 [20096/50000]\tLoss: 4.4776\tLR: 2.939898\n",
      "Training Epoch: 30 [20224/50000]\tLoss: 4.3284\tLR: 2.940153\n",
      "Training Epoch: 30 [20352/50000]\tLoss: 4.3830\tLR: 2.940409\n",
      "Training Epoch: 30 [20480/50000]\tLoss: 4.5650\tLR: 2.940665\n",
      "Training Epoch: 30 [20608/50000]\tLoss: 4.4546\tLR: 2.940921\n",
      "Training Epoch: 30 [20736/50000]\tLoss: 4.3548\tLR: 2.941176\n",
      "Training Epoch: 30 [20864/50000]\tLoss: 4.3704\tLR: 2.941432\n",
      "Training Epoch: 30 [20992/50000]\tLoss: 4.4435\tLR: 2.941688\n",
      "Training Epoch: 30 [21120/50000]\tLoss: 4.4058\tLR: 2.941944\n",
      "Training Epoch: 30 [21248/50000]\tLoss: 4.3353\tLR: 2.942199\n",
      "Training Epoch: 30 [21376/50000]\tLoss: 4.4533\tLR: 2.942455\n",
      "Training Epoch: 30 [21504/50000]\tLoss: 4.2621\tLR: 2.942711\n",
      "Training Epoch: 30 [21632/50000]\tLoss: 4.3615\tLR: 2.942967\n",
      "Training Epoch: 30 [21760/50000]\tLoss: 4.2521\tLR: 2.943223\n",
      "Training Epoch: 30 [21888/50000]\tLoss: 4.2700\tLR: 2.943478\n",
      "Training Epoch: 30 [22016/50000]\tLoss: 4.3035\tLR: 2.943734\n",
      "Training Epoch: 30 [22144/50000]\tLoss: 4.2815\tLR: 2.943990\n",
      "Training Epoch: 30 [22272/50000]\tLoss: 4.2598\tLR: 2.944246\n",
      "Training Epoch: 30 [22400/50000]\tLoss: 4.2584\tLR: 2.944501\n",
      "Training Epoch: 30 [22528/50000]\tLoss: 4.3089\tLR: 2.944757\n",
      "Training Epoch: 30 [22656/50000]\tLoss: 4.2679\tLR: 2.945013\n",
      "Training Epoch: 30 [22784/50000]\tLoss: 4.4938\tLR: 2.945269\n",
      "Training Epoch: 30 [22912/50000]\tLoss: 4.3126\tLR: 2.945524\n",
      "Training Epoch: 30 [23040/50000]\tLoss: 4.3525\tLR: 2.945780\n",
      "Training Epoch: 30 [23168/50000]\tLoss: 4.2966\tLR: 2.946036\n",
      "Training Epoch: 30 [23296/50000]\tLoss: 4.2248\tLR: 2.946292\n",
      "Training Epoch: 30 [23424/50000]\tLoss: 4.3903\tLR: 2.946547\n",
      "Training Epoch: 30 [23552/50000]\tLoss: 4.3332\tLR: 2.946803\n",
      "Training Epoch: 30 [23680/50000]\tLoss: 4.2412\tLR: 2.947059\n",
      "Training Epoch: 30 [23808/50000]\tLoss: 4.3527\tLR: 2.947315\n",
      "Training Epoch: 30 [23936/50000]\tLoss: 4.3686\tLR: 2.947570\n",
      "Training Epoch: 30 [24064/50000]\tLoss: 4.2625\tLR: 2.947826\n",
      "Training Epoch: 30 [24192/50000]\tLoss: 4.4054\tLR: 2.948082\n",
      "Training Epoch: 30 [24320/50000]\tLoss: 4.3598\tLR: 2.948338\n",
      "Training Epoch: 30 [24448/50000]\tLoss: 4.3642\tLR: 2.948593\n",
      "Training Epoch: 30 [24576/50000]\tLoss: 4.3377\tLR: 2.948849\n",
      "Training Epoch: 30 [24704/50000]\tLoss: 4.3233\tLR: 2.949105\n",
      "Training Epoch: 30 [24832/50000]\tLoss: 4.4543\tLR: 2.949361\n",
      "Training Epoch: 30 [24960/50000]\tLoss: 4.1998\tLR: 2.949616\n",
      "Training Epoch: 30 [25088/50000]\tLoss: 4.1788\tLR: 2.949872\n",
      "Training Epoch: 30 [25216/50000]\tLoss: 4.3162\tLR: 2.950128\n",
      "Training Epoch: 30 [25344/50000]\tLoss: 4.3130\tLR: 2.950384\n",
      "Training Epoch: 30 [25472/50000]\tLoss: 4.4445\tLR: 2.950639\n",
      "Training Epoch: 30 [25600/50000]\tLoss: 4.3279\tLR: 2.950895\n",
      "Training Epoch: 30 [25728/50000]\tLoss: 4.3720\tLR: 2.951151\n",
      "Training Epoch: 30 [25856/50000]\tLoss: 4.3159\tLR: 2.951407\n",
      "Training Epoch: 30 [25984/50000]\tLoss: 4.3183\tLR: 2.951662\n",
      "Training Epoch: 30 [26112/50000]\tLoss: 4.3487\tLR: 2.951918\n",
      "Training Epoch: 30 [26240/50000]\tLoss: 4.3422\tLR: 2.952174\n",
      "Training Epoch: 30 [26368/50000]\tLoss: 4.4082\tLR: 2.952430\n",
      "Training Epoch: 30 [26496/50000]\tLoss: 4.4131\tLR: 2.952685\n",
      "Training Epoch: 30 [26624/50000]\tLoss: 4.3681\tLR: 2.952941\n",
      "Training Epoch: 30 [26752/50000]\tLoss: 4.1604\tLR: 2.953197\n",
      "Training Epoch: 30 [26880/50000]\tLoss: 4.2504\tLR: 2.953453\n",
      "Training Epoch: 30 [27008/50000]\tLoss: 4.3403\tLR: 2.953708\n",
      "Training Epoch: 30 [27136/50000]\tLoss: 4.2138\tLR: 2.953964\n",
      "Training Epoch: 30 [27264/50000]\tLoss: 4.3540\tLR: 2.954220\n",
      "Training Epoch: 30 [27392/50000]\tLoss: 4.3587\tLR: 2.954476\n",
      "Training Epoch: 30 [27520/50000]\tLoss: 4.4026\tLR: 2.954731\n",
      "Training Epoch: 30 [27648/50000]\tLoss: 4.3208\tLR: 2.954987\n",
      "Training Epoch: 30 [27776/50000]\tLoss: 4.3854\tLR: 2.955243\n",
      "Training Epoch: 30 [27904/50000]\tLoss: 4.3161\tLR: 2.955499\n",
      "Training Epoch: 30 [28032/50000]\tLoss: 4.2778\tLR: 2.955754\n",
      "Training Epoch: 30 [28160/50000]\tLoss: 4.2786\tLR: 2.956010\n",
      "Training Epoch: 30 [28288/50000]\tLoss: 4.3849\tLR: 2.956266\n",
      "Training Epoch: 30 [28416/50000]\tLoss: 4.2076\tLR: 2.956522\n",
      "Training Epoch: 30 [28544/50000]\tLoss: 4.5439\tLR: 2.956777\n",
      "Training Epoch: 30 [28672/50000]\tLoss: 4.2530\tLR: 2.957033\n",
      "Training Epoch: 30 [28800/50000]\tLoss: 4.3090\tLR: 2.957289\n",
      "Training Epoch: 30 [28928/50000]\tLoss: 4.3010\tLR: 2.957545\n",
      "Training Epoch: 30 [29056/50000]\tLoss: 4.2901\tLR: 2.957801\n",
      "Training Epoch: 30 [29184/50000]\tLoss: 4.2448\tLR: 2.958056\n",
      "Training Epoch: 30 [29312/50000]\tLoss: 4.3643\tLR: 2.958312\n",
      "Training Epoch: 30 [29440/50000]\tLoss: 4.3496\tLR: 2.958568\n",
      "Training Epoch: 30 [29568/50000]\tLoss: 4.2391\tLR: 2.958824\n",
      "Training Epoch: 30 [29696/50000]\tLoss: 4.2963\tLR: 2.959079\n",
      "Training Epoch: 30 [29824/50000]\tLoss: 4.2429\tLR: 2.959335\n",
      "Training Epoch: 30 [29952/50000]\tLoss: 4.6483\tLR: 2.959591\n",
      "Training Epoch: 30 [30080/50000]\tLoss: 4.3034\tLR: 2.959847\n",
      "Training Epoch: 30 [30208/50000]\tLoss: 4.4055\tLR: 2.960102\n",
      "Training Epoch: 30 [30336/50000]\tLoss: 4.3268\tLR: 2.960358\n",
      "Training Epoch: 30 [30464/50000]\tLoss: 4.4051\tLR: 2.960614\n",
      "Training Epoch: 30 [30592/50000]\tLoss: 4.2491\tLR: 2.960870\n",
      "Training Epoch: 30 [30720/50000]\tLoss: 4.5390\tLR: 2.961125\n",
      "Training Epoch: 30 [30848/50000]\tLoss: 4.2924\tLR: 2.961381\n",
      "Training Epoch: 30 [30976/50000]\tLoss: 4.3113\tLR: 2.961637\n",
      "Training Epoch: 30 [31104/50000]\tLoss: 4.2898\tLR: 2.961893\n",
      "Training Epoch: 30 [31232/50000]\tLoss: 4.5774\tLR: 2.962148\n",
      "Training Epoch: 30 [31360/50000]\tLoss: 4.4645\tLR: 2.962404\n",
      "Training Epoch: 30 [31488/50000]\tLoss: 4.3592\tLR: 2.962660\n",
      "Training Epoch: 30 [31616/50000]\tLoss: 4.5406\tLR: 2.962916\n",
      "Training Epoch: 30 [31744/50000]\tLoss: 4.4352\tLR: 2.963171\n",
      "Training Epoch: 30 [31872/50000]\tLoss: 4.5758\tLR: 2.963427\n",
      "Training Epoch: 30 [32000/50000]\tLoss: 4.4042\tLR: 2.963683\n",
      "Training Epoch: 30 [32128/50000]\tLoss: 4.4443\tLR: 2.963939\n",
      "Training Epoch: 30 [32256/50000]\tLoss: 4.5408\tLR: 2.964194\n",
      "Training Epoch: 30 [32384/50000]\tLoss: 4.4044\tLR: 2.964450\n",
      "Training Epoch: 30 [32512/50000]\tLoss: 4.4774\tLR: 2.964706\n",
      "Training Epoch: 30 [32640/50000]\tLoss: 4.4052\tLR: 2.964962\n",
      "Training Epoch: 30 [32768/50000]\tLoss: 4.3768\tLR: 2.965217\n",
      "Training Epoch: 30 [32896/50000]\tLoss: 4.3856\tLR: 2.965473\n",
      "Training Epoch: 30 [33024/50000]\tLoss: 4.4032\tLR: 2.965729\n",
      "Training Epoch: 30 [33152/50000]\tLoss: 4.3249\tLR: 2.965985\n",
      "Training Epoch: 30 [33280/50000]\tLoss: 4.3974\tLR: 2.966240\n",
      "Training Epoch: 30 [33408/50000]\tLoss: 4.4012\tLR: 2.966496\n",
      "Training Epoch: 30 [33536/50000]\tLoss: 4.3459\tLR: 2.966752\n",
      "Training Epoch: 30 [33664/50000]\tLoss: 4.3288\tLR: 2.967008\n",
      "Training Epoch: 30 [33792/50000]\tLoss: 4.3851\tLR: 2.967263\n",
      "Training Epoch: 30 [33920/50000]\tLoss: 4.4786\tLR: 2.967519\n",
      "Training Epoch: 30 [34048/50000]\tLoss: 4.4019\tLR: 2.967775\n",
      "Training Epoch: 30 [34176/50000]\tLoss: 4.4115\tLR: 2.968031\n",
      "Training Epoch: 30 [34304/50000]\tLoss: 4.4973\tLR: 2.968286\n",
      "Training Epoch: 30 [34432/50000]\tLoss: 4.3097\tLR: 2.968542\n",
      "Training Epoch: 30 [34560/50000]\tLoss: 4.4724\tLR: 2.968798\n",
      "Training Epoch: 30 [34688/50000]\tLoss: 4.2841\tLR: 2.969054\n",
      "Training Epoch: 30 [34816/50000]\tLoss: 4.3942\tLR: 2.969309\n",
      "Training Epoch: 30 [34944/50000]\tLoss: 4.2918\tLR: 2.969565\n",
      "Training Epoch: 30 [35072/50000]\tLoss: 4.3297\tLR: 2.969821\n",
      "Training Epoch: 30 [35200/50000]\tLoss: 4.2884\tLR: 2.970077\n",
      "Training Epoch: 30 [35328/50000]\tLoss: 4.3349\tLR: 2.970332\n",
      "Training Epoch: 30 [35456/50000]\tLoss: 4.4074\tLR: 2.970588\n",
      "Training Epoch: 30 [35584/50000]\tLoss: 4.4597\tLR: 2.970844\n",
      "Training Epoch: 30 [35712/50000]\tLoss: 4.4686\tLR: 2.971100\n",
      "Training Epoch: 30 [35840/50000]\tLoss: 4.4454\tLR: 2.971355\n",
      "Training Epoch: 30 [35968/50000]\tLoss: 4.3037\tLR: 2.971611\n",
      "Training Epoch: 30 [36096/50000]\tLoss: 4.2363\tLR: 2.971867\n",
      "Training Epoch: 30 [36224/50000]\tLoss: 4.5055\tLR: 2.972123\n",
      "Training Epoch: 30 [36352/50000]\tLoss: 4.3657\tLR: 2.972379\n",
      "Training Epoch: 30 [36480/50000]\tLoss: 4.4472\tLR: 2.972634\n",
      "Training Epoch: 30 [36608/50000]\tLoss: 4.4065\tLR: 2.972890\n",
      "Training Epoch: 30 [36736/50000]\tLoss: 4.3293\tLR: 2.973146\n",
      "Training Epoch: 30 [36864/50000]\tLoss: 4.3827\tLR: 2.973402\n",
      "Training Epoch: 30 [36992/50000]\tLoss: 4.1809\tLR: 2.973657\n",
      "Training Epoch: 30 [37120/50000]\tLoss: 4.2376\tLR: 2.973913\n",
      "Training Epoch: 30 [37248/50000]\tLoss: 4.3088\tLR: 2.974169\n",
      "Training Epoch: 30 [37376/50000]\tLoss: 4.3630\tLR: 2.974425\n",
      "Training Epoch: 30 [37504/50000]\tLoss: 4.3068\tLR: 2.974680\n",
      "Training Epoch: 30 [37632/50000]\tLoss: 4.3331\tLR: 2.974936\n",
      "Training Epoch: 30 [37760/50000]\tLoss: 4.3263\tLR: 2.975192\n",
      "Training Epoch: 30 [37888/50000]\tLoss: 4.4518\tLR: 2.975448\n",
      "Training Epoch: 30 [38016/50000]\tLoss: 4.3689\tLR: 2.975703\n",
      "Training Epoch: 30 [38144/50000]\tLoss: 4.3384\tLR: 2.975959\n",
      "Training Epoch: 30 [38272/50000]\tLoss: 4.3990\tLR: 2.976215\n",
      "Training Epoch: 30 [38400/50000]\tLoss: 4.2124\tLR: 2.976471\n",
      "Training Epoch: 30 [38528/50000]\tLoss: 4.4817\tLR: 2.976726\n",
      "Training Epoch: 30 [38656/50000]\tLoss: 4.2847\tLR: 2.976982\n",
      "Training Epoch: 30 [38784/50000]\tLoss: 4.2758\tLR: 2.977238\n",
      "Training Epoch: 30 [38912/50000]\tLoss: 4.3952\tLR: 2.977494\n",
      "Training Epoch: 30 [39040/50000]\tLoss: 4.3339\tLR: 2.977749\n",
      "Training Epoch: 30 [39168/50000]\tLoss: 4.3764\tLR: 2.978005\n",
      "Training Epoch: 30 [39296/50000]\tLoss: 4.2611\tLR: 2.978261\n",
      "Training Epoch: 30 [39424/50000]\tLoss: 4.3882\tLR: 2.978517\n",
      "Training Epoch: 30 [39552/50000]\tLoss: 4.4421\tLR: 2.978772\n",
      "Training Epoch: 30 [39680/50000]\tLoss: 4.3221\tLR: 2.979028\n",
      "Training Epoch: 30 [39808/50000]\tLoss: 4.3866\tLR: 2.979284\n",
      "Training Epoch: 30 [39936/50000]\tLoss: 4.2270\tLR: 2.979540\n",
      "Training Epoch: 30 [40064/50000]\tLoss: 4.4225\tLR: 2.979795\n",
      "Training Epoch: 30 [40192/50000]\tLoss: 4.2931\tLR: 2.980051\n",
      "Training Epoch: 30 [40320/50000]\tLoss: 4.1898\tLR: 2.980307\n",
      "Training Epoch: 30 [40448/50000]\tLoss: 4.3918\tLR: 2.980563\n",
      "Training Epoch: 30 [40576/50000]\tLoss: 4.3333\tLR: 2.980818\n",
      "Training Epoch: 30 [40704/50000]\tLoss: 4.2614\tLR: 2.981074\n",
      "Training Epoch: 30 [40832/50000]\tLoss: 4.3836\tLR: 2.981330\n",
      "Training Epoch: 30 [40960/50000]\tLoss: 4.3411\tLR: 2.981586\n",
      "Training Epoch: 30 [41088/50000]\tLoss: 4.4003\tLR: 2.981841\n",
      "Training Epoch: 30 [41216/50000]\tLoss: 4.3672\tLR: 2.982097\n",
      "Training Epoch: 30 [41344/50000]\tLoss: 4.3571\tLR: 2.982353\n",
      "Training Epoch: 30 [41472/50000]\tLoss: 4.3346\tLR: 2.982609\n",
      "Training Epoch: 30 [41600/50000]\tLoss: 4.2507\tLR: 2.982864\n",
      "Training Epoch: 30 [41728/50000]\tLoss: 4.3114\tLR: 2.983120\n",
      "Training Epoch: 30 [41856/50000]\tLoss: 4.2503\tLR: 2.983376\n",
      "Training Epoch: 30 [41984/50000]\tLoss: 4.2577\tLR: 2.983632\n",
      "Training Epoch: 30 [42112/50000]\tLoss: 4.3511\tLR: 2.983887\n",
      "Training Epoch: 30 [42240/50000]\tLoss: 4.3477\tLR: 2.984143\n",
      "Training Epoch: 30 [42368/50000]\tLoss: 4.3214\tLR: 2.984399\n",
      "Training Epoch: 30 [42496/50000]\tLoss: 4.2461\tLR: 2.984655\n",
      "Training Epoch: 30 [42624/50000]\tLoss: 4.3667\tLR: 2.984910\n",
      "Training Epoch: 30 [42752/50000]\tLoss: 4.4823\tLR: 2.985166\n",
      "Training Epoch: 30 [42880/50000]\tLoss: 4.3227\tLR: 2.985422\n",
      "Training Epoch: 30 [43008/50000]\tLoss: 4.4084\tLR: 2.985678\n",
      "Training Epoch: 30 [43136/50000]\tLoss: 4.2510\tLR: 2.985934\n",
      "Training Epoch: 30 [43264/50000]\tLoss: 4.2827\tLR: 2.986189\n",
      "Training Epoch: 30 [43392/50000]\tLoss: 4.2987\tLR: 2.986445\n",
      "Training Epoch: 30 [43520/50000]\tLoss: 4.3645\tLR: 2.986701\n",
      "Training Epoch: 30 [43648/50000]\tLoss: 4.4200\tLR: 2.986957\n",
      "Training Epoch: 30 [43776/50000]\tLoss: 4.2944\tLR: 2.987212\n",
      "Training Epoch: 30 [43904/50000]\tLoss: 4.3806\tLR: 2.987468\n",
      "Training Epoch: 30 [44032/50000]\tLoss: 4.3556\tLR: 2.987724\n",
      "Training Epoch: 30 [44160/50000]\tLoss: 4.2749\tLR: 2.987980\n",
      "Training Epoch: 30 [44288/50000]\tLoss: 4.3219\tLR: 2.988235\n",
      "Training Epoch: 30 [44416/50000]\tLoss: 4.4397\tLR: 2.988491\n",
      "Training Epoch: 30 [44544/50000]\tLoss: 4.2680\tLR: 2.988747\n",
      "Training Epoch: 30 [44672/50000]\tLoss: 4.3330\tLR: 2.989003\n",
      "Training Epoch: 30 [44800/50000]\tLoss: 4.3025\tLR: 2.989258\n",
      "Training Epoch: 30 [44928/50000]\tLoss: 4.2513\tLR: 2.989514\n",
      "Training Epoch: 30 [45056/50000]\tLoss: 4.3868\tLR: 2.989770\n",
      "Training Epoch: 30 [45184/50000]\tLoss: 4.4289\tLR: 2.990026\n",
      "Training Epoch: 30 [45312/50000]\tLoss: 4.3823\tLR: 2.990281\n",
      "Training Epoch: 30 [45440/50000]\tLoss: 4.4426\tLR: 2.990537\n",
      "Training Epoch: 30 [45568/50000]\tLoss: 4.3766\tLR: 2.990793\n",
      "Training Epoch: 30 [45696/50000]\tLoss: 4.4727\tLR: 2.991049\n",
      "Training Epoch: 30 [45824/50000]\tLoss: 4.4438\tLR: 2.991304\n",
      "Training Epoch: 30 [45952/50000]\tLoss: 4.5057\tLR: 2.991560\n",
      "Training Epoch: 30 [46080/50000]\tLoss: 4.6037\tLR: 2.991816\n",
      "Training Epoch: 30 [46208/50000]\tLoss: 4.5064\tLR: 2.992072\n",
      "Training Epoch: 30 [46336/50000]\tLoss: 4.5019\tLR: 2.992327\n",
      "Training Epoch: 30 [46464/50000]\tLoss: 4.4819\tLR: 2.992583\n",
      "Training Epoch: 30 [46592/50000]\tLoss: 4.4114\tLR: 2.992839\n",
      "Training Epoch: 30 [46720/50000]\tLoss: 4.3245\tLR: 2.993095\n",
      "Training Epoch: 30 [46848/50000]\tLoss: 4.3381\tLR: 2.993350\n",
      "Training Epoch: 30 [46976/50000]\tLoss: 4.3188\tLR: 2.993606\n",
      "Training Epoch: 30 [47104/50000]\tLoss: 4.3255\tLR: 2.993862\n",
      "Training Epoch: 30 [47232/50000]\tLoss: 4.3770\tLR: 2.994118\n",
      "Training Epoch: 30 [47360/50000]\tLoss: 4.3133\tLR: 2.994373\n",
      "Training Epoch: 30 [47488/50000]\tLoss: 4.1505\tLR: 2.994629\n",
      "Training Epoch: 30 [47616/50000]\tLoss: 4.2848\tLR: 2.994885\n",
      "Training Epoch: 30 [47744/50000]\tLoss: 4.4189\tLR: 2.995141\n",
      "Training Epoch: 30 [47872/50000]\tLoss: 4.4359\tLR: 2.995396\n",
      "Training Epoch: 30 [48000/50000]\tLoss: 4.2507\tLR: 2.995652\n",
      "Training Epoch: 30 [48128/50000]\tLoss: 4.2627\tLR: 2.995908\n",
      "Training Epoch: 30 [48256/50000]\tLoss: 4.4240\tLR: 2.996164\n",
      "Training Epoch: 30 [48384/50000]\tLoss: 4.3635\tLR: 2.996419\n",
      "Training Epoch: 30 [48512/50000]\tLoss: 4.4285\tLR: 2.996675\n",
      "Training Epoch: 30 [48640/50000]\tLoss: 4.2124\tLR: 2.996931\n",
      "Training Epoch: 30 [48768/50000]\tLoss: 4.2637\tLR: 2.997187\n",
      "Training Epoch: 30 [48896/50000]\tLoss: 4.4691\tLR: 2.997442\n",
      "Training Epoch: 30 [49024/50000]\tLoss: 4.3621\tLR: 2.997698\n",
      "Training Epoch: 30 [49152/50000]\tLoss: 4.1985\tLR: 2.997954\n",
      "Training Epoch: 30 [49280/50000]\tLoss: 4.4340\tLR: 2.998210\n",
      "Training Epoch: 30 [49408/50000]\tLoss: 4.3309\tLR: 2.998465\n",
      "Training Epoch: 30 [49536/50000]\tLoss: 4.2772\tLR: 2.998721\n",
      "Training Epoch: 30 [49664/50000]\tLoss: 4.2394\tLR: 2.998977\n",
      "Training Epoch: 30 [49792/50000]\tLoss: 4.4679\tLR: 2.999233\n",
      "Training Epoch: 30 [49920/50000]\tLoss: 4.4274\tLR: 2.999488\n",
      "Training Epoch: 30 [50000/50000]\tLoss: 4.5385\tLR: 2.999744\n",
      "epoch 30 training time consumed: 488.95s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   42059 GB |   42058 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   41929 GB |   41929 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     129 GB |     129 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   42059 GB |   42058 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   41929 GB |   41929 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     129 GB |     129 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   41465 GB |   41465 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   41336 GB |   41336 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     129 GB |     129 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    4459 K  |    4459 K  |\n",
      "|       from large pool |      24    |      65    |    1901 K  |    1901 K  |\n",
      "|       from small pool |     231    |     274    |    2558 K  |    2558 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    4459 K  |    4459 K  |\n",
      "|       from large pool |      24    |      65    |    1901 K  |    1901 K  |\n",
      "|       from small pool |     231    |     274    |    2558 K  |    2558 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      46    |    2584 K  |    2584 K  |\n",
      "|       from large pool |      10    |      23    |     913 K  |     913 K  |\n",
      "|       from small pool |      26    |      35    |    1670 K  |    1670 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 30, Average loss: 0.0360, Accuracy: 0.0250, Time consumed:31.13s\n",
      "\n",
      "saving weights file to checkpoint\\resnet18\\Monday_25_July_2022_16h_56m_47s\\resnet18-30-regular.pth\n",
      "Training Epoch: 31 [128/50000]\tLoss: 4.4483\tLR: 0.100000\n",
      "Training Epoch: 31 [256/50000]\tLoss: 4.3326\tLR: 3.000256\n",
      "Training Epoch: 31 [384/50000]\tLoss: 4.3318\tLR: 3.000512\n",
      "Training Epoch: 31 [512/50000]\tLoss: 4.4601\tLR: 3.000767\n",
      "Training Epoch: 31 [640/50000]\tLoss: 4.3847\tLR: 3.001023\n",
      "Training Epoch: 31 [768/50000]\tLoss: 4.3660\tLR: 3.001279\n",
      "Training Epoch: 31 [896/50000]\tLoss: 4.3995\tLR: 3.001535\n",
      "Training Epoch: 31 [1024/50000]\tLoss: 4.3456\tLR: 3.001790\n",
      "Training Epoch: 31 [1152/50000]\tLoss: 4.3637\tLR: 3.002046\n",
      "Training Epoch: 31 [1280/50000]\tLoss: 4.3583\tLR: 3.002302\n",
      "Training Epoch: 31 [1408/50000]\tLoss: 4.2087\tLR: 3.002558\n",
      "Training Epoch: 31 [1536/50000]\tLoss: 4.2757\tLR: 3.002813\n",
      "Training Epoch: 31 [1664/50000]\tLoss: 4.5367\tLR: 3.003069\n",
      "Training Epoch: 31 [1792/50000]\tLoss: 4.3895\tLR: 3.003325\n",
      "Training Epoch: 31 [1920/50000]\tLoss: 4.5057\tLR: 3.003581\n",
      "Training Epoch: 31 [2048/50000]\tLoss: 4.3991\tLR: 3.003836\n",
      "Training Epoch: 31 [2176/50000]\tLoss: 4.3527\tLR: 3.004092\n",
      "Training Epoch: 31 [2304/50000]\tLoss: 4.4270\tLR: 3.004348\n",
      "Training Epoch: 31 [2432/50000]\tLoss: 4.3983\tLR: 3.004604\n",
      "Training Epoch: 31 [2560/50000]\tLoss: 4.3350\tLR: 3.004859\n",
      "Training Epoch: 31 [2688/50000]\tLoss: 4.4131\tLR: 3.005115\n",
      "Training Epoch: 31 [2816/50000]\tLoss: 4.3948\tLR: 3.005371\n",
      "Training Epoch: 31 [2944/50000]\tLoss: 4.4163\tLR: 3.005627\n",
      "Training Epoch: 31 [3072/50000]\tLoss: 4.3107\tLR: 3.005882\n",
      "Training Epoch: 31 [3200/50000]\tLoss: 4.3629\tLR: 3.006138\n",
      "Training Epoch: 31 [3328/50000]\tLoss: 4.4086\tLR: 3.006394\n",
      "Training Epoch: 31 [3456/50000]\tLoss: 4.3546\tLR: 3.006650\n",
      "Training Epoch: 31 [3584/50000]\tLoss: 4.4760\tLR: 3.006905\n",
      "Training Epoch: 31 [3712/50000]\tLoss: 4.4830\tLR: 3.007161\n",
      "Training Epoch: 31 [3840/50000]\tLoss: 4.3857\tLR: 3.007417\n",
      "Training Epoch: 31 [3968/50000]\tLoss: 4.4117\tLR: 3.007673\n",
      "Training Epoch: 31 [4096/50000]\tLoss: 4.3061\tLR: 3.007928\n",
      "Training Epoch: 31 [4224/50000]\tLoss: 4.3596\tLR: 3.008184\n",
      "Training Epoch: 31 [4352/50000]\tLoss: 4.4923\tLR: 3.008440\n",
      "Training Epoch: 31 [4480/50000]\tLoss: 4.3332\tLR: 3.008696\n",
      "Training Epoch: 31 [4608/50000]\tLoss: 4.3850\tLR: 3.008951\n",
      "Training Epoch: 31 [4736/50000]\tLoss: 4.3979\tLR: 3.009207\n",
      "Training Epoch: 31 [4864/50000]\tLoss: 4.4102\tLR: 3.009463\n",
      "Training Epoch: 31 [4992/50000]\tLoss: 4.5021\tLR: 3.009719\n",
      "Training Epoch: 31 [5120/50000]\tLoss: 4.3659\tLR: 3.009974\n",
      "Training Epoch: 31 [5248/50000]\tLoss: 4.2562\tLR: 3.010230\n",
      "Training Epoch: 31 [5376/50000]\tLoss: 4.2872\tLR: 3.010486\n",
      "Training Epoch: 31 [5504/50000]\tLoss: 4.3641\tLR: 3.010742\n",
      "Training Epoch: 31 [5632/50000]\tLoss: 4.2776\tLR: 3.010997\n",
      "Training Epoch: 31 [5760/50000]\tLoss: 4.4959\tLR: 3.011253\n",
      "Training Epoch: 31 [5888/50000]\tLoss: 4.3738\tLR: 3.011509\n",
      "Training Epoch: 31 [6016/50000]\tLoss: 4.3725\tLR: 3.011765\n",
      "Training Epoch: 31 [6144/50000]\tLoss: 4.3727\tLR: 3.012020\n",
      "Training Epoch: 31 [6272/50000]\tLoss: 4.3642\tLR: 3.012276\n",
      "Training Epoch: 31 [6400/50000]\tLoss: 4.3343\tLR: 3.012532\n",
      "Training Epoch: 31 [6528/50000]\tLoss: 4.3123\tLR: 3.012788\n",
      "Training Epoch: 31 [6656/50000]\tLoss: 4.3404\tLR: 3.013043\n",
      "Training Epoch: 31 [6784/50000]\tLoss: 4.3841\tLR: 3.013299\n",
      "Training Epoch: 31 [6912/50000]\tLoss: 4.4166\tLR: 3.013555\n",
      "Training Epoch: 31 [7040/50000]\tLoss: 4.2290\tLR: 3.013811\n",
      "Training Epoch: 31 [7168/50000]\tLoss: 4.3014\tLR: 3.014066\n",
      "Training Epoch: 31 [7296/50000]\tLoss: 4.3990\tLR: 3.014322\n",
      "Training Epoch: 31 [7424/50000]\tLoss: 4.2976\tLR: 3.014578\n",
      "Training Epoch: 31 [7552/50000]\tLoss: 4.4192\tLR: 3.014834\n",
      "Training Epoch: 31 [7680/50000]\tLoss: 4.3344\tLR: 3.015090\n",
      "Training Epoch: 31 [7808/50000]\tLoss: 4.2016\tLR: 3.015345\n",
      "Training Epoch: 31 [7936/50000]\tLoss: 4.5369\tLR: 3.015601\n",
      "Training Epoch: 31 [8064/50000]\tLoss: 4.3613\tLR: 3.015857\n",
      "Training Epoch: 31 [8192/50000]\tLoss: 4.2710\tLR: 3.016113\n",
      "Training Epoch: 31 [8320/50000]\tLoss: 4.4276\tLR: 3.016368\n",
      "Training Epoch: 31 [8448/50000]\tLoss: 4.3364\tLR: 3.016624\n",
      "Training Epoch: 31 [8576/50000]\tLoss: 4.3873\tLR: 3.016880\n",
      "Training Epoch: 31 [8704/50000]\tLoss: 4.4764\tLR: 3.017136\n",
      "Training Epoch: 31 [8832/50000]\tLoss: 4.4153\tLR: 3.017391\n",
      "Training Epoch: 31 [8960/50000]\tLoss: 4.3779\tLR: 3.017647\n",
      "Training Epoch: 31 [9088/50000]\tLoss: 4.2402\tLR: 3.017903\n",
      "Training Epoch: 31 [9216/50000]\tLoss: 4.3440\tLR: 3.018159\n",
      "Training Epoch: 31 [9344/50000]\tLoss: 4.2060\tLR: 3.018414\n",
      "Training Epoch: 31 [9472/50000]\tLoss: 4.5737\tLR: 3.018670\n",
      "Training Epoch: 31 [9600/50000]\tLoss: 4.3572\tLR: 3.018926\n",
      "Training Epoch: 31 [9728/50000]\tLoss: 4.4042\tLR: 3.019182\n",
      "Training Epoch: 31 [9856/50000]\tLoss: 4.3081\tLR: 3.019437\n",
      "Training Epoch: 31 [9984/50000]\tLoss: 4.2790\tLR: 3.019693\n",
      "Training Epoch: 31 [10112/50000]\tLoss: 4.3477\tLR: 3.019949\n",
      "Training Epoch: 31 [10240/50000]\tLoss: 4.2304\tLR: 3.020205\n",
      "Training Epoch: 31 [10368/50000]\tLoss: 4.3015\tLR: 3.020460\n",
      "Training Epoch: 31 [10496/50000]\tLoss: 4.3401\tLR: 3.020716\n",
      "Training Epoch: 31 [10624/50000]\tLoss: 4.4282\tLR: 3.020972\n",
      "Training Epoch: 31 [10752/50000]\tLoss: 4.4140\tLR: 3.021228\n",
      "Training Epoch: 31 [10880/50000]\tLoss: 4.6332\tLR: 3.021483\n",
      "Training Epoch: 31 [11008/50000]\tLoss: 4.4080\tLR: 3.021739\n",
      "Training Epoch: 31 [11136/50000]\tLoss: 4.4027\tLR: 3.021995\n",
      "Training Epoch: 31 [11264/50000]\tLoss: 4.4216\tLR: 3.022251\n",
      "Training Epoch: 31 [11392/50000]\tLoss: 4.5076\tLR: 3.022506\n",
      "Training Epoch: 31 [11520/50000]\tLoss: 4.3823\tLR: 3.022762\n",
      "Training Epoch: 31 [11648/50000]\tLoss: 4.3548\tLR: 3.023018\n",
      "Training Epoch: 31 [11776/50000]\tLoss: 4.4298\tLR: 3.023274\n",
      "Training Epoch: 31 [11904/50000]\tLoss: 4.3138\tLR: 3.023529\n",
      "Training Epoch: 31 [12032/50000]\tLoss: 4.3288\tLR: 3.023785\n",
      "Training Epoch: 31 [12160/50000]\tLoss: 4.4054\tLR: 3.024041\n",
      "Training Epoch: 31 [12288/50000]\tLoss: 4.3469\tLR: 3.024297\n",
      "Training Epoch: 31 [12416/50000]\tLoss: 4.3652\tLR: 3.024552\n",
      "Training Epoch: 31 [12544/50000]\tLoss: 4.3267\tLR: 3.024808\n",
      "Training Epoch: 31 [12672/50000]\tLoss: 4.3635\tLR: 3.025064\n",
      "Training Epoch: 31 [12800/50000]\tLoss: 4.2352\tLR: 3.025320\n",
      "Training Epoch: 31 [12928/50000]\tLoss: 4.4379\tLR: 3.025575\n",
      "Training Epoch: 31 [13056/50000]\tLoss: 4.4835\tLR: 3.025831\n",
      "Training Epoch: 31 [13184/50000]\tLoss: 4.3664\tLR: 3.026087\n",
      "Training Epoch: 31 [13312/50000]\tLoss: 4.4675\tLR: 3.026343\n",
      "Training Epoch: 31 [13440/50000]\tLoss: 4.4296\tLR: 3.026598\n",
      "Training Epoch: 31 [13568/50000]\tLoss: 4.3126\tLR: 3.026854\n",
      "Training Epoch: 31 [13696/50000]\tLoss: 4.2184\tLR: 3.027110\n",
      "Training Epoch: 31 [13824/50000]\tLoss: 4.3401\tLR: 3.027366\n",
      "Training Epoch: 31 [13952/50000]\tLoss: 4.3791\tLR: 3.027621\n",
      "Training Epoch: 31 [14080/50000]\tLoss: 4.5051\tLR: 3.027877\n",
      "Training Epoch: 31 [14208/50000]\tLoss: 4.5682\tLR: 3.028133\n",
      "Training Epoch: 31 [14336/50000]\tLoss: 4.3849\tLR: 3.028389\n",
      "Training Epoch: 31 [14464/50000]\tLoss: 4.4735\tLR: 3.028645\n",
      "Training Epoch: 31 [14592/50000]\tLoss: 4.3903\tLR: 3.028900\n",
      "Training Epoch: 31 [14720/50000]\tLoss: 4.5017\tLR: 3.029156\n",
      "Training Epoch: 31 [14848/50000]\tLoss: 4.3179\tLR: 3.029412\n",
      "Training Epoch: 31 [14976/50000]\tLoss: 4.7040\tLR: 3.029668\n",
      "Training Epoch: 31 [15104/50000]\tLoss: 4.5381\tLR: 3.029923\n",
      "Training Epoch: 31 [15232/50000]\tLoss: 4.3471\tLR: 3.030179\n",
      "Training Epoch: 31 [15360/50000]\tLoss: 4.4410\tLR: 3.030435\n",
      "Training Epoch: 31 [15488/50000]\tLoss: 4.3859\tLR: 3.030691\n",
      "Training Epoch: 31 [15616/50000]\tLoss: 4.3393\tLR: 3.030946\n",
      "Training Epoch: 31 [15744/50000]\tLoss: 4.3851\tLR: 3.031202\n",
      "Training Epoch: 31 [15872/50000]\tLoss: 4.3323\tLR: 3.031458\n",
      "Training Epoch: 31 [16000/50000]\tLoss: 4.5379\tLR: 3.031714\n",
      "Training Epoch: 31 [16128/50000]\tLoss: 4.3397\tLR: 3.031969\n",
      "Training Epoch: 31 [16256/50000]\tLoss: 4.3261\tLR: 3.032225\n",
      "Training Epoch: 31 [16384/50000]\tLoss: 4.3192\tLR: 3.032481\n",
      "Training Epoch: 31 [16512/50000]\tLoss: 4.2105\tLR: 3.032737\n",
      "Training Epoch: 31 [16640/50000]\tLoss: 4.4869\tLR: 3.032992\n",
      "Training Epoch: 31 [16768/50000]\tLoss: 4.4337\tLR: 3.033248\n",
      "Training Epoch: 31 [16896/50000]\tLoss: 4.5656\tLR: 3.033504\n",
      "Training Epoch: 31 [17024/50000]\tLoss: 4.4921\tLR: 3.033760\n",
      "Training Epoch: 31 [17152/50000]\tLoss: 4.5753\tLR: 3.034015\n",
      "Training Epoch: 31 [17280/50000]\tLoss: 4.4777\tLR: 3.034271\n",
      "Training Epoch: 31 [17408/50000]\tLoss: 4.4569\tLR: 3.034527\n",
      "Training Epoch: 31 [17536/50000]\tLoss: 4.5286\tLR: 3.034783\n",
      "Training Epoch: 31 [17664/50000]\tLoss: 4.4015\tLR: 3.035038\n",
      "Training Epoch: 31 [17792/50000]\tLoss: 4.4313\tLR: 3.035294\n",
      "Training Epoch: 31 [17920/50000]\tLoss: 4.4727\tLR: 3.035550\n",
      "Training Epoch: 31 [18048/50000]\tLoss: 4.5283\tLR: 3.035806\n",
      "Training Epoch: 31 [18176/50000]\tLoss: 4.5081\tLR: 3.036061\n",
      "Training Epoch: 31 [18304/50000]\tLoss: 4.4869\tLR: 3.036317\n",
      "Training Epoch: 31 [18432/50000]\tLoss: 4.4079\tLR: 3.036573\n",
      "Training Epoch: 31 [18560/50000]\tLoss: 4.5643\tLR: 3.036829\n",
      "Training Epoch: 31 [18688/50000]\tLoss: 4.5165\tLR: 3.037084\n",
      "Training Epoch: 31 [18816/50000]\tLoss: 4.6267\tLR: 3.037340\n",
      "Training Epoch: 31 [18944/50000]\tLoss: 4.3903\tLR: 3.037596\n",
      "Training Epoch: 31 [19072/50000]\tLoss: 4.3904\tLR: 3.037852\n",
      "Training Epoch: 31 [19200/50000]\tLoss: 4.4044\tLR: 3.038107\n",
      "Training Epoch: 31 [19328/50000]\tLoss: 4.5577\tLR: 3.038363\n",
      "Training Epoch: 31 [19456/50000]\tLoss: 4.6622\tLR: 3.038619\n",
      "Training Epoch: 31 [19584/50000]\tLoss: 4.5119\tLR: 3.038875\n",
      "Training Epoch: 31 [19712/50000]\tLoss: 4.5084\tLR: 3.039130\n",
      "Training Epoch: 31 [19840/50000]\tLoss: 4.3000\tLR: 3.039386\n",
      "Training Epoch: 31 [19968/50000]\tLoss: 4.3727\tLR: 3.039642\n",
      "Training Epoch: 31 [20096/50000]\tLoss: 4.2998\tLR: 3.039898\n",
      "Training Epoch: 31 [20224/50000]\tLoss: 4.3603\tLR: 3.040153\n",
      "Training Epoch: 31 [20352/50000]\tLoss: 4.2603\tLR: 3.040409\n",
      "Training Epoch: 31 [20480/50000]\tLoss: 4.3731\tLR: 3.040665\n",
      "Training Epoch: 31 [20608/50000]\tLoss: 4.2552\tLR: 3.040921\n",
      "Training Epoch: 31 [20736/50000]\tLoss: 4.4394\tLR: 3.041176\n",
      "Training Epoch: 31 [20864/50000]\tLoss: 4.3756\tLR: 3.041432\n",
      "Training Epoch: 31 [20992/50000]\tLoss: 4.3595\tLR: 3.041688\n",
      "Training Epoch: 31 [21120/50000]\tLoss: 4.3065\tLR: 3.041944\n",
      "Training Epoch: 31 [21248/50000]\tLoss: 4.3860\tLR: 3.042199\n",
      "Training Epoch: 31 [21376/50000]\tLoss: 4.5262\tLR: 3.042455\n",
      "Training Epoch: 31 [21504/50000]\tLoss: 4.3980\tLR: 3.042711\n",
      "Training Epoch: 31 [21632/50000]\tLoss: 4.4213\tLR: 3.042967\n",
      "Training Epoch: 31 [21760/50000]\tLoss: 4.2714\tLR: 3.043223\n",
      "Training Epoch: 31 [21888/50000]\tLoss: 4.4389\tLR: 3.043478\n",
      "Training Epoch: 31 [22016/50000]\tLoss: 4.5577\tLR: 3.043734\n",
      "Training Epoch: 31 [22144/50000]\tLoss: 4.3453\tLR: 3.043990\n",
      "Training Epoch: 31 [22272/50000]\tLoss: 4.4895\tLR: 3.044246\n",
      "Training Epoch: 31 [22400/50000]\tLoss: 4.3356\tLR: 3.044501\n",
      "Training Epoch: 31 [22528/50000]\tLoss: 4.3797\tLR: 3.044757\n",
      "Training Epoch: 31 [22656/50000]\tLoss: 4.2882\tLR: 3.045013\n",
      "Training Epoch: 31 [22784/50000]\tLoss: 4.4620\tLR: 3.045269\n",
      "Training Epoch: 31 [22912/50000]\tLoss: 4.3001\tLR: 3.045524\n",
      "Training Epoch: 31 [23040/50000]\tLoss: 4.1968\tLR: 3.045780\n",
      "Training Epoch: 31 [23168/50000]\tLoss: 4.4622\tLR: 3.046036\n",
      "Training Epoch: 31 [23296/50000]\tLoss: 4.4010\tLR: 3.046292\n",
      "Training Epoch: 31 [23424/50000]\tLoss: 4.3495\tLR: 3.046547\n",
      "Training Epoch: 31 [23552/50000]\tLoss: 4.3654\tLR: 3.046803\n",
      "Training Epoch: 31 [23680/50000]\tLoss: 4.3560\tLR: 3.047059\n",
      "Training Epoch: 31 [23808/50000]\tLoss: 4.3649\tLR: 3.047315\n",
      "Training Epoch: 31 [23936/50000]\tLoss: 4.4325\tLR: 3.047570\n",
      "Training Epoch: 31 [24064/50000]\tLoss: 4.2144\tLR: 3.047826\n",
      "Training Epoch: 31 [24192/50000]\tLoss: 4.3478\tLR: 3.048082\n",
      "Training Epoch: 31 [24320/50000]\tLoss: 4.3467\tLR: 3.048338\n",
      "Training Epoch: 31 [24448/50000]\tLoss: 4.3480\tLR: 3.048593\n",
      "Training Epoch: 31 [24576/50000]\tLoss: 4.3821\tLR: 3.048849\n",
      "Training Epoch: 31 [24704/50000]\tLoss: 4.3439\tLR: 3.049105\n",
      "Training Epoch: 31 [24832/50000]\tLoss: 4.4132\tLR: 3.049361\n",
      "Training Epoch: 31 [24960/50000]\tLoss: 4.3318\tLR: 3.049616\n",
      "Training Epoch: 31 [25088/50000]\tLoss: 4.2700\tLR: 3.049872\n",
      "Training Epoch: 31 [25216/50000]\tLoss: 4.3013\tLR: 3.050128\n",
      "Training Epoch: 31 [25344/50000]\tLoss: 4.1190\tLR: 3.050384\n",
      "Training Epoch: 31 [25472/50000]\tLoss: 4.3903\tLR: 3.050639\n",
      "Training Epoch: 31 [25600/50000]\tLoss: 4.1303\tLR: 3.050895\n",
      "Training Epoch: 31 [25728/50000]\tLoss: 4.1138\tLR: 3.051151\n",
      "Training Epoch: 31 [25856/50000]\tLoss: 4.2047\tLR: 3.051407\n",
      "Training Epoch: 31 [25984/50000]\tLoss: 4.4762\tLR: 3.051662\n",
      "Training Epoch: 31 [26112/50000]\tLoss: 4.5211\tLR: 3.051918\n",
      "Training Epoch: 31 [26240/50000]\tLoss: 4.4025\tLR: 3.052174\n",
      "Training Epoch: 31 [26368/50000]\tLoss: 4.4631\tLR: 3.052430\n",
      "Training Epoch: 31 [26496/50000]\tLoss: 4.4654\tLR: 3.052685\n",
      "Training Epoch: 31 [26624/50000]\tLoss: 4.4511\tLR: 3.052941\n",
      "Training Epoch: 31 [26752/50000]\tLoss: 4.4437\tLR: 3.053197\n",
      "Training Epoch: 31 [26880/50000]\tLoss: 4.3346\tLR: 3.053453\n",
      "Training Epoch: 31 [27008/50000]\tLoss: 4.4104\tLR: 3.053708\n",
      "Training Epoch: 31 [27136/50000]\tLoss: 4.3987\tLR: 3.053964\n",
      "Training Epoch: 31 [27264/50000]\tLoss: 4.2791\tLR: 3.054220\n",
      "Training Epoch: 31 [27392/50000]\tLoss: 4.3968\tLR: 3.054476\n",
      "Training Epoch: 31 [27520/50000]\tLoss: 4.3540\tLR: 3.054731\n",
      "Training Epoch: 31 [27648/50000]\tLoss: 4.3429\tLR: 3.054987\n",
      "Training Epoch: 31 [27776/50000]\tLoss: 4.4739\tLR: 3.055243\n",
      "Training Epoch: 31 [27904/50000]\tLoss: 4.4150\tLR: 3.055499\n",
      "Training Epoch: 31 [28032/50000]\tLoss: 4.3497\tLR: 3.055754\n",
      "Training Epoch: 31 [28160/50000]\tLoss: 4.4128\tLR: 3.056010\n",
      "Training Epoch: 31 [28288/50000]\tLoss: 4.4618\tLR: 3.056266\n",
      "Training Epoch: 31 [28416/50000]\tLoss: 4.3627\tLR: 3.056522\n",
      "Training Epoch: 31 [28544/50000]\tLoss: 4.2799\tLR: 3.056777\n",
      "Training Epoch: 31 [28672/50000]\tLoss: 4.3244\tLR: 3.057033\n",
      "Training Epoch: 31 [28800/50000]\tLoss: 4.4657\tLR: 3.057289\n",
      "Training Epoch: 31 [28928/50000]\tLoss: 4.4927\tLR: 3.057545\n",
      "Training Epoch: 31 [29056/50000]\tLoss: 4.4856\tLR: 3.057801\n",
      "Training Epoch: 31 [29184/50000]\tLoss: 4.5889\tLR: 3.058056\n",
      "Training Epoch: 31 [29312/50000]\tLoss: 4.6124\tLR: 3.058312\n",
      "Training Epoch: 31 [29440/50000]\tLoss: 4.4906\tLR: 3.058568\n",
      "Training Epoch: 31 [29568/50000]\tLoss: 4.4617\tLR: 3.058824\n",
      "Training Epoch: 31 [29696/50000]\tLoss: 4.4525\tLR: 3.059079\n",
      "Training Epoch: 31 [29824/50000]\tLoss: 4.2732\tLR: 3.059335\n",
      "Training Epoch: 31 [29952/50000]\tLoss: 4.5595\tLR: 3.059591\n",
      "Training Epoch: 31 [30080/50000]\tLoss: 4.4119\tLR: 3.059847\n",
      "Training Epoch: 31 [30208/50000]\tLoss: 4.4109\tLR: 3.060102\n",
      "Training Epoch: 31 [30336/50000]\tLoss: 4.4545\tLR: 3.060358\n",
      "Training Epoch: 31 [30464/50000]\tLoss: 4.4095\tLR: 3.060614\n",
      "Training Epoch: 31 [30592/50000]\tLoss: 4.5235\tLR: 3.060870\n",
      "Training Epoch: 31 [30720/50000]\tLoss: 4.4481\tLR: 3.061125\n",
      "Training Epoch: 31 [30848/50000]\tLoss: 4.4357\tLR: 3.061381\n",
      "Training Epoch: 31 [30976/50000]\tLoss: 4.4268\tLR: 3.061637\n",
      "Training Epoch: 31 [31104/50000]\tLoss: 4.4007\tLR: 3.061893\n",
      "Training Epoch: 31 [31232/50000]\tLoss: 4.3264\tLR: 3.062148\n",
      "Training Epoch: 31 [31360/50000]\tLoss: 4.3368\tLR: 3.062404\n",
      "Training Epoch: 31 [31488/50000]\tLoss: 4.4643\tLR: 3.062660\n",
      "Training Epoch: 31 [31616/50000]\tLoss: 4.2970\tLR: 3.062916\n",
      "Training Epoch: 31 [31744/50000]\tLoss: 4.3576\tLR: 3.063171\n",
      "Training Epoch: 31 [31872/50000]\tLoss: 4.3782\tLR: 3.063427\n",
      "Training Epoch: 31 [32000/50000]\tLoss: 4.4090\tLR: 3.063683\n",
      "Training Epoch: 31 [32128/50000]\tLoss: 4.4675\tLR: 3.063939\n",
      "Training Epoch: 31 [32256/50000]\tLoss: 4.3790\tLR: 3.064194\n",
      "Training Epoch: 31 [32384/50000]\tLoss: 4.5450\tLR: 3.064450\n",
      "Training Epoch: 31 [32512/50000]\tLoss: 4.3279\tLR: 3.064706\n",
      "Training Epoch: 31 [32640/50000]\tLoss: 4.4221\tLR: 3.064962\n",
      "Training Epoch: 31 [32768/50000]\tLoss: 4.5010\tLR: 3.065217\n",
      "Training Epoch: 31 [32896/50000]\tLoss: 4.2984\tLR: 3.065473\n",
      "Training Epoch: 31 [33024/50000]\tLoss: 4.5465\tLR: 3.065729\n",
      "Training Epoch: 31 [33152/50000]\tLoss: 4.6335\tLR: 3.065985\n",
      "Training Epoch: 31 [33280/50000]\tLoss: 4.4046\tLR: 3.066240\n",
      "Training Epoch: 31 [33408/50000]\tLoss: 4.3901\tLR: 3.066496\n",
      "Training Epoch: 31 [33536/50000]\tLoss: 4.4308\tLR: 3.066752\n",
      "Training Epoch: 31 [33664/50000]\tLoss: 4.4608\tLR: 3.067008\n",
      "Training Epoch: 31 [33792/50000]\tLoss: 4.4428\tLR: 3.067263\n",
      "Training Epoch: 31 [33920/50000]\tLoss: 4.5768\tLR: 3.067519\n",
      "Training Epoch: 31 [34048/50000]\tLoss: 4.4168\tLR: 3.067775\n",
      "Training Epoch: 31 [34176/50000]\tLoss: 4.5851\tLR: 3.068031\n",
      "Training Epoch: 31 [34304/50000]\tLoss: 4.4039\tLR: 3.068286\n",
      "Training Epoch: 31 [34432/50000]\tLoss: 4.4384\tLR: 3.068542\n",
      "Training Epoch: 31 [34560/50000]\tLoss: 4.3311\tLR: 3.068798\n",
      "Training Epoch: 31 [34688/50000]\tLoss: 4.5303\tLR: 3.069054\n",
      "Training Epoch: 31 [34816/50000]\tLoss: 4.4237\tLR: 3.069309\n",
      "Training Epoch: 31 [34944/50000]\tLoss: 4.3651\tLR: 3.069565\n",
      "Training Epoch: 31 [35072/50000]\tLoss: 4.4418\tLR: 3.069821\n",
      "Training Epoch: 31 [35200/50000]\tLoss: 4.3725\tLR: 3.070077\n",
      "Training Epoch: 31 [35328/50000]\tLoss: 4.3333\tLR: 3.070332\n",
      "Training Epoch: 31 [35456/50000]\tLoss: 4.3513\tLR: 3.070588\n",
      "Training Epoch: 31 [35584/50000]\tLoss: 4.2428\tLR: 3.070844\n",
      "Training Epoch: 31 [35712/50000]\tLoss: 4.4294\tLR: 3.071100\n",
      "Training Epoch: 31 [35840/50000]\tLoss: 4.5210\tLR: 3.071355\n",
      "Training Epoch: 31 [35968/50000]\tLoss: 4.5335\tLR: 3.071611\n",
      "Training Epoch: 31 [36096/50000]\tLoss: 4.4473\tLR: 3.071867\n",
      "Training Epoch: 31 [36224/50000]\tLoss: 4.4244\tLR: 3.072123\n",
      "Training Epoch: 31 [36352/50000]\tLoss: 4.3825\tLR: 3.072379\n",
      "Training Epoch: 31 [36480/50000]\tLoss: 4.3065\tLR: 3.072634\n",
      "Training Epoch: 31 [36608/50000]\tLoss: 4.2816\tLR: 3.072890\n",
      "Training Epoch: 31 [36736/50000]\tLoss: 4.4393\tLR: 3.073146\n",
      "Training Epoch: 31 [36864/50000]\tLoss: 4.4762\tLR: 3.073402\n",
      "Training Epoch: 31 [36992/50000]\tLoss: 4.4167\tLR: 3.073657\n",
      "Training Epoch: 31 [37120/50000]\tLoss: 4.4596\tLR: 3.073913\n",
      "Training Epoch: 31 [37248/50000]\tLoss: 4.5687\tLR: 3.074169\n",
      "Training Epoch: 31 [37376/50000]\tLoss: 4.3936\tLR: 3.074425\n",
      "Training Epoch: 31 [37504/50000]\tLoss: 4.5149\tLR: 3.074680\n",
      "Training Epoch: 31 [37632/50000]\tLoss: 4.4943\tLR: 3.074936\n",
      "Training Epoch: 31 [37760/50000]\tLoss: 4.5137\tLR: 3.075192\n",
      "Training Epoch: 31 [37888/50000]\tLoss: 4.4245\tLR: 3.075448\n",
      "Training Epoch: 31 [38016/50000]\tLoss: 4.3286\tLR: 3.075703\n",
      "Training Epoch: 31 [38144/50000]\tLoss: 4.4894\tLR: 3.075959\n",
      "Training Epoch: 31 [38272/50000]\tLoss: 4.3924\tLR: 3.076215\n",
      "Training Epoch: 31 [38400/50000]\tLoss: 4.3787\tLR: 3.076471\n",
      "Training Epoch: 31 [38528/50000]\tLoss: 4.4876\tLR: 3.076726\n",
      "Training Epoch: 31 [38656/50000]\tLoss: 4.4826\tLR: 3.076982\n",
      "Training Epoch: 31 [38784/50000]\tLoss: 4.4706\tLR: 3.077238\n",
      "Training Epoch: 31 [38912/50000]\tLoss: 4.4344\tLR: 3.077494\n",
      "Training Epoch: 31 [39040/50000]\tLoss: 4.3014\tLR: 3.077749\n",
      "Training Epoch: 31 [39168/50000]\tLoss: 4.4965\tLR: 3.078005\n",
      "Training Epoch: 31 [39296/50000]\tLoss: 4.2955\tLR: 3.078261\n",
      "Training Epoch: 31 [39424/50000]\tLoss: 4.4356\tLR: 3.078517\n",
      "Training Epoch: 31 [39552/50000]\tLoss: 4.5377\tLR: 3.078772\n",
      "Training Epoch: 31 [39680/50000]\tLoss: 4.4174\tLR: 3.079028\n",
      "Training Epoch: 31 [39808/50000]\tLoss: 4.4176\tLR: 3.079284\n",
      "Training Epoch: 31 [39936/50000]\tLoss: 4.4081\tLR: 3.079540\n",
      "Training Epoch: 31 [40064/50000]\tLoss: 4.5273\tLR: 3.079795\n",
      "Training Epoch: 31 [40192/50000]\tLoss: 4.3995\tLR: 3.080051\n",
      "Training Epoch: 31 [40320/50000]\tLoss: 4.4039\tLR: 3.080307\n",
      "Training Epoch: 31 [40448/50000]\tLoss: 4.3923\tLR: 3.080563\n",
      "Training Epoch: 31 [40576/50000]\tLoss: 4.5540\tLR: 3.080818\n",
      "Training Epoch: 31 [40704/50000]\tLoss: 4.2949\tLR: 3.081074\n",
      "Training Epoch: 31 [40832/50000]\tLoss: 4.4443\tLR: 3.081330\n",
      "Training Epoch: 31 [40960/50000]\tLoss: 4.2979\tLR: 3.081586\n",
      "Training Epoch: 31 [41088/50000]\tLoss: 4.4575\tLR: 3.081841\n",
      "Training Epoch: 31 [41216/50000]\tLoss: 4.5488\tLR: 3.082097\n",
      "Training Epoch: 31 [41344/50000]\tLoss: 4.3815\tLR: 3.082353\n",
      "Training Epoch: 31 [41472/50000]\tLoss: 4.4808\tLR: 3.082609\n",
      "Training Epoch: 31 [41600/50000]\tLoss: 4.4179\tLR: 3.082864\n",
      "Training Epoch: 31 [41728/50000]\tLoss: 4.4217\tLR: 3.083120\n",
      "Training Epoch: 31 [41856/50000]\tLoss: 4.3123\tLR: 3.083376\n",
      "Training Epoch: 31 [41984/50000]\tLoss: 4.4111\tLR: 3.083632\n",
      "Training Epoch: 31 [42112/50000]\tLoss: 4.4782\tLR: 3.083887\n",
      "Training Epoch: 31 [42240/50000]\tLoss: 4.3457\tLR: 3.084143\n",
      "Training Epoch: 31 [42368/50000]\tLoss: 4.3142\tLR: 3.084399\n",
      "Training Epoch: 31 [42496/50000]\tLoss: 4.4617\tLR: 3.084655\n",
      "Training Epoch: 31 [42624/50000]\tLoss: 4.2328\tLR: 3.084910\n",
      "Training Epoch: 31 [42752/50000]\tLoss: 4.3648\tLR: 3.085166\n",
      "Training Epoch: 31 [42880/50000]\tLoss: 4.4030\tLR: 3.085422\n",
      "Training Epoch: 31 [43008/50000]\tLoss: 4.4018\tLR: 3.085678\n",
      "Training Epoch: 31 [43136/50000]\tLoss: 4.4103\tLR: 3.085934\n",
      "Training Epoch: 31 [43264/50000]\tLoss: 4.4180\tLR: 3.086189\n",
      "Training Epoch: 31 [43392/50000]\tLoss: 4.1366\tLR: 3.086445\n",
      "Training Epoch: 31 [43520/50000]\tLoss: 4.2711\tLR: 3.086701\n",
      "Training Epoch: 31 [43648/50000]\tLoss: 4.3434\tLR: 3.086957\n",
      "Training Epoch: 31 [43776/50000]\tLoss: 4.5066\tLR: 3.087212\n",
      "Training Epoch: 31 [43904/50000]\tLoss: 4.5116\tLR: 3.087468\n",
      "Training Epoch: 31 [44032/50000]\tLoss: 4.3794\tLR: 3.087724\n",
      "Training Epoch: 31 [44160/50000]\tLoss: 4.3351\tLR: 3.087980\n",
      "Training Epoch: 31 [44288/50000]\tLoss: 4.2825\tLR: 3.088235\n",
      "Training Epoch: 31 [44416/50000]\tLoss: 4.4282\tLR: 3.088491\n",
      "Training Epoch: 31 [44544/50000]\tLoss: 4.3997\tLR: 3.088747\n",
      "Training Epoch: 31 [44672/50000]\tLoss: 4.3578\tLR: 3.089003\n",
      "Training Epoch: 31 [44800/50000]\tLoss: 4.3709\tLR: 3.089258\n",
      "Training Epoch: 31 [44928/50000]\tLoss: 4.4351\tLR: 3.089514\n",
      "Training Epoch: 31 [45056/50000]\tLoss: 4.2927\tLR: 3.089770\n",
      "Training Epoch: 31 [45184/50000]\tLoss: 4.4215\tLR: 3.090026\n",
      "Training Epoch: 31 [45312/50000]\tLoss: 4.5173\tLR: 3.090281\n",
      "Training Epoch: 31 [45440/50000]\tLoss: 4.5066\tLR: 3.090537\n",
      "Training Epoch: 31 [45568/50000]\tLoss: 4.4988\tLR: 3.090793\n",
      "Training Epoch: 31 [45696/50000]\tLoss: 4.4236\tLR: 3.091049\n",
      "Training Epoch: 31 [45824/50000]\tLoss: 4.5890\tLR: 3.091304\n",
      "Training Epoch: 31 [45952/50000]\tLoss: 4.4343\tLR: 3.091560\n",
      "Training Epoch: 31 [46080/50000]\tLoss: 4.3344\tLR: 3.091816\n",
      "Training Epoch: 31 [46208/50000]\tLoss: 4.6842\tLR: 3.092072\n",
      "Training Epoch: 31 [46336/50000]\tLoss: 4.3657\tLR: 3.092327\n",
      "Training Epoch: 31 [46464/50000]\tLoss: 4.4308\tLR: 3.092583\n",
      "Training Epoch: 31 [46592/50000]\tLoss: 4.4518\tLR: 3.092839\n",
      "Training Epoch: 31 [46720/50000]\tLoss: 4.3574\tLR: 3.093095\n",
      "Training Epoch: 31 [46848/50000]\tLoss: 4.3371\tLR: 3.093350\n",
      "Training Epoch: 31 [46976/50000]\tLoss: 4.5049\tLR: 3.093606\n",
      "Training Epoch: 31 [47104/50000]\tLoss: 4.4791\tLR: 3.093862\n",
      "Training Epoch: 31 [47232/50000]\tLoss: 4.3606\tLR: 3.094118\n",
      "Training Epoch: 31 [47360/50000]\tLoss: 4.3732\tLR: 3.094373\n",
      "Training Epoch: 31 [47488/50000]\tLoss: 4.3874\tLR: 3.094629\n",
      "Training Epoch: 31 [47616/50000]\tLoss: 4.4082\tLR: 3.094885\n",
      "Training Epoch: 31 [47744/50000]\tLoss: 4.2957\tLR: 3.095141\n",
      "Training Epoch: 31 [47872/50000]\tLoss: 4.3238\tLR: 3.095396\n",
      "Training Epoch: 31 [48000/50000]\tLoss: 4.3779\tLR: 3.095652\n",
      "Training Epoch: 31 [48128/50000]\tLoss: 4.3028\tLR: 3.095908\n",
      "Training Epoch: 31 [48256/50000]\tLoss: 4.3044\tLR: 3.096164\n",
      "Training Epoch: 31 [48384/50000]\tLoss: 4.4047\tLR: 3.096419\n",
      "Training Epoch: 31 [48512/50000]\tLoss: 4.2830\tLR: 3.096675\n",
      "Training Epoch: 31 [48640/50000]\tLoss: 4.4709\tLR: 3.096931\n",
      "Training Epoch: 31 [48768/50000]\tLoss: 4.4455\tLR: 3.097187\n",
      "Training Epoch: 31 [48896/50000]\tLoss: 4.4147\tLR: 3.097442\n",
      "Training Epoch: 31 [49024/50000]\tLoss: 4.3435\tLR: 3.097698\n",
      "Training Epoch: 31 [49152/50000]\tLoss: 4.2885\tLR: 3.097954\n",
      "Training Epoch: 31 [49280/50000]\tLoss: 4.2838\tLR: 3.098210\n",
      "Training Epoch: 31 [49408/50000]\tLoss: 4.3983\tLR: 3.098465\n",
      "Training Epoch: 31 [49536/50000]\tLoss: 4.3735\tLR: 3.098721\n",
      "Training Epoch: 31 [49664/50000]\tLoss: 4.4173\tLR: 3.098977\n",
      "Training Epoch: 31 [49792/50000]\tLoss: 4.3728\tLR: 3.099233\n",
      "Training Epoch: 31 [49920/50000]\tLoss: 4.3800\tLR: 3.099488\n",
      "Training Epoch: 31 [50000/50000]\tLoss: 4.5358\tLR: 3.099744\n",
      "epoch 31 training time consumed: 492.92s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   43461 GB |   43460 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   43327 GB |   43327 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     133 GB |     133 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   43461 GB |   43460 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   43327 GB |   43327 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     133 GB |     133 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   42847 GB |   42847 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   42714 GB |   42714 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     133 GB |     133 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    4608 K  |    4608 K  |\n",
      "|       from large pool |      24    |      65    |    1964 K  |    1964 K  |\n",
      "|       from small pool |     231    |     274    |    2644 K  |    2643 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    4608 K  |    4608 K  |\n",
      "|       from large pool |      24    |      65    |    1964 K  |    1964 K  |\n",
      "|       from small pool |     231    |     274    |    2644 K  |    2643 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      46    |    2671 K  |    2671 K  |\n",
      "|       from large pool |      10    |      23    |     944 K  |     944 K  |\n",
      "|       from small pool |      27    |      35    |    1726 K  |    1726 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 31, Average loss: 0.0355, Accuracy: 0.0300, Time consumed:33.94s\n",
      "\n",
      "Training Epoch: 32 [128/50000]\tLoss: 4.3732\tLR: 0.100000\n",
      "Training Epoch: 32 [256/50000]\tLoss: 4.3830\tLR: 3.100256\n",
      "Training Epoch: 32 [384/50000]\tLoss: 4.3467\tLR: 3.100512\n",
      "Training Epoch: 32 [512/50000]\tLoss: 4.4175\tLR: 3.100767\n",
      "Training Epoch: 32 [640/50000]\tLoss: 4.4210\tLR: 3.101023\n",
      "Training Epoch: 32 [768/50000]\tLoss: 4.4609\tLR: 3.101279\n",
      "Training Epoch: 32 [896/50000]\tLoss: 4.4815\tLR: 3.101535\n",
      "Training Epoch: 32 [1024/50000]\tLoss: 4.4794\tLR: 3.101790\n",
      "Training Epoch: 32 [1152/50000]\tLoss: 4.4088\tLR: 3.102046\n",
      "Training Epoch: 32 [1280/50000]\tLoss: 4.3034\tLR: 3.102302\n",
      "Training Epoch: 32 [1408/50000]\tLoss: 4.4103\tLR: 3.102558\n",
      "Training Epoch: 32 [1536/50000]\tLoss: 4.4748\tLR: 3.102813\n",
      "Training Epoch: 32 [1664/50000]\tLoss: 4.4393\tLR: 3.103069\n",
      "Training Epoch: 32 [1792/50000]\tLoss: 4.3370\tLR: 3.103325\n",
      "Training Epoch: 32 [1920/50000]\tLoss: 4.3948\tLR: 3.103581\n",
      "Training Epoch: 32 [2048/50000]\tLoss: 4.4570\tLR: 3.103836\n",
      "Training Epoch: 32 [2176/50000]\tLoss: 4.4076\tLR: 3.104092\n",
      "Training Epoch: 32 [2304/50000]\tLoss: 4.5421\tLR: 3.104348\n",
      "Training Epoch: 32 [2432/50000]\tLoss: 4.3924\tLR: 3.104604\n",
      "Training Epoch: 32 [2560/50000]\tLoss: 4.3660\tLR: 3.104859\n",
      "Training Epoch: 32 [2688/50000]\tLoss: 4.3890\tLR: 3.105115\n",
      "Training Epoch: 32 [2816/50000]\tLoss: 4.3763\tLR: 3.105371\n",
      "Training Epoch: 32 [2944/50000]\tLoss: 4.3500\tLR: 3.105627\n",
      "Training Epoch: 32 [3072/50000]\tLoss: 4.4551\tLR: 3.105882\n",
      "Training Epoch: 32 [3200/50000]\tLoss: 4.4170\tLR: 3.106138\n",
      "Training Epoch: 32 [3328/50000]\tLoss: 4.5512\tLR: 3.106394\n",
      "Training Epoch: 32 [3456/50000]\tLoss: 4.4645\tLR: 3.106650\n",
      "Training Epoch: 32 [3584/50000]\tLoss: 4.5270\tLR: 3.106905\n",
      "Training Epoch: 32 [3712/50000]\tLoss: 4.5001\tLR: 3.107161\n",
      "Training Epoch: 32 [3840/50000]\tLoss: 4.2911\tLR: 3.107417\n",
      "Training Epoch: 32 [3968/50000]\tLoss: 4.4500\tLR: 3.107673\n",
      "Training Epoch: 32 [4096/50000]\tLoss: 4.5253\tLR: 3.107928\n",
      "Training Epoch: 32 [4224/50000]\tLoss: 4.3209\tLR: 3.108184\n",
      "Training Epoch: 32 [4352/50000]\tLoss: 4.3364\tLR: 3.108440\n",
      "Training Epoch: 32 [4480/50000]\tLoss: 4.3968\tLR: 3.108696\n",
      "Training Epoch: 32 [4608/50000]\tLoss: 4.3215\tLR: 3.108951\n",
      "Training Epoch: 32 [4736/50000]\tLoss: 4.4163\tLR: 3.109207\n",
      "Training Epoch: 32 [4864/50000]\tLoss: 4.4213\tLR: 3.109463\n",
      "Training Epoch: 32 [4992/50000]\tLoss: 4.4292\tLR: 3.109719\n",
      "Training Epoch: 32 [5120/50000]\tLoss: 4.4032\tLR: 3.109974\n",
      "Training Epoch: 32 [5248/50000]\tLoss: 4.4403\tLR: 3.110230\n",
      "Training Epoch: 32 [5376/50000]\tLoss: 4.4902\tLR: 3.110486\n",
      "Training Epoch: 32 [5504/50000]\tLoss: 4.2440\tLR: 3.110742\n",
      "Training Epoch: 32 [5632/50000]\tLoss: 4.3704\tLR: 3.110997\n",
      "Training Epoch: 32 [5760/50000]\tLoss: 4.4122\tLR: 3.111253\n",
      "Training Epoch: 32 [5888/50000]\tLoss: 4.3978\tLR: 3.111509\n",
      "Training Epoch: 32 [6016/50000]\tLoss: 4.3569\tLR: 3.111765\n",
      "Training Epoch: 32 [6144/50000]\tLoss: 4.4925\tLR: 3.112020\n",
      "Training Epoch: 32 [6272/50000]\tLoss: 4.5640\tLR: 3.112276\n",
      "Training Epoch: 32 [6400/50000]\tLoss: 4.5006\tLR: 3.112532\n",
      "Training Epoch: 32 [6528/50000]\tLoss: 4.5225\tLR: 3.112788\n",
      "Training Epoch: 32 [6656/50000]\tLoss: 4.4803\tLR: 3.113043\n",
      "Training Epoch: 32 [6784/50000]\tLoss: 4.3816\tLR: 3.113299\n",
      "Training Epoch: 32 [6912/50000]\tLoss: 4.4174\tLR: 3.113555\n",
      "Training Epoch: 32 [7040/50000]\tLoss: 4.4054\tLR: 3.113811\n",
      "Training Epoch: 32 [7168/50000]\tLoss: 4.3819\tLR: 3.114066\n",
      "Training Epoch: 32 [7296/50000]\tLoss: 4.4891\tLR: 3.114322\n",
      "Training Epoch: 32 [7424/50000]\tLoss: 4.4855\tLR: 3.114578\n",
      "Training Epoch: 32 [7552/50000]\tLoss: 4.5733\tLR: 3.114834\n",
      "Training Epoch: 32 [7680/50000]\tLoss: 4.4185\tLR: 3.115090\n",
      "Training Epoch: 32 [7808/50000]\tLoss: 4.4574\tLR: 3.115345\n",
      "Training Epoch: 32 [7936/50000]\tLoss: 4.4907\tLR: 3.115601\n",
      "Training Epoch: 32 [8064/50000]\tLoss: 4.4424\tLR: 3.115857\n",
      "Training Epoch: 32 [8192/50000]\tLoss: 4.4487\tLR: 3.116113\n",
      "Training Epoch: 32 [8320/50000]\tLoss: 4.4512\tLR: 3.116368\n",
      "Training Epoch: 32 [8448/50000]\tLoss: 4.5686\tLR: 3.116624\n",
      "Training Epoch: 32 [8576/50000]\tLoss: 4.4464\tLR: 3.116880\n",
      "Training Epoch: 32 [8704/50000]\tLoss: 4.4559\tLR: 3.117136\n",
      "Training Epoch: 32 [8832/50000]\tLoss: 4.5399\tLR: 3.117391\n",
      "Training Epoch: 32 [8960/50000]\tLoss: 4.4324\tLR: 3.117647\n",
      "Training Epoch: 32 [9088/50000]\tLoss: 4.4916\tLR: 3.117903\n",
      "Training Epoch: 32 [9216/50000]\tLoss: 4.5261\tLR: 3.118159\n",
      "Training Epoch: 32 [9344/50000]\tLoss: 4.5064\tLR: 3.118414\n",
      "Training Epoch: 32 [9472/50000]\tLoss: 4.3829\tLR: 3.118670\n",
      "Training Epoch: 32 [9600/50000]\tLoss: 4.3000\tLR: 3.118926\n",
      "Training Epoch: 32 [9728/50000]\tLoss: 4.3087\tLR: 3.119182\n",
      "Training Epoch: 32 [9856/50000]\tLoss: 4.3469\tLR: 3.119437\n",
      "Training Epoch: 32 [9984/50000]\tLoss: 4.2835\tLR: 3.119693\n",
      "Training Epoch: 32 [10112/50000]\tLoss: 4.3086\tLR: 3.119949\n",
      "Training Epoch: 32 [10240/50000]\tLoss: 4.4533\tLR: 3.120205\n",
      "Training Epoch: 32 [10368/50000]\tLoss: 4.4883\tLR: 3.120460\n",
      "Training Epoch: 32 [10496/50000]\tLoss: 4.4252\tLR: 3.120716\n",
      "Training Epoch: 32 [10624/50000]\tLoss: 4.4130\tLR: 3.120972\n",
      "Training Epoch: 32 [10752/50000]\tLoss: 4.3480\tLR: 3.121228\n",
      "Training Epoch: 32 [10880/50000]\tLoss: 4.3693\tLR: 3.121483\n",
      "Training Epoch: 32 [11008/50000]\tLoss: 4.4960\tLR: 3.121739\n",
      "Training Epoch: 32 [11136/50000]\tLoss: 4.3704\tLR: 3.121995\n",
      "Training Epoch: 32 [11264/50000]\tLoss: 4.3157\tLR: 3.122251\n",
      "Training Epoch: 32 [11392/50000]\tLoss: 4.3935\tLR: 3.122506\n",
      "Training Epoch: 32 [11520/50000]\tLoss: 4.3798\tLR: 3.122762\n",
      "Training Epoch: 32 [11648/50000]\tLoss: 4.4673\tLR: 3.123018\n",
      "Training Epoch: 32 [11776/50000]\tLoss: 4.5289\tLR: 3.123274\n",
      "Training Epoch: 32 [11904/50000]\tLoss: 4.5302\tLR: 3.123529\n",
      "Training Epoch: 32 [12032/50000]\tLoss: 4.4619\tLR: 3.123785\n",
      "Training Epoch: 32 [12160/50000]\tLoss: 4.4424\tLR: 3.124041\n",
      "Training Epoch: 32 [12288/50000]\tLoss: 4.4266\tLR: 3.124297\n",
      "Training Epoch: 32 [12416/50000]\tLoss: 4.3335\tLR: 3.124552\n",
      "Training Epoch: 32 [12544/50000]\tLoss: 4.5034\tLR: 3.124808\n",
      "Training Epoch: 32 [12672/50000]\tLoss: 4.3369\tLR: 3.125064\n",
      "Training Epoch: 32 [12800/50000]\tLoss: 4.2991\tLR: 3.125320\n",
      "Training Epoch: 32 [12928/50000]\tLoss: 4.3310\tLR: 3.125575\n",
      "Training Epoch: 32 [13056/50000]\tLoss: 4.3688\tLR: 3.125831\n",
      "Training Epoch: 32 [13184/50000]\tLoss: 4.5086\tLR: 3.126087\n",
      "Training Epoch: 32 [13312/50000]\tLoss: 4.3395\tLR: 3.126343\n",
      "Training Epoch: 32 [13440/50000]\tLoss: 4.4225\tLR: 3.126598\n",
      "Training Epoch: 32 [13568/50000]\tLoss: 4.3414\tLR: 3.126854\n",
      "Training Epoch: 32 [13696/50000]\tLoss: 4.3729\tLR: 3.127110\n",
      "Training Epoch: 32 [13824/50000]\tLoss: 4.3957\tLR: 3.127366\n",
      "Training Epoch: 32 [13952/50000]\tLoss: 4.3109\tLR: 3.127621\n",
      "Training Epoch: 32 [14080/50000]\tLoss: 4.5226\tLR: 3.127877\n",
      "Training Epoch: 32 [14208/50000]\tLoss: 4.3850\tLR: 3.128133\n",
      "Training Epoch: 32 [14336/50000]\tLoss: 4.5222\tLR: 3.128389\n",
      "Training Epoch: 32 [14464/50000]\tLoss: 4.3801\tLR: 3.128645\n",
      "Training Epoch: 32 [14592/50000]\tLoss: 4.2541\tLR: 3.128900\n",
      "Training Epoch: 32 [14720/50000]\tLoss: 4.3452\tLR: 3.129156\n",
      "Training Epoch: 32 [14848/50000]\tLoss: 4.3840\tLR: 3.129412\n",
      "Training Epoch: 32 [14976/50000]\tLoss: 4.3534\tLR: 3.129668\n",
      "Training Epoch: 32 [15104/50000]\tLoss: 4.4391\tLR: 3.129923\n",
      "Training Epoch: 32 [15232/50000]\tLoss: 4.4212\tLR: 3.130179\n",
      "Training Epoch: 32 [15360/50000]\tLoss: 4.4874\tLR: 3.130435\n",
      "Training Epoch: 32 [15488/50000]\tLoss: 4.4254\tLR: 3.130691\n",
      "Training Epoch: 32 [15616/50000]\tLoss: 4.3322\tLR: 3.130946\n",
      "Training Epoch: 32 [15744/50000]\tLoss: 4.4709\tLR: 3.131202\n",
      "Training Epoch: 32 [15872/50000]\tLoss: 4.5397\tLR: 3.131458\n",
      "Training Epoch: 32 [16000/50000]\tLoss: 4.4414\tLR: 3.131714\n",
      "Training Epoch: 32 [16128/50000]\tLoss: 4.3458\tLR: 3.131969\n",
      "Training Epoch: 32 [16256/50000]\tLoss: 4.3664\tLR: 3.132225\n",
      "Training Epoch: 32 [16384/50000]\tLoss: 4.1999\tLR: 3.132481\n",
      "Training Epoch: 32 [16512/50000]\tLoss: 4.3908\tLR: 3.132737\n",
      "Training Epoch: 32 [16640/50000]\tLoss: 4.5131\tLR: 3.132992\n",
      "Training Epoch: 32 [16768/50000]\tLoss: 4.2561\tLR: 3.133248\n",
      "Training Epoch: 32 [16896/50000]\tLoss: 4.3288\tLR: 3.133504\n",
      "Training Epoch: 32 [17024/50000]\tLoss: 4.3007\tLR: 3.133760\n",
      "Training Epoch: 32 [17152/50000]\tLoss: 4.3739\tLR: 3.134015\n",
      "Training Epoch: 32 [17280/50000]\tLoss: 4.2294\tLR: 3.134271\n",
      "Training Epoch: 32 [17408/50000]\tLoss: 4.3666\tLR: 3.134527\n",
      "Training Epoch: 32 [17536/50000]\tLoss: 4.3380\tLR: 3.134783\n",
      "Training Epoch: 32 [17664/50000]\tLoss: 4.4049\tLR: 3.135038\n",
      "Training Epoch: 32 [17792/50000]\tLoss: 4.3982\tLR: 3.135294\n",
      "Training Epoch: 32 [17920/50000]\tLoss: 4.2647\tLR: 3.135550\n",
      "Training Epoch: 32 [18048/50000]\tLoss: 4.4263\tLR: 3.135806\n",
      "Training Epoch: 32 [18176/50000]\tLoss: 4.5977\tLR: 3.136061\n",
      "Training Epoch: 32 [18304/50000]\tLoss: 4.3721\tLR: 3.136317\n",
      "Training Epoch: 32 [18432/50000]\tLoss: 4.3718\tLR: 3.136573\n",
      "Training Epoch: 32 [18560/50000]\tLoss: 4.3743\tLR: 3.136829\n",
      "Training Epoch: 32 [18688/50000]\tLoss: 4.3717\tLR: 3.137084\n",
      "Training Epoch: 32 [18816/50000]\tLoss: 4.5038\tLR: 3.137340\n",
      "Training Epoch: 32 [18944/50000]\tLoss: 4.4306\tLR: 3.137596\n",
      "Training Epoch: 32 [19072/50000]\tLoss: 4.3096\tLR: 3.137852\n",
      "Training Epoch: 32 [19200/50000]\tLoss: 4.3348\tLR: 3.138107\n",
      "Training Epoch: 32 [19328/50000]\tLoss: 4.5054\tLR: 3.138363\n",
      "Training Epoch: 32 [19456/50000]\tLoss: 4.4476\tLR: 3.138619\n",
      "Training Epoch: 32 [19584/50000]\tLoss: 4.4582\tLR: 3.138875\n",
      "Training Epoch: 32 [19712/50000]\tLoss: 4.4996\tLR: 3.139130\n",
      "Training Epoch: 32 [19840/50000]\tLoss: 4.4341\tLR: 3.139386\n",
      "Training Epoch: 32 [19968/50000]\tLoss: 4.4081\tLR: 3.139642\n",
      "Training Epoch: 32 [20096/50000]\tLoss: 4.4327\tLR: 3.139898\n",
      "Training Epoch: 32 [20224/50000]\tLoss: 4.4992\tLR: 3.140153\n",
      "Training Epoch: 32 [20352/50000]\tLoss: 4.4245\tLR: 3.140409\n",
      "Training Epoch: 32 [20480/50000]\tLoss: 4.2963\tLR: 3.140665\n",
      "Training Epoch: 32 [20608/50000]\tLoss: 4.4418\tLR: 3.140921\n",
      "Training Epoch: 32 [20736/50000]\tLoss: 4.4561\tLR: 3.141176\n",
      "Training Epoch: 32 [20864/50000]\tLoss: 4.4828\tLR: 3.141432\n",
      "Training Epoch: 32 [20992/50000]\tLoss: 4.2906\tLR: 3.141688\n",
      "Training Epoch: 32 [21120/50000]\tLoss: 4.5547\tLR: 3.141944\n",
      "Training Epoch: 32 [21248/50000]\tLoss: 4.3976\tLR: 3.142199\n",
      "Training Epoch: 32 [21376/50000]\tLoss: 4.4221\tLR: 3.142455\n",
      "Training Epoch: 32 [21504/50000]\tLoss: 4.3480\tLR: 3.142711\n",
      "Training Epoch: 32 [21632/50000]\tLoss: 4.4617\tLR: 3.142967\n",
      "Training Epoch: 32 [21760/50000]\tLoss: 4.4165\tLR: 3.143223\n",
      "Training Epoch: 32 [21888/50000]\tLoss: 4.4297\tLR: 3.143478\n",
      "Training Epoch: 32 [22016/50000]\tLoss: 4.3576\tLR: 3.143734\n",
      "Training Epoch: 32 [22144/50000]\tLoss: 4.3843\tLR: 3.143990\n",
      "Training Epoch: 32 [22272/50000]\tLoss: 4.3319\tLR: 3.144246\n",
      "Training Epoch: 32 [22400/50000]\tLoss: 4.3124\tLR: 3.144501\n",
      "Training Epoch: 32 [22528/50000]\tLoss: 4.4115\tLR: 3.144757\n",
      "Training Epoch: 32 [22656/50000]\tLoss: 4.2976\tLR: 3.145013\n",
      "Training Epoch: 32 [22784/50000]\tLoss: 4.3057\tLR: 3.145269\n",
      "Training Epoch: 32 [22912/50000]\tLoss: 4.3497\tLR: 3.145524\n",
      "Training Epoch: 32 [23040/50000]\tLoss: 4.4683\tLR: 3.145780\n",
      "Training Epoch: 32 [23168/50000]\tLoss: 4.4431\tLR: 3.146036\n",
      "Training Epoch: 32 [23296/50000]\tLoss: 4.3865\tLR: 3.146292\n",
      "Training Epoch: 32 [23424/50000]\tLoss: 4.4991\tLR: 3.146547\n",
      "Training Epoch: 32 [23552/50000]\tLoss: 4.4742\tLR: 3.146803\n",
      "Training Epoch: 32 [23680/50000]\tLoss: 4.5487\tLR: 3.147059\n",
      "Training Epoch: 32 [23808/50000]\tLoss: 4.3487\tLR: 3.147315\n",
      "Training Epoch: 32 [23936/50000]\tLoss: 4.4728\tLR: 3.147570\n",
      "Training Epoch: 32 [24064/50000]\tLoss: 4.2576\tLR: 3.147826\n",
      "Training Epoch: 32 [24192/50000]\tLoss: 4.3214\tLR: 3.148082\n",
      "Training Epoch: 32 [24320/50000]\tLoss: 4.3213\tLR: 3.148338\n",
      "Training Epoch: 32 [24448/50000]\tLoss: 4.4247\tLR: 3.148593\n",
      "Training Epoch: 32 [24576/50000]\tLoss: 4.3115\tLR: 3.148849\n",
      "Training Epoch: 32 [24704/50000]\tLoss: 4.4012\tLR: 3.149105\n",
      "Training Epoch: 32 [24832/50000]\tLoss: 4.4906\tLR: 3.149361\n",
      "Training Epoch: 32 [24960/50000]\tLoss: 4.3738\tLR: 3.149616\n",
      "Training Epoch: 32 [25088/50000]\tLoss: 4.4190\tLR: 3.149872\n",
      "Training Epoch: 32 [25216/50000]\tLoss: 4.3997\tLR: 3.150128\n",
      "Training Epoch: 32 [25344/50000]\tLoss: 4.2505\tLR: 3.150384\n",
      "Training Epoch: 32 [25472/50000]\tLoss: 4.2352\tLR: 3.150639\n",
      "Training Epoch: 32 [25600/50000]\tLoss: 4.3788\tLR: 3.150895\n",
      "Training Epoch: 32 [25728/50000]\tLoss: 4.4260\tLR: 3.151151\n",
      "Training Epoch: 32 [25856/50000]\tLoss: 4.3647\tLR: 3.151407\n",
      "Training Epoch: 32 [25984/50000]\tLoss: 4.4341\tLR: 3.151662\n",
      "Training Epoch: 32 [26112/50000]\tLoss: 4.3848\tLR: 3.151918\n",
      "Training Epoch: 32 [26240/50000]\tLoss: 4.3113\tLR: 3.152174\n",
      "Training Epoch: 32 [26368/50000]\tLoss: 4.3863\tLR: 3.152430\n",
      "Training Epoch: 32 [26496/50000]\tLoss: 4.4043\tLR: 3.152685\n",
      "Training Epoch: 32 [26624/50000]\tLoss: 4.4073\tLR: 3.152941\n",
      "Training Epoch: 32 [26752/50000]\tLoss: 4.5160\tLR: 3.153197\n",
      "Training Epoch: 32 [26880/50000]\tLoss: 4.5044\tLR: 3.153453\n",
      "Training Epoch: 32 [27008/50000]\tLoss: 4.4560\tLR: 3.153708\n",
      "Training Epoch: 32 [27136/50000]\tLoss: 4.4730\tLR: 3.153964\n",
      "Training Epoch: 32 [27264/50000]\tLoss: 4.6830\tLR: 3.154220\n",
      "Training Epoch: 32 [27392/50000]\tLoss: 4.4551\tLR: 3.154476\n",
      "Training Epoch: 32 [27520/50000]\tLoss: 4.4211\tLR: 3.154731\n",
      "Training Epoch: 32 [27648/50000]\tLoss: 4.5954\tLR: 3.154987\n",
      "Training Epoch: 32 [27776/50000]\tLoss: 4.4332\tLR: 3.155243\n",
      "Training Epoch: 32 [27904/50000]\tLoss: 4.4334\tLR: 3.155499\n",
      "Training Epoch: 32 [28032/50000]\tLoss: 4.4874\tLR: 3.155754\n",
      "Training Epoch: 32 [28160/50000]\tLoss: 4.4534\tLR: 3.156010\n",
      "Training Epoch: 32 [28288/50000]\tLoss: 4.3944\tLR: 3.156266\n",
      "Training Epoch: 32 [28416/50000]\tLoss: 4.3770\tLR: 3.156522\n",
      "Training Epoch: 32 [28544/50000]\tLoss: 4.4496\tLR: 3.156777\n",
      "Training Epoch: 32 [28672/50000]\tLoss: 4.4835\tLR: 3.157033\n",
      "Training Epoch: 32 [28800/50000]\tLoss: 4.4033\tLR: 3.157289\n",
      "Training Epoch: 32 [28928/50000]\tLoss: 4.3855\tLR: 3.157545\n",
      "Training Epoch: 32 [29056/50000]\tLoss: 4.4650\tLR: 3.157801\n",
      "Training Epoch: 32 [29184/50000]\tLoss: 4.4279\tLR: 3.158056\n",
      "Training Epoch: 32 [29312/50000]\tLoss: 4.3163\tLR: 3.158312\n",
      "Training Epoch: 32 [29440/50000]\tLoss: 4.3517\tLR: 3.158568\n",
      "Training Epoch: 32 [29568/50000]\tLoss: 4.4218\tLR: 3.158824\n",
      "Training Epoch: 32 [29696/50000]\tLoss: 4.3126\tLR: 3.159079\n",
      "Training Epoch: 32 [29824/50000]\tLoss: 4.4949\tLR: 3.159335\n",
      "Training Epoch: 32 [29952/50000]\tLoss: 4.4236\tLR: 3.159591\n",
      "Training Epoch: 32 [30080/50000]\tLoss: 4.4203\tLR: 3.159847\n",
      "Training Epoch: 32 [30208/50000]\tLoss: 4.4589\tLR: 3.160102\n",
      "Training Epoch: 32 [30336/50000]\tLoss: 4.3835\tLR: 3.160358\n",
      "Training Epoch: 32 [30464/50000]\tLoss: 4.5564\tLR: 3.160614\n",
      "Training Epoch: 32 [30592/50000]\tLoss: 4.5542\tLR: 3.160870\n",
      "Training Epoch: 32 [30720/50000]\tLoss: 4.5164\tLR: 3.161125\n",
      "Training Epoch: 32 [30848/50000]\tLoss: 4.4741\tLR: 3.161381\n",
      "Training Epoch: 32 [30976/50000]\tLoss: 4.3956\tLR: 3.161637\n",
      "Training Epoch: 32 [31104/50000]\tLoss: 4.6146\tLR: 3.161893\n",
      "Training Epoch: 32 [31232/50000]\tLoss: 4.4306\tLR: 3.162148\n",
      "Training Epoch: 32 [31360/50000]\tLoss: 4.3871\tLR: 3.162404\n",
      "Training Epoch: 32 [31488/50000]\tLoss: 4.3749\tLR: 3.162660\n",
      "Training Epoch: 32 [31616/50000]\tLoss: 4.3802\tLR: 3.162916\n",
      "Training Epoch: 32 [31744/50000]\tLoss: 4.3496\tLR: 3.163171\n",
      "Training Epoch: 32 [31872/50000]\tLoss: 4.4587\tLR: 3.163427\n",
      "Training Epoch: 32 [32000/50000]\tLoss: 4.4904\tLR: 3.163683\n",
      "Training Epoch: 32 [32128/50000]\tLoss: 4.3798\tLR: 3.163939\n",
      "Training Epoch: 32 [32256/50000]\tLoss: 4.3875\tLR: 3.164194\n",
      "Training Epoch: 32 [32384/50000]\tLoss: 4.3197\tLR: 3.164450\n",
      "Training Epoch: 32 [32512/50000]\tLoss: 4.5189\tLR: 3.164706\n",
      "Training Epoch: 32 [32640/50000]\tLoss: 4.4757\tLR: 3.164962\n",
      "Training Epoch: 32 [32768/50000]\tLoss: 4.2989\tLR: 3.165217\n",
      "Training Epoch: 32 [32896/50000]\tLoss: 4.4983\tLR: 3.165473\n",
      "Training Epoch: 32 [33024/50000]\tLoss: 4.5127\tLR: 3.165729\n",
      "Training Epoch: 32 [33152/50000]\tLoss: 4.4030\tLR: 3.165985\n",
      "Training Epoch: 32 [33280/50000]\tLoss: 4.4544\tLR: 3.166240\n",
      "Training Epoch: 32 [33408/50000]\tLoss: 4.4346\tLR: 3.166496\n",
      "Training Epoch: 32 [33536/50000]\tLoss: 4.3431\tLR: 3.166752\n",
      "Training Epoch: 32 [33664/50000]\tLoss: 4.4426\tLR: 3.167008\n",
      "Training Epoch: 32 [33792/50000]\tLoss: 4.4883\tLR: 3.167263\n",
      "Training Epoch: 32 [33920/50000]\tLoss: 4.3640\tLR: 3.167519\n",
      "Training Epoch: 32 [34048/50000]\tLoss: 4.4322\tLR: 3.167775\n",
      "Training Epoch: 32 [34176/50000]\tLoss: 4.5124\tLR: 3.168031\n",
      "Training Epoch: 32 [34304/50000]\tLoss: 4.4279\tLR: 3.168286\n",
      "Training Epoch: 32 [34432/50000]\tLoss: 4.4481\tLR: 3.168542\n",
      "Training Epoch: 32 [34560/50000]\tLoss: 4.3718\tLR: 3.168798\n",
      "Training Epoch: 32 [34688/50000]\tLoss: 4.3535\tLR: 3.169054\n",
      "Training Epoch: 32 [34816/50000]\tLoss: 4.4129\tLR: 3.169309\n",
      "Training Epoch: 32 [34944/50000]\tLoss: 4.4148\tLR: 3.169565\n",
      "Training Epoch: 32 [35072/50000]\tLoss: 4.3917\tLR: 3.169821\n",
      "Training Epoch: 32 [35200/50000]\tLoss: 4.3163\tLR: 3.170077\n",
      "Training Epoch: 32 [35328/50000]\tLoss: 4.3287\tLR: 3.170332\n",
      "Training Epoch: 32 [35456/50000]\tLoss: 4.3487\tLR: 3.170588\n",
      "Training Epoch: 32 [35584/50000]\tLoss: 4.7318\tLR: 3.170844\n",
      "Training Epoch: 32 [35712/50000]\tLoss: 4.3332\tLR: 3.171100\n",
      "Training Epoch: 32 [35840/50000]\tLoss: 4.4172\tLR: 3.171355\n",
      "Training Epoch: 32 [35968/50000]\tLoss: 4.4043\tLR: 3.171611\n",
      "Training Epoch: 32 [36096/50000]\tLoss: 4.4736\tLR: 3.171867\n",
      "Training Epoch: 32 [36224/50000]\tLoss: 4.3918\tLR: 3.172123\n",
      "Training Epoch: 32 [36352/50000]\tLoss: 4.5893\tLR: 3.172379\n",
      "Training Epoch: 32 [36480/50000]\tLoss: 4.4709\tLR: 3.172634\n",
      "Training Epoch: 32 [36608/50000]\tLoss: 4.7032\tLR: 3.172890\n",
      "Training Epoch: 32 [36736/50000]\tLoss: 4.6031\tLR: 3.173146\n",
      "Training Epoch: 32 [36864/50000]\tLoss: 4.5344\tLR: 3.173402\n",
      "Training Epoch: 32 [36992/50000]\tLoss: 4.4604\tLR: 3.173657\n",
      "Training Epoch: 32 [37120/50000]\tLoss: 4.5535\tLR: 3.173913\n",
      "Training Epoch: 32 [37248/50000]\tLoss: 4.5267\tLR: 3.174169\n",
      "Training Epoch: 32 [37376/50000]\tLoss: 4.5968\tLR: 3.174425\n",
      "Training Epoch: 32 [37504/50000]\tLoss: 4.5513\tLR: 3.174680\n",
      "Training Epoch: 32 [37632/50000]\tLoss: 4.5297\tLR: 3.174936\n",
      "Training Epoch: 32 [37760/50000]\tLoss: 4.5213\tLR: 3.175192\n",
      "Training Epoch: 32 [37888/50000]\tLoss: 4.5154\tLR: 3.175448\n",
      "Training Epoch: 32 [38016/50000]\tLoss: 4.4515\tLR: 3.175703\n",
      "Training Epoch: 32 [38144/50000]\tLoss: 4.4417\tLR: 3.175959\n",
      "Training Epoch: 32 [38272/50000]\tLoss: 4.6117\tLR: 3.176215\n",
      "Training Epoch: 32 [38400/50000]\tLoss: 4.4683\tLR: 3.176471\n",
      "Training Epoch: 32 [38528/50000]\tLoss: 4.5394\tLR: 3.176726\n",
      "Training Epoch: 32 [38656/50000]\tLoss: 4.5164\tLR: 3.176982\n",
      "Training Epoch: 32 [38784/50000]\tLoss: 4.5114\tLR: 3.177238\n",
      "Training Epoch: 32 [38912/50000]\tLoss: 4.6047\tLR: 3.177494\n",
      "Training Epoch: 32 [39040/50000]\tLoss: 4.3726\tLR: 3.177749\n",
      "Training Epoch: 32 [39168/50000]\tLoss: 4.4286\tLR: 3.178005\n",
      "Training Epoch: 32 [39296/50000]\tLoss: 4.4940\tLR: 3.178261\n",
      "Training Epoch: 32 [39424/50000]\tLoss: 4.5682\tLR: 3.178517\n",
      "Training Epoch: 32 [39552/50000]\tLoss: 4.5780\tLR: 3.178772\n",
      "Training Epoch: 32 [39680/50000]\tLoss: 4.4707\tLR: 3.179028\n",
      "Training Epoch: 32 [39808/50000]\tLoss: 4.4557\tLR: 3.179284\n",
      "Training Epoch: 32 [39936/50000]\tLoss: 4.5898\tLR: 3.179540\n",
      "Training Epoch: 32 [40064/50000]\tLoss: 4.5201\tLR: 3.179795\n",
      "Training Epoch: 32 [40192/50000]\tLoss: 4.5321\tLR: 3.180051\n",
      "Training Epoch: 32 [40320/50000]\tLoss: 4.5440\tLR: 3.180307\n",
      "Training Epoch: 32 [40448/50000]\tLoss: 4.5336\tLR: 3.180563\n",
      "Training Epoch: 32 [40576/50000]\tLoss: 4.4894\tLR: 3.180818\n",
      "Training Epoch: 32 [40704/50000]\tLoss: 4.4621\tLR: 3.181074\n",
      "Training Epoch: 32 [40832/50000]\tLoss: 4.4848\tLR: 3.181330\n",
      "Training Epoch: 32 [40960/50000]\tLoss: 4.4753\tLR: 3.181586\n",
      "Training Epoch: 32 [41088/50000]\tLoss: 4.5526\tLR: 3.181841\n",
      "Training Epoch: 32 [41216/50000]\tLoss: 4.4688\tLR: 3.182097\n",
      "Training Epoch: 32 [41344/50000]\tLoss: 4.4874\tLR: 3.182353\n",
      "Training Epoch: 32 [41472/50000]\tLoss: 4.4624\tLR: 3.182609\n",
      "Training Epoch: 32 [41600/50000]\tLoss: 4.5340\tLR: 3.182864\n",
      "Training Epoch: 32 [41728/50000]\tLoss: 4.4187\tLR: 3.183120\n",
      "Training Epoch: 32 [41856/50000]\tLoss: 4.5050\tLR: 3.183376\n",
      "Training Epoch: 32 [41984/50000]\tLoss: 4.5073\tLR: 3.183632\n",
      "Training Epoch: 32 [42112/50000]\tLoss: 4.4206\tLR: 3.183887\n",
      "Training Epoch: 32 [42240/50000]\tLoss: 4.4090\tLR: 3.184143\n",
      "Training Epoch: 32 [42368/50000]\tLoss: 4.4492\tLR: 3.184399\n",
      "Training Epoch: 32 [42496/50000]\tLoss: 4.4105\tLR: 3.184655\n",
      "Training Epoch: 32 [42624/50000]\tLoss: 4.3495\tLR: 3.184910\n",
      "Training Epoch: 32 [42752/50000]\tLoss: 4.4727\tLR: 3.185166\n",
      "Training Epoch: 32 [42880/50000]\tLoss: 4.6511\tLR: 3.185422\n",
      "Training Epoch: 32 [43008/50000]\tLoss: 4.5579\tLR: 3.185678\n",
      "Training Epoch: 32 [43136/50000]\tLoss: 4.4203\tLR: 3.185934\n",
      "Training Epoch: 32 [43264/50000]\tLoss: 4.5169\tLR: 3.186189\n",
      "Training Epoch: 32 [43392/50000]\tLoss: 4.4521\tLR: 3.186445\n",
      "Training Epoch: 32 [43520/50000]\tLoss: 4.5727\tLR: 3.186701\n",
      "Training Epoch: 32 [43648/50000]\tLoss: 4.3834\tLR: 3.186957\n",
      "Training Epoch: 32 [43776/50000]\tLoss: 4.2940\tLR: 3.187212\n",
      "Training Epoch: 32 [43904/50000]\tLoss: 4.5533\tLR: 3.187468\n",
      "Training Epoch: 32 [44032/50000]\tLoss: 4.3369\tLR: 3.187724\n",
      "Training Epoch: 32 [44160/50000]\tLoss: 4.5151\tLR: 3.187980\n",
      "Training Epoch: 32 [44288/50000]\tLoss: 4.4915\tLR: 3.188235\n",
      "Training Epoch: 32 [44416/50000]\tLoss: 4.5936\tLR: 3.188491\n",
      "Training Epoch: 32 [44544/50000]\tLoss: 4.5468\tLR: 3.188747\n",
      "Training Epoch: 32 [44672/50000]\tLoss: 4.6390\tLR: 3.189003\n",
      "Training Epoch: 32 [44800/50000]\tLoss: 4.5341\tLR: 3.189258\n",
      "Training Epoch: 32 [44928/50000]\tLoss: 4.5277\tLR: 3.189514\n",
      "Training Epoch: 32 [45056/50000]\tLoss: 4.4415\tLR: 3.189770\n",
      "Training Epoch: 32 [45184/50000]\tLoss: 4.4679\tLR: 3.190026\n",
      "Training Epoch: 32 [45312/50000]\tLoss: 4.4808\tLR: 3.190281\n",
      "Training Epoch: 32 [45440/50000]\tLoss: 4.4829\tLR: 3.190537\n",
      "Training Epoch: 32 [45568/50000]\tLoss: 4.5242\tLR: 3.190793\n",
      "Training Epoch: 32 [45696/50000]\tLoss: 4.3625\tLR: 3.191049\n",
      "Training Epoch: 32 [45824/50000]\tLoss: 4.4331\tLR: 3.191304\n",
      "Training Epoch: 32 [45952/50000]\tLoss: 4.4993\tLR: 3.191560\n",
      "Training Epoch: 32 [46080/50000]\tLoss: 4.5494\tLR: 3.191816\n",
      "Training Epoch: 32 [46208/50000]\tLoss: 4.5533\tLR: 3.192072\n",
      "Training Epoch: 32 [46336/50000]\tLoss: 4.5428\tLR: 3.192327\n",
      "Training Epoch: 32 [46464/50000]\tLoss: 4.5749\tLR: 3.192583\n",
      "Training Epoch: 32 [46592/50000]\tLoss: 4.6279\tLR: 3.192839\n",
      "Training Epoch: 32 [46720/50000]\tLoss: 4.5524\tLR: 3.193095\n",
      "Training Epoch: 32 [46848/50000]\tLoss: 4.5692\tLR: 3.193350\n",
      "Training Epoch: 32 [46976/50000]\tLoss: 4.6825\tLR: 3.193606\n",
      "Training Epoch: 32 [47104/50000]\tLoss: 4.7119\tLR: 3.193862\n",
      "Training Epoch: 32 [47232/50000]\tLoss: 4.6120\tLR: 3.194118\n",
      "Training Epoch: 32 [47360/50000]\tLoss: 4.6724\tLR: 3.194373\n",
      "Training Epoch: 32 [47488/50000]\tLoss: 4.5846\tLR: 3.194629\n",
      "Training Epoch: 32 [47616/50000]\tLoss: 4.6012\tLR: 3.194885\n",
      "Training Epoch: 32 [47744/50000]\tLoss: 4.7713\tLR: 3.195141\n",
      "Training Epoch: 32 [47872/50000]\tLoss: 4.6127\tLR: 3.195396\n",
      "Training Epoch: 32 [48000/50000]\tLoss: 4.6433\tLR: 3.195652\n",
      "Training Epoch: 32 [48128/50000]\tLoss: 4.6037\tLR: 3.195908\n",
      "Training Epoch: 32 [48256/50000]\tLoss: 4.6690\tLR: 3.196164\n",
      "Training Epoch: 32 [48384/50000]\tLoss: 4.7402\tLR: 3.196419\n",
      "Training Epoch: 32 [48512/50000]\tLoss: 4.6724\tLR: 3.196675\n",
      "Training Epoch: 32 [48640/50000]\tLoss: 4.7937\tLR: 3.196931\n",
      "Training Epoch: 32 [48768/50000]\tLoss: 4.7368\tLR: 3.197187\n",
      "Training Epoch: 32 [48896/50000]\tLoss: 4.6478\tLR: 3.197442\n",
      "Training Epoch: 32 [49024/50000]\tLoss: 4.6927\tLR: 3.197698\n",
      "Training Epoch: 32 [49152/50000]\tLoss: 4.6200\tLR: 3.197954\n",
      "Training Epoch: 32 [49280/50000]\tLoss: 4.5957\tLR: 3.198210\n",
      "Training Epoch: 32 [49408/50000]\tLoss: 4.6743\tLR: 3.198465\n",
      "Training Epoch: 32 [49536/50000]\tLoss: 4.6303\tLR: 3.198721\n",
      "Training Epoch: 32 [49664/50000]\tLoss: 4.5364\tLR: 3.198977\n",
      "Training Epoch: 32 [49792/50000]\tLoss: 4.5975\tLR: 3.199233\n",
      "Training Epoch: 32 [49920/50000]\tLoss: 4.6786\tLR: 3.199488\n",
      "Training Epoch: 32 [50000/50000]\tLoss: 4.4850\tLR: 3.199744\n",
      "epoch 32 training time consumed: 492.95s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   44862 GB |   44862 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   44725 GB |   44725 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     137 GB |     137 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   44862 GB |   44862 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   44725 GB |   44725 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     137 GB |     137 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   44230 GB |   44230 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   44092 GB |   44092 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     137 GB |     137 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    4757 K  |    4757 K  |\n",
      "|       from large pool |      24    |      65    |    2027 K  |    2027 K  |\n",
      "|       from small pool |     231    |     274    |    2729 K  |    2729 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    4757 K  |    4757 K  |\n",
      "|       from large pool |      24    |      65    |    2027 K  |    2027 K  |\n",
      "|       from small pool |     231    |     274    |    2729 K  |    2729 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      46    |    2757 K  |    2756 K  |\n",
      "|       from large pool |      10    |      23    |     974 K  |     974 K  |\n",
      "|       from small pool |      26    |      35    |    1782 K  |    1782 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 32, Average loss: 0.0366, Accuracy: 0.0151, Time consumed:31.47s\n",
      "\n",
      "Training Epoch: 33 [128/50000]\tLoss: 4.6773\tLR: 0.100000\n",
      "Training Epoch: 33 [256/50000]\tLoss: 4.5938\tLR: 3.200256\n",
      "Training Epoch: 33 [384/50000]\tLoss: 4.6955\tLR: 3.200512\n",
      "Training Epoch: 33 [512/50000]\tLoss: 4.6333\tLR: 3.200767\n",
      "Training Epoch: 33 [640/50000]\tLoss: 4.6588\tLR: 3.201023\n",
      "Training Epoch: 33 [768/50000]\tLoss: 4.5718\tLR: 3.201279\n",
      "Training Epoch: 33 [896/50000]\tLoss: 4.6265\tLR: 3.201535\n",
      "Training Epoch: 33 [1024/50000]\tLoss: 4.6349\tLR: 3.201790\n",
      "Training Epoch: 33 [1152/50000]\tLoss: 4.5408\tLR: 3.202046\n",
      "Training Epoch: 33 [1280/50000]\tLoss: 4.5102\tLR: 3.202302\n",
      "Training Epoch: 33 [1408/50000]\tLoss: 4.5075\tLR: 3.202558\n",
      "Training Epoch: 33 [1536/50000]\tLoss: 4.5048\tLR: 3.202813\n",
      "Training Epoch: 33 [1664/50000]\tLoss: 4.4221\tLR: 3.203069\n",
      "Training Epoch: 33 [1792/50000]\tLoss: 4.6535\tLR: 3.203325\n",
      "Training Epoch: 33 [1920/50000]\tLoss: 4.5547\tLR: 3.203581\n",
      "Training Epoch: 33 [2048/50000]\tLoss: 4.5348\tLR: 3.203836\n",
      "Training Epoch: 33 [2176/50000]\tLoss: 4.6220\tLR: 3.204092\n",
      "Training Epoch: 33 [2304/50000]\tLoss: 4.6015\tLR: 3.204348\n",
      "Training Epoch: 33 [2432/50000]\tLoss: 4.5009\tLR: 3.204604\n",
      "Training Epoch: 33 [2560/50000]\tLoss: 4.5380\tLR: 3.204859\n",
      "Training Epoch: 33 [2688/50000]\tLoss: 4.6534\tLR: 3.205115\n",
      "Training Epoch: 33 [2816/50000]\tLoss: 4.6342\tLR: 3.205371\n",
      "Training Epoch: 33 [2944/50000]\tLoss: 4.5159\tLR: 3.205627\n",
      "Training Epoch: 33 [3072/50000]\tLoss: 4.5512\tLR: 3.205882\n",
      "Training Epoch: 33 [3200/50000]\tLoss: 4.6007\tLR: 3.206138\n",
      "Training Epoch: 33 [3328/50000]\tLoss: 4.4035\tLR: 3.206394\n",
      "Training Epoch: 33 [3456/50000]\tLoss: 4.4757\tLR: 3.206650\n",
      "Training Epoch: 33 [3584/50000]\tLoss: 4.5745\tLR: 3.206905\n",
      "Training Epoch: 33 [3712/50000]\tLoss: 4.4454\tLR: 3.207161\n",
      "Training Epoch: 33 [3840/50000]\tLoss: 4.4967\tLR: 3.207417\n",
      "Training Epoch: 33 [3968/50000]\tLoss: 4.4709\tLR: 3.207673\n",
      "Training Epoch: 33 [4096/50000]\tLoss: 4.6168\tLR: 3.207928\n",
      "Training Epoch: 33 [4224/50000]\tLoss: 4.4858\tLR: 3.208184\n",
      "Training Epoch: 33 [4352/50000]\tLoss: 4.4196\tLR: 3.208440\n",
      "Training Epoch: 33 [4480/50000]\tLoss: 4.5170\tLR: 3.208696\n",
      "Training Epoch: 33 [4608/50000]\tLoss: 4.5168\tLR: 3.208951\n",
      "Training Epoch: 33 [4736/50000]\tLoss: 4.8860\tLR: 3.209207\n",
      "Training Epoch: 33 [4864/50000]\tLoss: 4.6705\tLR: 3.209463\n",
      "Training Epoch: 33 [4992/50000]\tLoss: 4.6766\tLR: 3.209719\n",
      "Training Epoch: 33 [5120/50000]\tLoss: 4.6535\tLR: 3.209974\n",
      "Training Epoch: 33 [5248/50000]\tLoss: 4.6202\tLR: 3.210230\n",
      "Training Epoch: 33 [5376/50000]\tLoss: 4.6660\tLR: 3.210486\n",
      "Training Epoch: 33 [5504/50000]\tLoss: 4.6217\tLR: 3.210742\n",
      "Training Epoch: 33 [5632/50000]\tLoss: 4.5536\tLR: 3.210997\n",
      "Training Epoch: 33 [5760/50000]\tLoss: 4.6151\tLR: 3.211253\n",
      "Training Epoch: 33 [5888/50000]\tLoss: 4.7021\tLR: 3.211509\n",
      "Training Epoch: 33 [6016/50000]\tLoss: 4.7254\tLR: 3.211765\n",
      "Training Epoch: 33 [6144/50000]\tLoss: 4.6149\tLR: 3.212020\n",
      "Training Epoch: 33 [6272/50000]\tLoss: 4.6310\tLR: 3.212276\n",
      "Training Epoch: 33 [6400/50000]\tLoss: 4.6682\tLR: 3.212532\n",
      "Training Epoch: 33 [6528/50000]\tLoss: 4.5964\tLR: 3.212788\n",
      "Training Epoch: 33 [6656/50000]\tLoss: 4.5711\tLR: 3.213043\n",
      "Training Epoch: 33 [6784/50000]\tLoss: 4.6624\tLR: 3.213299\n",
      "Training Epoch: 33 [6912/50000]\tLoss: 4.6493\tLR: 3.213555\n",
      "Training Epoch: 33 [7040/50000]\tLoss: 4.5890\tLR: 3.213811\n",
      "Training Epoch: 33 [7168/50000]\tLoss: 4.6411\tLR: 3.214066\n",
      "Training Epoch: 33 [7296/50000]\tLoss: 4.6419\tLR: 3.214322\n",
      "Training Epoch: 33 [7424/50000]\tLoss: 4.5157\tLR: 3.214578\n",
      "Training Epoch: 33 [7552/50000]\tLoss: 4.6364\tLR: 3.214834\n",
      "Training Epoch: 33 [7680/50000]\tLoss: 4.6132\tLR: 3.215090\n",
      "Training Epoch: 33 [7808/50000]\tLoss: 4.6354\tLR: 3.215345\n",
      "Training Epoch: 33 [7936/50000]\tLoss: 4.5691\tLR: 3.215601\n",
      "Training Epoch: 33 [8064/50000]\tLoss: 4.6279\tLR: 3.215857\n",
      "Training Epoch: 33 [8192/50000]\tLoss: 4.5597\tLR: 3.216113\n",
      "Training Epoch: 33 [8320/50000]\tLoss: 4.5995\tLR: 3.216368\n",
      "Training Epoch: 33 [8448/50000]\tLoss: 4.4803\tLR: 3.216624\n",
      "Training Epoch: 33 [8576/50000]\tLoss: 4.5137\tLR: 3.216880\n",
      "Training Epoch: 33 [8704/50000]\tLoss: 4.4622\tLR: 3.217136\n",
      "Training Epoch: 33 [8832/50000]\tLoss: 4.5576\tLR: 3.217391\n",
      "Training Epoch: 33 [8960/50000]\tLoss: 4.5827\tLR: 3.217647\n",
      "Training Epoch: 33 [9088/50000]\tLoss: 4.5056\tLR: 3.217903\n",
      "Training Epoch: 33 [9216/50000]\tLoss: 4.5662\tLR: 3.218159\n",
      "Training Epoch: 33 [9344/50000]\tLoss: 4.4239\tLR: 3.218414\n",
      "Training Epoch: 33 [9472/50000]\tLoss: 4.6006\tLR: 3.218670\n",
      "Training Epoch: 33 [9600/50000]\tLoss: 4.5472\tLR: 3.218926\n",
      "Training Epoch: 33 [9728/50000]\tLoss: 4.5994\tLR: 3.219182\n",
      "Training Epoch: 33 [9856/50000]\tLoss: 4.7083\tLR: 3.219437\n",
      "Training Epoch: 33 [9984/50000]\tLoss: 4.6820\tLR: 3.219693\n",
      "Training Epoch: 33 [10112/50000]\tLoss: 4.5465\tLR: 3.219949\n",
      "Training Epoch: 33 [10240/50000]\tLoss: 4.6913\tLR: 3.220205\n",
      "Training Epoch: 33 [10368/50000]\tLoss: 4.5321\tLR: 3.220460\n",
      "Training Epoch: 33 [10496/50000]\tLoss: 4.5847\tLR: 3.220716\n",
      "Training Epoch: 33 [10624/50000]\tLoss: 4.4681\tLR: 3.220972\n",
      "Training Epoch: 33 [10752/50000]\tLoss: 4.4520\tLR: 3.221228\n",
      "Training Epoch: 33 [10880/50000]\tLoss: 4.5191\tLR: 3.221483\n",
      "Training Epoch: 33 [11008/50000]\tLoss: 4.5533\tLR: 3.221739\n",
      "Training Epoch: 33 [11136/50000]\tLoss: 4.5747\tLR: 3.221995\n",
      "Training Epoch: 33 [11264/50000]\tLoss: 4.5529\tLR: 3.222251\n",
      "Training Epoch: 33 [11392/50000]\tLoss: 4.5524\tLR: 3.222506\n",
      "Training Epoch: 33 [11520/50000]\tLoss: 4.6232\tLR: 3.222762\n",
      "Training Epoch: 33 [11648/50000]\tLoss: 4.5516\tLR: 3.223018\n",
      "Training Epoch: 33 [11776/50000]\tLoss: 4.5333\tLR: 3.223274\n",
      "Training Epoch: 33 [11904/50000]\tLoss: 4.5596\tLR: 3.223529\n",
      "Training Epoch: 33 [12032/50000]\tLoss: 4.5967\tLR: 3.223785\n",
      "Training Epoch: 33 [12160/50000]\tLoss: 4.5323\tLR: 3.224041\n",
      "Training Epoch: 33 [12288/50000]\tLoss: 4.5191\tLR: 3.224297\n",
      "Training Epoch: 33 [12416/50000]\tLoss: 4.4610\tLR: 3.224552\n",
      "Training Epoch: 33 [12544/50000]\tLoss: 4.4918\tLR: 3.224808\n",
      "Training Epoch: 33 [12672/50000]\tLoss: 4.6560\tLR: 3.225064\n",
      "Training Epoch: 33 [12800/50000]\tLoss: 4.5997\tLR: 3.225320\n",
      "Training Epoch: 33 [12928/50000]\tLoss: 4.4855\tLR: 3.225575\n",
      "Training Epoch: 33 [13056/50000]\tLoss: 4.5079\tLR: 3.225831\n",
      "Training Epoch: 33 [13184/50000]\tLoss: 4.5536\tLR: 3.226087\n",
      "Training Epoch: 33 [13312/50000]\tLoss: 4.4665\tLR: 3.226343\n",
      "Training Epoch: 33 [13440/50000]\tLoss: 4.6453\tLR: 3.226598\n",
      "Training Epoch: 33 [13568/50000]\tLoss: 4.4344\tLR: 3.226854\n",
      "Training Epoch: 33 [13696/50000]\tLoss: 4.5194\tLR: 3.227110\n",
      "Training Epoch: 33 [13824/50000]\tLoss: 4.6103\tLR: 3.227366\n",
      "Training Epoch: 33 [13952/50000]\tLoss: 4.5238\tLR: 3.227621\n",
      "Training Epoch: 33 [14080/50000]\tLoss: 4.4931\tLR: 3.227877\n",
      "Training Epoch: 33 [14208/50000]\tLoss: 4.5177\tLR: 3.228133\n",
      "Training Epoch: 33 [14336/50000]\tLoss: 4.4391\tLR: 3.228389\n",
      "Training Epoch: 33 [14464/50000]\tLoss: 4.4323\tLR: 3.228645\n",
      "Training Epoch: 33 [14592/50000]\tLoss: 4.4413\tLR: 3.228900\n",
      "Training Epoch: 33 [14720/50000]\tLoss: 4.4581\tLR: 3.229156\n",
      "Training Epoch: 33 [14848/50000]\tLoss: 4.3554\tLR: 3.229412\n",
      "Training Epoch: 33 [14976/50000]\tLoss: 4.5182\tLR: 3.229668\n",
      "Training Epoch: 33 [15104/50000]\tLoss: 4.4753\tLR: 3.229923\n",
      "Training Epoch: 33 [15232/50000]\tLoss: 4.4893\tLR: 3.230179\n",
      "Training Epoch: 33 [15360/50000]\tLoss: 4.5173\tLR: 3.230435\n",
      "Training Epoch: 33 [15488/50000]\tLoss: 4.2602\tLR: 3.230691\n",
      "Training Epoch: 33 [15616/50000]\tLoss: 4.3098\tLR: 3.230946\n",
      "Training Epoch: 33 [15744/50000]\tLoss: 4.4551\tLR: 3.231202\n",
      "Training Epoch: 33 [15872/50000]\tLoss: 4.4830\tLR: 3.231458\n",
      "Training Epoch: 33 [16000/50000]\tLoss: 4.4676\tLR: 3.231714\n",
      "Training Epoch: 33 [16128/50000]\tLoss: 4.4103\tLR: 3.231969\n",
      "Training Epoch: 33 [16256/50000]\tLoss: 4.5060\tLR: 3.232225\n",
      "Training Epoch: 33 [16384/50000]\tLoss: 4.3810\tLR: 3.232481\n",
      "Training Epoch: 33 [16512/50000]\tLoss: 4.5073\tLR: 3.232737\n",
      "Training Epoch: 33 [16640/50000]\tLoss: 4.5670\tLR: 3.232992\n",
      "Training Epoch: 33 [16768/50000]\tLoss: 4.5266\tLR: 3.233248\n",
      "Training Epoch: 33 [16896/50000]\tLoss: 4.4832\tLR: 3.233504\n",
      "Training Epoch: 33 [17024/50000]\tLoss: 4.2974\tLR: 3.233760\n",
      "Training Epoch: 33 [17152/50000]\tLoss: 4.4668\tLR: 3.234015\n",
      "Training Epoch: 33 [17280/50000]\tLoss: 4.4939\tLR: 3.234271\n",
      "Training Epoch: 33 [17408/50000]\tLoss: 4.5648\tLR: 3.234527\n",
      "Training Epoch: 33 [17536/50000]\tLoss: 4.3995\tLR: 3.234783\n",
      "Training Epoch: 33 [17664/50000]\tLoss: 4.3569\tLR: 3.235038\n",
      "Training Epoch: 33 [17792/50000]\tLoss: 4.4831\tLR: 3.235294\n",
      "Training Epoch: 33 [17920/50000]\tLoss: 4.4086\tLR: 3.235550\n",
      "Training Epoch: 33 [18048/50000]\tLoss: 4.4773\tLR: 3.235806\n",
      "Training Epoch: 33 [18176/50000]\tLoss: 4.5040\tLR: 3.236061\n",
      "Training Epoch: 33 [18304/50000]\tLoss: 4.4715\tLR: 3.236317\n",
      "Training Epoch: 33 [18432/50000]\tLoss: 4.3999\tLR: 3.236573\n",
      "Training Epoch: 33 [18560/50000]\tLoss: 4.5965\tLR: 3.236829\n",
      "Training Epoch: 33 [18688/50000]\tLoss: 4.5484\tLR: 3.237084\n",
      "Training Epoch: 33 [18816/50000]\tLoss: 4.5377\tLR: 3.237340\n",
      "Training Epoch: 33 [18944/50000]\tLoss: 4.5212\tLR: 3.237596\n",
      "Training Epoch: 33 [19072/50000]\tLoss: 4.5531\tLR: 3.237852\n",
      "Training Epoch: 33 [19200/50000]\tLoss: 4.6098\tLR: 3.238107\n",
      "Training Epoch: 33 [19328/50000]\tLoss: 4.5811\tLR: 3.238363\n",
      "Training Epoch: 33 [19456/50000]\tLoss: 4.3855\tLR: 3.238619\n",
      "Training Epoch: 33 [19584/50000]\tLoss: 4.5658\tLR: 3.238875\n",
      "Training Epoch: 33 [19712/50000]\tLoss: 4.4199\tLR: 3.239130\n",
      "Training Epoch: 33 [19840/50000]\tLoss: 4.5652\tLR: 3.239386\n",
      "Training Epoch: 33 [19968/50000]\tLoss: 4.4278\tLR: 3.239642\n",
      "Training Epoch: 33 [20096/50000]\tLoss: 4.4717\tLR: 3.239898\n",
      "Training Epoch: 33 [20224/50000]\tLoss: 4.5305\tLR: 3.240153\n",
      "Training Epoch: 33 [20352/50000]\tLoss: 4.5419\tLR: 3.240409\n",
      "Training Epoch: 33 [20480/50000]\tLoss: 4.4999\tLR: 3.240665\n",
      "Training Epoch: 33 [20608/50000]\tLoss: 4.4919\tLR: 3.240921\n",
      "Training Epoch: 33 [20736/50000]\tLoss: 4.6135\tLR: 3.241176\n",
      "Training Epoch: 33 [20864/50000]\tLoss: 4.6227\tLR: 3.241432\n",
      "Training Epoch: 33 [20992/50000]\tLoss: 4.5145\tLR: 3.241688\n",
      "Training Epoch: 33 [21120/50000]\tLoss: 4.4492\tLR: 3.241944\n",
      "Training Epoch: 33 [21248/50000]\tLoss: 4.8694\tLR: 3.242199\n",
      "Training Epoch: 33 [21376/50000]\tLoss: 4.6937\tLR: 3.242455\n",
      "Training Epoch: 33 [21504/50000]\tLoss: 4.6670\tLR: 3.242711\n",
      "Training Epoch: 33 [21632/50000]\tLoss: 4.6301\tLR: 3.242967\n",
      "Training Epoch: 33 [21760/50000]\tLoss: 4.5998\tLR: 3.243223\n",
      "Training Epoch: 33 [21888/50000]\tLoss: 4.5869\tLR: 3.243478\n",
      "Training Epoch: 33 [22016/50000]\tLoss: 4.5882\tLR: 3.243734\n",
      "Training Epoch: 33 [22144/50000]\tLoss: 4.6078\tLR: 3.243990\n",
      "Training Epoch: 33 [22272/50000]\tLoss: 4.5646\tLR: 3.244246\n",
      "Training Epoch: 33 [22400/50000]\tLoss: 4.5456\tLR: 3.244501\n",
      "Training Epoch: 33 [22528/50000]\tLoss: 4.5809\tLR: 3.244757\n",
      "Training Epoch: 33 [22656/50000]\tLoss: 4.4845\tLR: 3.245013\n",
      "Training Epoch: 33 [22784/50000]\tLoss: 4.5884\tLR: 3.245269\n",
      "Training Epoch: 33 [22912/50000]\tLoss: 4.6107\tLR: 3.245524\n",
      "Training Epoch: 33 [23040/50000]\tLoss: 4.5861\tLR: 3.245780\n",
      "Training Epoch: 33 [23168/50000]\tLoss: 4.5491\tLR: 3.246036\n",
      "Training Epoch: 33 [23296/50000]\tLoss: 4.6476\tLR: 3.246292\n",
      "Training Epoch: 33 [23424/50000]\tLoss: 4.5419\tLR: 3.246547\n",
      "Training Epoch: 33 [23552/50000]\tLoss: 4.5998\tLR: 3.246803\n",
      "Training Epoch: 33 [23680/50000]\tLoss: 4.5376\tLR: 3.247059\n",
      "Training Epoch: 33 [23808/50000]\tLoss: 4.5978\tLR: 3.247315\n",
      "Training Epoch: 33 [23936/50000]\tLoss: 4.5351\tLR: 3.247570\n",
      "Training Epoch: 33 [24064/50000]\tLoss: 4.5747\tLR: 3.247826\n",
      "Training Epoch: 33 [24192/50000]\tLoss: 4.5361\tLR: 3.248082\n",
      "Training Epoch: 33 [24320/50000]\tLoss: 4.5741\tLR: 3.248338\n",
      "Training Epoch: 33 [24448/50000]\tLoss: 4.4954\tLR: 3.248593\n",
      "Training Epoch: 33 [24576/50000]\tLoss: 4.6248\tLR: 3.248849\n",
      "Training Epoch: 33 [24704/50000]\tLoss: 4.5455\tLR: 3.249105\n",
      "Training Epoch: 33 [24832/50000]\tLoss: 4.5413\tLR: 3.249361\n",
      "Training Epoch: 33 [24960/50000]\tLoss: 4.6148\tLR: 3.249616\n",
      "Training Epoch: 33 [25088/50000]\tLoss: 4.6772\tLR: 3.249872\n",
      "Training Epoch: 33 [25216/50000]\tLoss: 4.5622\tLR: 3.250128\n",
      "Training Epoch: 33 [25344/50000]\tLoss: 4.6339\tLR: 3.250384\n",
      "Training Epoch: 33 [25472/50000]\tLoss: 4.5721\tLR: 3.250639\n",
      "Training Epoch: 33 [25600/50000]\tLoss: 4.5821\tLR: 3.250895\n",
      "Training Epoch: 33 [25728/50000]\tLoss: 4.6770\tLR: 3.251151\n",
      "Training Epoch: 33 [25856/50000]\tLoss: 4.6074\tLR: 3.251407\n",
      "Training Epoch: 33 [25984/50000]\tLoss: 4.5944\tLR: 3.251662\n",
      "Training Epoch: 33 [26112/50000]\tLoss: 4.5081\tLR: 3.251918\n",
      "Training Epoch: 33 [26240/50000]\tLoss: 4.6001\tLR: 3.252174\n",
      "Training Epoch: 33 [26368/50000]\tLoss: 4.5036\tLR: 3.252430\n",
      "Training Epoch: 33 [26496/50000]\tLoss: 4.6028\tLR: 3.252685\n",
      "Training Epoch: 33 [26624/50000]\tLoss: 4.5893\tLR: 3.252941\n",
      "Training Epoch: 33 [26752/50000]\tLoss: 4.4036\tLR: 3.253197\n",
      "Training Epoch: 33 [26880/50000]\tLoss: 4.5436\tLR: 3.253453\n",
      "Training Epoch: 33 [27008/50000]\tLoss: 4.5221\tLR: 3.253708\n",
      "Training Epoch: 33 [27136/50000]\tLoss: 4.5888\tLR: 3.253964\n",
      "Training Epoch: 33 [27264/50000]\tLoss: 4.5385\tLR: 3.254220\n",
      "Training Epoch: 33 [27392/50000]\tLoss: 4.5319\tLR: 3.254476\n",
      "Training Epoch: 33 [27520/50000]\tLoss: 4.5422\tLR: 3.254731\n",
      "Training Epoch: 33 [27648/50000]\tLoss: 4.6007\tLR: 3.254987\n",
      "Training Epoch: 33 [27776/50000]\tLoss: 4.3783\tLR: 3.255243\n",
      "Training Epoch: 33 [27904/50000]\tLoss: 4.6090\tLR: 3.255499\n",
      "Training Epoch: 33 [28032/50000]\tLoss: 4.5976\tLR: 3.255754\n",
      "Training Epoch: 33 [28160/50000]\tLoss: 4.6001\tLR: 3.256010\n",
      "Training Epoch: 33 [28288/50000]\tLoss: 4.6987\tLR: 3.256266\n",
      "Training Epoch: 33 [28416/50000]\tLoss: 4.6174\tLR: 3.256522\n",
      "Training Epoch: 33 [28544/50000]\tLoss: 4.6528\tLR: 3.256777\n",
      "Training Epoch: 33 [28672/50000]\tLoss: 4.5179\tLR: 3.257033\n",
      "Training Epoch: 33 [28800/50000]\tLoss: 4.6355\tLR: 3.257289\n",
      "Training Epoch: 33 [28928/50000]\tLoss: 4.5045\tLR: 3.257545\n",
      "Training Epoch: 33 [29056/50000]\tLoss: 4.6063\tLR: 3.257801\n",
      "Training Epoch: 33 [29184/50000]\tLoss: 4.5850\tLR: 3.258056\n",
      "Training Epoch: 33 [29312/50000]\tLoss: 4.6420\tLR: 3.258312\n",
      "Training Epoch: 33 [29440/50000]\tLoss: 4.6261\tLR: 3.258568\n",
      "Training Epoch: 33 [29568/50000]\tLoss: 4.5478\tLR: 3.258824\n",
      "Training Epoch: 33 [29696/50000]\tLoss: 4.6176\tLR: 3.259079\n",
      "Training Epoch: 33 [29824/50000]\tLoss: 4.5627\tLR: 3.259335\n",
      "Training Epoch: 33 [29952/50000]\tLoss: 4.5252\tLR: 3.259591\n",
      "Training Epoch: 33 [30080/50000]\tLoss: 4.6569\tLR: 3.259847\n",
      "Training Epoch: 33 [30208/50000]\tLoss: 4.4820\tLR: 3.260102\n",
      "Training Epoch: 33 [30336/50000]\tLoss: 4.5098\tLR: 3.260358\n",
      "Training Epoch: 33 [30464/50000]\tLoss: 4.5917\tLR: 3.260614\n",
      "Training Epoch: 33 [30592/50000]\tLoss: 4.5350\tLR: 3.260870\n",
      "Training Epoch: 33 [30720/50000]\tLoss: 4.5516\tLR: 3.261125\n",
      "Training Epoch: 33 [30848/50000]\tLoss: 4.6196\tLR: 3.261381\n",
      "Training Epoch: 33 [30976/50000]\tLoss: 4.5280\tLR: 3.261637\n",
      "Training Epoch: 33 [31104/50000]\tLoss: 4.5333\tLR: 3.261893\n",
      "Training Epoch: 33 [31232/50000]\tLoss: 4.5659\tLR: 3.262148\n",
      "Training Epoch: 33 [31360/50000]\tLoss: 4.6495\tLR: 3.262404\n",
      "Training Epoch: 33 [31488/50000]\tLoss: 4.6154\tLR: 3.262660\n",
      "Training Epoch: 33 [31616/50000]\tLoss: 4.5926\tLR: 3.262916\n",
      "Training Epoch: 33 [31744/50000]\tLoss: 4.5063\tLR: 3.263171\n",
      "Training Epoch: 33 [31872/50000]\tLoss: 4.5067\tLR: 3.263427\n",
      "Training Epoch: 33 [32000/50000]\tLoss: 4.4732\tLR: 3.263683\n",
      "Training Epoch: 33 [32128/50000]\tLoss: 4.5157\tLR: 3.263939\n",
      "Training Epoch: 33 [32256/50000]\tLoss: 4.5591\tLR: 3.264194\n",
      "Training Epoch: 33 [32384/50000]\tLoss: 4.5582\tLR: 3.264450\n",
      "Training Epoch: 33 [32512/50000]\tLoss: 4.5356\tLR: 3.264706\n",
      "Training Epoch: 33 [32640/50000]\tLoss: 4.6017\tLR: 3.264962\n",
      "Training Epoch: 33 [32768/50000]\tLoss: 4.6315\tLR: 3.265217\n",
      "Training Epoch: 33 [32896/50000]\tLoss: 4.5426\tLR: 3.265473\n",
      "Training Epoch: 33 [33024/50000]\tLoss: 4.6517\tLR: 3.265729\n",
      "Training Epoch: 33 [33152/50000]\tLoss: 4.5567\tLR: 3.265985\n",
      "Training Epoch: 33 [33280/50000]\tLoss: 4.7110\tLR: 3.266240\n",
      "Training Epoch: 33 [33408/50000]\tLoss: 4.4572\tLR: 3.266496\n",
      "Training Epoch: 33 [33536/50000]\tLoss: 4.5799\tLR: 3.266752\n",
      "Training Epoch: 33 [33664/50000]\tLoss: 4.5247\tLR: 3.267008\n",
      "Training Epoch: 33 [33792/50000]\tLoss: 4.5278\tLR: 3.267263\n",
      "Training Epoch: 33 [33920/50000]\tLoss: 4.4993\tLR: 3.267519\n",
      "Training Epoch: 33 [34048/50000]\tLoss: 4.5262\tLR: 3.267775\n",
      "Training Epoch: 33 [34176/50000]\tLoss: 4.5739\tLR: 3.268031\n",
      "Training Epoch: 33 [34304/50000]\tLoss: 4.4636\tLR: 3.268286\n",
      "Training Epoch: 33 [34432/50000]\tLoss: 4.5436\tLR: 3.268542\n",
      "Training Epoch: 33 [34560/50000]\tLoss: 4.5602\tLR: 3.268798\n",
      "Training Epoch: 33 [34688/50000]\tLoss: 4.5345\tLR: 3.269054\n",
      "Training Epoch: 33 [34816/50000]\tLoss: 4.5508\tLR: 3.269309\n",
      "Training Epoch: 33 [34944/50000]\tLoss: 4.5188\tLR: 3.269565\n",
      "Training Epoch: 33 [35072/50000]\tLoss: 4.5435\tLR: 3.269821\n",
      "Training Epoch: 33 [35200/50000]\tLoss: 4.6041\tLR: 3.270077\n",
      "Training Epoch: 33 [35328/50000]\tLoss: 4.4959\tLR: 3.270332\n",
      "Training Epoch: 33 [35456/50000]\tLoss: 4.5971\tLR: 3.270588\n",
      "Training Epoch: 33 [35584/50000]\tLoss: 4.5173\tLR: 3.270844\n",
      "Training Epoch: 33 [35712/50000]\tLoss: 4.5274\tLR: 3.271100\n",
      "Training Epoch: 33 [35840/50000]\tLoss: 4.6004\tLR: 3.271355\n",
      "Training Epoch: 33 [35968/50000]\tLoss: 4.4918\tLR: 3.271611\n",
      "Training Epoch: 33 [36096/50000]\tLoss: 4.5099\tLR: 3.271867\n",
      "Training Epoch: 33 [36224/50000]\tLoss: 4.5423\tLR: 3.272123\n",
      "Training Epoch: 33 [36352/50000]\tLoss: 4.7099\tLR: 3.272379\n",
      "Training Epoch: 33 [36480/50000]\tLoss: 4.4993\tLR: 3.272634\n",
      "Training Epoch: 33 [36608/50000]\tLoss: 4.6335\tLR: 3.272890\n",
      "Training Epoch: 33 [36736/50000]\tLoss: 4.6262\tLR: 3.273146\n",
      "Training Epoch: 33 [36864/50000]\tLoss: 4.6462\tLR: 3.273402\n",
      "Training Epoch: 33 [36992/50000]\tLoss: 4.6200\tLR: 3.273657\n",
      "Training Epoch: 33 [37120/50000]\tLoss: 4.5827\tLR: 3.273913\n",
      "Training Epoch: 33 [37248/50000]\tLoss: 4.4393\tLR: 3.274169\n",
      "Training Epoch: 33 [37376/50000]\tLoss: 4.7041\tLR: 3.274425\n",
      "Training Epoch: 33 [37504/50000]\tLoss: 4.4852\tLR: 3.274680\n",
      "Training Epoch: 33 [37632/50000]\tLoss: 4.5586\tLR: 3.274936\n",
      "Training Epoch: 33 [37760/50000]\tLoss: 4.6235\tLR: 3.275192\n",
      "Training Epoch: 33 [37888/50000]\tLoss: 4.6202\tLR: 3.275448\n",
      "Training Epoch: 33 [38016/50000]\tLoss: 4.6261\tLR: 3.275703\n",
      "Training Epoch: 33 [38144/50000]\tLoss: 4.5717\tLR: 3.275959\n",
      "Training Epoch: 33 [38272/50000]\tLoss: 4.5239\tLR: 3.276215\n",
      "Training Epoch: 33 [38400/50000]\tLoss: 4.6683\tLR: 3.276471\n",
      "Training Epoch: 33 [38528/50000]\tLoss: 4.6347\tLR: 3.276726\n",
      "Training Epoch: 33 [38656/50000]\tLoss: 4.6370\tLR: 3.276982\n",
      "Training Epoch: 33 [38784/50000]\tLoss: 4.6144\tLR: 3.277238\n",
      "Training Epoch: 33 [38912/50000]\tLoss: 4.5215\tLR: 3.277494\n",
      "Training Epoch: 33 [39040/50000]\tLoss: 4.5731\tLR: 3.277749\n",
      "Training Epoch: 33 [39168/50000]\tLoss: 4.6546\tLR: 3.278005\n",
      "Training Epoch: 33 [39296/50000]\tLoss: 4.5180\tLR: 3.278261\n",
      "Training Epoch: 33 [39424/50000]\tLoss: 4.5503\tLR: 3.278517\n",
      "Training Epoch: 33 [39552/50000]\tLoss: 4.4370\tLR: 3.278772\n",
      "Training Epoch: 33 [39680/50000]\tLoss: 4.4998\tLR: 3.279028\n",
      "Training Epoch: 33 [39808/50000]\tLoss: 4.4577\tLR: 3.279284\n",
      "Training Epoch: 33 [39936/50000]\tLoss: 4.5475\tLR: 3.279540\n",
      "Training Epoch: 33 [40064/50000]\tLoss: 4.5932\tLR: 3.279795\n",
      "Training Epoch: 33 [40192/50000]\tLoss: 4.5357\tLR: 3.280051\n",
      "Training Epoch: 33 [40320/50000]\tLoss: 4.4987\tLR: 3.280307\n",
      "Training Epoch: 33 [40448/50000]\tLoss: 4.5020\tLR: 3.280563\n",
      "Training Epoch: 33 [40576/50000]\tLoss: 4.4461\tLR: 3.280818\n",
      "Training Epoch: 33 [40704/50000]\tLoss: 4.5311\tLR: 3.281074\n",
      "Training Epoch: 33 [40832/50000]\tLoss: 4.5436\tLR: 3.281330\n",
      "Training Epoch: 33 [40960/50000]\tLoss: 4.4973\tLR: 3.281586\n",
      "Training Epoch: 33 [41088/50000]\tLoss: 4.6552\tLR: 3.281841\n",
      "Training Epoch: 33 [41216/50000]\tLoss: 4.5423\tLR: 3.282097\n",
      "Training Epoch: 33 [41344/50000]\tLoss: 4.5099\tLR: 3.282353\n",
      "Training Epoch: 33 [41472/50000]\tLoss: 4.4592\tLR: 3.282609\n",
      "Training Epoch: 33 [41600/50000]\tLoss: 4.5170\tLR: 3.282864\n",
      "Training Epoch: 33 [41728/50000]\tLoss: 4.4435\tLR: 3.283120\n",
      "Training Epoch: 33 [41856/50000]\tLoss: 4.5562\tLR: 3.283376\n",
      "Training Epoch: 33 [41984/50000]\tLoss: 4.4901\tLR: 3.283632\n",
      "Training Epoch: 33 [42112/50000]\tLoss: 4.5061\tLR: 3.283887\n",
      "Training Epoch: 33 [42240/50000]\tLoss: 4.5792\tLR: 3.284143\n",
      "Training Epoch: 33 [42368/50000]\tLoss: 4.5833\tLR: 3.284399\n",
      "Training Epoch: 33 [42496/50000]\tLoss: 4.5390\tLR: 3.284655\n",
      "Training Epoch: 33 [42624/50000]\tLoss: 4.5067\tLR: 3.284910\n",
      "Training Epoch: 33 [42752/50000]\tLoss: 4.5787\tLR: 3.285166\n",
      "Training Epoch: 33 [42880/50000]\tLoss: 4.4892\tLR: 3.285422\n",
      "Training Epoch: 33 [43008/50000]\tLoss: 4.5264\tLR: 3.285678\n",
      "Training Epoch: 33 [43136/50000]\tLoss: 4.5575\tLR: 3.285934\n",
      "Training Epoch: 33 [43264/50000]\tLoss: 4.5427\tLR: 3.286189\n",
      "Training Epoch: 33 [43392/50000]\tLoss: 4.6167\tLR: 3.286445\n",
      "Training Epoch: 33 [43520/50000]\tLoss: 4.6031\tLR: 3.286701\n",
      "Training Epoch: 33 [43648/50000]\tLoss: 4.5869\tLR: 3.286957\n",
      "Training Epoch: 33 [43776/50000]\tLoss: 4.4703\tLR: 3.287212\n",
      "Training Epoch: 33 [43904/50000]\tLoss: 4.5656\tLR: 3.287468\n",
      "Training Epoch: 33 [44032/50000]\tLoss: 4.5875\tLR: 3.287724\n",
      "Training Epoch: 33 [44160/50000]\tLoss: 4.5243\tLR: 3.287980\n",
      "Training Epoch: 33 [44288/50000]\tLoss: 4.5568\tLR: 3.288235\n",
      "Training Epoch: 33 [44416/50000]\tLoss: 4.5630\tLR: 3.288491\n",
      "Training Epoch: 33 [44544/50000]\tLoss: 4.5471\tLR: 3.288747\n",
      "Training Epoch: 33 [44672/50000]\tLoss: 4.5515\tLR: 3.289003\n",
      "Training Epoch: 33 [44800/50000]\tLoss: 4.4693\tLR: 3.289258\n",
      "Training Epoch: 33 [44928/50000]\tLoss: 4.5387\tLR: 3.289514\n",
      "Training Epoch: 33 [45056/50000]\tLoss: 5.4404\tLR: 3.289770\n",
      "Training Epoch: 33 [45184/50000]\tLoss: 4.6809\tLR: 3.290026\n",
      "Training Epoch: 33 [45312/50000]\tLoss: 4.8068\tLR: 3.290281\n",
      "Training Epoch: 33 [45440/50000]\tLoss: 4.7539\tLR: 3.290537\n",
      "Training Epoch: 33 [45568/50000]\tLoss: 4.7030\tLR: 3.290793\n",
      "Training Epoch: 33 [45696/50000]\tLoss: 4.6290\tLR: 3.291049\n",
      "Training Epoch: 33 [45824/50000]\tLoss: 4.7353\tLR: 3.291304\n",
      "Training Epoch: 33 [45952/50000]\tLoss: 4.6452\tLR: 3.291560\n",
      "Training Epoch: 33 [46080/50000]\tLoss: 4.6688\tLR: 3.291816\n",
      "Training Epoch: 33 [46208/50000]\tLoss: 4.7089\tLR: 3.292072\n",
      "Training Epoch: 33 [46336/50000]\tLoss: 4.6543\tLR: 3.292327\n",
      "Training Epoch: 33 [46464/50000]\tLoss: 4.6948\tLR: 3.292583\n",
      "Training Epoch: 33 [46592/50000]\tLoss: 4.6784\tLR: 3.292839\n",
      "Training Epoch: 33 [46720/50000]\tLoss: 4.6268\tLR: 3.293095\n",
      "Training Epoch: 33 [46848/50000]\tLoss: 4.6458\tLR: 3.293350\n",
      "Training Epoch: 33 [46976/50000]\tLoss: 4.6864\tLR: 3.293606\n",
      "Training Epoch: 33 [47104/50000]\tLoss: 4.6199\tLR: 3.293862\n",
      "Training Epoch: 33 [47232/50000]\tLoss: 4.5571\tLR: 3.294118\n",
      "Training Epoch: 33 [47360/50000]\tLoss: 4.6601\tLR: 3.294373\n",
      "Training Epoch: 33 [47488/50000]\tLoss: 4.6385\tLR: 3.294629\n",
      "Training Epoch: 33 [47616/50000]\tLoss: 4.6635\tLR: 3.294885\n",
      "Training Epoch: 33 [47744/50000]\tLoss: 4.6789\tLR: 3.295141\n",
      "Training Epoch: 33 [47872/50000]\tLoss: 4.6649\tLR: 3.295396\n",
      "Training Epoch: 33 [48000/50000]\tLoss: 4.6761\tLR: 3.295652\n",
      "Training Epoch: 33 [48128/50000]\tLoss: 4.6207\tLR: 3.295908\n",
      "Training Epoch: 33 [48256/50000]\tLoss: 4.6992\tLR: 3.296164\n",
      "Training Epoch: 33 [48384/50000]\tLoss: 4.5788\tLR: 3.296419\n",
      "Training Epoch: 33 [48512/50000]\tLoss: 4.6371\tLR: 3.296675\n",
      "Training Epoch: 33 [48640/50000]\tLoss: 4.5920\tLR: 3.296931\n",
      "Training Epoch: 33 [48768/50000]\tLoss: 4.5056\tLR: 3.297187\n",
      "Training Epoch: 33 [48896/50000]\tLoss: 4.6533\tLR: 3.297442\n",
      "Training Epoch: 33 [49024/50000]\tLoss: 4.6104\tLR: 3.297698\n",
      "Training Epoch: 33 [49152/50000]\tLoss: 4.5919\tLR: 3.297954\n",
      "Training Epoch: 33 [49280/50000]\tLoss: 4.5864\tLR: 3.298210\n",
      "Training Epoch: 33 [49408/50000]\tLoss: 4.5729\tLR: 3.298465\n",
      "Training Epoch: 33 [49536/50000]\tLoss: 4.6035\tLR: 3.298721\n",
      "Training Epoch: 33 [49664/50000]\tLoss: 4.6488\tLR: 3.298977\n",
      "Training Epoch: 33 [49792/50000]\tLoss: 4.7418\tLR: 3.299233\n",
      "Training Epoch: 33 [49920/50000]\tLoss: 4.6414\tLR: 3.299488\n",
      "Training Epoch: 33 [50000/50000]\tLoss: 4.5772\tLR: 3.299744\n",
      "epoch 33 training time consumed: 489.15s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   46264 GB |   46264 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   46122 GB |   46122 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     142 GB |     142 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   46264 GB |   46264 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   46122 GB |   46122 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     142 GB |     142 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   45612 GB |   45612 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   45470 GB |   45470 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     142 GB |     142 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    4905 K  |    4905 K  |\n",
      "|       from large pool |      24    |      65    |    2091 K  |    2091 K  |\n",
      "|       from small pool |     231    |     274    |    2814 K  |    2814 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    4905 K  |    4905 K  |\n",
      "|       from large pool |      24    |      65    |    2091 K  |    2091 K  |\n",
      "|       from small pool |     231    |     274    |    2814 K  |    2814 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      46    |    2842 K  |    2842 K  |\n",
      "|       from large pool |      10    |      23    |    1005 K  |    1005 K  |\n",
      "|       from small pool |      25    |      35    |    1837 K  |    1837 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 33, Average loss: 0.0370, Accuracy: 0.0100, Time consumed:30.91s\n",
      "\n",
      "Training Epoch: 34 [128/50000]\tLoss: 4.5916\tLR: 0.100000\n",
      "Training Epoch: 34 [256/50000]\tLoss: 4.7092\tLR: 3.300256\n",
      "Training Epoch: 34 [384/50000]\tLoss: 4.5903\tLR: 3.300512\n",
      "Training Epoch: 34 [512/50000]\tLoss: 4.5956\tLR: 3.300767\n",
      "Training Epoch: 34 [640/50000]\tLoss: 4.5632\tLR: 3.301023\n",
      "Training Epoch: 34 [768/50000]\tLoss: 4.6210\tLR: 3.301279\n",
      "Training Epoch: 34 [896/50000]\tLoss: 4.5313\tLR: 3.301535\n",
      "Training Epoch: 34 [1024/50000]\tLoss: 4.5836\tLR: 3.301790\n",
      "Training Epoch: 34 [1152/50000]\tLoss: 4.5359\tLR: 3.302046\n",
      "Training Epoch: 34 [1280/50000]\tLoss: 4.6891\tLR: 3.302302\n",
      "Training Epoch: 34 [1408/50000]\tLoss: 4.5944\tLR: 3.302558\n",
      "Training Epoch: 34 [1536/50000]\tLoss: 4.6572\tLR: 3.302813\n",
      "Training Epoch: 34 [1664/50000]\tLoss: 4.6039\tLR: 3.303069\n",
      "Training Epoch: 34 [1792/50000]\tLoss: 4.6431\tLR: 3.303325\n",
      "Training Epoch: 34 [1920/50000]\tLoss: 4.6648\tLR: 3.303581\n",
      "Training Epoch: 34 [2048/50000]\tLoss: 4.6179\tLR: 3.303836\n",
      "Training Epoch: 34 [2176/50000]\tLoss: 4.5860\tLR: 3.304092\n",
      "Training Epoch: 34 [2304/50000]\tLoss: 4.6518\tLR: 3.304348\n",
      "Training Epoch: 34 [2432/50000]\tLoss: 4.6390\tLR: 3.304604\n",
      "Training Epoch: 34 [2560/50000]\tLoss: 4.6017\tLR: 3.304859\n",
      "Training Epoch: 34 [2688/50000]\tLoss: 4.5810\tLR: 3.305115\n",
      "Training Epoch: 34 [2816/50000]\tLoss: 4.6256\tLR: 3.305371\n",
      "Training Epoch: 34 [2944/50000]\tLoss: 4.5708\tLR: 3.305627\n",
      "Training Epoch: 34 [3072/50000]\tLoss: 4.6404\tLR: 3.305882\n",
      "Training Epoch: 34 [3200/50000]\tLoss: 4.5815\tLR: 3.306138\n",
      "Training Epoch: 34 [3328/50000]\tLoss: 4.6246\tLR: 3.306394\n",
      "Training Epoch: 34 [3456/50000]\tLoss: 4.6407\tLR: 3.306650\n",
      "Training Epoch: 34 [3584/50000]\tLoss: 4.5916\tLR: 3.306905\n",
      "Training Epoch: 34 [3712/50000]\tLoss: 4.5556\tLR: 3.307161\n",
      "Training Epoch: 34 [3840/50000]\tLoss: 4.6264\tLR: 3.307417\n",
      "Training Epoch: 34 [3968/50000]\tLoss: 4.5595\tLR: 3.307673\n",
      "Training Epoch: 34 [4096/50000]\tLoss: 4.5323\tLR: 3.307928\n",
      "Training Epoch: 34 [4224/50000]\tLoss: 4.7047\tLR: 3.308184\n",
      "Training Epoch: 34 [4352/50000]\tLoss: 4.4949\tLR: 3.308440\n",
      "Training Epoch: 34 [4480/50000]\tLoss: 4.5968\tLR: 3.308696\n",
      "Training Epoch: 34 [4608/50000]\tLoss: 4.5694\tLR: 3.308951\n",
      "Training Epoch: 34 [4736/50000]\tLoss: 4.5226\tLR: 3.309207\n",
      "Training Epoch: 34 [4864/50000]\tLoss: 4.5603\tLR: 3.309463\n",
      "Training Epoch: 34 [4992/50000]\tLoss: 4.6119\tLR: 3.309719\n",
      "Training Epoch: 34 [5120/50000]\tLoss: 4.5518\tLR: 3.309974\n",
      "Training Epoch: 34 [5248/50000]\tLoss: 4.6020\tLR: 3.310230\n",
      "Training Epoch: 34 [5376/50000]\tLoss: 4.5503\tLR: 3.310486\n",
      "Training Epoch: 34 [5504/50000]\tLoss: 4.5696\tLR: 3.310742\n",
      "Training Epoch: 34 [5632/50000]\tLoss: 4.6203\tLR: 3.310997\n",
      "Training Epoch: 34 [5760/50000]\tLoss: 4.4607\tLR: 3.311253\n",
      "Training Epoch: 34 [5888/50000]\tLoss: 4.5332\tLR: 3.311509\n",
      "Training Epoch: 34 [6016/50000]\tLoss: 4.4904\tLR: 3.311765\n",
      "Training Epoch: 34 [6144/50000]\tLoss: 4.6581\tLR: 3.312020\n",
      "Training Epoch: 34 [6272/50000]\tLoss: 4.5096\tLR: 3.312276\n",
      "Training Epoch: 34 [6400/50000]\tLoss: 4.5065\tLR: 3.312532\n",
      "Training Epoch: 34 [6528/50000]\tLoss: 4.5593\tLR: 3.312788\n",
      "Training Epoch: 34 [6656/50000]\tLoss: 4.5719\tLR: 3.313043\n",
      "Training Epoch: 34 [6784/50000]\tLoss: 4.6046\tLR: 3.313299\n",
      "Training Epoch: 34 [6912/50000]\tLoss: 4.5255\tLR: 3.313555\n",
      "Training Epoch: 34 [7040/50000]\tLoss: 4.6079\tLR: 3.313811\n",
      "Training Epoch: 34 [7168/50000]\tLoss: 4.6049\tLR: 3.314066\n",
      "Training Epoch: 34 [7296/50000]\tLoss: 4.5380\tLR: 3.314322\n",
      "Training Epoch: 34 [7424/50000]\tLoss: 4.5864\tLR: 3.314578\n",
      "Training Epoch: 34 [7552/50000]\tLoss: 4.6699\tLR: 3.314834\n",
      "Training Epoch: 34 [7680/50000]\tLoss: 4.6152\tLR: 3.315090\n",
      "Training Epoch: 34 [7808/50000]\tLoss: 4.6219\tLR: 3.315345\n",
      "Training Epoch: 34 [7936/50000]\tLoss: 4.5618\tLR: 3.315601\n",
      "Training Epoch: 34 [8064/50000]\tLoss: 4.5062\tLR: 3.315857\n",
      "Training Epoch: 34 [8192/50000]\tLoss: 4.5308\tLR: 3.316113\n",
      "Training Epoch: 34 [8320/50000]\tLoss: 4.4518\tLR: 3.316368\n",
      "Training Epoch: 34 [8448/50000]\tLoss: 4.4848\tLR: 3.316624\n",
      "Training Epoch: 34 [8576/50000]\tLoss: 4.6650\tLR: 3.316880\n",
      "Training Epoch: 34 [8704/50000]\tLoss: 4.5214\tLR: 3.317136\n",
      "Training Epoch: 34 [8832/50000]\tLoss: 4.5524\tLR: 3.317391\n",
      "Training Epoch: 34 [8960/50000]\tLoss: 4.5546\tLR: 3.317647\n",
      "Training Epoch: 34 [9088/50000]\tLoss: 4.6770\tLR: 3.317903\n",
      "Training Epoch: 34 [9216/50000]\tLoss: 4.4660\tLR: 3.318159\n",
      "Training Epoch: 34 [9344/50000]\tLoss: 4.5082\tLR: 3.318414\n",
      "Training Epoch: 34 [9472/50000]\tLoss: 4.6094\tLR: 3.318670\n",
      "Training Epoch: 34 [9600/50000]\tLoss: 4.4672\tLR: 3.318926\n",
      "Training Epoch: 34 [9728/50000]\tLoss: 4.6551\tLR: 3.319182\n",
      "Training Epoch: 34 [9856/50000]\tLoss: 4.6505\tLR: 3.319437\n",
      "Training Epoch: 34 [9984/50000]\tLoss: 4.5330\tLR: 3.319693\n",
      "Training Epoch: 34 [10112/50000]\tLoss: 4.5065\tLR: 3.319949\n",
      "Training Epoch: 34 [10240/50000]\tLoss: 4.5247\tLR: 3.320205\n",
      "Training Epoch: 34 [10368/50000]\tLoss: 4.5493\tLR: 3.320460\n",
      "Training Epoch: 34 [10496/50000]\tLoss: 4.4600\tLR: 3.320716\n",
      "Training Epoch: 34 [10624/50000]\tLoss: 4.5208\tLR: 3.320972\n",
      "Training Epoch: 34 [10752/50000]\tLoss: 4.5313\tLR: 3.321228\n",
      "Training Epoch: 34 [10880/50000]\tLoss: 4.5224\tLR: 3.321483\n",
      "Training Epoch: 34 [11008/50000]\tLoss: 4.6460\tLR: 3.321739\n",
      "Training Epoch: 34 [11136/50000]\tLoss: 4.5894\tLR: 3.321995\n",
      "Training Epoch: 34 [11264/50000]\tLoss: 4.6097\tLR: 3.322251\n",
      "Training Epoch: 34 [11392/50000]\tLoss: 4.5285\tLR: 3.322506\n",
      "Training Epoch: 34 [11520/50000]\tLoss: 4.5518\tLR: 3.322762\n",
      "Training Epoch: 34 [11648/50000]\tLoss: 4.4928\tLR: 3.323018\n",
      "Training Epoch: 34 [11776/50000]\tLoss: 4.6003\tLR: 3.323274\n",
      "Training Epoch: 34 [11904/50000]\tLoss: 4.4893\tLR: 3.323529\n",
      "Training Epoch: 34 [12032/50000]\tLoss: 4.5220\tLR: 3.323785\n",
      "Training Epoch: 34 [12160/50000]\tLoss: 4.4598\tLR: 3.324041\n",
      "Training Epoch: 34 [12288/50000]\tLoss: 4.6161\tLR: 3.324297\n",
      "Training Epoch: 34 [12416/50000]\tLoss: 4.5217\tLR: 3.324552\n",
      "Training Epoch: 34 [12544/50000]\tLoss: 4.5787\tLR: 3.324808\n",
      "Training Epoch: 34 [12672/50000]\tLoss: 4.5957\tLR: 3.325064\n",
      "Training Epoch: 34 [12800/50000]\tLoss: 4.6075\tLR: 3.325320\n",
      "Training Epoch: 34 [12928/50000]\tLoss: 4.6088\tLR: 3.325575\n",
      "Training Epoch: 34 [13056/50000]\tLoss: 4.5497\tLR: 3.325831\n",
      "Training Epoch: 34 [13184/50000]\tLoss: 4.5814\tLR: 3.326087\n",
      "Training Epoch: 34 [13312/50000]\tLoss: 4.5888\tLR: 3.326343\n",
      "Training Epoch: 34 [13440/50000]\tLoss: 4.5728\tLR: 3.326598\n",
      "Training Epoch: 34 [13568/50000]\tLoss: 4.5133\tLR: 3.326854\n",
      "Training Epoch: 34 [13696/50000]\tLoss: 4.6039\tLR: 3.327110\n",
      "Training Epoch: 34 [13824/50000]\tLoss: 4.5660\tLR: 3.327366\n",
      "Training Epoch: 34 [13952/50000]\tLoss: 4.6369\tLR: 3.327621\n",
      "Training Epoch: 34 [14080/50000]\tLoss: 4.6358\tLR: 3.327877\n",
      "Training Epoch: 34 [14208/50000]\tLoss: 4.5346\tLR: 3.328133\n",
      "Training Epoch: 34 [14336/50000]\tLoss: 4.5841\tLR: 3.328389\n",
      "Training Epoch: 34 [14464/50000]\tLoss: 4.5873\tLR: 3.328645\n",
      "Training Epoch: 34 [14592/50000]\tLoss: 4.6627\tLR: 3.328900\n",
      "Training Epoch: 34 [14720/50000]\tLoss: 4.5057\tLR: 3.329156\n",
      "Training Epoch: 34 [14848/50000]\tLoss: 4.4526\tLR: 3.329412\n",
      "Training Epoch: 34 [14976/50000]\tLoss: 5.1929\tLR: 3.329668\n",
      "Training Epoch: 34 [15104/50000]\tLoss: 4.7366\tLR: 3.329923\n",
      "Training Epoch: 34 [15232/50000]\tLoss: 4.6551\tLR: 3.330179\n",
      "Training Epoch: 34 [15360/50000]\tLoss: 4.7671\tLR: 3.330435\n",
      "Training Epoch: 34 [15488/50000]\tLoss: 4.6439\tLR: 3.330691\n",
      "Training Epoch: 34 [15616/50000]\tLoss: 4.6576\tLR: 3.330946\n",
      "Training Epoch: 34 [15744/50000]\tLoss: 4.6604\tLR: 3.331202\n",
      "Training Epoch: 34 [15872/50000]\tLoss: 4.8050\tLR: 3.331458\n",
      "Training Epoch: 34 [16000/50000]\tLoss: 4.6811\tLR: 3.331714\n",
      "Training Epoch: 34 [16128/50000]\tLoss: 4.6257\tLR: 3.331969\n",
      "Training Epoch: 34 [16256/50000]\tLoss: 4.6361\tLR: 3.332225\n",
      "Training Epoch: 34 [16384/50000]\tLoss: 4.6475\tLR: 3.332481\n",
      "Training Epoch: 34 [16512/50000]\tLoss: 4.7171\tLR: 3.332737\n",
      "Training Epoch: 34 [16640/50000]\tLoss: 4.7072\tLR: 3.332992\n",
      "Training Epoch: 34 [16768/50000]\tLoss: 4.6953\tLR: 3.333248\n",
      "Training Epoch: 34 [16896/50000]\tLoss: 4.6470\tLR: 3.333504\n",
      "Training Epoch: 34 [17024/50000]\tLoss: 4.6712\tLR: 3.333760\n",
      "Training Epoch: 34 [17152/50000]\tLoss: 4.6747\tLR: 3.334015\n",
      "Training Epoch: 34 [17280/50000]\tLoss: 4.6831\tLR: 3.334271\n",
      "Training Epoch: 34 [17408/50000]\tLoss: 4.6537\tLR: 3.334527\n",
      "Training Epoch: 34 [17536/50000]\tLoss: 4.6543\tLR: 3.334783\n",
      "Training Epoch: 34 [17664/50000]\tLoss: 4.6447\tLR: 3.335038\n",
      "Training Epoch: 34 [17792/50000]\tLoss: 4.6563\tLR: 3.335294\n",
      "Training Epoch: 34 [17920/50000]\tLoss: 4.6879\tLR: 3.335550\n",
      "Training Epoch: 34 [18048/50000]\tLoss: 4.6894\tLR: 3.335806\n",
      "Training Epoch: 34 [18176/50000]\tLoss: 4.6985\tLR: 3.336061\n",
      "Training Epoch: 34 [18304/50000]\tLoss: 4.7033\tLR: 3.336317\n",
      "Training Epoch: 34 [18432/50000]\tLoss: 4.6662\tLR: 3.336573\n",
      "Training Epoch: 34 [18560/50000]\tLoss: 4.6966\tLR: 3.336829\n",
      "Training Epoch: 34 [18688/50000]\tLoss: 4.6500\tLR: 3.337084\n",
      "Training Epoch: 34 [18816/50000]\tLoss: 4.6897\tLR: 3.337340\n",
      "Training Epoch: 34 [18944/50000]\tLoss: 4.6474\tLR: 3.337596\n",
      "Training Epoch: 34 [19072/50000]\tLoss: 4.6740\tLR: 3.337852\n",
      "Training Epoch: 34 [19200/50000]\tLoss: 4.7233\tLR: 3.338107\n",
      "Training Epoch: 34 [19328/50000]\tLoss: 4.5882\tLR: 3.338363\n",
      "Training Epoch: 34 [19456/50000]\tLoss: 4.6366\tLR: 3.338619\n",
      "Training Epoch: 34 [19584/50000]\tLoss: 4.6798\tLR: 3.338875\n",
      "Training Epoch: 34 [19712/50000]\tLoss: 4.6639\tLR: 3.339130\n",
      "Training Epoch: 34 [19840/50000]\tLoss: 4.6359\tLR: 3.339386\n",
      "Training Epoch: 34 [19968/50000]\tLoss: 4.6604\tLR: 3.339642\n",
      "Training Epoch: 34 [20096/50000]\tLoss: 4.6402\tLR: 3.339898\n",
      "Training Epoch: 34 [20224/50000]\tLoss: 4.5974\tLR: 3.340153\n",
      "Training Epoch: 34 [20352/50000]\tLoss: 4.6397\tLR: 3.340409\n",
      "Training Epoch: 34 [20480/50000]\tLoss: 4.6644\tLR: 3.340665\n",
      "Training Epoch: 34 [20608/50000]\tLoss: 4.7206\tLR: 3.340921\n",
      "Training Epoch: 34 [20736/50000]\tLoss: 4.7219\tLR: 3.341176\n",
      "Training Epoch: 34 [20864/50000]\tLoss: 4.7319\tLR: 3.341432\n",
      "Training Epoch: 34 [20992/50000]\tLoss: 4.6670\tLR: 3.341688\n",
      "Training Epoch: 34 [21120/50000]\tLoss: 4.6266\tLR: 3.341944\n",
      "Training Epoch: 34 [21248/50000]\tLoss: 4.6915\tLR: 3.342199\n",
      "Training Epoch: 34 [21376/50000]\tLoss: 4.7065\tLR: 3.342455\n",
      "Training Epoch: 34 [21504/50000]\tLoss: 4.6473\tLR: 3.342711\n",
      "Training Epoch: 34 [21632/50000]\tLoss: 4.6534\tLR: 3.342967\n",
      "Training Epoch: 34 [21760/50000]\tLoss: 4.6620\tLR: 3.343223\n",
      "Training Epoch: 34 [21888/50000]\tLoss: 4.6128\tLR: 3.343478\n",
      "Training Epoch: 34 [22016/50000]\tLoss: 4.6277\tLR: 3.343734\n",
      "Training Epoch: 34 [22144/50000]\tLoss: 4.6749\tLR: 3.343990\n",
      "Training Epoch: 34 [22272/50000]\tLoss: 4.6708\tLR: 3.344246\n",
      "Training Epoch: 34 [22400/50000]\tLoss: 4.6795\tLR: 3.344501\n",
      "Training Epoch: 34 [22528/50000]\tLoss: 4.6817\tLR: 3.344757\n",
      "Training Epoch: 34 [22656/50000]\tLoss: 4.6825\tLR: 3.345013\n",
      "Training Epoch: 34 [22784/50000]\tLoss: 4.6639\tLR: 3.345269\n",
      "Training Epoch: 34 [22912/50000]\tLoss: 4.6844\tLR: 3.345524\n",
      "Training Epoch: 34 [23040/50000]\tLoss: 4.6493\tLR: 3.345780\n",
      "Training Epoch: 34 [23168/50000]\tLoss: 4.6666\tLR: 3.346036\n",
      "Training Epoch: 34 [23296/50000]\tLoss: 4.6938\tLR: 3.346292\n",
      "Training Epoch: 34 [23424/50000]\tLoss: 4.6693\tLR: 3.346547\n",
      "Training Epoch: 34 [23552/50000]\tLoss: 4.6711\tLR: 3.346803\n",
      "Training Epoch: 34 [23680/50000]\tLoss: 4.7123\tLR: 3.347059\n",
      "Training Epoch: 34 [23808/50000]\tLoss: 4.6700\tLR: 3.347315\n",
      "Training Epoch: 34 [23936/50000]\tLoss: 4.6854\tLR: 3.347570\n",
      "Training Epoch: 34 [24064/50000]\tLoss: 4.6830\tLR: 3.347826\n",
      "Training Epoch: 34 [24192/50000]\tLoss: 4.7448\tLR: 3.348082\n",
      "Training Epoch: 34 [24320/50000]\tLoss: 4.6840\tLR: 3.348338\n",
      "Training Epoch: 34 [24448/50000]\tLoss: 4.6224\tLR: 3.348593\n",
      "Training Epoch: 34 [24576/50000]\tLoss: 4.6700\tLR: 3.348849\n",
      "Training Epoch: 34 [24704/50000]\tLoss: 4.6594\tLR: 3.349105\n",
      "Training Epoch: 34 [24832/50000]\tLoss: 4.5828\tLR: 3.349361\n",
      "Training Epoch: 34 [24960/50000]\tLoss: 4.7093\tLR: 3.349616\n",
      "Training Epoch: 34 [25088/50000]\tLoss: 4.7037\tLR: 3.349872\n",
      "Training Epoch: 34 [25216/50000]\tLoss: 4.6962\tLR: 3.350128\n",
      "Training Epoch: 34 [25344/50000]\tLoss: 4.6940\tLR: 3.350384\n",
      "Training Epoch: 34 [25472/50000]\tLoss: 4.6466\tLR: 3.350639\n",
      "Training Epoch: 34 [25600/50000]\tLoss: 4.6638\tLR: 3.350895\n",
      "Training Epoch: 34 [25728/50000]\tLoss: 4.6673\tLR: 3.351151\n",
      "Training Epoch: 34 [25856/50000]\tLoss: 4.6353\tLR: 3.351407\n",
      "Training Epoch: 34 [25984/50000]\tLoss: 4.5991\tLR: 3.351662\n",
      "Training Epoch: 34 [26112/50000]\tLoss: 4.6987\tLR: 3.351918\n",
      "Training Epoch: 34 [26240/50000]\tLoss: 4.6591\tLR: 3.352174\n",
      "Training Epoch: 34 [26368/50000]\tLoss: 4.6723\tLR: 3.352430\n",
      "Training Epoch: 34 [26496/50000]\tLoss: 4.6695\tLR: 3.352685\n",
      "Training Epoch: 34 [26624/50000]\tLoss: 4.6723\tLR: 3.352941\n",
      "Training Epoch: 34 [26752/50000]\tLoss: 4.6747\tLR: 3.353197\n",
      "Training Epoch: 34 [26880/50000]\tLoss: 4.6328\tLR: 3.353453\n",
      "Training Epoch: 34 [27008/50000]\tLoss: 4.6748\tLR: 3.353708\n",
      "Training Epoch: 34 [27136/50000]\tLoss: 4.6305\tLR: 3.353964\n",
      "Training Epoch: 34 [27264/50000]\tLoss: 4.6918\tLR: 3.354220\n",
      "Training Epoch: 34 [27392/50000]\tLoss: 4.6300\tLR: 3.354476\n",
      "Training Epoch: 34 [27520/50000]\tLoss: 4.6729\tLR: 3.354731\n",
      "Training Epoch: 34 [27648/50000]\tLoss: 4.6531\tLR: 3.354987\n",
      "Training Epoch: 34 [27776/50000]\tLoss: 4.7067\tLR: 3.355243\n",
      "Training Epoch: 34 [27904/50000]\tLoss: 4.7378\tLR: 3.355499\n",
      "Training Epoch: 34 [28032/50000]\tLoss: 4.6467\tLR: 3.355754\n",
      "Training Epoch: 34 [28160/50000]\tLoss: 4.6656\tLR: 3.356010\n",
      "Training Epoch: 34 [28288/50000]\tLoss: 4.7186\tLR: 3.356266\n",
      "Training Epoch: 34 [28416/50000]\tLoss: 4.6887\tLR: 3.356522\n",
      "Training Epoch: 34 [28544/50000]\tLoss: 4.6463\tLR: 3.356777\n",
      "Training Epoch: 34 [28672/50000]\tLoss: 4.7059\tLR: 3.357033\n",
      "Training Epoch: 34 [28800/50000]\tLoss: 4.6198\tLR: 3.357289\n",
      "Training Epoch: 34 [28928/50000]\tLoss: 4.6820\tLR: 3.357545\n",
      "Training Epoch: 34 [29056/50000]\tLoss: 4.6200\tLR: 3.357801\n",
      "Training Epoch: 34 [29184/50000]\tLoss: 4.6540\tLR: 3.358056\n",
      "Training Epoch: 34 [29312/50000]\tLoss: 4.6556\tLR: 3.358312\n",
      "Training Epoch: 34 [29440/50000]\tLoss: 4.6993\tLR: 3.358568\n",
      "Training Epoch: 34 [29568/50000]\tLoss: 4.6697\tLR: 3.358824\n",
      "Training Epoch: 34 [29696/50000]\tLoss: 4.6659\tLR: 3.359079\n",
      "Training Epoch: 34 [29824/50000]\tLoss: 4.6457\tLR: 3.359335\n",
      "Training Epoch: 34 [29952/50000]\tLoss: 4.6478\tLR: 3.359591\n",
      "Training Epoch: 34 [30080/50000]\tLoss: 4.7139\tLR: 3.359847\n",
      "Training Epoch: 34 [30208/50000]\tLoss: 4.6226\tLR: 3.360102\n",
      "Training Epoch: 34 [30336/50000]\tLoss: 4.6717\tLR: 3.360358\n",
      "Training Epoch: 34 [30464/50000]\tLoss: 4.6724\tLR: 3.360614\n",
      "Training Epoch: 34 [30592/50000]\tLoss: 4.6603\tLR: 3.360870\n",
      "Training Epoch: 34 [30720/50000]\tLoss: 4.6576\tLR: 3.361125\n",
      "Training Epoch: 34 [30848/50000]\tLoss: 4.6325\tLR: 3.361381\n",
      "Training Epoch: 34 [30976/50000]\tLoss: 4.6766\tLR: 3.361637\n",
      "Training Epoch: 34 [31104/50000]\tLoss: 4.6581\tLR: 3.361893\n",
      "Training Epoch: 34 [31232/50000]\tLoss: 4.5813\tLR: 3.362148\n",
      "Training Epoch: 34 [31360/50000]\tLoss: 4.6721\tLR: 3.362404\n",
      "Training Epoch: 34 [31488/50000]\tLoss: 4.6238\tLR: 3.362660\n",
      "Training Epoch: 34 [31616/50000]\tLoss: 4.6936\tLR: 3.362916\n",
      "Training Epoch: 34 [31744/50000]\tLoss: 4.7209\tLR: 3.363171\n",
      "Training Epoch: 34 [31872/50000]\tLoss: 4.6595\tLR: 3.363427\n",
      "Training Epoch: 34 [32000/50000]\tLoss: 4.6387\tLR: 3.363683\n",
      "Training Epoch: 34 [32128/50000]\tLoss: 4.6788\tLR: 3.363939\n",
      "Training Epoch: 34 [32256/50000]\tLoss: 4.7035\tLR: 3.364194\n",
      "Training Epoch: 34 [32384/50000]\tLoss: 4.6875\tLR: 3.364450\n",
      "Training Epoch: 34 [32512/50000]\tLoss: 4.6714\tLR: 3.364706\n",
      "Training Epoch: 34 [32640/50000]\tLoss: 4.6365\tLR: 3.364962\n",
      "Training Epoch: 34 [32768/50000]\tLoss: 4.6613\tLR: 3.365217\n",
      "Training Epoch: 34 [32896/50000]\tLoss: 4.6961\tLR: 3.365473\n",
      "Training Epoch: 34 [33024/50000]\tLoss: 4.6824\tLR: 3.365729\n",
      "Training Epoch: 34 [33152/50000]\tLoss: 4.6975\tLR: 3.365985\n",
      "Training Epoch: 34 [33280/50000]\tLoss: 4.6625\tLR: 3.366240\n",
      "Training Epoch: 34 [33408/50000]\tLoss: 4.6609\tLR: 3.366496\n",
      "Training Epoch: 34 [33536/50000]\tLoss: 4.6359\tLR: 3.366752\n",
      "Training Epoch: 34 [33664/50000]\tLoss: 4.6562\tLR: 3.367008\n",
      "Training Epoch: 34 [33792/50000]\tLoss: 4.7165\tLR: 3.367263\n",
      "Training Epoch: 34 [33920/50000]\tLoss: 4.7024\tLR: 3.367519\n",
      "Training Epoch: 34 [34048/50000]\tLoss: 4.6466\tLR: 3.367775\n",
      "Training Epoch: 34 [34176/50000]\tLoss: 4.6311\tLR: 3.368031\n",
      "Training Epoch: 34 [34304/50000]\tLoss: 4.6960\tLR: 3.368286\n",
      "Training Epoch: 34 [34432/50000]\tLoss: 4.6938\tLR: 3.368542\n",
      "Training Epoch: 34 [34560/50000]\tLoss: 4.6840\tLR: 3.368798\n",
      "Training Epoch: 34 [34688/50000]\tLoss: 4.7369\tLR: 3.369054\n",
      "Training Epoch: 34 [34816/50000]\tLoss: 4.6648\tLR: 3.369309\n",
      "Training Epoch: 34 [34944/50000]\tLoss: 4.6576\tLR: 3.369565\n",
      "Training Epoch: 34 [35072/50000]\tLoss: 4.6976\tLR: 3.369821\n",
      "Training Epoch: 34 [35200/50000]\tLoss: 4.6530\tLR: 3.370077\n",
      "Training Epoch: 34 [35328/50000]\tLoss: 4.7240\tLR: 3.370332\n",
      "Training Epoch: 34 [35456/50000]\tLoss: 4.6564\tLR: 3.370588\n",
      "Training Epoch: 34 [35584/50000]\tLoss: 4.6516\tLR: 3.370844\n",
      "Training Epoch: 34 [35712/50000]\tLoss: 4.6908\tLR: 3.371100\n",
      "Training Epoch: 34 [35840/50000]\tLoss: 4.6177\tLR: 3.371355\n",
      "Training Epoch: 34 [35968/50000]\tLoss: 4.6807\tLR: 3.371611\n",
      "Training Epoch: 34 [36096/50000]\tLoss: 4.6981\tLR: 3.371867\n",
      "Training Epoch: 34 [36224/50000]\tLoss: 4.6303\tLR: 3.372123\n",
      "Training Epoch: 34 [36352/50000]\tLoss: 4.6989\tLR: 3.372379\n",
      "Training Epoch: 34 [36480/50000]\tLoss: 4.6086\tLR: 3.372634\n",
      "Training Epoch: 34 [36608/50000]\tLoss: 4.6740\tLR: 3.372890\n",
      "Training Epoch: 34 [36736/50000]\tLoss: 4.6515\tLR: 3.373146\n",
      "Training Epoch: 34 [36864/50000]\tLoss: 4.6575\tLR: 3.373402\n",
      "Training Epoch: 34 [36992/50000]\tLoss: 4.7385\tLR: 3.373657\n",
      "Training Epoch: 34 [37120/50000]\tLoss: 4.6983\tLR: 3.373913\n",
      "Training Epoch: 34 [37248/50000]\tLoss: 4.6384\tLR: 3.374169\n",
      "Training Epoch: 34 [37376/50000]\tLoss: 4.6944\tLR: 3.374425\n",
      "Training Epoch: 34 [37504/50000]\tLoss: 4.6414\tLR: 3.374680\n",
      "Training Epoch: 34 [37632/50000]\tLoss: 4.7065\tLR: 3.374936\n",
      "Training Epoch: 34 [37760/50000]\tLoss: 4.6792\tLR: 3.375192\n",
      "Training Epoch: 34 [37888/50000]\tLoss: 4.6557\tLR: 3.375448\n",
      "Training Epoch: 34 [38016/50000]\tLoss: 4.7264\tLR: 3.375703\n",
      "Training Epoch: 34 [38144/50000]\tLoss: 4.6785\tLR: 3.375959\n",
      "Training Epoch: 34 [38272/50000]\tLoss: 4.6795\tLR: 3.376215\n",
      "Training Epoch: 34 [38400/50000]\tLoss: 4.6916\tLR: 3.376471\n",
      "Training Epoch: 34 [38528/50000]\tLoss: 4.6449\tLR: 3.376726\n",
      "Training Epoch: 34 [38656/50000]\tLoss: 4.6005\tLR: 3.376982\n",
      "Training Epoch: 34 [38784/50000]\tLoss: 4.6737\tLR: 3.377238\n",
      "Training Epoch: 34 [38912/50000]\tLoss: 4.6516\tLR: 3.377494\n",
      "Training Epoch: 34 [39040/50000]\tLoss: 4.6412\tLR: 3.377749\n",
      "Training Epoch: 34 [39168/50000]\tLoss: 4.6255\tLR: 3.378005\n",
      "Training Epoch: 34 [39296/50000]\tLoss: 4.7246\tLR: 3.378261\n",
      "Training Epoch: 34 [39424/50000]\tLoss: 4.6675\tLR: 3.378517\n",
      "Training Epoch: 34 [39552/50000]\tLoss: 4.6220\tLR: 3.378772\n",
      "Training Epoch: 34 [39680/50000]\tLoss: 4.6708\tLR: 3.379028\n",
      "Training Epoch: 34 [39808/50000]\tLoss: 4.6866\tLR: 3.379284\n",
      "Training Epoch: 34 [39936/50000]\tLoss: 4.6267\tLR: 3.379540\n",
      "Training Epoch: 34 [40064/50000]\tLoss: 4.6651\tLR: 3.379795\n",
      "Training Epoch: 34 [40192/50000]\tLoss: 4.6168\tLR: 3.380051\n",
      "Training Epoch: 34 [40320/50000]\tLoss: 4.5988\tLR: 3.380307\n",
      "Training Epoch: 34 [40448/50000]\tLoss: 4.6473\tLR: 3.380563\n",
      "Training Epoch: 34 [40576/50000]\tLoss: 4.6034\tLR: 3.380818\n",
      "Training Epoch: 34 [40704/50000]\tLoss: 4.6658\tLR: 3.381074\n",
      "Training Epoch: 34 [40832/50000]\tLoss: 4.6728\tLR: 3.381330\n",
      "Training Epoch: 34 [40960/50000]\tLoss: 4.6976\tLR: 3.381586\n",
      "Training Epoch: 34 [41088/50000]\tLoss: 4.7010\tLR: 3.381841\n",
      "Training Epoch: 34 [41216/50000]\tLoss: 4.7055\tLR: 3.382097\n",
      "Training Epoch: 34 [41344/50000]\tLoss: 4.6976\tLR: 3.382353\n",
      "Training Epoch: 34 [41472/50000]\tLoss: 4.6835\tLR: 3.382609\n",
      "Training Epoch: 34 [41600/50000]\tLoss: 4.6709\tLR: 3.382864\n",
      "Training Epoch: 34 [41728/50000]\tLoss: 4.6244\tLR: 3.383120\n",
      "Training Epoch: 34 [41856/50000]\tLoss: 4.6852\tLR: 3.383376\n",
      "Training Epoch: 34 [41984/50000]\tLoss: 4.6284\tLR: 3.383632\n",
      "Training Epoch: 34 [42112/50000]\tLoss: 4.7003\tLR: 3.383887\n",
      "Training Epoch: 34 [42240/50000]\tLoss: 4.6137\tLR: 3.384143\n",
      "Training Epoch: 34 [42368/50000]\tLoss: 4.6886\tLR: 3.384399\n",
      "Training Epoch: 34 [42496/50000]\tLoss: 4.6596\tLR: 3.384655\n",
      "Training Epoch: 34 [42624/50000]\tLoss: 4.6738\tLR: 3.384910\n",
      "Training Epoch: 34 [42752/50000]\tLoss: 4.6919\tLR: 3.385166\n",
      "Training Epoch: 34 [42880/50000]\tLoss: 4.6626\tLR: 3.385422\n",
      "Training Epoch: 34 [43008/50000]\tLoss: 4.6439\tLR: 3.385678\n",
      "Training Epoch: 34 [43136/50000]\tLoss: 4.6208\tLR: 3.385934\n",
      "Training Epoch: 34 [43264/50000]\tLoss: 4.7443\tLR: 3.386189\n",
      "Training Epoch: 34 [43392/50000]\tLoss: 4.6303\tLR: 3.386445\n",
      "Training Epoch: 34 [43520/50000]\tLoss: 4.7325\tLR: 3.386701\n",
      "Training Epoch: 34 [43648/50000]\tLoss: 4.6419\tLR: 3.386957\n",
      "Training Epoch: 34 [43776/50000]\tLoss: 4.6337\tLR: 3.387212\n",
      "Training Epoch: 34 [43904/50000]\tLoss: 4.6324\tLR: 3.387468\n",
      "Training Epoch: 34 [44032/50000]\tLoss: 4.7088\tLR: 3.387724\n",
      "Training Epoch: 34 [44160/50000]\tLoss: 4.6491\tLR: 3.387980\n",
      "Training Epoch: 34 [44288/50000]\tLoss: 4.6979\tLR: 3.388235\n",
      "Training Epoch: 34 [44416/50000]\tLoss: 4.6280\tLR: 3.388491\n",
      "Training Epoch: 34 [44544/50000]\tLoss: 4.6395\tLR: 3.388747\n",
      "Training Epoch: 34 [44672/50000]\tLoss: 4.7439\tLR: 3.389003\n",
      "Training Epoch: 34 [44800/50000]\tLoss: 4.6236\tLR: 3.389258\n",
      "Training Epoch: 34 [44928/50000]\tLoss: 4.7294\tLR: 3.389514\n",
      "Training Epoch: 34 [45056/50000]\tLoss: 4.6720\tLR: 3.389770\n",
      "Training Epoch: 34 [45184/50000]\tLoss: 4.6537\tLR: 3.390026\n",
      "Training Epoch: 34 [45312/50000]\tLoss: 4.6575\tLR: 3.390281\n",
      "Training Epoch: 34 [45440/50000]\tLoss: 4.7227\tLR: 3.390537\n",
      "Training Epoch: 34 [45568/50000]\tLoss: 4.6625\tLR: 3.390793\n",
      "Training Epoch: 34 [45696/50000]\tLoss: 4.7283\tLR: 3.391049\n",
      "Training Epoch: 34 [45824/50000]\tLoss: 4.6251\tLR: 3.391304\n",
      "Training Epoch: 34 [45952/50000]\tLoss: 4.6575\tLR: 3.391560\n",
      "Training Epoch: 34 [46080/50000]\tLoss: 4.7124\tLR: 3.391816\n",
      "Training Epoch: 34 [46208/50000]\tLoss: 4.6478\tLR: 3.392072\n",
      "Training Epoch: 34 [46336/50000]\tLoss: 4.7361\tLR: 3.392327\n",
      "Training Epoch: 34 [46464/50000]\tLoss: 4.6683\tLR: 3.392583\n",
      "Training Epoch: 34 [46592/50000]\tLoss: 4.6882\tLR: 3.392839\n",
      "Training Epoch: 34 [46720/50000]\tLoss: 4.6628\tLR: 3.393095\n",
      "Training Epoch: 34 [46848/50000]\tLoss: 4.6623\tLR: 3.393350\n",
      "Training Epoch: 34 [46976/50000]\tLoss: 4.6301\tLR: 3.393606\n",
      "Training Epoch: 34 [47104/50000]\tLoss: 4.6918\tLR: 3.393862\n",
      "Training Epoch: 34 [47232/50000]\tLoss: 4.6258\tLR: 3.394118\n",
      "Training Epoch: 34 [47360/50000]\tLoss: 4.6695\tLR: 3.394373\n",
      "Training Epoch: 34 [47488/50000]\tLoss: 4.6172\tLR: 3.394629\n",
      "Training Epoch: 34 [47616/50000]\tLoss: 4.6658\tLR: 3.394885\n",
      "Training Epoch: 34 [47744/50000]\tLoss: 4.7441\tLR: 3.395141\n",
      "Training Epoch: 34 [47872/50000]\tLoss: 4.6927\tLR: 3.395396\n",
      "Training Epoch: 34 [48000/50000]\tLoss: 4.6850\tLR: 3.395652\n",
      "Training Epoch: 34 [48128/50000]\tLoss: 4.6998\tLR: 3.395908\n",
      "Training Epoch: 34 [48256/50000]\tLoss: 4.6460\tLR: 3.396164\n",
      "Training Epoch: 34 [48384/50000]\tLoss: 4.6992\tLR: 3.396419\n",
      "Training Epoch: 34 [48512/50000]\tLoss: 4.6614\tLR: 3.396675\n",
      "Training Epoch: 34 [48640/50000]\tLoss: 4.6607\tLR: 3.396931\n",
      "Training Epoch: 34 [48768/50000]\tLoss: 4.6522\tLR: 3.397187\n",
      "Training Epoch: 34 [48896/50000]\tLoss: 4.7084\tLR: 3.397442\n",
      "Training Epoch: 34 [49024/50000]\tLoss: 4.6956\tLR: 3.397698\n",
      "Training Epoch: 34 [49152/50000]\tLoss: 4.7137\tLR: 3.397954\n",
      "Training Epoch: 34 [49280/50000]\tLoss: 4.6207\tLR: 3.398210\n",
      "Training Epoch: 34 [49408/50000]\tLoss: 4.7174\tLR: 3.398465\n",
      "Training Epoch: 34 [49536/50000]\tLoss: 4.6818\tLR: 3.398721\n",
      "Training Epoch: 34 [49664/50000]\tLoss: 4.6486\tLR: 3.398977\n",
      "Training Epoch: 34 [49792/50000]\tLoss: 4.6701\tLR: 3.399233\n",
      "Training Epoch: 34 [49920/50000]\tLoss: 4.6369\tLR: 3.399488\n",
      "Training Epoch: 34 [50000/50000]\tLoss: 4.7509\tLR: 3.399744\n",
      "epoch 34 training time consumed: 488.77s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   47666 GB |   47666 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   47520 GB |   47520 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     146 GB |     146 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   47666 GB |   47666 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   47520 GB |   47520 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     146 GB |     146 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   46994 GB |   46994 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   46848 GB |   46848 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     146 GB |     146 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    5054 K  |    5054 K  |\n",
      "|       from large pool |      24    |      65    |    2154 K  |    2154 K  |\n",
      "|       from small pool |     231    |     274    |    2899 K  |    2899 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    5054 K  |    5054 K  |\n",
      "|       from large pool |      24    |      65    |    2154 K  |    2154 K  |\n",
      "|       from small pool |     231    |     274    |    2899 K  |    2899 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      46    |    2929 K  |    2929 K  |\n",
      "|       from large pool |      10    |      23    |    1035 K  |    1035 K  |\n",
      "|       from small pool |      26    |      35    |    1893 K  |    1893 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 34, Average loss: 0.0370, Accuracy: 0.0100, Time consumed:31.12s\n",
      "\n",
      "Training Epoch: 35 [128/50000]\tLoss: 4.7015\tLR: 0.100000\n",
      "Training Epoch: 35 [256/50000]\tLoss: 4.6554\tLR: 3.400256\n",
      "Training Epoch: 35 [384/50000]\tLoss: 4.6655\tLR: 3.400512\n",
      "Training Epoch: 35 [512/50000]\tLoss: 4.6830\tLR: 3.400767\n",
      "Training Epoch: 35 [640/50000]\tLoss: 4.6744\tLR: 3.401023\n",
      "Training Epoch: 35 [768/50000]\tLoss: 4.7141\tLR: 3.401279\n",
      "Training Epoch: 35 [896/50000]\tLoss: 4.7107\tLR: 3.401535\n",
      "Training Epoch: 35 [1024/50000]\tLoss: 4.6438\tLR: 3.401790\n",
      "Training Epoch: 35 [1152/50000]\tLoss: 4.6511\tLR: 3.402046\n",
      "Training Epoch: 35 [1280/50000]\tLoss: 4.6659\tLR: 3.402302\n",
      "Training Epoch: 35 [1408/50000]\tLoss: 4.7114\tLR: 3.402558\n",
      "Training Epoch: 35 [1536/50000]\tLoss: 4.6560\tLR: 3.402813\n",
      "Training Epoch: 35 [1664/50000]\tLoss: 4.7127\tLR: 3.403069\n",
      "Training Epoch: 35 [1792/50000]\tLoss: 4.6626\tLR: 3.403325\n",
      "Training Epoch: 35 [1920/50000]\tLoss: 4.6632\tLR: 3.403581\n",
      "Training Epoch: 35 [2048/50000]\tLoss: 4.7020\tLR: 3.403836\n",
      "Training Epoch: 35 [2176/50000]\tLoss: 4.6153\tLR: 3.404092\n",
      "Training Epoch: 35 [2304/50000]\tLoss: 4.6272\tLR: 3.404348\n",
      "Training Epoch: 35 [2432/50000]\tLoss: 4.7416\tLR: 3.404604\n",
      "Training Epoch: 35 [2560/50000]\tLoss: 4.7144\tLR: 3.404859\n",
      "Training Epoch: 35 [2688/50000]\tLoss: 4.6212\tLR: 3.405115\n",
      "Training Epoch: 35 [2816/50000]\tLoss: 4.7266\tLR: 3.405371\n",
      "Training Epoch: 35 [2944/50000]\tLoss: 4.7057\tLR: 3.405627\n",
      "Training Epoch: 35 [3072/50000]\tLoss: 4.6670\tLR: 3.405882\n",
      "Training Epoch: 35 [3200/50000]\tLoss: 4.6739\tLR: 3.406138\n",
      "Training Epoch: 35 [3328/50000]\tLoss: 4.7116\tLR: 3.406394\n",
      "Training Epoch: 35 [3456/50000]\tLoss: 4.6638\tLR: 3.406650\n",
      "Training Epoch: 35 [3584/50000]\tLoss: 4.7075\tLR: 3.406905\n",
      "Training Epoch: 35 [3712/50000]\tLoss: 4.7030\tLR: 3.407161\n",
      "Training Epoch: 35 [3840/50000]\tLoss: 4.6850\tLR: 3.407417\n",
      "Training Epoch: 35 [3968/50000]\tLoss: 4.6311\tLR: 3.407673\n",
      "Training Epoch: 35 [4096/50000]\tLoss: 4.6705\tLR: 3.407928\n",
      "Training Epoch: 35 [4224/50000]\tLoss: 4.5936\tLR: 3.408184\n",
      "Training Epoch: 35 [4352/50000]\tLoss: 4.6428\tLR: 3.408440\n",
      "Training Epoch: 35 [4480/50000]\tLoss: 4.6310\tLR: 3.408696\n",
      "Training Epoch: 35 [4608/50000]\tLoss: 4.5964\tLR: 3.408951\n",
      "Training Epoch: 35 [4736/50000]\tLoss: 4.6473\tLR: 3.409207\n",
      "Training Epoch: 35 [4864/50000]\tLoss: 4.6447\tLR: 3.409463\n",
      "Training Epoch: 35 [4992/50000]\tLoss: 4.7162\tLR: 3.409719\n",
      "Training Epoch: 35 [5120/50000]\tLoss: 4.6843\tLR: 3.409974\n",
      "Training Epoch: 35 [5248/50000]\tLoss: 4.6823\tLR: 3.410230\n",
      "Training Epoch: 35 [5376/50000]\tLoss: 4.6669\tLR: 3.410486\n",
      "Training Epoch: 35 [5504/50000]\tLoss: 4.7093\tLR: 3.410742\n",
      "Training Epoch: 35 [5632/50000]\tLoss: 4.6918\tLR: 3.410997\n",
      "Training Epoch: 35 [5760/50000]\tLoss: 4.6244\tLR: 3.411253\n",
      "Training Epoch: 35 [5888/50000]\tLoss: 4.6457\tLR: 3.411509\n",
      "Training Epoch: 35 [6016/50000]\tLoss: 4.6619\tLR: 3.411765\n",
      "Training Epoch: 35 [6144/50000]\tLoss: 4.6848\tLR: 3.412020\n",
      "Training Epoch: 35 [6272/50000]\tLoss: 4.6336\tLR: 3.412276\n",
      "Training Epoch: 35 [6400/50000]\tLoss: 4.6905\tLR: 3.412532\n",
      "Training Epoch: 35 [6528/50000]\tLoss: 4.6436\tLR: 3.412788\n",
      "Training Epoch: 35 [6656/50000]\tLoss: 4.6655\tLR: 3.413043\n",
      "Training Epoch: 35 [6784/50000]\tLoss: 4.6623\tLR: 3.413299\n",
      "Training Epoch: 35 [6912/50000]\tLoss: 4.6896\tLR: 3.413555\n",
      "Training Epoch: 35 [7040/50000]\tLoss: 4.6784\tLR: 3.413811\n",
      "Training Epoch: 35 [7168/50000]\tLoss: 4.7230\tLR: 3.414066\n",
      "Training Epoch: 35 [7296/50000]\tLoss: 4.6247\tLR: 3.414322\n",
      "Training Epoch: 35 [7424/50000]\tLoss: 4.6808\tLR: 3.414578\n",
      "Training Epoch: 35 [7552/50000]\tLoss: 4.6740\tLR: 3.414834\n",
      "Training Epoch: 35 [7680/50000]\tLoss: 4.6535\tLR: 3.415090\n",
      "Training Epoch: 35 [7808/50000]\tLoss: 4.6643\tLR: 3.415345\n",
      "Training Epoch: 35 [7936/50000]\tLoss: 4.6111\tLR: 3.415601\n",
      "Training Epoch: 35 [8064/50000]\tLoss: 4.6803\tLR: 3.415857\n",
      "Training Epoch: 35 [8192/50000]\tLoss: 4.6413\tLR: 3.416113\n",
      "Training Epoch: 35 [8320/50000]\tLoss: 4.6581\tLR: 3.416368\n",
      "Training Epoch: 35 [8448/50000]\tLoss: 4.6420\tLR: 3.416624\n",
      "Training Epoch: 35 [8576/50000]\tLoss: 4.6319\tLR: 3.416880\n",
      "Training Epoch: 35 [8704/50000]\tLoss: 4.7259\tLR: 3.417136\n",
      "Training Epoch: 35 [8832/50000]\tLoss: 4.6684\tLR: 3.417391\n",
      "Training Epoch: 35 [8960/50000]\tLoss: 4.6391\tLR: 3.417647\n",
      "Training Epoch: 35 [9088/50000]\tLoss: 4.6697\tLR: 3.417903\n",
      "Training Epoch: 35 [9216/50000]\tLoss: 4.6951\tLR: 3.418159\n",
      "Training Epoch: 35 [9344/50000]\tLoss: 4.6798\tLR: 3.418414\n",
      "Training Epoch: 35 [9472/50000]\tLoss: 4.7261\tLR: 3.418670\n",
      "Training Epoch: 35 [9600/50000]\tLoss: 4.6457\tLR: 3.418926\n",
      "Training Epoch: 35 [9728/50000]\tLoss: 4.6325\tLR: 3.419182\n",
      "Training Epoch: 35 [9856/50000]\tLoss: 4.6515\tLR: 3.419437\n",
      "Training Epoch: 35 [9984/50000]\tLoss: 4.6920\tLR: 3.419693\n",
      "Training Epoch: 35 [10112/50000]\tLoss: 4.6656\tLR: 3.419949\n",
      "Training Epoch: 35 [10240/50000]\tLoss: 4.6071\tLR: 3.420205\n",
      "Training Epoch: 35 [10368/50000]\tLoss: 4.6917\tLR: 3.420460\n",
      "Training Epoch: 35 [10496/50000]\tLoss: 4.6721\tLR: 3.420716\n",
      "Training Epoch: 35 [10624/50000]\tLoss: 4.7089\tLR: 3.420972\n",
      "Training Epoch: 35 [10752/50000]\tLoss: 4.6253\tLR: 3.421228\n",
      "Training Epoch: 35 [10880/50000]\tLoss: 4.6629\tLR: 3.421483\n",
      "Training Epoch: 35 [11008/50000]\tLoss: 4.6702\tLR: 3.421739\n",
      "Training Epoch: 35 [11136/50000]\tLoss: 4.7169\tLR: 3.421995\n",
      "Training Epoch: 35 [11264/50000]\tLoss: 4.6571\tLR: 3.422251\n",
      "Training Epoch: 35 [11392/50000]\tLoss: 4.6515\tLR: 3.422506\n",
      "Training Epoch: 35 [11520/50000]\tLoss: 4.6691\tLR: 3.422762\n",
      "Training Epoch: 35 [11648/50000]\tLoss: 4.6820\tLR: 3.423018\n",
      "Training Epoch: 35 [11776/50000]\tLoss: 4.7607\tLR: 3.423274\n",
      "Training Epoch: 35 [11904/50000]\tLoss: 4.7288\tLR: 3.423529\n",
      "Training Epoch: 35 [12032/50000]\tLoss: 4.7130\tLR: 3.423785\n",
      "Training Epoch: 35 [12160/50000]\tLoss: 4.6940\tLR: 3.424041\n",
      "Training Epoch: 35 [12288/50000]\tLoss: 4.6182\tLR: 3.424297\n",
      "Training Epoch: 35 [12416/50000]\tLoss: 4.6385\tLR: 3.424552\n",
      "Training Epoch: 35 [12544/50000]\tLoss: 4.5830\tLR: 3.424808\n",
      "Training Epoch: 35 [12672/50000]\tLoss: 4.6422\tLR: 3.425064\n",
      "Training Epoch: 35 [12800/50000]\tLoss: 4.6719\tLR: 3.425320\n",
      "Training Epoch: 35 [12928/50000]\tLoss: 4.6887\tLR: 3.425575\n",
      "Training Epoch: 35 [13056/50000]\tLoss: 4.6785\tLR: 3.425831\n",
      "Training Epoch: 35 [13184/50000]\tLoss: 4.7600\tLR: 3.426087\n",
      "Training Epoch: 35 [13312/50000]\tLoss: 4.6998\tLR: 3.426343\n",
      "Training Epoch: 35 [13440/50000]\tLoss: 4.6978\tLR: 3.426598\n",
      "Training Epoch: 35 [13568/50000]\tLoss: 4.6291\tLR: 3.426854\n",
      "Training Epoch: 35 [13696/50000]\tLoss: 4.6439\tLR: 3.427110\n",
      "Training Epoch: 35 [13824/50000]\tLoss: 4.6744\tLR: 3.427366\n",
      "Training Epoch: 35 [13952/50000]\tLoss: 4.6569\tLR: 3.427621\n",
      "Training Epoch: 35 [14080/50000]\tLoss: 4.6626\tLR: 3.427877\n",
      "Training Epoch: 35 [14208/50000]\tLoss: 4.6129\tLR: 3.428133\n",
      "Training Epoch: 35 [14336/50000]\tLoss: 4.6511\tLR: 3.428389\n",
      "Training Epoch: 35 [14464/50000]\tLoss: 4.7158\tLR: 3.428645\n",
      "Training Epoch: 35 [14592/50000]\tLoss: 4.6589\tLR: 3.428900\n",
      "Training Epoch: 35 [14720/50000]\tLoss: 4.6985\tLR: 3.429156\n",
      "Training Epoch: 35 [14848/50000]\tLoss: 4.6472\tLR: 3.429412\n",
      "Training Epoch: 35 [14976/50000]\tLoss: 4.6728\tLR: 3.429668\n",
      "Training Epoch: 35 [15104/50000]\tLoss: 4.7200\tLR: 3.429923\n",
      "Training Epoch: 35 [15232/50000]\tLoss: 4.7145\tLR: 3.430179\n",
      "Training Epoch: 35 [15360/50000]\tLoss: 4.6842\tLR: 3.430435\n",
      "Training Epoch: 35 [15488/50000]\tLoss: 4.6889\tLR: 3.430691\n",
      "Training Epoch: 35 [15616/50000]\tLoss: 4.7029\tLR: 3.430946\n",
      "Training Epoch: 35 [15744/50000]\tLoss: 4.6451\tLR: 3.431202\n",
      "Training Epoch: 35 [15872/50000]\tLoss: 4.7104\tLR: 3.431458\n",
      "Training Epoch: 35 [16000/50000]\tLoss: 4.6699\tLR: 3.431714\n",
      "Training Epoch: 35 [16128/50000]\tLoss: 4.6682\tLR: 3.431969\n",
      "Training Epoch: 35 [16256/50000]\tLoss: 4.6940\tLR: 3.432225\n",
      "Training Epoch: 35 [16384/50000]\tLoss: 4.6390\tLR: 3.432481\n",
      "Training Epoch: 35 [16512/50000]\tLoss: 4.6805\tLR: 3.432737\n",
      "Training Epoch: 35 [16640/50000]\tLoss: 4.6525\tLR: 3.432992\n",
      "Training Epoch: 35 [16768/50000]\tLoss: 4.6857\tLR: 3.433248\n",
      "Training Epoch: 35 [16896/50000]\tLoss: 4.6831\tLR: 3.433504\n",
      "Training Epoch: 35 [17024/50000]\tLoss: 4.6385\tLR: 3.433760\n",
      "Training Epoch: 35 [17152/50000]\tLoss: 4.6774\tLR: 3.434015\n",
      "Training Epoch: 35 [17280/50000]\tLoss: 4.6794\tLR: 3.434271\n",
      "Training Epoch: 35 [17408/50000]\tLoss: 4.6423\tLR: 3.434527\n",
      "Training Epoch: 35 [17536/50000]\tLoss: 4.7188\tLR: 3.434783\n",
      "Training Epoch: 35 [17664/50000]\tLoss: 4.6178\tLR: 3.435038\n",
      "Training Epoch: 35 [17792/50000]\tLoss: 4.6348\tLR: 3.435294\n",
      "Training Epoch: 35 [17920/50000]\tLoss: 4.6817\tLR: 3.435550\n",
      "Training Epoch: 35 [18048/50000]\tLoss: 4.6915\tLR: 3.435806\n",
      "Training Epoch: 35 [18176/50000]\tLoss: 4.6737\tLR: 3.436061\n",
      "Training Epoch: 35 [18304/50000]\tLoss: 4.7168\tLR: 3.436317\n",
      "Training Epoch: 35 [18432/50000]\tLoss: 4.6195\tLR: 3.436573\n",
      "Training Epoch: 35 [18560/50000]\tLoss: 4.7144\tLR: 3.436829\n",
      "Training Epoch: 35 [18688/50000]\tLoss: 4.6987\tLR: 3.437084\n",
      "Training Epoch: 35 [18816/50000]\tLoss: 4.6588\tLR: 3.437340\n",
      "Training Epoch: 35 [18944/50000]\tLoss: 4.6156\tLR: 3.437596\n",
      "Training Epoch: 35 [19072/50000]\tLoss: 4.6697\tLR: 3.437852\n",
      "Training Epoch: 35 [19200/50000]\tLoss: 4.6337\tLR: 3.438107\n",
      "Training Epoch: 35 [19328/50000]\tLoss: 4.6361\tLR: 3.438363\n",
      "Training Epoch: 35 [19456/50000]\tLoss: 4.6800\tLR: 3.438619\n",
      "Training Epoch: 35 [19584/50000]\tLoss: 4.7564\tLR: 3.438875\n",
      "Training Epoch: 35 [19712/50000]\tLoss: 4.6732\tLR: 3.439130\n",
      "Training Epoch: 35 [19840/50000]\tLoss: 4.6270\tLR: 3.439386\n",
      "Training Epoch: 35 [19968/50000]\tLoss: 4.7209\tLR: 3.439642\n",
      "Training Epoch: 35 [20096/50000]\tLoss: 4.7052\tLR: 3.439898\n",
      "Training Epoch: 35 [20224/50000]\tLoss: 4.6581\tLR: 3.440153\n",
      "Training Epoch: 35 [20352/50000]\tLoss: 4.6590\tLR: 3.440409\n",
      "Training Epoch: 35 [20480/50000]\tLoss: 4.6440\tLR: 3.440665\n",
      "Training Epoch: 35 [20608/50000]\tLoss: 4.6568\tLR: 3.440921\n",
      "Training Epoch: 35 [20736/50000]\tLoss: 4.6401\tLR: 3.441176\n",
      "Training Epoch: 35 [20864/50000]\tLoss: 4.6068\tLR: 3.441432\n",
      "Training Epoch: 35 [20992/50000]\tLoss: 4.6862\tLR: 3.441688\n",
      "Training Epoch: 35 [21120/50000]\tLoss: 4.6594\tLR: 3.441944\n",
      "Training Epoch: 35 [21248/50000]\tLoss: 4.7053\tLR: 3.442199\n",
      "Training Epoch: 35 [21376/50000]\tLoss: 4.6863\tLR: 3.442455\n",
      "Training Epoch: 35 [21504/50000]\tLoss: 4.7327\tLR: 3.442711\n",
      "Training Epoch: 35 [21632/50000]\tLoss: 4.7120\tLR: 3.442967\n",
      "Training Epoch: 35 [21760/50000]\tLoss: 4.7169\tLR: 3.443223\n",
      "Training Epoch: 35 [21888/50000]\tLoss: 4.6413\tLR: 3.443478\n",
      "Training Epoch: 35 [22016/50000]\tLoss: 4.6504\tLR: 3.443734\n",
      "Training Epoch: 35 [22144/50000]\tLoss: 4.6533\tLR: 3.443990\n",
      "Training Epoch: 35 [22272/50000]\tLoss: 4.6543\tLR: 3.444246\n",
      "Training Epoch: 35 [22400/50000]\tLoss: 4.6477\tLR: 3.444501\n",
      "Training Epoch: 35 [22528/50000]\tLoss: 4.6659\tLR: 3.444757\n",
      "Training Epoch: 35 [22656/50000]\tLoss: 4.6966\tLR: 3.445013\n",
      "Training Epoch: 35 [22784/50000]\tLoss: 4.6500\tLR: 3.445269\n",
      "Training Epoch: 35 [22912/50000]\tLoss: 4.6547\tLR: 3.445524\n",
      "Training Epoch: 35 [23040/50000]\tLoss: 4.6079\tLR: 3.445780\n",
      "Training Epoch: 35 [23168/50000]\tLoss: 4.7064\tLR: 3.446036\n",
      "Training Epoch: 35 [23296/50000]\tLoss: 4.6840\tLR: 3.446292\n",
      "Training Epoch: 35 [23424/50000]\tLoss: 4.6933\tLR: 3.446547\n",
      "Training Epoch: 35 [23552/50000]\tLoss: 4.6830\tLR: 3.446803\n",
      "Training Epoch: 35 [23680/50000]\tLoss: 4.7056\tLR: 3.447059\n",
      "Training Epoch: 35 [23808/50000]\tLoss: 4.6177\tLR: 3.447315\n",
      "Training Epoch: 35 [23936/50000]\tLoss: 4.6726\tLR: 3.447570\n",
      "Training Epoch: 35 [24064/50000]\tLoss: 4.6282\tLR: 3.447826\n",
      "Training Epoch: 35 [24192/50000]\tLoss: 4.5738\tLR: 3.448082\n",
      "Training Epoch: 35 [24320/50000]\tLoss: 4.6453\tLR: 3.448338\n",
      "Training Epoch: 35 [24448/50000]\tLoss: 4.7257\tLR: 3.448593\n",
      "Training Epoch: 35 [24576/50000]\tLoss: 4.7135\tLR: 3.448849\n",
      "Training Epoch: 35 [24704/50000]\tLoss: 4.6994\tLR: 3.449105\n",
      "Training Epoch: 35 [24832/50000]\tLoss: 4.7233\tLR: 3.449361\n",
      "Training Epoch: 35 [24960/50000]\tLoss: 4.7095\tLR: 3.449616\n",
      "Training Epoch: 35 [25088/50000]\tLoss: 4.6649\tLR: 3.449872\n",
      "Training Epoch: 35 [25216/50000]\tLoss: 4.7046\tLR: 3.450128\n",
      "Training Epoch: 35 [25344/50000]\tLoss: 4.6397\tLR: 3.450384\n",
      "Training Epoch: 35 [25472/50000]\tLoss: 4.6231\tLR: 3.450639\n",
      "Training Epoch: 35 [25600/50000]\tLoss: 4.6815\tLR: 3.450895\n",
      "Training Epoch: 35 [25728/50000]\tLoss: 4.6990\tLR: 3.451151\n",
      "Training Epoch: 35 [25856/50000]\tLoss: 4.6895\tLR: 3.451407\n",
      "Training Epoch: 35 [25984/50000]\tLoss: 4.7059\tLR: 3.451662\n",
      "Training Epoch: 35 [26112/50000]\tLoss: 4.6922\tLR: 3.451918\n",
      "Training Epoch: 35 [26240/50000]\tLoss: 4.6736\tLR: 3.452174\n",
      "Training Epoch: 35 [26368/50000]\tLoss: 4.6121\tLR: 3.452430\n",
      "Training Epoch: 35 [26496/50000]\tLoss: 4.6522\tLR: 3.452685\n",
      "Training Epoch: 35 [26624/50000]\tLoss: 4.6705\tLR: 3.452941\n",
      "Training Epoch: 35 [26752/50000]\tLoss: 4.6340\tLR: 3.453197\n",
      "Training Epoch: 35 [26880/50000]\tLoss: 4.6601\tLR: 3.453453\n",
      "Training Epoch: 35 [27008/50000]\tLoss: 4.6863\tLR: 3.453708\n",
      "Training Epoch: 35 [27136/50000]\tLoss: 4.6911\tLR: 3.453964\n",
      "Training Epoch: 35 [27264/50000]\tLoss: 4.6831\tLR: 3.454220\n",
      "Training Epoch: 35 [27392/50000]\tLoss: 4.6750\tLR: 3.454476\n",
      "Training Epoch: 35 [27520/50000]\tLoss: 4.6423\tLR: 3.454731\n",
      "Training Epoch: 35 [27648/50000]\tLoss: 4.6405\tLR: 3.454987\n",
      "Training Epoch: 35 [27776/50000]\tLoss: 4.6425\tLR: 3.455243\n",
      "Training Epoch: 35 [27904/50000]\tLoss: 4.6111\tLR: 3.455499\n",
      "Training Epoch: 35 [28032/50000]\tLoss: 4.6431\tLR: 3.455754\n",
      "Training Epoch: 35 [28160/50000]\tLoss: 4.7154\tLR: 3.456010\n",
      "Training Epoch: 35 [28288/50000]\tLoss: 4.7134\tLR: 3.456266\n",
      "Training Epoch: 35 [28416/50000]\tLoss: 4.6327\tLR: 3.456522\n",
      "Training Epoch: 35 [28544/50000]\tLoss: 4.6421\tLR: 3.456777\n",
      "Training Epoch: 35 [28672/50000]\tLoss: 4.6490\tLR: 3.457033\n",
      "Training Epoch: 35 [28800/50000]\tLoss: 4.6086\tLR: 3.457289\n",
      "Training Epoch: 35 [28928/50000]\tLoss: 4.6474\tLR: 3.457545\n",
      "Training Epoch: 35 [29056/50000]\tLoss: 4.6488\tLR: 3.457801\n",
      "Training Epoch: 35 [29184/50000]\tLoss: 4.6483\tLR: 3.458056\n",
      "Training Epoch: 35 [29312/50000]\tLoss: 4.6812\tLR: 3.458312\n",
      "Training Epoch: 35 [29440/50000]\tLoss: 4.6689\tLR: 3.458568\n",
      "Training Epoch: 35 [29568/50000]\tLoss: 4.7125\tLR: 3.458824\n",
      "Training Epoch: 35 [29696/50000]\tLoss: 4.6313\tLR: 3.459079\n",
      "Training Epoch: 35 [29824/50000]\tLoss: 4.6628\tLR: 3.459335\n",
      "Training Epoch: 35 [29952/50000]\tLoss: 4.6719\tLR: 3.459591\n",
      "Training Epoch: 35 [30080/50000]\tLoss: 4.6815\tLR: 3.459847\n",
      "Training Epoch: 35 [30208/50000]\tLoss: 4.6467\tLR: 3.460102\n",
      "Training Epoch: 35 [30336/50000]\tLoss: 4.7026\tLR: 3.460358\n",
      "Training Epoch: 35 [30464/50000]\tLoss: 4.7030\tLR: 3.460614\n",
      "Training Epoch: 35 [30592/50000]\tLoss: 4.7121\tLR: 3.460870\n",
      "Training Epoch: 35 [30720/50000]\tLoss: 4.7335\tLR: 3.461125\n",
      "Training Epoch: 35 [30848/50000]\tLoss: 4.6638\tLR: 3.461381\n",
      "Training Epoch: 35 [30976/50000]\tLoss: 4.6672\tLR: 3.461637\n",
      "Training Epoch: 35 [31104/50000]\tLoss: 4.6434\tLR: 3.461893\n",
      "Training Epoch: 35 [31232/50000]\tLoss: 4.6647\tLR: 3.462148\n",
      "Training Epoch: 35 [31360/50000]\tLoss: 4.6419\tLR: 3.462404\n",
      "Training Epoch: 35 [31488/50000]\tLoss: 4.6533\tLR: 3.462660\n",
      "Training Epoch: 35 [31616/50000]\tLoss: 4.7426\tLR: 3.462916\n",
      "Training Epoch: 35 [31744/50000]\tLoss: 4.6579\tLR: 3.463171\n",
      "Training Epoch: 35 [31872/50000]\tLoss: 4.6435\tLR: 3.463427\n",
      "Training Epoch: 35 [32000/50000]\tLoss: 4.7724\tLR: 3.463683\n",
      "Training Epoch: 35 [32128/50000]\tLoss: 4.6802\tLR: 3.463939\n",
      "Training Epoch: 35 [32256/50000]\tLoss: 4.6625\tLR: 3.464194\n",
      "Training Epoch: 35 [32384/50000]\tLoss: 4.6489\tLR: 3.464450\n",
      "Training Epoch: 35 [32512/50000]\tLoss: 4.6982\tLR: 3.464706\n",
      "Training Epoch: 35 [32640/50000]\tLoss: 4.6730\tLR: 3.464962\n",
      "Training Epoch: 35 [32768/50000]\tLoss: 4.7010\tLR: 3.465217\n",
      "Training Epoch: 35 [32896/50000]\tLoss: 4.6456\tLR: 3.465473\n",
      "Training Epoch: 35 [33024/50000]\tLoss: 4.7172\tLR: 3.465729\n",
      "Training Epoch: 35 [33152/50000]\tLoss: 4.7078\tLR: 3.465985\n",
      "Training Epoch: 35 [33280/50000]\tLoss: 4.7872\tLR: 3.466240\n",
      "Training Epoch: 35 [33408/50000]\tLoss: 4.6612\tLR: 3.466496\n",
      "Training Epoch: 35 [33536/50000]\tLoss: 4.6961\tLR: 3.466752\n",
      "Training Epoch: 35 [33664/50000]\tLoss: 4.6254\tLR: 3.467008\n",
      "Training Epoch: 35 [33792/50000]\tLoss: 4.6426\tLR: 3.467263\n",
      "Training Epoch: 35 [33920/50000]\tLoss: 4.6380\tLR: 3.467519\n",
      "Training Epoch: 35 [34048/50000]\tLoss: 4.6599\tLR: 3.467775\n",
      "Training Epoch: 35 [34176/50000]\tLoss: 4.6559\tLR: 3.468031\n",
      "Training Epoch: 35 [34304/50000]\tLoss: 4.6694\tLR: 3.468286\n",
      "Training Epoch: 35 [34432/50000]\tLoss: 4.6240\tLR: 3.468542\n",
      "Training Epoch: 35 [34560/50000]\tLoss: 4.6347\tLR: 3.468798\n",
      "Training Epoch: 35 [34688/50000]\tLoss: 4.6663\tLR: 3.469054\n",
      "Training Epoch: 35 [34816/50000]\tLoss: 4.6721\tLR: 3.469309\n",
      "Training Epoch: 35 [34944/50000]\tLoss: 4.6917\tLR: 3.469565\n",
      "Training Epoch: 35 [35072/50000]\tLoss: 4.7182\tLR: 3.469821\n",
      "Training Epoch: 35 [35200/50000]\tLoss: 4.6500\tLR: 3.470077\n",
      "Training Epoch: 35 [35328/50000]\tLoss: 4.6496\tLR: 3.470332\n",
      "Training Epoch: 35 [35456/50000]\tLoss: 4.7090\tLR: 3.470588\n",
      "Training Epoch: 35 [35584/50000]\tLoss: 4.6771\tLR: 3.470844\n",
      "Training Epoch: 35 [35712/50000]\tLoss: 4.6930\tLR: 3.471100\n",
      "Training Epoch: 35 [35840/50000]\tLoss: 4.7070\tLR: 3.471355\n",
      "Training Epoch: 35 [35968/50000]\tLoss: 4.6260\tLR: 3.471611\n",
      "Training Epoch: 35 [36096/50000]\tLoss: 4.6863\tLR: 3.471867\n",
      "Training Epoch: 35 [36224/50000]\tLoss: 4.6886\tLR: 3.472123\n",
      "Training Epoch: 35 [36352/50000]\tLoss: 4.7010\tLR: 3.472379\n",
      "Training Epoch: 35 [36480/50000]\tLoss: 4.6444\tLR: 3.472634\n",
      "Training Epoch: 35 [36608/50000]\tLoss: 4.6703\tLR: 3.472890\n",
      "Training Epoch: 35 [36736/50000]\tLoss: 4.6363\tLR: 3.473146\n",
      "Training Epoch: 35 [36864/50000]\tLoss: 4.7309\tLR: 3.473402\n",
      "Training Epoch: 35 [36992/50000]\tLoss: 4.6969\tLR: 3.473657\n",
      "Training Epoch: 35 [37120/50000]\tLoss: 4.6919\tLR: 3.473913\n",
      "Training Epoch: 35 [37248/50000]\tLoss: 4.7111\tLR: 3.474169\n",
      "Training Epoch: 35 [37376/50000]\tLoss: 4.6553\tLR: 3.474425\n",
      "Training Epoch: 35 [37504/50000]\tLoss: 4.6920\tLR: 3.474680\n",
      "Training Epoch: 35 [37632/50000]\tLoss: 4.6614\tLR: 3.474936\n",
      "Training Epoch: 35 [37760/50000]\tLoss: 4.6946\tLR: 3.475192\n",
      "Training Epoch: 35 [37888/50000]\tLoss: 4.6779\tLR: 3.475448\n",
      "Training Epoch: 35 [38016/50000]\tLoss: 4.6454\tLR: 3.475703\n",
      "Training Epoch: 35 [38144/50000]\tLoss: 4.6559\tLR: 3.475959\n",
      "Training Epoch: 35 [38272/50000]\tLoss: 4.6911\tLR: 3.476215\n",
      "Training Epoch: 35 [38400/50000]\tLoss: 4.7116\tLR: 3.476471\n",
      "Training Epoch: 35 [38528/50000]\tLoss: 4.6769\tLR: 3.476726\n",
      "Training Epoch: 35 [38656/50000]\tLoss: 4.6839\tLR: 3.476982\n",
      "Training Epoch: 35 [38784/50000]\tLoss: 4.7036\tLR: 3.477238\n",
      "Training Epoch: 35 [38912/50000]\tLoss: 4.6457\tLR: 3.477494\n",
      "Training Epoch: 35 [39040/50000]\tLoss: 4.6546\tLR: 3.477749\n",
      "Training Epoch: 35 [39168/50000]\tLoss: 4.6953\tLR: 3.478005\n",
      "Training Epoch: 35 [39296/50000]\tLoss: 4.6834\tLR: 3.478261\n",
      "Training Epoch: 35 [39424/50000]\tLoss: 4.7484\tLR: 3.478517\n",
      "Training Epoch: 35 [39552/50000]\tLoss: 4.6929\tLR: 3.478772\n",
      "Training Epoch: 35 [39680/50000]\tLoss: 4.6599\tLR: 3.479028\n",
      "Training Epoch: 35 [39808/50000]\tLoss: 4.6788\tLR: 3.479284\n",
      "Training Epoch: 35 [39936/50000]\tLoss: 4.6591\tLR: 3.479540\n",
      "Training Epoch: 35 [40064/50000]\tLoss: 4.7395\tLR: 3.479795\n",
      "Training Epoch: 35 [40192/50000]\tLoss: 4.6581\tLR: 3.480051\n",
      "Training Epoch: 35 [40320/50000]\tLoss: 4.6955\tLR: 3.480307\n",
      "Training Epoch: 35 [40448/50000]\tLoss: 4.6914\tLR: 3.480563\n",
      "Training Epoch: 35 [40576/50000]\tLoss: 4.6325\tLR: 3.480818\n",
      "Training Epoch: 35 [40704/50000]\tLoss: 4.7101\tLR: 3.481074\n",
      "Training Epoch: 35 [40832/50000]\tLoss: 4.6459\tLR: 3.481330\n",
      "Training Epoch: 35 [40960/50000]\tLoss: 4.6793\tLR: 3.481586\n",
      "Training Epoch: 35 [41088/50000]\tLoss: 4.6386\tLR: 3.481841\n",
      "Training Epoch: 35 [41216/50000]\tLoss: 4.6506\tLR: 3.482097\n",
      "Training Epoch: 35 [41344/50000]\tLoss: 4.6579\tLR: 3.482353\n",
      "Training Epoch: 35 [41472/50000]\tLoss: 4.6470\tLR: 3.482609\n",
      "Training Epoch: 35 [41600/50000]\tLoss: 4.6309\tLR: 3.482864\n",
      "Training Epoch: 35 [41728/50000]\tLoss: 4.6547\tLR: 3.483120\n",
      "Training Epoch: 35 [41856/50000]\tLoss: 4.6834\tLR: 3.483376\n",
      "Training Epoch: 35 [41984/50000]\tLoss: 4.6734\tLR: 3.483632\n",
      "Training Epoch: 35 [42112/50000]\tLoss: 4.6582\tLR: 3.483887\n",
      "Training Epoch: 35 [42240/50000]\tLoss: 4.6578\tLR: 3.484143\n",
      "Training Epoch: 35 [42368/50000]\tLoss: 4.7208\tLR: 3.484399\n",
      "Training Epoch: 35 [42496/50000]\tLoss: 4.7101\tLR: 3.484655\n",
      "Training Epoch: 35 [42624/50000]\tLoss: 4.5971\tLR: 3.484910\n",
      "Training Epoch: 35 [42752/50000]\tLoss: 4.6444\tLR: 3.485166\n",
      "Training Epoch: 35 [42880/50000]\tLoss: 4.6182\tLR: 3.485422\n",
      "Training Epoch: 35 [43008/50000]\tLoss: 4.6565\tLR: 3.485678\n",
      "Training Epoch: 35 [43136/50000]\tLoss: 4.6167\tLR: 3.485934\n",
      "Training Epoch: 35 [43264/50000]\tLoss: 4.6357\tLR: 3.486189\n",
      "Training Epoch: 35 [43392/50000]\tLoss: 4.6126\tLR: 3.486445\n",
      "Training Epoch: 35 [43520/50000]\tLoss: 4.6906\tLR: 3.486701\n",
      "Training Epoch: 35 [43648/50000]\tLoss: 4.7111\tLR: 3.486957\n",
      "Training Epoch: 35 [43776/50000]\tLoss: 4.6970\tLR: 3.487212\n",
      "Training Epoch: 35 [43904/50000]\tLoss: 4.6967\tLR: 3.487468\n",
      "Training Epoch: 35 [44032/50000]\tLoss: 4.6983\tLR: 3.487724\n",
      "Training Epoch: 35 [44160/50000]\tLoss: 4.6748\tLR: 3.487980\n",
      "Training Epoch: 35 [44288/50000]\tLoss: 4.6303\tLR: 3.488235\n",
      "Training Epoch: 35 [44416/50000]\tLoss: 4.6640\tLR: 3.488491\n",
      "Training Epoch: 35 [44544/50000]\tLoss: 4.7004\tLR: 3.488747\n",
      "Training Epoch: 35 [44672/50000]\tLoss: 4.5948\tLR: 3.489003\n",
      "Training Epoch: 35 [44800/50000]\tLoss: 4.6484\tLR: 3.489258\n",
      "Training Epoch: 35 [44928/50000]\tLoss: 4.7412\tLR: 3.489514\n",
      "Training Epoch: 35 [45056/50000]\tLoss: 4.6664\tLR: 3.489770\n",
      "Training Epoch: 35 [45184/50000]\tLoss: 4.7232\tLR: 3.490026\n",
      "Training Epoch: 35 [45312/50000]\tLoss: 4.6796\tLR: 3.490281\n",
      "Training Epoch: 35 [45440/50000]\tLoss: 4.6792\tLR: 3.490537\n",
      "Training Epoch: 35 [45568/50000]\tLoss: 4.6808\tLR: 3.490793\n",
      "Training Epoch: 35 [45696/50000]\tLoss: 4.6173\tLR: 3.491049\n",
      "Training Epoch: 35 [45824/50000]\tLoss: 4.6568\tLR: 3.491304\n",
      "Training Epoch: 35 [45952/50000]\tLoss: 4.6911\tLR: 3.491560\n",
      "Training Epoch: 35 [46080/50000]\tLoss: 4.6831\tLR: 3.491816\n",
      "Training Epoch: 35 [46208/50000]\tLoss: 4.6461\tLR: 3.492072\n",
      "Training Epoch: 35 [46336/50000]\tLoss: 4.7170\tLR: 3.492327\n",
      "Training Epoch: 35 [46464/50000]\tLoss: 4.7070\tLR: 3.492583\n",
      "Training Epoch: 35 [46592/50000]\tLoss: 4.7066\tLR: 3.492839\n",
      "Training Epoch: 35 [46720/50000]\tLoss: 4.7179\tLR: 3.493095\n",
      "Training Epoch: 35 [46848/50000]\tLoss: 4.6915\tLR: 3.493350\n",
      "Training Epoch: 35 [46976/50000]\tLoss: 4.6127\tLR: 3.493606\n",
      "Training Epoch: 35 [47104/50000]\tLoss: 4.6751\tLR: 3.493862\n",
      "Training Epoch: 35 [47232/50000]\tLoss: 4.7164\tLR: 3.494118\n",
      "Training Epoch: 35 [47360/50000]\tLoss: 4.6799\tLR: 3.494373\n",
      "Training Epoch: 35 [47488/50000]\tLoss: 4.6267\tLR: 3.494629\n",
      "Training Epoch: 35 [47616/50000]\tLoss: 4.7274\tLR: 3.494885\n",
      "Training Epoch: 35 [47744/50000]\tLoss: 4.6559\tLR: 3.495141\n",
      "Training Epoch: 35 [47872/50000]\tLoss: 4.6736\tLR: 3.495396\n",
      "Training Epoch: 35 [48000/50000]\tLoss: 4.6618\tLR: 3.495652\n",
      "Training Epoch: 35 [48128/50000]\tLoss: 4.6860\tLR: 3.495908\n",
      "Training Epoch: 35 [48256/50000]\tLoss: 4.7123\tLR: 3.496164\n",
      "Training Epoch: 35 [48384/50000]\tLoss: 4.7223\tLR: 3.496419\n",
      "Training Epoch: 35 [48512/50000]\tLoss: 4.6894\tLR: 3.496675\n",
      "Training Epoch: 35 [48640/50000]\tLoss: 4.7051\tLR: 3.496931\n",
      "Training Epoch: 35 [48768/50000]\tLoss: 4.6796\tLR: 3.497187\n",
      "Training Epoch: 35 [48896/50000]\tLoss: 4.6596\tLR: 3.497442\n",
      "Training Epoch: 35 [49024/50000]\tLoss: 4.6605\tLR: 3.497698\n",
      "Training Epoch: 35 [49152/50000]\tLoss: 4.6943\tLR: 3.497954\n",
      "Training Epoch: 35 [49280/50000]\tLoss: 4.6251\tLR: 3.498210\n",
      "Training Epoch: 35 [49408/50000]\tLoss: 4.6280\tLR: 3.498465\n",
      "Training Epoch: 35 [49536/50000]\tLoss: 4.6613\tLR: 3.498721\n",
      "Training Epoch: 35 [49664/50000]\tLoss: 4.6517\tLR: 3.498977\n",
      "Training Epoch: 35 [49792/50000]\tLoss: 4.7225\tLR: 3.499233\n",
      "Training Epoch: 35 [49920/50000]\tLoss: 4.7432\tLR: 3.499488\n",
      "Training Epoch: 35 [50000/50000]\tLoss: 4.6703\tLR: 3.499744\n",
      "epoch 35 training time consumed: 488.85s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   49068 GB |   49068 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   48918 GB |   48917 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     150 GB |     150 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   49068 GB |   49068 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   48918 GB |   48917 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     150 GB |     150 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   48376 GB |   48376 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   48226 GB |   48226 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     150 GB |     150 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    5203 K  |    5202 K  |\n",
      "|       from large pool |      24    |      65    |    2217 K  |    2217 K  |\n",
      "|       from small pool |     231    |     274    |    2985 K  |    2985 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    5203 K  |    5202 K  |\n",
      "|       from large pool |      24    |      65    |    2217 K  |    2217 K  |\n",
      "|       from small pool |     231    |     274    |    2985 K  |    2985 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      46    |    3015 K  |    3015 K  |\n",
      "|       from large pool |      10    |      23    |    1066 K  |    1066 K  |\n",
      "|       from small pool |      27    |      35    |    1949 K  |    1949 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 35, Average loss: 0.0369, Accuracy: 0.0100, Time consumed:30.94s\n",
      "\n",
      "Training Epoch: 36 [128/50000]\tLoss: 4.7136\tLR: 0.100000\n",
      "Training Epoch: 36 [256/50000]\tLoss: 4.6915\tLR: 3.500256\n",
      "Training Epoch: 36 [384/50000]\tLoss: 4.6859\tLR: 3.500512\n",
      "Training Epoch: 36 [512/50000]\tLoss: 4.6364\tLR: 3.500767\n",
      "Training Epoch: 36 [640/50000]\tLoss: 4.6949\tLR: 3.501023\n",
      "Training Epoch: 36 [768/50000]\tLoss: 4.5966\tLR: 3.501279\n",
      "Training Epoch: 36 [896/50000]\tLoss: 4.6606\tLR: 3.501535\n",
      "Training Epoch: 36 [1024/50000]\tLoss: 4.6653\tLR: 3.501790\n",
      "Training Epoch: 36 [1152/50000]\tLoss: 4.6494\tLR: 3.502046\n",
      "Training Epoch: 36 [1280/50000]\tLoss: 4.6968\tLR: 3.502302\n",
      "Training Epoch: 36 [1408/50000]\tLoss: 4.7417\tLR: 3.502558\n",
      "Training Epoch: 36 [1536/50000]\tLoss: 4.6274\tLR: 3.502813\n",
      "Training Epoch: 36 [1664/50000]\tLoss: 4.6679\tLR: 3.503069\n",
      "Training Epoch: 36 [1792/50000]\tLoss: 4.7361\tLR: 3.503325\n",
      "Training Epoch: 36 [1920/50000]\tLoss: 4.6458\tLR: 3.503581\n",
      "Training Epoch: 36 [2048/50000]\tLoss: 4.6915\tLR: 3.503836\n",
      "Training Epoch: 36 [2176/50000]\tLoss: 4.7456\tLR: 3.504092\n",
      "Training Epoch: 36 [2304/50000]\tLoss: 4.6889\tLR: 3.504348\n",
      "Training Epoch: 36 [2432/50000]\tLoss: 4.7156\tLR: 3.504604\n",
      "Training Epoch: 36 [2560/50000]\tLoss: 4.7236\tLR: 3.504859\n",
      "Training Epoch: 36 [2688/50000]\tLoss: 4.6732\tLR: 3.505115\n",
      "Training Epoch: 36 [2816/50000]\tLoss: 4.6525\tLR: 3.505371\n",
      "Training Epoch: 36 [2944/50000]\tLoss: 4.6403\tLR: 3.505627\n",
      "Training Epoch: 36 [3072/50000]\tLoss: 4.6533\tLR: 3.505882\n",
      "Training Epoch: 36 [3200/50000]\tLoss: 4.6469\tLR: 3.506138\n",
      "Training Epoch: 36 [3328/50000]\tLoss: 4.6066\tLR: 3.506394\n",
      "Training Epoch: 36 [3456/50000]\tLoss: 4.6843\tLR: 3.506650\n",
      "Training Epoch: 36 [3584/50000]\tLoss: 4.6750\tLR: 3.506905\n",
      "Training Epoch: 36 [3712/50000]\tLoss: 4.6703\tLR: 3.507161\n",
      "Training Epoch: 36 [3840/50000]\tLoss: 4.6803\tLR: 3.507417\n",
      "Training Epoch: 36 [3968/50000]\tLoss: 4.6925\tLR: 3.507673\n",
      "Training Epoch: 36 [4096/50000]\tLoss: 4.6559\tLR: 3.507928\n",
      "Training Epoch: 36 [4224/50000]\tLoss: 4.6933\tLR: 3.508184\n",
      "Training Epoch: 36 [4352/50000]\tLoss: 4.6549\tLR: 3.508440\n",
      "Training Epoch: 36 [4480/50000]\tLoss: 4.7151\tLR: 3.508696\n",
      "Training Epoch: 36 [4608/50000]\tLoss: 4.6908\tLR: 3.508951\n",
      "Training Epoch: 36 [4736/50000]\tLoss: 4.6984\tLR: 3.509207\n",
      "Training Epoch: 36 [4864/50000]\tLoss: 4.6898\tLR: 3.509463\n",
      "Training Epoch: 36 [4992/50000]\tLoss: 4.6001\tLR: 3.509719\n",
      "Training Epoch: 36 [5120/50000]\tLoss: 4.6670\tLR: 3.509974\n",
      "Training Epoch: 36 [5248/50000]\tLoss: 4.6679\tLR: 3.510230\n",
      "Training Epoch: 36 [5376/50000]\tLoss: 4.7085\tLR: 3.510486\n",
      "Training Epoch: 36 [5504/50000]\tLoss: 4.7373\tLR: 3.510742\n",
      "Training Epoch: 36 [5632/50000]\tLoss: 4.6851\tLR: 3.510997\n",
      "Training Epoch: 36 [5760/50000]\tLoss: 4.6665\tLR: 3.511253\n",
      "Training Epoch: 36 [5888/50000]\tLoss: 4.6738\tLR: 3.511509\n",
      "Training Epoch: 36 [6016/50000]\tLoss: 4.6756\tLR: 3.511765\n",
      "Training Epoch: 36 [6144/50000]\tLoss: 4.6950\tLR: 3.512020\n",
      "Training Epoch: 36 [6272/50000]\tLoss: 4.6893\tLR: 3.512276\n",
      "Training Epoch: 36 [6400/50000]\tLoss: 4.6803\tLR: 3.512532\n",
      "Training Epoch: 36 [6528/50000]\tLoss: 4.6276\tLR: 3.512788\n",
      "Training Epoch: 36 [6656/50000]\tLoss: 4.7100\tLR: 3.513043\n",
      "Training Epoch: 36 [6784/50000]\tLoss: 4.7255\tLR: 3.513299\n",
      "Training Epoch: 36 [6912/50000]\tLoss: 4.6741\tLR: 3.513555\n",
      "Training Epoch: 36 [7040/50000]\tLoss: 4.6911\tLR: 3.513811\n",
      "Training Epoch: 36 [7168/50000]\tLoss: 4.7309\tLR: 3.514066\n",
      "Training Epoch: 36 [7296/50000]\tLoss: 4.6625\tLR: 3.514322\n",
      "Training Epoch: 36 [7424/50000]\tLoss: 4.6491\tLR: 3.514578\n",
      "Training Epoch: 36 [7552/50000]\tLoss: 4.7125\tLR: 3.514834\n",
      "Training Epoch: 36 [7680/50000]\tLoss: 4.6228\tLR: 3.515090\n",
      "Training Epoch: 36 [7808/50000]\tLoss: 4.6996\tLR: 3.515345\n",
      "Training Epoch: 36 [7936/50000]\tLoss: 4.6225\tLR: 3.515601\n",
      "Training Epoch: 36 [8064/50000]\tLoss: 4.7407\tLR: 3.515857\n",
      "Training Epoch: 36 [8192/50000]\tLoss: 4.6965\tLR: 3.516113\n",
      "Training Epoch: 36 [8320/50000]\tLoss: 4.7337\tLR: 3.516368\n",
      "Training Epoch: 36 [8448/50000]\tLoss: 4.6556\tLR: 3.516624\n",
      "Training Epoch: 36 [8576/50000]\tLoss: 4.6001\tLR: 3.516880\n",
      "Training Epoch: 36 [8704/50000]\tLoss: 4.5841\tLR: 3.517136\n",
      "Training Epoch: 36 [8832/50000]\tLoss: 4.6815\tLR: 3.517391\n",
      "Training Epoch: 36 [8960/50000]\tLoss: 4.6675\tLR: 3.517647\n",
      "Training Epoch: 36 [9088/50000]\tLoss: 4.7567\tLR: 3.517903\n",
      "Training Epoch: 36 [9216/50000]\tLoss: 4.6633\tLR: 3.518159\n",
      "Training Epoch: 36 [9344/50000]\tLoss: 4.6309\tLR: 3.518414\n",
      "Training Epoch: 36 [9472/50000]\tLoss: 4.7024\tLR: 3.518670\n",
      "Training Epoch: 36 [9600/50000]\tLoss: 4.7151\tLR: 3.518926\n",
      "Training Epoch: 36 [9728/50000]\tLoss: 4.6418\tLR: 3.519182\n",
      "Training Epoch: 36 [9856/50000]\tLoss: 4.6144\tLR: 3.519437\n",
      "Training Epoch: 36 [9984/50000]\tLoss: 4.6402\tLR: 3.519693\n",
      "Training Epoch: 36 [10112/50000]\tLoss: 4.5766\tLR: 3.519949\n",
      "Training Epoch: 36 [10240/50000]\tLoss: 4.6610\tLR: 3.520205\n",
      "Training Epoch: 36 [10368/50000]\tLoss: 4.7288\tLR: 3.520460\n",
      "Training Epoch: 36 [10496/50000]\tLoss: 4.6262\tLR: 3.520716\n",
      "Training Epoch: 36 [10624/50000]\tLoss: 4.6198\tLR: 3.520972\n",
      "Training Epoch: 36 [10752/50000]\tLoss: 4.7024\tLR: 3.521228\n",
      "Training Epoch: 36 [10880/50000]\tLoss: 4.6693\tLR: 3.521483\n",
      "Training Epoch: 36 [11008/50000]\tLoss: 4.6765\tLR: 3.521739\n",
      "Training Epoch: 36 [11136/50000]\tLoss: 4.6572\tLR: 3.521995\n",
      "Training Epoch: 36 [11264/50000]\tLoss: 4.6403\tLR: 3.522251\n",
      "Training Epoch: 36 [11392/50000]\tLoss: 4.6632\tLR: 3.522506\n",
      "Training Epoch: 36 [11520/50000]\tLoss: 4.6147\tLR: 3.522762\n",
      "Training Epoch: 36 [11648/50000]\tLoss: 4.6355\tLR: 3.523018\n",
      "Training Epoch: 36 [11776/50000]\tLoss: 4.6569\tLR: 3.523274\n",
      "Training Epoch: 36 [11904/50000]\tLoss: 4.6227\tLR: 3.523529\n",
      "Training Epoch: 36 [12032/50000]\tLoss: 4.6787\tLR: 3.523785\n",
      "Training Epoch: 36 [12160/50000]\tLoss: 4.7131\tLR: 3.524041\n",
      "Training Epoch: 36 [12288/50000]\tLoss: 4.6506\tLR: 3.524297\n",
      "Training Epoch: 36 [12416/50000]\tLoss: 4.6525\tLR: 3.524552\n",
      "Training Epoch: 36 [12544/50000]\tLoss: 4.7175\tLR: 3.524808\n",
      "Training Epoch: 36 [12672/50000]\tLoss: 4.6522\tLR: 3.525064\n",
      "Training Epoch: 36 [12800/50000]\tLoss: 4.7030\tLR: 3.525320\n",
      "Training Epoch: 36 [12928/50000]\tLoss: 4.6500\tLR: 3.525575\n",
      "Training Epoch: 36 [13056/50000]\tLoss: 4.7025\tLR: 3.525831\n",
      "Training Epoch: 36 [13184/50000]\tLoss: 4.6910\tLR: 3.526087\n",
      "Training Epoch: 36 [13312/50000]\tLoss: 4.6159\tLR: 3.526343\n",
      "Training Epoch: 36 [13440/50000]\tLoss: 4.6179\tLR: 3.526598\n",
      "Training Epoch: 36 [13568/50000]\tLoss: 4.6264\tLR: 3.526854\n",
      "Training Epoch: 36 [13696/50000]\tLoss: 4.6273\tLR: 3.527110\n",
      "Training Epoch: 36 [13824/50000]\tLoss: 4.6564\tLR: 3.527366\n",
      "Training Epoch: 36 [13952/50000]\tLoss: 4.6645\tLR: 3.527621\n",
      "Training Epoch: 36 [14080/50000]\tLoss: 4.7516\tLR: 3.527877\n",
      "Training Epoch: 36 [14208/50000]\tLoss: 4.7087\tLR: 3.528133\n",
      "Training Epoch: 36 [14336/50000]\tLoss: 4.6475\tLR: 3.528389\n",
      "Training Epoch: 36 [14464/50000]\tLoss: 4.6773\tLR: 3.528645\n",
      "Training Epoch: 36 [14592/50000]\tLoss: 4.6814\tLR: 3.528900\n",
      "Training Epoch: 36 [14720/50000]\tLoss: 4.6567\tLR: 3.529156\n",
      "Training Epoch: 36 [14848/50000]\tLoss: 4.6948\tLR: 3.529412\n",
      "Training Epoch: 36 [14976/50000]\tLoss: 4.6534\tLR: 3.529668\n",
      "Training Epoch: 36 [15104/50000]\tLoss: 4.6959\tLR: 3.529923\n",
      "Training Epoch: 36 [15232/50000]\tLoss: 4.6705\tLR: 3.530179\n",
      "Training Epoch: 36 [15360/50000]\tLoss: 4.6343\tLR: 3.530435\n",
      "Training Epoch: 36 [15488/50000]\tLoss: 4.6953\tLR: 3.530691\n",
      "Training Epoch: 36 [15616/50000]\tLoss: 4.6564\tLR: 3.530946\n",
      "Training Epoch: 36 [15744/50000]\tLoss: 4.6977\tLR: 3.531202\n",
      "Training Epoch: 36 [15872/50000]\tLoss: 4.7198\tLR: 3.531458\n",
      "Training Epoch: 36 [16000/50000]\tLoss: 4.7044\tLR: 3.531714\n",
      "Training Epoch: 36 [16128/50000]\tLoss: 4.6630\tLR: 3.531969\n",
      "Training Epoch: 36 [16256/50000]\tLoss: 4.7295\tLR: 3.532225\n",
      "Training Epoch: 36 [16384/50000]\tLoss: 4.6994\tLR: 3.532481\n",
      "Training Epoch: 36 [16512/50000]\tLoss: 4.6989\tLR: 3.532737\n",
      "Training Epoch: 36 [16640/50000]\tLoss: 4.6528\tLR: 3.532992\n",
      "Training Epoch: 36 [16768/50000]\tLoss: 4.6198\tLR: 3.533248\n",
      "Training Epoch: 36 [16896/50000]\tLoss: 4.6471\tLR: 3.533504\n",
      "Training Epoch: 36 [17024/50000]\tLoss: 4.7093\tLR: 3.533760\n",
      "Training Epoch: 36 [17152/50000]\tLoss: 4.6956\tLR: 3.534015\n",
      "Training Epoch: 36 [17280/50000]\tLoss: 4.7082\tLR: 3.534271\n",
      "Training Epoch: 36 [17408/50000]\tLoss: 4.6426\tLR: 3.534527\n",
      "Training Epoch: 36 [17536/50000]\tLoss: 4.6338\tLR: 3.534783\n",
      "Training Epoch: 36 [17664/50000]\tLoss: 4.7126\tLR: 3.535038\n",
      "Training Epoch: 36 [17792/50000]\tLoss: 4.6613\tLR: 3.535294\n",
      "Training Epoch: 36 [17920/50000]\tLoss: 4.6450\tLR: 3.535550\n",
      "Training Epoch: 36 [18048/50000]\tLoss: 4.6543\tLR: 3.535806\n",
      "Training Epoch: 36 [18176/50000]\tLoss: 4.6462\tLR: 3.536061\n",
      "Training Epoch: 36 [18304/50000]\tLoss: 4.6231\tLR: 3.536317\n",
      "Training Epoch: 36 [18432/50000]\tLoss: 4.6726\tLR: 3.536573\n",
      "Training Epoch: 36 [18560/50000]\tLoss: 4.6660\tLR: 3.536829\n",
      "Training Epoch: 36 [18688/50000]\tLoss: 4.7071\tLR: 3.537084\n",
      "Training Epoch: 36 [18816/50000]\tLoss: 4.6501\tLR: 3.537340\n",
      "Training Epoch: 36 [18944/50000]\tLoss: 4.6870\tLR: 3.537596\n",
      "Training Epoch: 36 [19072/50000]\tLoss: 4.6441\tLR: 3.537852\n",
      "Training Epoch: 36 [19200/50000]\tLoss: 4.6869\tLR: 3.538107\n",
      "Training Epoch: 36 [19328/50000]\tLoss: 4.6799\tLR: 3.538363\n",
      "Training Epoch: 36 [19456/50000]\tLoss: 4.7255\tLR: 3.538619\n",
      "Training Epoch: 36 [19584/50000]\tLoss: 4.6521\tLR: 3.538875\n",
      "Training Epoch: 36 [19712/50000]\tLoss: 4.6526\tLR: 3.539130\n",
      "Training Epoch: 36 [19840/50000]\tLoss: 4.6760\tLR: 3.539386\n",
      "Training Epoch: 36 [19968/50000]\tLoss: 4.6626\tLR: 3.539642\n",
      "Training Epoch: 36 [20096/50000]\tLoss: 4.6174\tLR: 3.539898\n",
      "Training Epoch: 36 [20224/50000]\tLoss: 4.6675\tLR: 3.540153\n",
      "Training Epoch: 36 [20352/50000]\tLoss: 4.7211\tLR: 3.540409\n",
      "Training Epoch: 36 [20480/50000]\tLoss: 4.6773\tLR: 3.540665\n",
      "Training Epoch: 36 [20608/50000]\tLoss: 4.7698\tLR: 3.540921\n",
      "Training Epoch: 36 [20736/50000]\tLoss: 4.6343\tLR: 3.541176\n",
      "Training Epoch: 36 [20864/50000]\tLoss: 4.6301\tLR: 3.541432\n",
      "Training Epoch: 36 [20992/50000]\tLoss: 4.6491\tLR: 3.541688\n",
      "Training Epoch: 36 [21120/50000]\tLoss: 4.6436\tLR: 3.541944\n",
      "Training Epoch: 36 [21248/50000]\tLoss: 4.7178\tLR: 3.542199\n",
      "Training Epoch: 36 [21376/50000]\tLoss: 4.7087\tLR: 3.542455\n",
      "Training Epoch: 36 [21504/50000]\tLoss: 4.6653\tLR: 3.542711\n",
      "Training Epoch: 36 [21632/50000]\tLoss: 4.7253\tLR: 3.542967\n",
      "Training Epoch: 36 [21760/50000]\tLoss: 4.6574\tLR: 3.543223\n",
      "Training Epoch: 36 [21888/50000]\tLoss: 4.6809\tLR: 3.543478\n",
      "Training Epoch: 36 [22016/50000]\tLoss: 4.6430\tLR: 3.543734\n",
      "Training Epoch: 36 [22144/50000]\tLoss: 4.6927\tLR: 3.543990\n",
      "Training Epoch: 36 [22272/50000]\tLoss: 4.6451\tLR: 3.544246\n",
      "Training Epoch: 36 [22400/50000]\tLoss: 4.7245\tLR: 3.544501\n",
      "Training Epoch: 36 [22528/50000]\tLoss: 4.6286\tLR: 3.544757\n",
      "Training Epoch: 36 [22656/50000]\tLoss: 4.6925\tLR: 3.545013\n",
      "Training Epoch: 36 [22784/50000]\tLoss: 4.6572\tLR: 3.545269\n",
      "Training Epoch: 36 [22912/50000]\tLoss: 4.6448\tLR: 3.545524\n",
      "Training Epoch: 36 [23040/50000]\tLoss: 4.6800\tLR: 3.545780\n",
      "Training Epoch: 36 [23168/50000]\tLoss: 4.6332\tLR: 3.546036\n",
      "Training Epoch: 36 [23296/50000]\tLoss: 4.7151\tLR: 3.546292\n",
      "Training Epoch: 36 [23424/50000]\tLoss: 4.6721\tLR: 3.546547\n",
      "Training Epoch: 36 [23552/50000]\tLoss: 4.7215\tLR: 3.546803\n",
      "Training Epoch: 36 [23680/50000]\tLoss: 4.7165\tLR: 3.547059\n",
      "Training Epoch: 36 [23808/50000]\tLoss: 4.6823\tLR: 3.547315\n",
      "Training Epoch: 36 [23936/50000]\tLoss: 4.6949\tLR: 3.547570\n",
      "Training Epoch: 36 [24064/50000]\tLoss: 4.6748\tLR: 3.547826\n",
      "Training Epoch: 36 [24192/50000]\tLoss: 4.6668\tLR: 3.548082\n",
      "Training Epoch: 36 [24320/50000]\tLoss: 4.6951\tLR: 3.548338\n",
      "Training Epoch: 36 [24448/50000]\tLoss: 4.6371\tLR: 3.548593\n",
      "Training Epoch: 36 [24576/50000]\tLoss: 4.6379\tLR: 3.548849\n",
      "Training Epoch: 36 [24704/50000]\tLoss: 4.6997\tLR: 3.549105\n",
      "Training Epoch: 36 [24832/50000]\tLoss: 4.5994\tLR: 3.549361\n",
      "Training Epoch: 36 [24960/50000]\tLoss: 4.6827\tLR: 3.549616\n",
      "Training Epoch: 36 [25088/50000]\tLoss: 4.7360\tLR: 3.549872\n",
      "Training Epoch: 36 [25216/50000]\tLoss: 4.6758\tLR: 3.550128\n",
      "Training Epoch: 36 [25344/50000]\tLoss: 4.7285\tLR: 3.550384\n",
      "Training Epoch: 36 [25472/50000]\tLoss: 4.6625\tLR: 3.550639\n",
      "Training Epoch: 36 [25600/50000]\tLoss: 4.6761\tLR: 3.550895\n",
      "Training Epoch: 36 [25728/50000]\tLoss: 4.7069\tLR: 3.551151\n",
      "Training Epoch: 36 [25856/50000]\tLoss: 4.6639\tLR: 3.551407\n",
      "Training Epoch: 36 [25984/50000]\tLoss: 4.6649\tLR: 3.551662\n",
      "Training Epoch: 36 [26112/50000]\tLoss: 4.6678\tLR: 3.551918\n",
      "Training Epoch: 36 [26240/50000]\tLoss: 4.6486\tLR: 3.552174\n",
      "Training Epoch: 36 [26368/50000]\tLoss: 4.6817\tLR: 3.552430\n",
      "Training Epoch: 36 [26496/50000]\tLoss: 4.6902\tLR: 3.552685\n",
      "Training Epoch: 36 [26624/50000]\tLoss: 4.6640\tLR: 3.552941\n",
      "Training Epoch: 36 [26752/50000]\tLoss: 4.6778\tLR: 3.553197\n",
      "Training Epoch: 36 [26880/50000]\tLoss: 4.6695\tLR: 3.553453\n",
      "Training Epoch: 36 [27008/50000]\tLoss: 4.6799\tLR: 3.553708\n",
      "Training Epoch: 36 [27136/50000]\tLoss: 4.6742\tLR: 3.553964\n",
      "Training Epoch: 36 [27264/50000]\tLoss: 4.6437\tLR: 3.554220\n",
      "Training Epoch: 36 [27392/50000]\tLoss: 4.6800\tLR: 3.554476\n",
      "Training Epoch: 36 [27520/50000]\tLoss: 4.6725\tLR: 3.554731\n",
      "Training Epoch: 36 [27648/50000]\tLoss: 4.6672\tLR: 3.554987\n",
      "Training Epoch: 36 [27776/50000]\tLoss: 4.6083\tLR: 3.555243\n",
      "Training Epoch: 36 [27904/50000]\tLoss: 4.6985\tLR: 3.555499\n",
      "Training Epoch: 36 [28032/50000]\tLoss: 4.6444\tLR: 3.555754\n",
      "Training Epoch: 36 [28160/50000]\tLoss: 4.6405\tLR: 3.556010\n",
      "Training Epoch: 36 [28288/50000]\tLoss: 4.6514\tLR: 3.556266\n",
      "Training Epoch: 36 [28416/50000]\tLoss: 4.7224\tLR: 3.556522\n",
      "Training Epoch: 36 [28544/50000]\tLoss: 4.7581\tLR: 3.556777\n",
      "Training Epoch: 36 [28672/50000]\tLoss: 4.6791\tLR: 3.557033\n",
      "Training Epoch: 36 [28800/50000]\tLoss: 4.6492\tLR: 3.557289\n",
      "Training Epoch: 36 [28928/50000]\tLoss: 4.7264\tLR: 3.557545\n",
      "Training Epoch: 36 [29056/50000]\tLoss: 4.7006\tLR: 3.557801\n",
      "Training Epoch: 36 [29184/50000]\tLoss: 4.6889\tLR: 3.558056\n",
      "Training Epoch: 36 [29312/50000]\tLoss: 4.6820\tLR: 3.558312\n",
      "Training Epoch: 36 [29440/50000]\tLoss: 4.6630\tLR: 3.558568\n",
      "Training Epoch: 36 [29568/50000]\tLoss: 4.6600\tLR: 3.558824\n",
      "Training Epoch: 36 [29696/50000]\tLoss: 4.6469\tLR: 3.559079\n",
      "Training Epoch: 36 [29824/50000]\tLoss: 4.6775\tLR: 3.559335\n",
      "Training Epoch: 36 [29952/50000]\tLoss: 4.7195\tLR: 3.559591\n",
      "Training Epoch: 36 [30080/50000]\tLoss: 4.6709\tLR: 3.559847\n",
      "Training Epoch: 36 [30208/50000]\tLoss: 4.6425\tLR: 3.560102\n",
      "Training Epoch: 36 [30336/50000]\tLoss: 4.6811\tLR: 3.560358\n",
      "Training Epoch: 36 [30464/50000]\tLoss: 4.7297\tLR: 3.560614\n",
      "Training Epoch: 36 [30592/50000]\tLoss: 4.6813\tLR: 3.560870\n",
      "Training Epoch: 36 [30720/50000]\tLoss: 4.6820\tLR: 3.561125\n",
      "Training Epoch: 36 [30848/50000]\tLoss: 4.6751\tLR: 3.561381\n",
      "Training Epoch: 36 [30976/50000]\tLoss: 4.7522\tLR: 3.561637\n",
      "Training Epoch: 36 [31104/50000]\tLoss: 4.7089\tLR: 3.561893\n",
      "Training Epoch: 36 [31232/50000]\tLoss: 4.6810\tLR: 3.562148\n",
      "Training Epoch: 36 [31360/50000]\tLoss: 4.6727\tLR: 3.562404\n",
      "Training Epoch: 36 [31488/50000]\tLoss: 4.7156\tLR: 3.562660\n",
      "Training Epoch: 36 [31616/50000]\tLoss: 4.7133\tLR: 3.562916\n",
      "Training Epoch: 36 [31744/50000]\tLoss: 4.6517\tLR: 3.563171\n",
      "Training Epoch: 36 [31872/50000]\tLoss: 4.6487\tLR: 3.563427\n",
      "Training Epoch: 36 [32000/50000]\tLoss: 4.6790\tLR: 3.563683\n",
      "Training Epoch: 36 [32128/50000]\tLoss: 4.7114\tLR: 3.563939\n",
      "Training Epoch: 36 [32256/50000]\tLoss: 4.6526\tLR: 3.564194\n",
      "Training Epoch: 36 [32384/50000]\tLoss: 4.5994\tLR: 3.564450\n",
      "Training Epoch: 36 [32512/50000]\tLoss: 4.7223\tLR: 3.564706\n",
      "Training Epoch: 36 [32640/50000]\tLoss: 4.7055\tLR: 3.564962\n",
      "Training Epoch: 36 [32768/50000]\tLoss: 4.7219\tLR: 3.565217\n",
      "Training Epoch: 36 [32896/50000]\tLoss: 4.6609\tLR: 3.565473\n",
      "Training Epoch: 36 [33024/50000]\tLoss: 4.6657\tLR: 3.565729\n",
      "Training Epoch: 36 [33152/50000]\tLoss: 4.6954\tLR: 3.565985\n",
      "Training Epoch: 36 [33280/50000]\tLoss: 4.7033\tLR: 3.566240\n",
      "Training Epoch: 36 [33408/50000]\tLoss: 4.6647\tLR: 3.566496\n",
      "Training Epoch: 36 [33536/50000]\tLoss: 4.6539\tLR: 3.566752\n",
      "Training Epoch: 36 [33664/50000]\tLoss: 4.6751\tLR: 3.567008\n",
      "Training Epoch: 36 [33792/50000]\tLoss: 4.6833\tLR: 3.567263\n",
      "Training Epoch: 36 [33920/50000]\tLoss: 4.6628\tLR: 3.567519\n",
      "Training Epoch: 36 [34048/50000]\tLoss: 4.6987\tLR: 3.567775\n",
      "Training Epoch: 36 [34176/50000]\tLoss: 4.7284\tLR: 3.568031\n",
      "Training Epoch: 36 [34304/50000]\tLoss: 4.6677\tLR: 3.568286\n",
      "Training Epoch: 36 [34432/50000]\tLoss: 4.6247\tLR: 3.568542\n",
      "Training Epoch: 36 [34560/50000]\tLoss: 4.6443\tLR: 3.568798\n",
      "Training Epoch: 36 [34688/50000]\tLoss: 4.6482\tLR: 3.569054\n",
      "Training Epoch: 36 [34816/50000]\tLoss: 4.6803\tLR: 3.569309\n",
      "Training Epoch: 36 [34944/50000]\tLoss: 4.6543\tLR: 3.569565\n",
      "Training Epoch: 36 [35072/50000]\tLoss: 4.6619\tLR: 3.569821\n",
      "Training Epoch: 36 [35200/50000]\tLoss: 4.6682\tLR: 3.570077\n",
      "Training Epoch: 36 [35328/50000]\tLoss: 4.6495\tLR: 3.570332\n",
      "Training Epoch: 36 [35456/50000]\tLoss: 4.6303\tLR: 3.570588\n",
      "Training Epoch: 36 [35584/50000]\tLoss: 4.6373\tLR: 3.570844\n",
      "Training Epoch: 36 [35712/50000]\tLoss: 4.7059\tLR: 3.571100\n",
      "Training Epoch: 36 [35840/50000]\tLoss: 4.6442\tLR: 3.571355\n",
      "Training Epoch: 36 [35968/50000]\tLoss: 4.7006\tLR: 3.571611\n",
      "Training Epoch: 36 [36096/50000]\tLoss: 4.7129\tLR: 3.571867\n",
      "Training Epoch: 36 [36224/50000]\tLoss: 4.6593\tLR: 3.572123\n",
      "Training Epoch: 36 [36352/50000]\tLoss: 4.6395\tLR: 3.572379\n",
      "Training Epoch: 36 [36480/50000]\tLoss: 4.7247\tLR: 3.572634\n",
      "Training Epoch: 36 [36608/50000]\tLoss: 4.7162\tLR: 3.572890\n",
      "Training Epoch: 36 [36736/50000]\tLoss: 4.6672\tLR: 3.573146\n",
      "Training Epoch: 36 [36864/50000]\tLoss: 4.6621\tLR: 3.573402\n",
      "Training Epoch: 36 [36992/50000]\tLoss: 4.6775\tLR: 3.573657\n",
      "Training Epoch: 36 [37120/50000]\tLoss: 4.7164\tLR: 3.573913\n",
      "Training Epoch: 36 [37248/50000]\tLoss: 4.6816\tLR: 3.574169\n",
      "Training Epoch: 36 [37376/50000]\tLoss: 4.6778\tLR: 3.574425\n",
      "Training Epoch: 36 [37504/50000]\tLoss: 4.7292\tLR: 3.574680\n",
      "Training Epoch: 36 [37632/50000]\tLoss: 4.6623\tLR: 3.574936\n",
      "Training Epoch: 36 [37760/50000]\tLoss: 4.6553\tLR: 3.575192\n",
      "Training Epoch: 36 [37888/50000]\tLoss: 4.7503\tLR: 3.575448\n",
      "Training Epoch: 36 [38016/50000]\tLoss: 4.6987\tLR: 3.575703\n",
      "Training Epoch: 36 [38144/50000]\tLoss: 4.6613\tLR: 3.575959\n",
      "Training Epoch: 36 [38272/50000]\tLoss: 4.6703\tLR: 3.576215\n",
      "Training Epoch: 36 [38400/50000]\tLoss: 4.6775\tLR: 3.576471\n",
      "Training Epoch: 36 [38528/50000]\tLoss: 4.6659\tLR: 3.576726\n",
      "Training Epoch: 36 [38656/50000]\tLoss: 4.6581\tLR: 3.576982\n",
      "Training Epoch: 36 [38784/50000]\tLoss: 4.6840\tLR: 3.577238\n",
      "Training Epoch: 36 [38912/50000]\tLoss: 4.6788\tLR: 3.577494\n",
      "Training Epoch: 36 [39040/50000]\tLoss: 4.7168\tLR: 3.577749\n",
      "Training Epoch: 36 [39168/50000]\tLoss: 4.6476\tLR: 3.578005\n",
      "Training Epoch: 36 [39296/50000]\tLoss: 4.6560\tLR: 3.578261\n",
      "Training Epoch: 36 [39424/50000]\tLoss: 4.6510\tLR: 3.578517\n",
      "Training Epoch: 36 [39552/50000]\tLoss: 4.6730\tLR: 3.578772\n",
      "Training Epoch: 36 [39680/50000]\tLoss: 4.6394\tLR: 3.579028\n",
      "Training Epoch: 36 [39808/50000]\tLoss: 4.6633\tLR: 3.579284\n",
      "Training Epoch: 36 [39936/50000]\tLoss: 4.6588\tLR: 3.579540\n",
      "Training Epoch: 36 [40064/50000]\tLoss: 4.6483\tLR: 3.579795\n",
      "Training Epoch: 36 [40192/50000]\tLoss: 4.6900\tLR: 3.580051\n",
      "Training Epoch: 36 [40320/50000]\tLoss: 4.6831\tLR: 3.580307\n",
      "Training Epoch: 36 [40448/50000]\tLoss: 4.6835\tLR: 3.580563\n",
      "Training Epoch: 36 [40576/50000]\tLoss: 4.7348\tLR: 3.580818\n",
      "Training Epoch: 36 [40704/50000]\tLoss: 4.7235\tLR: 3.581074\n",
      "Training Epoch: 36 [40832/50000]\tLoss: 4.6774\tLR: 3.581330\n",
      "Training Epoch: 36 [40960/50000]\tLoss: 4.6649\tLR: 3.581586\n",
      "Training Epoch: 36 [41088/50000]\tLoss: 4.6753\tLR: 3.581841\n",
      "Training Epoch: 36 [41216/50000]\tLoss: 4.6493\tLR: 3.582097\n",
      "Training Epoch: 36 [41344/50000]\tLoss: 4.6238\tLR: 3.582353\n",
      "Training Epoch: 36 [41472/50000]\tLoss: 4.6922\tLR: 3.582609\n",
      "Training Epoch: 36 [41600/50000]\tLoss: 4.7068\tLR: 3.582864\n",
      "Training Epoch: 36 [41728/50000]\tLoss: 4.6890\tLR: 3.583120\n",
      "Training Epoch: 36 [41856/50000]\tLoss: 4.6586\tLR: 3.583376\n",
      "Training Epoch: 36 [41984/50000]\tLoss: 4.5997\tLR: 3.583632\n",
      "Training Epoch: 36 [42112/50000]\tLoss: 4.7072\tLR: 3.583887\n",
      "Training Epoch: 36 [42240/50000]\tLoss: 4.6684\tLR: 3.584143\n",
      "Training Epoch: 36 [42368/50000]\tLoss: 4.6939\tLR: 3.584399\n",
      "Training Epoch: 36 [42496/50000]\tLoss: 4.6366\tLR: 3.584655\n",
      "Training Epoch: 36 [42624/50000]\tLoss: 4.6918\tLR: 3.584910\n",
      "Training Epoch: 36 [42752/50000]\tLoss: 4.6604\tLR: 3.585166\n",
      "Training Epoch: 36 [42880/50000]\tLoss: 4.6750\tLR: 3.585422\n",
      "Training Epoch: 36 [43008/50000]\tLoss: 4.7522\tLR: 3.585678\n",
      "Training Epoch: 36 [43136/50000]\tLoss: 4.6796\tLR: 3.585934\n",
      "Training Epoch: 36 [43264/50000]\tLoss: 4.6467\tLR: 3.586189\n",
      "Training Epoch: 36 [43392/50000]\tLoss: 4.6758\tLR: 3.586445\n",
      "Training Epoch: 36 [43520/50000]\tLoss: 4.6765\tLR: 3.586701\n",
      "Training Epoch: 36 [43648/50000]\tLoss: 4.6320\tLR: 3.586957\n",
      "Training Epoch: 36 [43776/50000]\tLoss: 4.6594\tLR: 3.587212\n",
      "Training Epoch: 36 [43904/50000]\tLoss: 4.6958\tLR: 3.587468\n",
      "Training Epoch: 36 [44032/50000]\tLoss: 4.6161\tLR: 3.587724\n",
      "Training Epoch: 36 [44160/50000]\tLoss: 4.6665\tLR: 3.587980\n",
      "Training Epoch: 36 [44288/50000]\tLoss: 4.6758\tLR: 3.588235\n",
      "Training Epoch: 36 [44416/50000]\tLoss: 4.6494\tLR: 3.588491\n",
      "Training Epoch: 36 [44544/50000]\tLoss: 4.6812\tLR: 3.588747\n",
      "Training Epoch: 36 [44672/50000]\tLoss: 4.6108\tLR: 3.589003\n",
      "Training Epoch: 36 [44800/50000]\tLoss: 4.6760\tLR: 3.589258\n",
      "Training Epoch: 36 [44928/50000]\tLoss: 4.7495\tLR: 3.589514\n",
      "Training Epoch: 36 [45056/50000]\tLoss: 4.6527\tLR: 3.589770\n",
      "Training Epoch: 36 [45184/50000]\tLoss: 4.6677\tLR: 3.590026\n",
      "Training Epoch: 36 [45312/50000]\tLoss: 4.6898\tLR: 3.590281\n",
      "Training Epoch: 36 [45440/50000]\tLoss: 4.7146\tLR: 3.590537\n",
      "Training Epoch: 36 [45568/50000]\tLoss: 4.6863\tLR: 3.590793\n",
      "Training Epoch: 36 [45696/50000]\tLoss: 4.7140\tLR: 3.591049\n",
      "Training Epoch: 36 [45824/50000]\tLoss: 4.6852\tLR: 3.591304\n",
      "Training Epoch: 36 [45952/50000]\tLoss: 4.6850\tLR: 3.591560\n",
      "Training Epoch: 36 [46080/50000]\tLoss: 4.6594\tLR: 3.591816\n",
      "Training Epoch: 36 [46208/50000]\tLoss: 4.6594\tLR: 3.592072\n",
      "Training Epoch: 36 [46336/50000]\tLoss: 4.6450\tLR: 3.592327\n",
      "Training Epoch: 36 [46464/50000]\tLoss: 4.6825\tLR: 3.592583\n",
      "Training Epoch: 36 [46592/50000]\tLoss: 4.6601\tLR: 3.592839\n",
      "Training Epoch: 36 [46720/50000]\tLoss: 4.6627\tLR: 3.593095\n",
      "Training Epoch: 36 [46848/50000]\tLoss: 4.7125\tLR: 3.593350\n",
      "Training Epoch: 36 [46976/50000]\tLoss: 4.6459\tLR: 3.593606\n",
      "Training Epoch: 36 [47104/50000]\tLoss: 4.6107\tLR: 3.593862\n",
      "Training Epoch: 36 [47232/50000]\tLoss: 4.7000\tLR: 3.594118\n",
      "Training Epoch: 36 [47360/50000]\tLoss: 4.6673\tLR: 3.594373\n",
      "Training Epoch: 36 [47488/50000]\tLoss: 4.6528\tLR: 3.594629\n",
      "Training Epoch: 36 [47616/50000]\tLoss: 4.6310\tLR: 3.594885\n",
      "Training Epoch: 36 [47744/50000]\tLoss: 4.6647\tLR: 3.595141\n",
      "Training Epoch: 36 [47872/50000]\tLoss: 4.6784\tLR: 3.595396\n",
      "Training Epoch: 36 [48000/50000]\tLoss: 4.6611\tLR: 3.595652\n",
      "Training Epoch: 36 [48128/50000]\tLoss: 4.6610\tLR: 3.595908\n",
      "Training Epoch: 36 [48256/50000]\tLoss: 4.7085\tLR: 3.596164\n",
      "Training Epoch: 36 [48384/50000]\tLoss: 4.6971\tLR: 3.596419\n",
      "Training Epoch: 36 [48512/50000]\tLoss: 4.7015\tLR: 3.596675\n",
      "Training Epoch: 36 [48640/50000]\tLoss: 4.6836\tLR: 3.596931\n",
      "Training Epoch: 36 [48768/50000]\tLoss: 4.6907\tLR: 3.597187\n",
      "Training Epoch: 36 [48896/50000]\tLoss: 4.6256\tLR: 3.597442\n",
      "Training Epoch: 36 [49024/50000]\tLoss: 4.5947\tLR: 3.597698\n",
      "Training Epoch: 36 [49152/50000]\tLoss: 4.6596\tLR: 3.597954\n",
      "Training Epoch: 36 [49280/50000]\tLoss: 4.7078\tLR: 3.598210\n",
      "Training Epoch: 36 [49408/50000]\tLoss: 4.7537\tLR: 3.598465\n",
      "Training Epoch: 36 [49536/50000]\tLoss: 4.6853\tLR: 3.598721\n",
      "Training Epoch: 36 [49664/50000]\tLoss: 4.7122\tLR: 3.598977\n",
      "Training Epoch: 36 [49792/50000]\tLoss: 4.6839\tLR: 3.599233\n",
      "Training Epoch: 36 [49920/50000]\tLoss: 4.6309\tLR: 3.599488\n",
      "Training Epoch: 36 [50000/50000]\tLoss: 4.6861\tLR: 3.599744\n",
      "epoch 36 training time consumed: 488.92s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   50470 GB |   50470 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   50315 GB |   50315 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     154 GB |     154 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   50470 GB |   50470 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   50315 GB |   50315 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     154 GB |     154 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   49759 GB |   49759 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   49604 GB |   49604 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     154 GB |     154 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    5351 K  |    5351 K  |\n",
      "|       from large pool |      24    |      65    |    2281 K  |    2281 K  |\n",
      "|       from small pool |     231    |     274    |    3070 K  |    3070 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    5351 K  |    5351 K  |\n",
      "|       from large pool |      24    |      65    |    2281 K  |    2281 K  |\n",
      "|       from small pool |     231    |     274    |    3070 K  |    3070 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      46    |    3101 K  |    3101 K  |\n",
      "|       from large pool |      10    |      23    |    1096 K  |    1096 K  |\n",
      "|       from small pool |      26    |      35    |    2005 K  |    2005 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 36, Average loss: 0.0369, Accuracy: 0.0100, Time consumed:30.98s\n",
      "\n",
      "Training Epoch: 37 [128/50000]\tLoss: 4.5985\tLR: 0.100000\n",
      "Training Epoch: 37 [256/50000]\tLoss: 4.6081\tLR: 3.600256\n",
      "Training Epoch: 37 [384/50000]\tLoss: 4.6465\tLR: 3.600512\n",
      "Training Epoch: 37 [512/50000]\tLoss: 4.7935\tLR: 3.600767\n",
      "Training Epoch: 37 [640/50000]\tLoss: 4.6807\tLR: 3.601023\n",
      "Training Epoch: 37 [768/50000]\tLoss: 4.6133\tLR: 3.601279\n",
      "Training Epoch: 37 [896/50000]\tLoss: 4.7179\tLR: 3.601535\n",
      "Training Epoch: 37 [1024/50000]\tLoss: 4.7670\tLR: 3.601790\n",
      "Training Epoch: 37 [1152/50000]\tLoss: 4.6352\tLR: 3.602046\n",
      "Training Epoch: 37 [1280/50000]\tLoss: 4.7232\tLR: 3.602302\n",
      "Training Epoch: 37 [1408/50000]\tLoss: 4.7236\tLR: 3.602558\n",
      "Training Epoch: 37 [1536/50000]\tLoss: 4.7100\tLR: 3.602813\n",
      "Training Epoch: 37 [1664/50000]\tLoss: 4.6437\tLR: 3.603069\n",
      "Training Epoch: 37 [1792/50000]\tLoss: 4.6517\tLR: 3.603325\n",
      "Training Epoch: 37 [1920/50000]\tLoss: 4.6543\tLR: 3.603581\n",
      "Training Epoch: 37 [2048/50000]\tLoss: 4.6808\tLR: 3.603836\n",
      "Training Epoch: 37 [2176/50000]\tLoss: 4.7165\tLR: 3.604092\n",
      "Training Epoch: 37 [2304/50000]\tLoss: 4.6650\tLR: 3.604348\n",
      "Training Epoch: 37 [2432/50000]\tLoss: 4.6106\tLR: 3.604604\n",
      "Training Epoch: 37 [2560/50000]\tLoss: 4.6195\tLR: 3.604859\n",
      "Training Epoch: 37 [2688/50000]\tLoss: 4.6853\tLR: 3.605115\n",
      "Training Epoch: 37 [2816/50000]\tLoss: 4.7252\tLR: 3.605371\n",
      "Training Epoch: 37 [2944/50000]\tLoss: 4.6290\tLR: 3.605627\n",
      "Training Epoch: 37 [3072/50000]\tLoss: 4.7043\tLR: 3.605882\n",
      "Training Epoch: 37 [3200/50000]\tLoss: 4.6279\tLR: 3.606138\n",
      "Training Epoch: 37 [3328/50000]\tLoss: 4.6588\tLR: 3.606394\n",
      "Training Epoch: 37 [3456/50000]\tLoss: 4.6620\tLR: 3.606650\n",
      "Training Epoch: 37 [3584/50000]\tLoss: 4.6898\tLR: 3.606905\n",
      "Training Epoch: 37 [3712/50000]\tLoss: 4.6942\tLR: 3.607161\n",
      "Training Epoch: 37 [3840/50000]\tLoss: 4.6845\tLR: 3.607417\n",
      "Training Epoch: 37 [3968/50000]\tLoss: 4.6729\tLR: 3.607673\n",
      "Training Epoch: 37 [4096/50000]\tLoss: 4.6286\tLR: 3.607928\n",
      "Training Epoch: 37 [4224/50000]\tLoss: 4.6560\tLR: 3.608184\n",
      "Training Epoch: 37 [4352/50000]\tLoss: 4.6660\tLR: 3.608440\n",
      "Training Epoch: 37 [4480/50000]\tLoss: 4.6971\tLR: 3.608696\n",
      "Training Epoch: 37 [4608/50000]\tLoss: 4.6735\tLR: 3.608951\n",
      "Training Epoch: 37 [4736/50000]\tLoss: 4.6538\tLR: 3.609207\n",
      "Training Epoch: 37 [4864/50000]\tLoss: 4.7101\tLR: 3.609463\n",
      "Training Epoch: 37 [4992/50000]\tLoss: 4.6290\tLR: 3.609719\n",
      "Training Epoch: 37 [5120/50000]\tLoss: 4.7106\tLR: 3.609974\n",
      "Training Epoch: 37 [5248/50000]\tLoss: 4.6592\tLR: 3.610230\n",
      "Training Epoch: 37 [5376/50000]\tLoss: 4.6555\tLR: 3.610486\n",
      "Training Epoch: 37 [5504/50000]\tLoss: 4.6263\tLR: 3.610742\n",
      "Training Epoch: 37 [5632/50000]\tLoss: 4.6660\tLR: 3.610997\n",
      "Training Epoch: 37 [5760/50000]\tLoss: 4.6332\tLR: 3.611253\n",
      "Training Epoch: 37 [5888/50000]\tLoss: 4.6606\tLR: 3.611509\n",
      "Training Epoch: 37 [6016/50000]\tLoss: 4.6611\tLR: 3.611765\n",
      "Training Epoch: 37 [6144/50000]\tLoss: 4.6427\tLR: 3.612020\n",
      "Training Epoch: 37 [6272/50000]\tLoss: 4.6830\tLR: 3.612276\n",
      "Training Epoch: 37 [6400/50000]\tLoss: 4.6777\tLR: 3.612532\n",
      "Training Epoch: 37 [6528/50000]\tLoss: 4.7087\tLR: 3.612788\n",
      "Training Epoch: 37 [6656/50000]\tLoss: 4.6827\tLR: 3.613043\n",
      "Training Epoch: 37 [6784/50000]\tLoss: 4.6234\tLR: 3.613299\n",
      "Training Epoch: 37 [6912/50000]\tLoss: 4.7018\tLR: 3.613555\n",
      "Training Epoch: 37 [7040/50000]\tLoss: 4.6719\tLR: 3.613811\n",
      "Training Epoch: 37 [7168/50000]\tLoss: 4.6753\tLR: 3.614066\n",
      "Training Epoch: 37 [7296/50000]\tLoss: 4.6676\tLR: 3.614322\n",
      "Training Epoch: 37 [7424/50000]\tLoss: 4.6297\tLR: 3.614578\n",
      "Training Epoch: 37 [7552/50000]\tLoss: 4.6624\tLR: 3.614834\n",
      "Training Epoch: 37 [7680/50000]\tLoss: 4.7065\tLR: 3.615090\n",
      "Training Epoch: 37 [7808/50000]\tLoss: 4.6765\tLR: 3.615345\n",
      "Training Epoch: 37 [7936/50000]\tLoss: 4.6218\tLR: 3.615601\n",
      "Training Epoch: 37 [8064/50000]\tLoss: 4.6860\tLR: 3.615857\n",
      "Training Epoch: 37 [8192/50000]\tLoss: 4.6889\tLR: 3.616113\n",
      "Training Epoch: 37 [8320/50000]\tLoss: 4.6324\tLR: 3.616368\n",
      "Training Epoch: 37 [8448/50000]\tLoss: 4.7213\tLR: 3.616624\n",
      "Training Epoch: 37 [8576/50000]\tLoss: 4.7428\tLR: 3.616880\n",
      "Training Epoch: 37 [8704/50000]\tLoss: 4.7261\tLR: 3.617136\n",
      "Training Epoch: 37 [8832/50000]\tLoss: 4.6107\tLR: 3.617391\n",
      "Training Epoch: 37 [8960/50000]\tLoss: 4.7014\tLR: 3.617647\n",
      "Training Epoch: 37 [9088/50000]\tLoss: 4.6449\tLR: 3.617903\n",
      "Training Epoch: 37 [9216/50000]\tLoss: 4.6984\tLR: 3.618159\n",
      "Training Epoch: 37 [9344/50000]\tLoss: 4.6350\tLR: 3.618414\n",
      "Training Epoch: 37 [9472/50000]\tLoss: 4.6619\tLR: 3.618670\n",
      "Training Epoch: 37 [9600/50000]\tLoss: 4.6991\tLR: 3.618926\n",
      "Training Epoch: 37 [9728/50000]\tLoss: 4.5918\tLR: 3.619182\n",
      "Training Epoch: 37 [9856/50000]\tLoss: 4.6735\tLR: 3.619437\n",
      "Training Epoch: 37 [9984/50000]\tLoss: 4.6670\tLR: 3.619693\n",
      "Training Epoch: 37 [10112/50000]\tLoss: 4.6220\tLR: 3.619949\n",
      "Training Epoch: 37 [10240/50000]\tLoss: 4.6585\tLR: 3.620205\n",
      "Training Epoch: 37 [10368/50000]\tLoss: 4.7022\tLR: 3.620460\n",
      "Training Epoch: 37 [10496/50000]\tLoss: 4.6513\tLR: 3.620716\n",
      "Training Epoch: 37 [10624/50000]\tLoss: 4.7084\tLR: 3.620972\n",
      "Training Epoch: 37 [10752/50000]\tLoss: 4.7129\tLR: 3.621228\n",
      "Training Epoch: 37 [10880/50000]\tLoss: 4.7175\tLR: 3.621483\n",
      "Training Epoch: 37 [11008/50000]\tLoss: 4.6596\tLR: 3.621739\n",
      "Training Epoch: 37 [11136/50000]\tLoss: 4.6516\tLR: 3.621995\n",
      "Training Epoch: 37 [11264/50000]\tLoss: 4.6657\tLR: 3.622251\n",
      "Training Epoch: 37 [11392/50000]\tLoss: 4.7162\tLR: 3.622506\n",
      "Training Epoch: 37 [11520/50000]\tLoss: 4.6647\tLR: 3.622762\n",
      "Training Epoch: 37 [11648/50000]\tLoss: 4.6841\tLR: 3.623018\n",
      "Training Epoch: 37 [11776/50000]\tLoss: 4.6747\tLR: 3.623274\n",
      "Training Epoch: 37 [11904/50000]\tLoss: 4.6381\tLR: 3.623529\n",
      "Training Epoch: 37 [12032/50000]\tLoss: 4.6597\tLR: 3.623785\n",
      "Training Epoch: 37 [12160/50000]\tLoss: 4.6818\tLR: 3.624041\n",
      "Training Epoch: 37 [12288/50000]\tLoss: 4.7122\tLR: 3.624297\n",
      "Training Epoch: 37 [12416/50000]\tLoss: 4.6901\tLR: 3.624552\n",
      "Training Epoch: 37 [12544/50000]\tLoss: 4.6573\tLR: 3.624808\n",
      "Training Epoch: 37 [12672/50000]\tLoss: 4.7169\tLR: 3.625064\n",
      "Training Epoch: 37 [12800/50000]\tLoss: 4.7062\tLR: 3.625320\n",
      "Training Epoch: 37 [12928/50000]\tLoss: 4.6455\tLR: 3.625575\n",
      "Training Epoch: 37 [13056/50000]\tLoss: 4.6760\tLR: 3.625831\n",
      "Training Epoch: 37 [13184/50000]\tLoss: 4.6997\tLR: 3.626087\n",
      "Training Epoch: 37 [13312/50000]\tLoss: 4.7223\tLR: 3.626343\n",
      "Training Epoch: 37 [13440/50000]\tLoss: 4.6608\tLR: 3.626598\n",
      "Training Epoch: 37 [13568/50000]\tLoss: 4.6360\tLR: 3.626854\n",
      "Training Epoch: 37 [13696/50000]\tLoss: 4.7271\tLR: 3.627110\n",
      "Training Epoch: 37 [13824/50000]\tLoss: 4.6410\tLR: 3.627366\n",
      "Training Epoch: 37 [13952/50000]\tLoss: 4.5979\tLR: 3.627621\n",
      "Training Epoch: 37 [14080/50000]\tLoss: 4.6777\tLR: 3.627877\n",
      "Training Epoch: 37 [14208/50000]\tLoss: 4.6846\tLR: 3.628133\n",
      "Training Epoch: 37 [14336/50000]\tLoss: 4.7185\tLR: 3.628389\n",
      "Training Epoch: 37 [14464/50000]\tLoss: 4.6996\tLR: 3.628645\n",
      "Training Epoch: 37 [14592/50000]\tLoss: 4.7257\tLR: 3.628900\n",
      "Training Epoch: 37 [14720/50000]\tLoss: 4.6950\tLR: 3.629156\n",
      "Training Epoch: 37 [14848/50000]\tLoss: 4.6828\tLR: 3.629412\n",
      "Training Epoch: 37 [14976/50000]\tLoss: 4.6993\tLR: 3.629668\n",
      "Training Epoch: 37 [15104/50000]\tLoss: 4.6936\tLR: 3.629923\n",
      "Training Epoch: 37 [15232/50000]\tLoss: 4.7170\tLR: 3.630179\n",
      "Training Epoch: 37 [15360/50000]\tLoss: 4.6195\tLR: 3.630435\n",
      "Training Epoch: 37 [15488/50000]\tLoss: 4.6935\tLR: 3.630691\n",
      "Training Epoch: 37 [15616/50000]\tLoss: 4.5836\tLR: 3.630946\n",
      "Training Epoch: 37 [15744/50000]\tLoss: 4.6155\tLR: 3.631202\n",
      "Training Epoch: 37 [15872/50000]\tLoss: 4.6872\tLR: 3.631458\n",
      "Training Epoch: 37 [16000/50000]\tLoss: 4.6543\tLR: 3.631714\n",
      "Training Epoch: 37 [16128/50000]\tLoss: 4.7503\tLR: 3.631969\n",
      "Training Epoch: 37 [16256/50000]\tLoss: 4.6880\tLR: 3.632225\n",
      "Training Epoch: 37 [16384/50000]\tLoss: 4.7252\tLR: 3.632481\n",
      "Training Epoch: 37 [16512/50000]\tLoss: 4.6740\tLR: 3.632737\n",
      "Training Epoch: 37 [16640/50000]\tLoss: 4.6688\tLR: 3.632992\n",
      "Training Epoch: 37 [16768/50000]\tLoss: 4.6874\tLR: 3.633248\n",
      "Training Epoch: 37 [16896/50000]\tLoss: 4.6845\tLR: 3.633504\n",
      "Training Epoch: 37 [17024/50000]\tLoss: 4.6895\tLR: 3.633760\n",
      "Training Epoch: 37 [17152/50000]\tLoss: 4.6799\tLR: 3.634015\n",
      "Training Epoch: 37 [17280/50000]\tLoss: 4.6179\tLR: 3.634271\n",
      "Training Epoch: 37 [17408/50000]\tLoss: 4.7505\tLR: 3.634527\n",
      "Training Epoch: 37 [17536/50000]\tLoss: 4.7615\tLR: 3.634783\n",
      "Training Epoch: 37 [17664/50000]\tLoss: 4.7241\tLR: 3.635038\n",
      "Training Epoch: 37 [17792/50000]\tLoss: 4.7053\tLR: 3.635294\n",
      "Training Epoch: 37 [17920/50000]\tLoss: 4.6423\tLR: 3.635550\n",
      "Training Epoch: 37 [18048/50000]\tLoss: 4.6717\tLR: 3.635806\n",
      "Training Epoch: 37 [18176/50000]\tLoss: 4.6732\tLR: 3.636061\n",
      "Training Epoch: 37 [18304/50000]\tLoss: 4.6582\tLR: 3.636317\n",
      "Training Epoch: 37 [18432/50000]\tLoss: 4.6757\tLR: 3.636573\n",
      "Training Epoch: 37 [18560/50000]\tLoss: 4.7123\tLR: 3.636829\n",
      "Training Epoch: 37 [18688/50000]\tLoss: 4.6529\tLR: 3.637084\n",
      "Training Epoch: 37 [18816/50000]\tLoss: 4.7036\tLR: 3.637340\n",
      "Training Epoch: 37 [18944/50000]\tLoss: 4.7095\tLR: 3.637596\n",
      "Training Epoch: 37 [19072/50000]\tLoss: 4.7114\tLR: 3.637852\n",
      "Training Epoch: 37 [19200/50000]\tLoss: 4.6921\tLR: 3.638107\n",
      "Training Epoch: 37 [19328/50000]\tLoss: 4.6946\tLR: 3.638363\n",
      "Training Epoch: 37 [19456/50000]\tLoss: 4.6274\tLR: 3.638619\n",
      "Training Epoch: 37 [19584/50000]\tLoss: 4.6798\tLR: 3.638875\n",
      "Training Epoch: 37 [19712/50000]\tLoss: 4.7149\tLR: 3.639130\n",
      "Training Epoch: 37 [19840/50000]\tLoss: 4.7069\tLR: 3.639386\n",
      "Training Epoch: 37 [19968/50000]\tLoss: 4.6617\tLR: 3.639642\n",
      "Training Epoch: 37 [20096/50000]\tLoss: 4.7223\tLR: 3.639898\n",
      "Training Epoch: 37 [20224/50000]\tLoss: 4.6303\tLR: 3.640153\n",
      "Training Epoch: 37 [20352/50000]\tLoss: 4.6985\tLR: 3.640409\n",
      "Training Epoch: 37 [20480/50000]\tLoss: 4.6784\tLR: 3.640665\n",
      "Training Epoch: 37 [20608/50000]\tLoss: 4.6719\tLR: 3.640921\n",
      "Training Epoch: 37 [20736/50000]\tLoss: 4.6819\tLR: 3.641176\n",
      "Training Epoch: 37 [20864/50000]\tLoss: 4.6499\tLR: 3.641432\n",
      "Training Epoch: 37 [20992/50000]\tLoss: 4.6399\tLR: 3.641688\n",
      "Training Epoch: 37 [21120/50000]\tLoss: 4.6724\tLR: 3.641944\n",
      "Training Epoch: 37 [21248/50000]\tLoss: 4.6351\tLR: 3.642199\n",
      "Training Epoch: 37 [21376/50000]\tLoss: 4.6760\tLR: 3.642455\n",
      "Training Epoch: 37 [21504/50000]\tLoss: 4.6480\tLR: 3.642711\n",
      "Training Epoch: 37 [21632/50000]\tLoss: 4.6801\tLR: 3.642967\n",
      "Training Epoch: 37 [21760/50000]\tLoss: 4.6574\tLR: 3.643223\n",
      "Training Epoch: 37 [21888/50000]\tLoss: 4.6926\tLR: 3.643478\n",
      "Training Epoch: 37 [22016/50000]\tLoss: 4.7257\tLR: 3.643734\n",
      "Training Epoch: 37 [22144/50000]\tLoss: 4.6585\tLR: 3.643990\n",
      "Training Epoch: 37 [22272/50000]\tLoss: 4.6413\tLR: 3.644246\n",
      "Training Epoch: 37 [22400/50000]\tLoss: 4.6713\tLR: 3.644501\n",
      "Training Epoch: 37 [22528/50000]\tLoss: 4.6703\tLR: 3.644757\n",
      "Training Epoch: 37 [22656/50000]\tLoss: 4.6450\tLR: 3.645013\n",
      "Training Epoch: 37 [22784/50000]\tLoss: 4.6602\tLR: 3.645269\n",
      "Training Epoch: 37 [22912/50000]\tLoss: 4.7013\tLR: 3.645524\n",
      "Training Epoch: 37 [23040/50000]\tLoss: 4.6875\tLR: 3.645780\n",
      "Training Epoch: 37 [23168/50000]\tLoss: 4.6497\tLR: 3.646036\n",
      "Training Epoch: 37 [23296/50000]\tLoss: 4.7369\tLR: 3.646292\n",
      "Training Epoch: 37 [23424/50000]\tLoss: 4.6089\tLR: 3.646547\n",
      "Training Epoch: 37 [23552/50000]\tLoss: 4.6887\tLR: 3.646803\n",
      "Training Epoch: 37 [23680/50000]\tLoss: 4.6721\tLR: 3.647059\n",
      "Training Epoch: 37 [23808/50000]\tLoss: 4.6625\tLR: 3.647315\n",
      "Training Epoch: 37 [23936/50000]\tLoss: 4.6830\tLR: 3.647570\n",
      "Training Epoch: 37 [24064/50000]\tLoss: 4.6712\tLR: 3.647826\n",
      "Training Epoch: 37 [24192/50000]\tLoss: 4.7313\tLR: 3.648082\n",
      "Training Epoch: 37 [24320/50000]\tLoss: 4.6706\tLR: 3.648338\n",
      "Training Epoch: 37 [24448/50000]\tLoss: 4.6882\tLR: 3.648593\n",
      "Training Epoch: 37 [24576/50000]\tLoss: 4.6544\tLR: 3.648849\n",
      "Training Epoch: 37 [24704/50000]\tLoss: 4.6174\tLR: 3.649105\n",
      "Training Epoch: 37 [24832/50000]\tLoss: 4.6746\tLR: 3.649361\n",
      "Training Epoch: 37 [24960/50000]\tLoss: 4.6855\tLR: 3.649616\n",
      "Training Epoch: 37 [25088/50000]\tLoss: 4.7109\tLR: 3.649872\n",
      "Training Epoch: 37 [25216/50000]\tLoss: 4.7008\tLR: 3.650128\n",
      "Training Epoch: 37 [25344/50000]\tLoss: 4.7152\tLR: 3.650384\n",
      "Training Epoch: 37 [25472/50000]\tLoss: 4.7824\tLR: 3.650639\n",
      "Training Epoch: 37 [25600/50000]\tLoss: 4.6830\tLR: 3.650895\n",
      "Training Epoch: 37 [25728/50000]\tLoss: 4.7133\tLR: 3.651151\n",
      "Training Epoch: 37 [25856/50000]\tLoss: 4.6111\tLR: 3.651407\n",
      "Training Epoch: 37 [25984/50000]\tLoss: 4.6921\tLR: 3.651662\n",
      "Training Epoch: 37 [26112/50000]\tLoss: 4.7208\tLR: 3.651918\n",
      "Training Epoch: 37 [26240/50000]\tLoss: 4.6948\tLR: 3.652174\n",
      "Training Epoch: 37 [26368/50000]\tLoss: 4.6784\tLR: 3.652430\n",
      "Training Epoch: 37 [26496/50000]\tLoss: 4.6349\tLR: 3.652685\n",
      "Training Epoch: 37 [26624/50000]\tLoss: 4.6892\tLR: 3.652941\n",
      "Training Epoch: 37 [26752/50000]\tLoss: 4.6734\tLR: 3.653197\n",
      "Training Epoch: 37 [26880/50000]\tLoss: 4.7328\tLR: 3.653453\n",
      "Training Epoch: 37 [27008/50000]\tLoss: 4.7322\tLR: 3.653708\n",
      "Training Epoch: 37 [27136/50000]\tLoss: 4.6813\tLR: 3.653964\n",
      "Training Epoch: 37 [27264/50000]\tLoss: 4.6462\tLR: 3.654220\n",
      "Training Epoch: 37 [27392/50000]\tLoss: 4.6300\tLR: 3.654476\n",
      "Training Epoch: 37 [27520/50000]\tLoss: 4.6792\tLR: 3.654731\n",
      "Training Epoch: 37 [27648/50000]\tLoss: 4.6037\tLR: 3.654987\n",
      "Training Epoch: 37 [27776/50000]\tLoss: 4.7593\tLR: 3.655243\n",
      "Training Epoch: 37 [27904/50000]\tLoss: 4.6657\tLR: 3.655499\n",
      "Training Epoch: 37 [28032/50000]\tLoss: 4.6720\tLR: 3.655754\n",
      "Training Epoch: 37 [28160/50000]\tLoss: 4.7109\tLR: 3.656010\n",
      "Training Epoch: 37 [28288/50000]\tLoss: 4.6865\tLR: 3.656266\n",
      "Training Epoch: 37 [28416/50000]\tLoss: 4.6818\tLR: 3.656522\n",
      "Training Epoch: 37 [28544/50000]\tLoss: 4.6691\tLR: 3.656777\n",
      "Training Epoch: 37 [28672/50000]\tLoss: 4.6653\tLR: 3.657033\n",
      "Training Epoch: 37 [28800/50000]\tLoss: 4.6977\tLR: 3.657289\n",
      "Training Epoch: 37 [28928/50000]\tLoss: 4.6369\tLR: 3.657545\n",
      "Training Epoch: 37 [29056/50000]\tLoss: 4.6545\tLR: 3.657801\n",
      "Training Epoch: 37 [29184/50000]\tLoss: 4.6398\tLR: 3.658056\n",
      "Training Epoch: 37 [29312/50000]\tLoss: 4.7135\tLR: 3.658312\n",
      "Training Epoch: 37 [29440/50000]\tLoss: 4.7005\tLR: 3.658568\n",
      "Training Epoch: 37 [29568/50000]\tLoss: 4.7046\tLR: 3.658824\n",
      "Training Epoch: 37 [29696/50000]\tLoss: 4.6640\tLR: 3.659079\n",
      "Training Epoch: 37 [29824/50000]\tLoss: 4.6311\tLR: 3.659335\n",
      "Training Epoch: 37 [29952/50000]\tLoss: 4.6620\tLR: 3.659591\n",
      "Training Epoch: 37 [30080/50000]\tLoss: 4.6223\tLR: 3.659847\n",
      "Training Epoch: 37 [30208/50000]\tLoss: 4.6755\tLR: 3.660102\n",
      "Training Epoch: 37 [30336/50000]\tLoss: 4.6278\tLR: 3.660358\n",
      "Training Epoch: 37 [30464/50000]\tLoss: 4.7061\tLR: 3.660614\n",
      "Training Epoch: 37 [30592/50000]\tLoss: 4.8038\tLR: 3.660870\n",
      "Training Epoch: 37 [30720/50000]\tLoss: 4.6427\tLR: 3.661125\n",
      "Training Epoch: 37 [30848/50000]\tLoss: 4.7067\tLR: 3.661381\n",
      "Training Epoch: 37 [30976/50000]\tLoss: 4.6529\tLR: 3.661637\n",
      "Training Epoch: 37 [31104/50000]\tLoss: 4.7161\tLR: 3.661893\n",
      "Training Epoch: 37 [31232/50000]\tLoss: 4.5963\tLR: 3.662148\n",
      "Training Epoch: 37 [31360/50000]\tLoss: 4.5962\tLR: 3.662404\n",
      "Training Epoch: 37 [31488/50000]\tLoss: 4.6728\tLR: 3.662660\n",
      "Training Epoch: 37 [31616/50000]\tLoss: 4.6696\tLR: 3.662916\n",
      "Training Epoch: 37 [31744/50000]\tLoss: 4.6992\tLR: 3.663171\n",
      "Training Epoch: 37 [31872/50000]\tLoss: 4.6971\tLR: 3.663427\n",
      "Training Epoch: 37 [32000/50000]\tLoss: 4.6814\tLR: 3.663683\n",
      "Training Epoch: 37 [32128/50000]\tLoss: 4.6736\tLR: 3.663939\n",
      "Training Epoch: 37 [32256/50000]\tLoss: 4.6639\tLR: 3.664194\n",
      "Training Epoch: 37 [32384/50000]\tLoss: 4.6403\tLR: 3.664450\n",
      "Training Epoch: 37 [32512/50000]\tLoss: 4.7065\tLR: 3.664706\n",
      "Training Epoch: 37 [32640/50000]\tLoss: 4.6441\tLR: 3.664962\n",
      "Training Epoch: 37 [32768/50000]\tLoss: 4.6883\tLR: 3.665217\n",
      "Training Epoch: 37 [32896/50000]\tLoss: 4.6919\tLR: 3.665473\n",
      "Training Epoch: 37 [33024/50000]\tLoss: 4.6877\tLR: 3.665729\n",
      "Training Epoch: 37 [33152/50000]\tLoss: 4.7149\tLR: 3.665985\n",
      "Training Epoch: 37 [33280/50000]\tLoss: 4.7352\tLR: 3.666240\n",
      "Training Epoch: 37 [33408/50000]\tLoss: 4.6824\tLR: 3.666496\n",
      "Training Epoch: 37 [33536/50000]\tLoss: 4.7042\tLR: 3.666752\n",
      "Training Epoch: 37 [33664/50000]\tLoss: 4.6474\tLR: 3.667008\n",
      "Training Epoch: 37 [33792/50000]\tLoss: 4.6373\tLR: 3.667263\n",
      "Training Epoch: 37 [33920/50000]\tLoss: 4.6107\tLR: 3.667519\n",
      "Training Epoch: 37 [34048/50000]\tLoss: 4.6765\tLR: 3.667775\n",
      "Training Epoch: 37 [34176/50000]\tLoss: 4.6688\tLR: 3.668031\n",
      "Training Epoch: 37 [34304/50000]\tLoss: 4.6688\tLR: 3.668286\n",
      "Training Epoch: 37 [34432/50000]\tLoss: 4.6562\tLR: 3.668542\n",
      "Training Epoch: 37 [34560/50000]\tLoss: 4.6770\tLR: 3.668798\n",
      "Training Epoch: 37 [34688/50000]\tLoss: 4.7055\tLR: 3.669054\n",
      "Training Epoch: 37 [34816/50000]\tLoss: 4.6894\tLR: 3.669309\n",
      "Training Epoch: 37 [34944/50000]\tLoss: 4.6655\tLR: 3.669565\n",
      "Training Epoch: 37 [35072/50000]\tLoss: 4.6216\tLR: 3.669821\n",
      "Training Epoch: 37 [35200/50000]\tLoss: 4.6846\tLR: 3.670077\n",
      "Training Epoch: 37 [35328/50000]\tLoss: 4.6624\tLR: 3.670332\n",
      "Training Epoch: 37 [35456/50000]\tLoss: 4.6971\tLR: 3.670588\n",
      "Training Epoch: 37 [35584/50000]\tLoss: 4.7096\tLR: 3.670844\n",
      "Training Epoch: 37 [35712/50000]\tLoss: 4.6440\tLR: 3.671100\n",
      "Training Epoch: 37 [35840/50000]\tLoss: 4.6227\tLR: 3.671355\n",
      "Training Epoch: 37 [35968/50000]\tLoss: 4.6769\tLR: 3.671611\n",
      "Training Epoch: 37 [36096/50000]\tLoss: 4.6515\tLR: 3.671867\n",
      "Training Epoch: 37 [36224/50000]\tLoss: 4.6304\tLR: 3.672123\n",
      "Training Epoch: 37 [36352/50000]\tLoss: 4.6815\tLR: 3.672379\n",
      "Training Epoch: 37 [36480/50000]\tLoss: 4.6516\tLR: 3.672634\n",
      "Training Epoch: 37 [36608/50000]\tLoss: 4.6871\tLR: 3.672890\n",
      "Training Epoch: 37 [36736/50000]\tLoss: 4.6964\tLR: 3.673146\n",
      "Training Epoch: 37 [36864/50000]\tLoss: 4.6904\tLR: 3.673402\n",
      "Training Epoch: 37 [36992/50000]\tLoss: 4.6962\tLR: 3.673657\n",
      "Training Epoch: 37 [37120/50000]\tLoss: 4.6596\tLR: 3.673913\n",
      "Training Epoch: 37 [37248/50000]\tLoss: 4.6946\tLR: 3.674169\n",
      "Training Epoch: 37 [37376/50000]\tLoss: 4.6345\tLR: 3.674425\n",
      "Training Epoch: 37 [37504/50000]\tLoss: 4.6177\tLR: 3.674680\n",
      "Training Epoch: 37 [37632/50000]\tLoss: 4.6620\tLR: 3.674936\n",
      "Training Epoch: 37 [37760/50000]\tLoss: 4.6446\tLR: 3.675192\n",
      "Training Epoch: 37 [37888/50000]\tLoss: 4.6348\tLR: 3.675448\n",
      "Training Epoch: 37 [38016/50000]\tLoss: 4.7068\tLR: 3.675703\n",
      "Training Epoch: 37 [38144/50000]\tLoss: 4.6471\tLR: 3.675959\n",
      "Training Epoch: 37 [38272/50000]\tLoss: 4.6830\tLR: 3.676215\n",
      "Training Epoch: 37 [38400/50000]\tLoss: 4.6987\tLR: 3.676471\n",
      "Training Epoch: 37 [38528/50000]\tLoss: 4.7044\tLR: 3.676726\n",
      "Training Epoch: 37 [38656/50000]\tLoss: 4.7012\tLR: 3.676982\n",
      "Training Epoch: 37 [38784/50000]\tLoss: 4.7103\tLR: 3.677238\n",
      "Training Epoch: 37 [38912/50000]\tLoss: 4.6891\tLR: 3.677494\n",
      "Training Epoch: 37 [39040/50000]\tLoss: 4.6103\tLR: 3.677749\n",
      "Training Epoch: 37 [39168/50000]\tLoss: 4.6581\tLR: 3.678005\n",
      "Training Epoch: 37 [39296/50000]\tLoss: 4.6724\tLR: 3.678261\n",
      "Training Epoch: 37 [39424/50000]\tLoss: 4.6823\tLR: 3.678517\n",
      "Training Epoch: 37 [39552/50000]\tLoss: 4.6274\tLR: 3.678772\n",
      "Training Epoch: 37 [39680/50000]\tLoss: 4.6755\tLR: 3.679028\n",
      "Training Epoch: 37 [39808/50000]\tLoss: 4.7173\tLR: 3.679284\n",
      "Training Epoch: 37 [39936/50000]\tLoss: 4.7109\tLR: 3.679540\n",
      "Training Epoch: 37 [40064/50000]\tLoss: 4.6783\tLR: 3.679795\n",
      "Training Epoch: 37 [40192/50000]\tLoss: 4.6190\tLR: 3.680051\n",
      "Training Epoch: 37 [40320/50000]\tLoss: 4.5938\tLR: 3.680307\n",
      "Training Epoch: 37 [40448/50000]\tLoss: 4.6500\tLR: 3.680563\n",
      "Training Epoch: 37 [40576/50000]\tLoss: 4.6484\tLR: 3.680818\n",
      "Training Epoch: 37 [40704/50000]\tLoss: 4.6426\tLR: 3.681074\n",
      "Training Epoch: 37 [40832/50000]\tLoss: 4.6987\tLR: 3.681330\n",
      "Training Epoch: 37 [40960/50000]\tLoss: 4.6463\tLR: 3.681586\n",
      "Training Epoch: 37 [41088/50000]\tLoss: 4.7228\tLR: 3.681841\n",
      "Training Epoch: 37 [41216/50000]\tLoss: 4.6916\tLR: 3.682097\n",
      "Training Epoch: 37 [41344/50000]\tLoss: 4.6972\tLR: 3.682353\n",
      "Training Epoch: 37 [41472/50000]\tLoss: 4.6366\tLR: 3.682609\n",
      "Training Epoch: 37 [41600/50000]\tLoss: 4.5979\tLR: 3.682864\n",
      "Training Epoch: 37 [41728/50000]\tLoss: 4.7164\tLR: 3.683120\n",
      "Training Epoch: 37 [41856/50000]\tLoss: 4.7540\tLR: 3.683376\n",
      "Training Epoch: 37 [41984/50000]\tLoss: 4.6859\tLR: 3.683632\n",
      "Training Epoch: 37 [42112/50000]\tLoss: 4.7349\tLR: 3.683887\n",
      "Training Epoch: 37 [42240/50000]\tLoss: 4.7122\tLR: 3.684143\n",
      "Training Epoch: 37 [42368/50000]\tLoss: 4.7413\tLR: 3.684399\n",
      "Training Epoch: 37 [42496/50000]\tLoss: 4.6296\tLR: 3.684655\n",
      "Training Epoch: 37 [42624/50000]\tLoss: 4.7393\tLR: 3.684910\n",
      "Training Epoch: 37 [42752/50000]\tLoss: 4.6845\tLR: 3.685166\n",
      "Training Epoch: 37 [42880/50000]\tLoss: 4.6983\tLR: 3.685422\n",
      "Training Epoch: 37 [43008/50000]\tLoss: 4.6999\tLR: 3.685678\n",
      "Training Epoch: 37 [43136/50000]\tLoss: 4.7095\tLR: 3.685934\n",
      "Training Epoch: 37 [43264/50000]\tLoss: 4.6761\tLR: 3.686189\n",
      "Training Epoch: 37 [43392/50000]\tLoss: 4.6890\tLR: 3.686445\n",
      "Training Epoch: 37 [43520/50000]\tLoss: 4.6899\tLR: 3.686701\n",
      "Training Epoch: 37 [43648/50000]\tLoss: 4.6540\tLR: 3.686957\n",
      "Training Epoch: 37 [43776/50000]\tLoss: 4.6717\tLR: 3.687212\n",
      "Training Epoch: 37 [43904/50000]\tLoss: 4.7140\tLR: 3.687468\n",
      "Training Epoch: 37 [44032/50000]\tLoss: 4.6372\tLR: 3.687724\n",
      "Training Epoch: 37 [44160/50000]\tLoss: 4.6902\tLR: 3.687980\n",
      "Training Epoch: 37 [44288/50000]\tLoss: 4.6597\tLR: 3.688235\n",
      "Training Epoch: 37 [44416/50000]\tLoss: 4.6329\tLR: 3.688491\n",
      "Training Epoch: 37 [44544/50000]\tLoss: 4.7378\tLR: 3.688747\n",
      "Training Epoch: 37 [44672/50000]\tLoss: 4.6874\tLR: 3.689003\n",
      "Training Epoch: 37 [44800/50000]\tLoss: 4.7089\tLR: 3.689258\n",
      "Training Epoch: 37 [44928/50000]\tLoss: 4.6981\tLR: 3.689514\n",
      "Training Epoch: 37 [45056/50000]\tLoss: 4.7594\tLR: 3.689770\n",
      "Training Epoch: 37 [45184/50000]\tLoss: 4.7129\tLR: 3.690026\n",
      "Training Epoch: 37 [45312/50000]\tLoss: 4.6397\tLR: 3.690281\n",
      "Training Epoch: 37 [45440/50000]\tLoss: 4.6847\tLR: 3.690537\n",
      "Training Epoch: 37 [45568/50000]\tLoss: 4.6844\tLR: 3.690793\n",
      "Training Epoch: 37 [45696/50000]\tLoss: 4.6713\tLR: 3.691049\n",
      "Training Epoch: 37 [45824/50000]\tLoss: 4.6494\tLR: 3.691304\n",
      "Training Epoch: 37 [45952/50000]\tLoss: 4.6619\tLR: 3.691560\n",
      "Training Epoch: 37 [46080/50000]\tLoss: 4.6858\tLR: 3.691816\n",
      "Training Epoch: 37 [46208/50000]\tLoss: 4.6315\tLR: 3.692072\n",
      "Training Epoch: 37 [46336/50000]\tLoss: 4.6214\tLR: 3.692327\n",
      "Training Epoch: 37 [46464/50000]\tLoss: 4.6224\tLR: 3.692583\n",
      "Training Epoch: 37 [46592/50000]\tLoss: 4.7015\tLR: 3.692839\n",
      "Training Epoch: 37 [46720/50000]\tLoss: 4.7237\tLR: 3.693095\n",
      "Training Epoch: 37 [46848/50000]\tLoss: 4.6282\tLR: 3.693350\n",
      "Training Epoch: 37 [46976/50000]\tLoss: 4.7095\tLR: 3.693606\n",
      "Training Epoch: 37 [47104/50000]\tLoss: 4.6484\tLR: 3.693862\n",
      "Training Epoch: 37 [47232/50000]\tLoss: 4.6790\tLR: 3.694118\n",
      "Training Epoch: 37 [47360/50000]\tLoss: 4.6740\tLR: 3.694373\n",
      "Training Epoch: 37 [47488/50000]\tLoss: 4.6964\tLR: 3.694629\n",
      "Training Epoch: 37 [47616/50000]\tLoss: 4.7083\tLR: 3.694885\n",
      "Training Epoch: 37 [47744/50000]\tLoss: 4.6585\tLR: 3.695141\n",
      "Training Epoch: 37 [47872/50000]\tLoss: 4.6908\tLR: 3.695396\n",
      "Training Epoch: 37 [48000/50000]\tLoss: 4.6785\tLR: 3.695652\n",
      "Training Epoch: 37 [48128/50000]\tLoss: 4.7328\tLR: 3.695908\n",
      "Training Epoch: 37 [48256/50000]\tLoss: 4.5858\tLR: 3.696164\n",
      "Training Epoch: 37 [48384/50000]\tLoss: 4.6471\tLR: 3.696419\n",
      "Training Epoch: 37 [48512/50000]\tLoss: 4.7201\tLR: 3.696675\n",
      "Training Epoch: 37 [48640/50000]\tLoss: 4.6508\tLR: 3.696931\n",
      "Training Epoch: 37 [48768/50000]\tLoss: 4.7063\tLR: 3.697187\n",
      "Training Epoch: 37 [48896/50000]\tLoss: 4.6856\tLR: 3.697442\n",
      "Training Epoch: 37 [49024/50000]\tLoss: 4.6911\tLR: 3.697698\n",
      "Training Epoch: 37 [49152/50000]\tLoss: 4.6799\tLR: 3.697954\n",
      "Training Epoch: 37 [49280/50000]\tLoss: 4.7194\tLR: 3.698210\n",
      "Training Epoch: 37 [49408/50000]\tLoss: 4.6828\tLR: 3.698465\n",
      "Training Epoch: 37 [49536/50000]\tLoss: 4.6584\tLR: 3.698721\n",
      "Training Epoch: 37 [49664/50000]\tLoss: 4.6213\tLR: 3.698977\n",
      "Training Epoch: 37 [49792/50000]\tLoss: 4.6742\tLR: 3.699233\n",
      "Training Epoch: 37 [49920/50000]\tLoss: 4.6773\tLR: 3.699488\n",
      "Training Epoch: 37 [50000/50000]\tLoss: 4.6753\tLR: 3.699744\n",
      "epoch 37 training time consumed: 488.70s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   51872 GB |   51872 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   51713 GB |   51713 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     159 GB |     159 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   51872 GB |   51872 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   51713 GB |   51713 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     159 GB |     159 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   51141 GB |   51141 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   50982 GB |   50982 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     159 GB |     159 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    5500 K  |    5500 K  |\n",
      "|       from large pool |      24    |      65    |    2344 K  |    2344 K  |\n",
      "|       from small pool |     231    |     274    |    3155 K  |    3155 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    5500 K  |    5500 K  |\n",
      "|       from large pool |      24    |      65    |    2344 K  |    2344 K  |\n",
      "|       from small pool |     231    |     274    |    3155 K  |    3155 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      46    |    3187 K  |    3187 K  |\n",
      "|       from large pool |      10    |      23    |    1127 K  |    1127 K  |\n",
      "|       from small pool |      26    |      35    |    2060 K  |    2060 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 37, Average loss: 0.0370, Accuracy: 0.0100, Time consumed:31.16s\n",
      "\n",
      "Training Epoch: 38 [128/50000]\tLoss: 4.6198\tLR: 0.100000\n",
      "Training Epoch: 38 [256/50000]\tLoss: 4.6592\tLR: 3.700256\n",
      "Training Epoch: 38 [384/50000]\tLoss: 4.6012\tLR: 3.700512\n",
      "Training Epoch: 38 [512/50000]\tLoss: 4.6977\tLR: 3.700767\n",
      "Training Epoch: 38 [640/50000]\tLoss: 4.6828\tLR: 3.701023\n",
      "Training Epoch: 38 [768/50000]\tLoss: 4.6396\tLR: 3.701279\n",
      "Training Epoch: 38 [896/50000]\tLoss: 4.7124\tLR: 3.701535\n",
      "Training Epoch: 38 [1024/50000]\tLoss: 4.6153\tLR: 3.701790\n",
      "Training Epoch: 38 [1152/50000]\tLoss: 4.6608\tLR: 3.702046\n",
      "Training Epoch: 38 [1280/50000]\tLoss: 4.6730\tLR: 3.702302\n",
      "Training Epoch: 38 [1408/50000]\tLoss: 4.6118\tLR: 3.702558\n",
      "Training Epoch: 38 [1536/50000]\tLoss: 4.6944\tLR: 3.702813\n",
      "Training Epoch: 38 [1664/50000]\tLoss: 4.6456\tLR: 3.703069\n",
      "Training Epoch: 38 [1792/50000]\tLoss: 4.6832\tLR: 3.703325\n",
      "Training Epoch: 38 [1920/50000]\tLoss: 4.7168\tLR: 3.703581\n",
      "Training Epoch: 38 [2048/50000]\tLoss: 4.6565\tLR: 3.703836\n",
      "Training Epoch: 38 [2176/50000]\tLoss: 4.6616\tLR: 3.704092\n",
      "Training Epoch: 38 [2304/50000]\tLoss: 4.6069\tLR: 3.704348\n",
      "Training Epoch: 38 [2432/50000]\tLoss: 4.6369\tLR: 3.704604\n",
      "Training Epoch: 38 [2560/50000]\tLoss: 4.6562\tLR: 3.704859\n",
      "Training Epoch: 38 [2688/50000]\tLoss: 4.7350\tLR: 3.705115\n",
      "Training Epoch: 38 [2816/50000]\tLoss: 4.6840\tLR: 3.705371\n",
      "Training Epoch: 38 [2944/50000]\tLoss: 4.6133\tLR: 3.705627\n",
      "Training Epoch: 38 [3072/50000]\tLoss: 4.6564\tLR: 3.705882\n",
      "Training Epoch: 38 [3200/50000]\tLoss: 4.6622\tLR: 3.706138\n",
      "Training Epoch: 38 [3328/50000]\tLoss: 4.6650\tLR: 3.706394\n",
      "Training Epoch: 38 [3456/50000]\tLoss: 4.7204\tLR: 3.706650\n",
      "Training Epoch: 38 [3584/50000]\tLoss: 4.7679\tLR: 3.706905\n",
      "Training Epoch: 38 [3712/50000]\tLoss: 4.6341\tLR: 3.707161\n",
      "Training Epoch: 38 [3840/50000]\tLoss: 4.7102\tLR: 3.707417\n",
      "Training Epoch: 38 [3968/50000]\tLoss: 4.6267\tLR: 3.707673\n",
      "Training Epoch: 38 [4096/50000]\tLoss: 4.6973\tLR: 3.707928\n",
      "Training Epoch: 38 [4224/50000]\tLoss: 4.6028\tLR: 3.708184\n",
      "Training Epoch: 38 [4352/50000]\tLoss: 4.6494\tLR: 3.708440\n",
      "Training Epoch: 38 [4480/50000]\tLoss: 4.6037\tLR: 3.708696\n",
      "Training Epoch: 38 [4608/50000]\tLoss: 4.6199\tLR: 3.708951\n",
      "Training Epoch: 38 [4736/50000]\tLoss: 4.7047\tLR: 3.709207\n",
      "Training Epoch: 38 [4864/50000]\tLoss: 4.6714\tLR: 3.709463\n",
      "Training Epoch: 38 [4992/50000]\tLoss: 4.7266\tLR: 3.709719\n",
      "Training Epoch: 38 [5120/50000]\tLoss: 4.6824\tLR: 3.709974\n",
      "Training Epoch: 38 [5248/50000]\tLoss: 4.6956\tLR: 3.710230\n",
      "Training Epoch: 38 [5376/50000]\tLoss: 4.6209\tLR: 3.710486\n",
      "Training Epoch: 38 [5504/50000]\tLoss: 4.7182\tLR: 3.710742\n",
      "Training Epoch: 38 [5632/50000]\tLoss: 4.6861\tLR: 3.710997\n",
      "Training Epoch: 38 [5760/50000]\tLoss: 4.7103\tLR: 3.711253\n",
      "Training Epoch: 38 [5888/50000]\tLoss: 4.6708\tLR: 3.711509\n",
      "Training Epoch: 38 [6016/50000]\tLoss: 4.6861\tLR: 3.711765\n",
      "Training Epoch: 38 [6144/50000]\tLoss: 4.6283\tLR: 3.712020\n",
      "Training Epoch: 38 [6272/50000]\tLoss: 4.6696\tLR: 3.712276\n",
      "Training Epoch: 38 [6400/50000]\tLoss: 4.6655\tLR: 3.712532\n",
      "Training Epoch: 38 [6528/50000]\tLoss: 4.7314\tLR: 3.712788\n",
      "Training Epoch: 38 [6656/50000]\tLoss: 4.6647\tLR: 3.713043\n",
      "Training Epoch: 38 [6784/50000]\tLoss: 4.7090\tLR: 3.713299\n",
      "Training Epoch: 38 [6912/50000]\tLoss: 4.6957\tLR: 3.713555\n",
      "Training Epoch: 38 [7040/50000]\tLoss: 4.7261\tLR: 3.713811\n",
      "Training Epoch: 38 [7168/50000]\tLoss: 4.6528\tLR: 3.714066\n",
      "Training Epoch: 38 [7296/50000]\tLoss: 4.6813\tLR: 3.714322\n",
      "Training Epoch: 38 [7424/50000]\tLoss: 4.6516\tLR: 3.714578\n",
      "Training Epoch: 38 [7552/50000]\tLoss: 4.6501\tLR: 3.714834\n",
      "Training Epoch: 38 [7680/50000]\tLoss: 4.6857\tLR: 3.715090\n",
      "Training Epoch: 38 [7808/50000]\tLoss: 4.7624\tLR: 3.715345\n",
      "Training Epoch: 38 [7936/50000]\tLoss: 4.6768\tLR: 3.715601\n",
      "Training Epoch: 38 [8064/50000]\tLoss: 4.6979\tLR: 3.715857\n",
      "Training Epoch: 38 [8192/50000]\tLoss: 4.6579\tLR: 3.716113\n",
      "Training Epoch: 38 [8320/50000]\tLoss: 4.6904\tLR: 3.716368\n",
      "Training Epoch: 38 [8448/50000]\tLoss: 4.6533\tLR: 3.716624\n",
      "Training Epoch: 38 [8576/50000]\tLoss: 4.6830\tLR: 3.716880\n",
      "Training Epoch: 38 [8704/50000]\tLoss: 4.6730\tLR: 3.717136\n",
      "Training Epoch: 38 [8832/50000]\tLoss: 4.6272\tLR: 3.717391\n",
      "Training Epoch: 38 [8960/50000]\tLoss: 4.7172\tLR: 3.717647\n",
      "Training Epoch: 38 [9088/50000]\tLoss: 4.7344\tLR: 3.717903\n",
      "Training Epoch: 38 [9216/50000]\tLoss: 4.7201\tLR: 3.718159\n",
      "Training Epoch: 38 [9344/50000]\tLoss: 4.6451\tLR: 3.718414\n",
      "Training Epoch: 38 [9472/50000]\tLoss: 4.6775\tLR: 3.718670\n",
      "Training Epoch: 38 [9600/50000]\tLoss: 4.6755\tLR: 3.718926\n",
      "Training Epoch: 38 [9728/50000]\tLoss: 4.6814\tLR: 3.719182\n",
      "Training Epoch: 38 [9856/50000]\tLoss: 4.6909\tLR: 3.719437\n",
      "Training Epoch: 38 [9984/50000]\tLoss: 4.6520\tLR: 3.719693\n",
      "Training Epoch: 38 [10112/50000]\tLoss: 4.6508\tLR: 3.719949\n",
      "Training Epoch: 38 [10240/50000]\tLoss: 4.6962\tLR: 3.720205\n",
      "Training Epoch: 38 [10368/50000]\tLoss: 4.6942\tLR: 3.720460\n",
      "Training Epoch: 38 [10496/50000]\tLoss: 4.6599\tLR: 3.720716\n",
      "Training Epoch: 38 [10624/50000]\tLoss: 4.7324\tLR: 3.720972\n",
      "Training Epoch: 38 [10752/50000]\tLoss: 4.7155\tLR: 3.721228\n",
      "Training Epoch: 38 [10880/50000]\tLoss: 4.7034\tLR: 3.721483\n",
      "Training Epoch: 38 [11008/50000]\tLoss: 4.7068\tLR: 3.721739\n",
      "Training Epoch: 38 [11136/50000]\tLoss: 4.6356\tLR: 3.721995\n",
      "Training Epoch: 38 [11264/50000]\tLoss: 4.6626\tLR: 3.722251\n",
      "Training Epoch: 38 [11392/50000]\tLoss: 4.6945\tLR: 3.722506\n",
      "Training Epoch: 38 [11520/50000]\tLoss: 4.7430\tLR: 3.722762\n",
      "Training Epoch: 38 [11648/50000]\tLoss: 4.6819\tLR: 3.723018\n",
      "Training Epoch: 38 [11776/50000]\tLoss: 4.6588\tLR: 3.723274\n",
      "Training Epoch: 38 [11904/50000]\tLoss: 4.6121\tLR: 3.723529\n",
      "Training Epoch: 38 [12032/50000]\tLoss: 4.6354\tLR: 3.723785\n",
      "Training Epoch: 38 [12160/50000]\tLoss: 4.6679\tLR: 3.724041\n",
      "Training Epoch: 38 [12288/50000]\tLoss: 4.6762\tLR: 3.724297\n",
      "Training Epoch: 38 [12416/50000]\tLoss: 4.6218\tLR: 3.724552\n",
      "Training Epoch: 38 [12544/50000]\tLoss: 4.6763\tLR: 3.724808\n",
      "Training Epoch: 38 [12672/50000]\tLoss: 4.6809\tLR: 3.725064\n",
      "Training Epoch: 38 [12800/50000]\tLoss: 4.6776\tLR: 3.725320\n",
      "Training Epoch: 38 [12928/50000]\tLoss: 4.7034\tLR: 3.725575\n",
      "Training Epoch: 38 [13056/50000]\tLoss: 4.6659\tLR: 3.725831\n",
      "Training Epoch: 38 [13184/50000]\tLoss: 4.7076\tLR: 3.726087\n",
      "Training Epoch: 38 [13312/50000]\tLoss: 4.6580\tLR: 3.726343\n",
      "Training Epoch: 38 [13440/50000]\tLoss: 4.7171\tLR: 3.726598\n",
      "Training Epoch: 38 [13568/50000]\tLoss: 4.6966\tLR: 3.726854\n",
      "Training Epoch: 38 [13696/50000]\tLoss: 4.6208\tLR: 3.727110\n",
      "Training Epoch: 38 [13824/50000]\tLoss: 4.6805\tLR: 3.727366\n",
      "Training Epoch: 38 [13952/50000]\tLoss: 4.6974\tLR: 3.727621\n",
      "Training Epoch: 38 [14080/50000]\tLoss: 4.6287\tLR: 3.727877\n",
      "Training Epoch: 38 [14208/50000]\tLoss: 4.6659\tLR: 3.728133\n",
      "Training Epoch: 38 [14336/50000]\tLoss: 4.6914\tLR: 3.728389\n",
      "Training Epoch: 38 [14464/50000]\tLoss: 4.6556\tLR: 3.728645\n",
      "Training Epoch: 38 [14592/50000]\tLoss: 4.7312\tLR: 3.728900\n",
      "Training Epoch: 38 [14720/50000]\tLoss: 4.6763\tLR: 3.729156\n",
      "Training Epoch: 38 [14848/50000]\tLoss: 4.6703\tLR: 3.729412\n",
      "Training Epoch: 38 [14976/50000]\tLoss: 4.6549\tLR: 3.729668\n",
      "Training Epoch: 38 [15104/50000]\tLoss: 4.6706\tLR: 3.729923\n",
      "Training Epoch: 38 [15232/50000]\tLoss: 4.6681\tLR: 3.730179\n",
      "Training Epoch: 38 [15360/50000]\tLoss: 4.6754\tLR: 3.730435\n",
      "Training Epoch: 38 [15488/50000]\tLoss: 4.6406\tLR: 3.730691\n",
      "Training Epoch: 38 [15616/50000]\tLoss: 4.6679\tLR: 3.730946\n",
      "Training Epoch: 38 [15744/50000]\tLoss: 4.6785\tLR: 3.731202\n",
      "Training Epoch: 38 [15872/50000]\tLoss: 4.6318\tLR: 3.731458\n",
      "Training Epoch: 38 [16000/50000]\tLoss: 4.6399\tLR: 3.731714\n",
      "Training Epoch: 38 [16128/50000]\tLoss: 4.7054\tLR: 3.731969\n",
      "Training Epoch: 38 [16256/50000]\tLoss: 4.5956\tLR: 3.732225\n",
      "Training Epoch: 38 [16384/50000]\tLoss: 4.7249\tLR: 3.732481\n",
      "Training Epoch: 38 [16512/50000]\tLoss: 4.6886\tLR: 3.732737\n",
      "Training Epoch: 38 [16640/50000]\tLoss: 4.6704\tLR: 3.732992\n",
      "Training Epoch: 38 [16768/50000]\tLoss: 4.6629\tLR: 3.733248\n",
      "Training Epoch: 38 [16896/50000]\tLoss: 4.7416\tLR: 3.733504\n",
      "Training Epoch: 38 [17024/50000]\tLoss: 4.6315\tLR: 3.733760\n",
      "Training Epoch: 38 [17152/50000]\tLoss: 4.6884\tLR: 3.734015\n",
      "Training Epoch: 38 [17280/50000]\tLoss: 4.7221\tLR: 3.734271\n",
      "Training Epoch: 38 [17408/50000]\tLoss: 4.7216\tLR: 3.734527\n",
      "Training Epoch: 38 [17536/50000]\tLoss: 4.6787\tLR: 3.734783\n",
      "Training Epoch: 38 [17664/50000]\tLoss: 4.6816\tLR: 3.735038\n",
      "Training Epoch: 38 [17792/50000]\tLoss: 4.6565\tLR: 3.735294\n",
      "Training Epoch: 38 [17920/50000]\tLoss: 4.6367\tLR: 3.735550\n",
      "Training Epoch: 38 [18048/50000]\tLoss: 4.6430\tLR: 3.735806\n",
      "Training Epoch: 38 [18176/50000]\tLoss: 4.6273\tLR: 3.736061\n",
      "Training Epoch: 38 [18304/50000]\tLoss: 4.6645\tLR: 3.736317\n",
      "Training Epoch: 38 [18432/50000]\tLoss: 4.6583\tLR: 3.736573\n",
      "Training Epoch: 38 [18560/50000]\tLoss: 4.6706\tLR: 3.736829\n",
      "Training Epoch: 38 [18688/50000]\tLoss: 4.6988\tLR: 3.737084\n",
      "Training Epoch: 38 [18816/50000]\tLoss: 4.6000\tLR: 3.737340\n",
      "Training Epoch: 38 [18944/50000]\tLoss: 4.7057\tLR: 3.737596\n",
      "Training Epoch: 38 [19072/50000]\tLoss: 4.6637\tLR: 3.737852\n",
      "Training Epoch: 38 [19200/50000]\tLoss: 4.6666\tLR: 3.738107\n",
      "Training Epoch: 38 [19328/50000]\tLoss: 4.6768\tLR: 3.738363\n",
      "Training Epoch: 38 [19456/50000]\tLoss: 4.7313\tLR: 3.738619\n",
      "Training Epoch: 38 [19584/50000]\tLoss: 4.7125\tLR: 3.738875\n",
      "Training Epoch: 38 [19712/50000]\tLoss: 4.6541\tLR: 3.739130\n",
      "Training Epoch: 38 [19840/50000]\tLoss: 4.6907\tLR: 3.739386\n",
      "Training Epoch: 38 [19968/50000]\tLoss: 4.7172\tLR: 3.739642\n",
      "Training Epoch: 38 [20096/50000]\tLoss: 4.6380\tLR: 3.739898\n",
      "Training Epoch: 38 [20224/50000]\tLoss: 4.6597\tLR: 3.740153\n",
      "Training Epoch: 38 [20352/50000]\tLoss: 4.6345\tLR: 3.740409\n",
      "Training Epoch: 38 [20480/50000]\tLoss: 4.7030\tLR: 3.740665\n",
      "Training Epoch: 38 [20608/50000]\tLoss: 4.6959\tLR: 3.740921\n",
      "Training Epoch: 38 [20736/50000]\tLoss: 4.6987\tLR: 3.741176\n",
      "Training Epoch: 38 [20864/50000]\tLoss: 4.6864\tLR: 3.741432\n",
      "Training Epoch: 38 [20992/50000]\tLoss: 4.6442\tLR: 3.741688\n",
      "Training Epoch: 38 [21120/50000]\tLoss: 4.7017\tLR: 3.741944\n",
      "Training Epoch: 38 [21248/50000]\tLoss: 4.6648\tLR: 3.742199\n",
      "Training Epoch: 38 [21376/50000]\tLoss: 4.6368\tLR: 3.742455\n",
      "Training Epoch: 38 [21504/50000]\tLoss: 4.7244\tLR: 3.742711\n",
      "Training Epoch: 38 [21632/50000]\tLoss: 4.6442\tLR: 3.742967\n",
      "Training Epoch: 38 [21760/50000]\tLoss: 4.7212\tLR: 3.743223\n",
      "Training Epoch: 38 [21888/50000]\tLoss: 4.6133\tLR: 3.743478\n",
      "Training Epoch: 38 [22016/50000]\tLoss: 4.7048\tLR: 3.743734\n",
      "Training Epoch: 38 [22144/50000]\tLoss: 4.7118\tLR: 3.743990\n",
      "Training Epoch: 38 [22272/50000]\tLoss: 4.6739\tLR: 3.744246\n",
      "Training Epoch: 38 [22400/50000]\tLoss: 4.6557\tLR: 3.744501\n",
      "Training Epoch: 38 [22528/50000]\tLoss: 4.6881\tLR: 3.744757\n",
      "Training Epoch: 38 [22656/50000]\tLoss: 4.7281\tLR: 3.745013\n",
      "Training Epoch: 38 [22784/50000]\tLoss: 4.6620\tLR: 3.745269\n",
      "Training Epoch: 38 [22912/50000]\tLoss: 4.6772\tLR: 3.745524\n",
      "Training Epoch: 38 [23040/50000]\tLoss: 4.6426\tLR: 3.745780\n",
      "Training Epoch: 38 [23168/50000]\tLoss: 4.6330\tLR: 3.746036\n",
      "Training Epoch: 38 [23296/50000]\tLoss: 4.5820\tLR: 3.746292\n",
      "Training Epoch: 38 [23424/50000]\tLoss: 4.6978\tLR: 3.746547\n",
      "Training Epoch: 38 [23552/50000]\tLoss: 4.7028\tLR: 3.746803\n",
      "Training Epoch: 38 [23680/50000]\tLoss: 4.6905\tLR: 3.747059\n",
      "Training Epoch: 38 [23808/50000]\tLoss: 4.6828\tLR: 3.747315\n",
      "Training Epoch: 38 [23936/50000]\tLoss: 4.8407\tLR: 3.747570\n",
      "Training Epoch: 38 [24064/50000]\tLoss: 4.7217\tLR: 3.747826\n",
      "Training Epoch: 38 [24192/50000]\tLoss: 4.7048\tLR: 3.748082\n",
      "Training Epoch: 38 [24320/50000]\tLoss: 4.7263\tLR: 3.748338\n",
      "Training Epoch: 38 [24448/50000]\tLoss: 4.7345\tLR: 3.748593\n",
      "Training Epoch: 38 [24576/50000]\tLoss: 4.6753\tLR: 3.748849\n",
      "Training Epoch: 38 [24704/50000]\tLoss: 4.7225\tLR: 3.749105\n",
      "Training Epoch: 38 [24832/50000]\tLoss: 4.6896\tLR: 3.749361\n",
      "Training Epoch: 38 [24960/50000]\tLoss: 4.6759\tLR: 3.749616\n",
      "Training Epoch: 38 [25088/50000]\tLoss: 4.6937\tLR: 3.749872\n",
      "Training Epoch: 38 [25216/50000]\tLoss: 4.6768\tLR: 3.750128\n",
      "Training Epoch: 38 [25344/50000]\tLoss: 4.6946\tLR: 3.750384\n",
      "Training Epoch: 38 [25472/50000]\tLoss: 4.7035\tLR: 3.750639\n",
      "Training Epoch: 38 [25600/50000]\tLoss: 4.6426\tLR: 3.750895\n",
      "Training Epoch: 38 [25728/50000]\tLoss: 4.7283\tLR: 3.751151\n",
      "Training Epoch: 38 [25856/50000]\tLoss: 4.6818\tLR: 3.751407\n",
      "Training Epoch: 38 [25984/50000]\tLoss: 4.7033\tLR: 3.751662\n",
      "Training Epoch: 38 [26112/50000]\tLoss: 4.6061\tLR: 3.751918\n",
      "Training Epoch: 38 [26240/50000]\tLoss: 4.7001\tLR: 3.752174\n",
      "Training Epoch: 38 [26368/50000]\tLoss: 4.6981\tLR: 3.752430\n",
      "Training Epoch: 38 [26496/50000]\tLoss: 4.6363\tLR: 3.752685\n",
      "Training Epoch: 38 [26624/50000]\tLoss: 4.6461\tLR: 3.752941\n",
      "Training Epoch: 38 [26752/50000]\tLoss: 4.6634\tLR: 3.753197\n",
      "Training Epoch: 38 [26880/50000]\tLoss: 4.6574\tLR: 3.753453\n",
      "Training Epoch: 38 [27008/50000]\tLoss: 4.6688\tLR: 3.753708\n",
      "Training Epoch: 38 [27136/50000]\tLoss: 4.6982\tLR: 3.753964\n",
      "Training Epoch: 38 [27264/50000]\tLoss: 4.6498\tLR: 3.754220\n",
      "Training Epoch: 38 [27392/50000]\tLoss: 4.6585\tLR: 3.754476\n",
      "Training Epoch: 38 [27520/50000]\tLoss: 4.6963\tLR: 3.754731\n",
      "Training Epoch: 38 [27648/50000]\tLoss: 4.7119\tLR: 3.754987\n",
      "Training Epoch: 38 [27776/50000]\tLoss: 4.6427\tLR: 3.755243\n",
      "Training Epoch: 38 [27904/50000]\tLoss: 4.6986\tLR: 3.755499\n",
      "Training Epoch: 38 [28032/50000]\tLoss: 4.6189\tLR: 3.755754\n",
      "Training Epoch: 38 [28160/50000]\tLoss: 4.6596\tLR: 3.756010\n",
      "Training Epoch: 38 [28288/50000]\tLoss: 4.6587\tLR: 3.756266\n",
      "Training Epoch: 38 [28416/50000]\tLoss: 4.6735\tLR: 3.756522\n",
      "Training Epoch: 38 [28544/50000]\tLoss: 4.6467\tLR: 3.756777\n",
      "Training Epoch: 38 [28672/50000]\tLoss: 4.6922\tLR: 3.757033\n",
      "Training Epoch: 38 [28800/50000]\tLoss: 4.6569\tLR: 3.757289\n",
      "Training Epoch: 38 [28928/50000]\tLoss: 4.6690\tLR: 3.757545\n",
      "Training Epoch: 38 [29056/50000]\tLoss: 4.6873\tLR: 3.757801\n",
      "Training Epoch: 38 [29184/50000]\tLoss: 4.6415\tLR: 3.758056\n",
      "Training Epoch: 38 [29312/50000]\tLoss: 4.6466\tLR: 3.758312\n",
      "Training Epoch: 38 [29440/50000]\tLoss: 4.6484\tLR: 3.758568\n",
      "Training Epoch: 38 [29568/50000]\tLoss: 4.6883\tLR: 3.758824\n",
      "Training Epoch: 38 [29696/50000]\tLoss: 4.6508\tLR: 3.759079\n",
      "Training Epoch: 38 [29824/50000]\tLoss: 4.7296\tLR: 3.759335\n",
      "Training Epoch: 38 [29952/50000]\tLoss: 4.7038\tLR: 3.759591\n",
      "Training Epoch: 38 [30080/50000]\tLoss: 4.7314\tLR: 3.759847\n",
      "Training Epoch: 38 [30208/50000]\tLoss: 4.6669\tLR: 3.760102\n",
      "Training Epoch: 38 [30336/50000]\tLoss: 4.6289\tLR: 3.760358\n",
      "Training Epoch: 38 [30464/50000]\tLoss: 4.7064\tLR: 3.760614\n",
      "Training Epoch: 38 [30592/50000]\tLoss: 4.6342\tLR: 3.760870\n",
      "Training Epoch: 38 [30720/50000]\tLoss: 4.6574\tLR: 3.761125\n",
      "Training Epoch: 38 [30848/50000]\tLoss: 4.6480\tLR: 3.761381\n",
      "Training Epoch: 38 [30976/50000]\tLoss: 4.7259\tLR: 3.761637\n",
      "Training Epoch: 38 [31104/50000]\tLoss: 4.6966\tLR: 3.761893\n",
      "Training Epoch: 38 [31232/50000]\tLoss: 4.6121\tLR: 3.762148\n",
      "Training Epoch: 38 [31360/50000]\tLoss: 4.6373\tLR: 3.762404\n",
      "Training Epoch: 38 [31488/50000]\tLoss: 4.7385\tLR: 3.762660\n",
      "Training Epoch: 38 [31616/50000]\tLoss: 4.7296\tLR: 3.762916\n",
      "Training Epoch: 38 [31744/50000]\tLoss: 4.7158\tLR: 3.763171\n",
      "Training Epoch: 38 [31872/50000]\tLoss: 4.6703\tLR: 3.763427\n",
      "Training Epoch: 38 [32000/50000]\tLoss: 4.6902\tLR: 3.763683\n",
      "Training Epoch: 38 [32128/50000]\tLoss: 4.6091\tLR: 3.763939\n",
      "Training Epoch: 38 [32256/50000]\tLoss: 4.6432\tLR: 3.764194\n",
      "Training Epoch: 38 [32384/50000]\tLoss: 4.6793\tLR: 3.764450\n",
      "Training Epoch: 38 [32512/50000]\tLoss: 4.6399\tLR: 3.764706\n",
      "Training Epoch: 38 [32640/50000]\tLoss: 4.6674\tLR: 3.764962\n",
      "Training Epoch: 38 [32768/50000]\tLoss: 4.6725\tLR: 3.765217\n",
      "Training Epoch: 38 [32896/50000]\tLoss: 4.6190\tLR: 3.765473\n",
      "Training Epoch: 38 [33024/50000]\tLoss: 4.6626\tLR: 3.765729\n",
      "Training Epoch: 38 [33152/50000]\tLoss: 4.6924\tLR: 3.765985\n",
      "Training Epoch: 38 [33280/50000]\tLoss: 4.6269\tLR: 3.766240\n",
      "Training Epoch: 38 [33408/50000]\tLoss: 4.6902\tLR: 3.766496\n",
      "Training Epoch: 38 [33536/50000]\tLoss: 4.7062\tLR: 3.766752\n",
      "Training Epoch: 38 [33664/50000]\tLoss: 4.6698\tLR: 3.767008\n",
      "Training Epoch: 38 [33792/50000]\tLoss: 4.6935\tLR: 3.767263\n",
      "Training Epoch: 38 [33920/50000]\tLoss: 4.6324\tLR: 3.767519\n",
      "Training Epoch: 38 [34048/50000]\tLoss: 4.6358\tLR: 3.767775\n",
      "Training Epoch: 38 [34176/50000]\tLoss: 4.7076\tLR: 3.768031\n",
      "Training Epoch: 38 [34304/50000]\tLoss: 4.6667\tLR: 3.768286\n",
      "Training Epoch: 38 [34432/50000]\tLoss: 4.7002\tLR: 3.768542\n",
      "Training Epoch: 38 [34560/50000]\tLoss: 4.6532\tLR: 3.768798\n",
      "Training Epoch: 38 [34688/50000]\tLoss: 4.6612\tLR: 3.769054\n",
      "Training Epoch: 38 [34816/50000]\tLoss: 4.7129\tLR: 3.769309\n",
      "Training Epoch: 38 [34944/50000]\tLoss: 4.7467\tLR: 3.769565\n",
      "Training Epoch: 38 [35072/50000]\tLoss: 4.6857\tLR: 3.769821\n",
      "Training Epoch: 38 [35200/50000]\tLoss: 4.6769\tLR: 3.770077\n",
      "Training Epoch: 38 [35328/50000]\tLoss: 4.6555\tLR: 3.770332\n",
      "Training Epoch: 38 [35456/50000]\tLoss: 4.6385\tLR: 3.770588\n",
      "Training Epoch: 38 [35584/50000]\tLoss: 4.7111\tLR: 3.770844\n",
      "Training Epoch: 38 [35712/50000]\tLoss: 4.6877\tLR: 3.771100\n",
      "Training Epoch: 38 [35840/50000]\tLoss: 4.7090\tLR: 3.771355\n",
      "Training Epoch: 38 [35968/50000]\tLoss: 4.6488\tLR: 3.771611\n",
      "Training Epoch: 38 [36096/50000]\tLoss: 4.7128\tLR: 3.771867\n",
      "Training Epoch: 38 [36224/50000]\tLoss: 4.6300\tLR: 3.772123\n",
      "Training Epoch: 38 [36352/50000]\tLoss: 4.7068\tLR: 3.772379\n",
      "Training Epoch: 38 [36480/50000]\tLoss: 4.7403\tLR: 3.772634\n",
      "Training Epoch: 38 [36608/50000]\tLoss: 4.7228\tLR: 3.772890\n",
      "Training Epoch: 38 [36736/50000]\tLoss: 4.7018\tLR: 3.773146\n",
      "Training Epoch: 38 [36864/50000]\tLoss: 4.6614\tLR: 3.773402\n",
      "Training Epoch: 38 [36992/50000]\tLoss: 4.6527\tLR: 3.773657\n",
      "Training Epoch: 38 [37120/50000]\tLoss: 4.6397\tLR: 3.773913\n",
      "Training Epoch: 38 [37248/50000]\tLoss: 4.7190\tLR: 3.774169\n",
      "Training Epoch: 38 [37376/50000]\tLoss: 4.6946\tLR: 3.774425\n",
      "Training Epoch: 38 [37504/50000]\tLoss: 4.6857\tLR: 3.774680\n",
      "Training Epoch: 38 [37632/50000]\tLoss: 4.7899\tLR: 3.774936\n",
      "Training Epoch: 38 [37760/50000]\tLoss: 4.5950\tLR: 3.775192\n",
      "Training Epoch: 38 [37888/50000]\tLoss: 4.6544\tLR: 3.775448\n",
      "Training Epoch: 38 [38016/50000]\tLoss: 4.6397\tLR: 3.775703\n",
      "Training Epoch: 38 [38144/50000]\tLoss: 4.6687\tLR: 3.775959\n",
      "Training Epoch: 38 [38272/50000]\tLoss: 4.6924\tLR: 3.776215\n",
      "Training Epoch: 38 [38400/50000]\tLoss: 4.6173\tLR: 3.776471\n",
      "Training Epoch: 38 [38528/50000]\tLoss: 4.7210\tLR: 3.776726\n",
      "Training Epoch: 38 [38656/50000]\tLoss: 4.6894\tLR: 3.776982\n",
      "Training Epoch: 38 [38784/50000]\tLoss: 4.7103\tLR: 3.777238\n",
      "Training Epoch: 38 [38912/50000]\tLoss: 4.6535\tLR: 3.777494\n",
      "Training Epoch: 38 [39040/50000]\tLoss: 4.6586\tLR: 3.777749\n",
      "Training Epoch: 38 [39168/50000]\tLoss: 4.6938\tLR: 3.778005\n",
      "Training Epoch: 38 [39296/50000]\tLoss: 4.6803\tLR: 3.778261\n",
      "Training Epoch: 38 [39424/50000]\tLoss: 4.6460\tLR: 3.778517\n",
      "Training Epoch: 38 [39552/50000]\tLoss: 4.6996\tLR: 3.778772\n",
      "Training Epoch: 38 [39680/50000]\tLoss: 4.6699\tLR: 3.779028\n",
      "Training Epoch: 38 [39808/50000]\tLoss: 4.6807\tLR: 3.779284\n",
      "Training Epoch: 38 [39936/50000]\tLoss: 4.6319\tLR: 3.779540\n",
      "Training Epoch: 38 [40064/50000]\tLoss: 4.7481\tLR: 3.779795\n",
      "Training Epoch: 38 [40192/50000]\tLoss: 4.6642\tLR: 3.780051\n",
      "Training Epoch: 38 [40320/50000]\tLoss: 4.6452\tLR: 3.780307\n",
      "Training Epoch: 38 [40448/50000]\tLoss: 4.6218\tLR: 3.780563\n",
      "Training Epoch: 38 [40576/50000]\tLoss: 4.7457\tLR: 3.780818\n",
      "Training Epoch: 38 [40704/50000]\tLoss: 4.7364\tLR: 3.781074\n",
      "Training Epoch: 38 [40832/50000]\tLoss: 4.6466\tLR: 3.781330\n",
      "Training Epoch: 38 [40960/50000]\tLoss: 4.6979\tLR: 3.781586\n",
      "Training Epoch: 38 [41088/50000]\tLoss: 4.7131\tLR: 3.781841\n",
      "Training Epoch: 38 [41216/50000]\tLoss: 4.6641\tLR: 3.782097\n",
      "Training Epoch: 38 [41344/50000]\tLoss: 4.5855\tLR: 3.782353\n",
      "Training Epoch: 38 [41472/50000]\tLoss: 4.6258\tLR: 3.782609\n",
      "Training Epoch: 38 [41600/50000]\tLoss: 4.7193\tLR: 3.782864\n",
      "Training Epoch: 38 [41728/50000]\tLoss: 4.7411\tLR: 3.783120\n",
      "Training Epoch: 38 [41856/50000]\tLoss: 4.7062\tLR: 3.783376\n",
      "Training Epoch: 38 [41984/50000]\tLoss: 4.6643\tLR: 3.783632\n",
      "Training Epoch: 38 [42112/50000]\tLoss: 4.6947\tLR: 3.783887\n",
      "Training Epoch: 38 [42240/50000]\tLoss: 4.6957\tLR: 3.784143\n",
      "Training Epoch: 38 [42368/50000]\tLoss: 4.6577\tLR: 3.784399\n",
      "Training Epoch: 38 [42496/50000]\tLoss: 4.6649\tLR: 3.784655\n",
      "Training Epoch: 38 [42624/50000]\tLoss: 4.6296\tLR: 3.784910\n",
      "Training Epoch: 38 [42752/50000]\tLoss: 4.6692\tLR: 3.785166\n",
      "Training Epoch: 38 [42880/50000]\tLoss: 4.6554\tLR: 3.785422\n",
      "Training Epoch: 38 [43008/50000]\tLoss: 4.6908\tLR: 3.785678\n",
      "Training Epoch: 38 [43136/50000]\tLoss: 4.6268\tLR: 3.785934\n",
      "Training Epoch: 38 [43264/50000]\tLoss: 4.6592\tLR: 3.786189\n",
      "Training Epoch: 38 [43392/50000]\tLoss: 4.6754\tLR: 3.786445\n",
      "Training Epoch: 38 [43520/50000]\tLoss: 4.7088\tLR: 3.786701\n",
      "Training Epoch: 38 [43648/50000]\tLoss: 4.7170\tLR: 3.786957\n",
      "Training Epoch: 38 [43776/50000]\tLoss: 4.6439\tLR: 3.787212\n",
      "Training Epoch: 38 [43904/50000]\tLoss: 4.6805\tLR: 3.787468\n",
      "Training Epoch: 38 [44032/50000]\tLoss: 4.7093\tLR: 3.787724\n",
      "Training Epoch: 38 [44160/50000]\tLoss: 4.6697\tLR: 3.787980\n",
      "Training Epoch: 38 [44288/50000]\tLoss: 4.7546\tLR: 3.788235\n",
      "Training Epoch: 38 [44416/50000]\tLoss: 4.6998\tLR: 3.788491\n",
      "Training Epoch: 38 [44544/50000]\tLoss: 4.6319\tLR: 3.788747\n",
      "Training Epoch: 38 [44672/50000]\tLoss: 4.6564\tLR: 3.789003\n",
      "Training Epoch: 38 [44800/50000]\tLoss: 4.5661\tLR: 3.789258\n",
      "Training Epoch: 38 [44928/50000]\tLoss: 4.6630\tLR: 3.789514\n",
      "Training Epoch: 38 [45056/50000]\tLoss: 4.6422\tLR: 3.789770\n",
      "Training Epoch: 38 [45184/50000]\tLoss: 4.6652\tLR: 3.790026\n",
      "Training Epoch: 38 [45312/50000]\tLoss: 4.6688\tLR: 3.790281\n",
      "Training Epoch: 38 [45440/50000]\tLoss: 4.6863\tLR: 3.790537\n",
      "Training Epoch: 38 [45568/50000]\tLoss: 4.6601\tLR: 3.790793\n",
      "Training Epoch: 38 [45696/50000]\tLoss: 4.5871\tLR: 3.791049\n",
      "Training Epoch: 38 [45824/50000]\tLoss: 4.6682\tLR: 3.791304\n",
      "Training Epoch: 38 [45952/50000]\tLoss: 4.7245\tLR: 3.791560\n",
      "Training Epoch: 38 [46080/50000]\tLoss: 4.7157\tLR: 3.791816\n",
      "Training Epoch: 38 [46208/50000]\tLoss: 4.6913\tLR: 3.792072\n",
      "Training Epoch: 38 [46336/50000]\tLoss: 4.7142\tLR: 3.792327\n",
      "Training Epoch: 38 [46464/50000]\tLoss: 4.7072\tLR: 3.792583\n",
      "Training Epoch: 38 [46592/50000]\tLoss: 4.6690\tLR: 3.792839\n",
      "Training Epoch: 38 [46720/50000]\tLoss: 4.6542\tLR: 3.793095\n",
      "Training Epoch: 38 [46848/50000]\tLoss: 4.6857\tLR: 3.793350\n",
      "Training Epoch: 38 [46976/50000]\tLoss: 4.6783\tLR: 3.793606\n",
      "Training Epoch: 38 [47104/50000]\tLoss: 4.6449\tLR: 3.793862\n",
      "Training Epoch: 38 [47232/50000]\tLoss: 4.6707\tLR: 3.794118\n",
      "Training Epoch: 38 [47360/50000]\tLoss: 4.6553\tLR: 3.794373\n",
      "Training Epoch: 38 [47488/50000]\tLoss: 4.6479\tLR: 3.794629\n",
      "Training Epoch: 38 [47616/50000]\tLoss: 4.6970\tLR: 3.794885\n",
      "Training Epoch: 38 [47744/50000]\tLoss: 4.6709\tLR: 3.795141\n",
      "Training Epoch: 38 [47872/50000]\tLoss: 4.6893\tLR: 3.795396\n",
      "Training Epoch: 38 [48000/50000]\tLoss: 4.6601\tLR: 3.795652\n",
      "Training Epoch: 38 [48128/50000]\tLoss: 4.6681\tLR: 3.795908\n",
      "Training Epoch: 38 [48256/50000]\tLoss: 4.6644\tLR: 3.796164\n",
      "Training Epoch: 38 [48384/50000]\tLoss: 4.6806\tLR: 3.796419\n",
      "Training Epoch: 38 [48512/50000]\tLoss: 4.6850\tLR: 3.796675\n",
      "Training Epoch: 38 [48640/50000]\tLoss: 4.6825\tLR: 3.796931\n",
      "Training Epoch: 38 [48768/50000]\tLoss: 4.7331\tLR: 3.797187\n",
      "Training Epoch: 38 [48896/50000]\tLoss: 4.6571\tLR: 3.797442\n",
      "Training Epoch: 38 [49024/50000]\tLoss: 4.6569\tLR: 3.797698\n",
      "Training Epoch: 38 [49152/50000]\tLoss: 4.6643\tLR: 3.797954\n",
      "Training Epoch: 38 [49280/50000]\tLoss: 4.6448\tLR: 3.798210\n",
      "Training Epoch: 38 [49408/50000]\tLoss: 4.6249\tLR: 3.798465\n",
      "Training Epoch: 38 [49536/50000]\tLoss: 4.6830\tLR: 3.798721\n",
      "Training Epoch: 38 [49664/50000]\tLoss: 4.7300\tLR: 3.798977\n",
      "Training Epoch: 38 [49792/50000]\tLoss: 4.6482\tLR: 3.799233\n",
      "Training Epoch: 38 [49920/50000]\tLoss: 4.7388\tLR: 3.799488\n",
      "Training Epoch: 38 [50000/50000]\tLoss: 4.6967\tLR: 3.799744\n",
      "epoch 38 training time consumed: 489.19s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   53274 GB |   53274 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   53110 GB |   53110 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     163 GB |     163 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   53274 GB |   53274 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   53110 GB |   53110 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     163 GB |     163 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   52523 GB |   52523 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   52360 GB |   52360 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     163 GB |     163 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    5649 K  |    5648 K  |\n",
      "|       from large pool |      24    |      65    |    2408 K  |    2408 K  |\n",
      "|       from small pool |     231    |     274    |    3241 K  |    3240 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    5649 K  |    5648 K  |\n",
      "|       from large pool |      24    |      65    |    2408 K  |    2408 K  |\n",
      "|       from small pool |     231    |     274    |    3241 K  |    3240 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      46    |    3274 K  |    3274 K  |\n",
      "|       from large pool |      10    |      23    |    1157 K  |    1157 K  |\n",
      "|       from small pool |      25    |      35    |    2116 K  |    2116 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 38, Average loss: 0.0370, Accuracy: 0.0100, Time consumed:31.07s\n",
      "\n",
      "Training Epoch: 39 [128/50000]\tLoss: 4.6918\tLR: 0.100000\n",
      "Training Epoch: 39 [256/50000]\tLoss: 4.6615\tLR: 3.800256\n",
      "Training Epoch: 39 [384/50000]\tLoss: 4.6569\tLR: 3.800512\n",
      "Training Epoch: 39 [512/50000]\tLoss: 4.6688\tLR: 3.800767\n",
      "Training Epoch: 39 [640/50000]\tLoss: 4.6270\tLR: 3.801023\n",
      "Training Epoch: 39 [768/50000]\tLoss: 4.6252\tLR: 3.801279\n",
      "Training Epoch: 39 [896/50000]\tLoss: 4.6747\tLR: 3.801535\n",
      "Training Epoch: 39 [1024/50000]\tLoss: 4.6807\tLR: 3.801790\n",
      "Training Epoch: 39 [1152/50000]\tLoss: 4.6665\tLR: 3.802046\n",
      "Training Epoch: 39 [1280/50000]\tLoss: 4.6224\tLR: 3.802302\n",
      "Training Epoch: 39 [1408/50000]\tLoss: 4.6801\tLR: 3.802558\n",
      "Training Epoch: 39 [1536/50000]\tLoss: 4.6740\tLR: 3.802813\n",
      "Training Epoch: 39 [1664/50000]\tLoss: 4.6363\tLR: 3.803069\n",
      "Training Epoch: 39 [1792/50000]\tLoss: 4.6635\tLR: 3.803325\n",
      "Training Epoch: 39 [1920/50000]\tLoss: 4.6766\tLR: 3.803581\n",
      "Training Epoch: 39 [2048/50000]\tLoss: 4.6288\tLR: 3.803836\n",
      "Training Epoch: 39 [2176/50000]\tLoss: 4.6621\tLR: 3.804092\n",
      "Training Epoch: 39 [2304/50000]\tLoss: 4.6235\tLR: 3.804348\n",
      "Training Epoch: 39 [2432/50000]\tLoss: 4.6417\tLR: 3.804604\n",
      "Training Epoch: 39 [2560/50000]\tLoss: 4.6904\tLR: 3.804859\n",
      "Training Epoch: 39 [2688/50000]\tLoss: 4.6854\tLR: 3.805115\n",
      "Training Epoch: 39 [2816/50000]\tLoss: 4.7199\tLR: 3.805371\n",
      "Training Epoch: 39 [2944/50000]\tLoss: 4.6843\tLR: 3.805627\n",
      "Training Epoch: 39 [3072/50000]\tLoss: 4.6598\tLR: 3.805882\n",
      "Training Epoch: 39 [3200/50000]\tLoss: 4.6169\tLR: 3.806138\n",
      "Training Epoch: 39 [3328/50000]\tLoss: 4.6683\tLR: 3.806394\n",
      "Training Epoch: 39 [3456/50000]\tLoss: 4.6513\tLR: 3.806650\n",
      "Training Epoch: 39 [3584/50000]\tLoss: 4.6221\tLR: 3.806905\n",
      "Training Epoch: 39 [3712/50000]\tLoss: 4.6745\tLR: 3.807161\n",
      "Training Epoch: 39 [3840/50000]\tLoss: 4.6697\tLR: 3.807417\n",
      "Training Epoch: 39 [3968/50000]\tLoss: 4.6884\tLR: 3.807673\n",
      "Training Epoch: 39 [4096/50000]\tLoss: 4.5903\tLR: 3.807928\n",
      "Training Epoch: 39 [4224/50000]\tLoss: 4.6600\tLR: 3.808184\n",
      "Training Epoch: 39 [4352/50000]\tLoss: 4.7217\tLR: 3.808440\n",
      "Training Epoch: 39 [4480/50000]\tLoss: 4.6936\tLR: 3.808696\n",
      "Training Epoch: 39 [4608/50000]\tLoss: 4.6465\tLR: 3.808951\n",
      "Training Epoch: 39 [4736/50000]\tLoss: 4.6364\tLR: 3.809207\n",
      "Training Epoch: 39 [4864/50000]\tLoss: 4.6669\tLR: 3.809463\n",
      "Training Epoch: 39 [4992/50000]\tLoss: 4.7085\tLR: 3.809719\n",
      "Training Epoch: 39 [5120/50000]\tLoss: 4.6498\tLR: 3.809974\n",
      "Training Epoch: 39 [5248/50000]\tLoss: 4.7087\tLR: 3.810230\n",
      "Training Epoch: 39 [5376/50000]\tLoss: 4.6901\tLR: 3.810486\n",
      "Training Epoch: 39 [5504/50000]\tLoss: 4.6494\tLR: 3.810742\n",
      "Training Epoch: 39 [5632/50000]\tLoss: 4.6568\tLR: 3.810997\n",
      "Training Epoch: 39 [5760/50000]\tLoss: 4.7205\tLR: 3.811253\n",
      "Training Epoch: 39 [5888/50000]\tLoss: 4.6491\tLR: 3.811509\n",
      "Training Epoch: 39 [6016/50000]\tLoss: 4.6228\tLR: 3.811765\n",
      "Training Epoch: 39 [6144/50000]\tLoss: 4.7098\tLR: 3.812020\n",
      "Training Epoch: 39 [6272/50000]\tLoss: 4.6498\tLR: 3.812276\n",
      "Training Epoch: 39 [6400/50000]\tLoss: 4.6775\tLR: 3.812532\n",
      "Training Epoch: 39 [6528/50000]\tLoss: 4.6505\tLR: 3.812788\n",
      "Training Epoch: 39 [6656/50000]\tLoss: 4.6681\tLR: 3.813043\n",
      "Training Epoch: 39 [6784/50000]\tLoss: 4.6818\tLR: 3.813299\n",
      "Training Epoch: 39 [6912/50000]\tLoss: 4.7224\tLR: 3.813555\n",
      "Training Epoch: 39 [7040/50000]\tLoss: 4.6607\tLR: 3.813811\n",
      "Training Epoch: 39 [7168/50000]\tLoss: 4.6897\tLR: 3.814066\n",
      "Training Epoch: 39 [7296/50000]\tLoss: 4.7020\tLR: 3.814322\n",
      "Training Epoch: 39 [7424/50000]\tLoss: 4.6609\tLR: 3.814578\n",
      "Training Epoch: 39 [7552/50000]\tLoss: 4.6327\tLR: 3.814834\n",
      "Training Epoch: 39 [7680/50000]\tLoss: 4.6614\tLR: 3.815090\n",
      "Training Epoch: 39 [7808/50000]\tLoss: 4.6137\tLR: 3.815345\n",
      "Training Epoch: 39 [7936/50000]\tLoss: 4.6415\tLR: 3.815601\n",
      "Training Epoch: 39 [8064/50000]\tLoss: 4.6623\tLR: 3.815857\n",
      "Training Epoch: 39 [8192/50000]\tLoss: 4.7375\tLR: 3.816113\n",
      "Training Epoch: 39 [8320/50000]\tLoss: 4.6930\tLR: 3.816368\n",
      "Training Epoch: 39 [8448/50000]\tLoss: 4.6916\tLR: 3.816624\n",
      "Training Epoch: 39 [8576/50000]\tLoss: 4.6482\tLR: 3.816880\n",
      "Training Epoch: 39 [8704/50000]\tLoss: 4.6841\tLR: 3.817136\n",
      "Training Epoch: 39 [8832/50000]\tLoss: 4.7146\tLR: 3.817391\n",
      "Training Epoch: 39 [8960/50000]\tLoss: 4.6431\tLR: 3.817647\n",
      "Training Epoch: 39 [9088/50000]\tLoss: 4.6436\tLR: 3.817903\n",
      "Training Epoch: 39 [9216/50000]\tLoss: 4.6385\tLR: 3.818159\n",
      "Training Epoch: 39 [9344/50000]\tLoss: 4.6658\tLR: 3.818414\n",
      "Training Epoch: 39 [9472/50000]\tLoss: 4.6480\tLR: 3.818670\n",
      "Training Epoch: 39 [9600/50000]\tLoss: 4.6642\tLR: 3.818926\n",
      "Training Epoch: 39 [9728/50000]\tLoss: 4.6165\tLR: 3.819182\n",
      "Training Epoch: 39 [9856/50000]\tLoss: 4.6873\tLR: 3.819437\n",
      "Training Epoch: 39 [9984/50000]\tLoss: 4.6524\tLR: 3.819693\n",
      "Training Epoch: 39 [10112/50000]\tLoss: 4.7009\tLR: 3.819949\n",
      "Training Epoch: 39 [10240/50000]\tLoss: 4.7327\tLR: 3.820205\n",
      "Training Epoch: 39 [10368/50000]\tLoss: 4.6956\tLR: 3.820460\n",
      "Training Epoch: 39 [10496/50000]\tLoss: 4.6714\tLR: 3.820716\n",
      "Training Epoch: 39 [10624/50000]\tLoss: 4.6691\tLR: 3.820972\n",
      "Training Epoch: 39 [10752/50000]\tLoss: 4.6762\tLR: 3.821228\n",
      "Training Epoch: 39 [10880/50000]\tLoss: 4.6927\tLR: 3.821483\n",
      "Training Epoch: 39 [11008/50000]\tLoss: 4.7143\tLR: 3.821739\n",
      "Training Epoch: 39 [11136/50000]\tLoss: 4.6738\tLR: 3.821995\n",
      "Training Epoch: 39 [11264/50000]\tLoss: 4.6635\tLR: 3.822251\n",
      "Training Epoch: 39 [11392/50000]\tLoss: 4.6306\tLR: 3.822506\n",
      "Training Epoch: 39 [11520/50000]\tLoss: 4.6731\tLR: 3.822762\n",
      "Training Epoch: 39 [11648/50000]\tLoss: 4.6965\tLR: 3.823018\n",
      "Training Epoch: 39 [11776/50000]\tLoss: 4.7071\tLR: 3.823274\n",
      "Training Epoch: 39 [11904/50000]\tLoss: 4.6555\tLR: 3.823529\n",
      "Training Epoch: 39 [12032/50000]\tLoss: 4.7409\tLR: 3.823785\n",
      "Training Epoch: 39 [12160/50000]\tLoss: 4.6855\tLR: 3.824041\n",
      "Training Epoch: 39 [12288/50000]\tLoss: 4.7479\tLR: 3.824297\n",
      "Training Epoch: 39 [12416/50000]\tLoss: 4.6655\tLR: 3.824552\n",
      "Training Epoch: 39 [12544/50000]\tLoss: 4.6867\tLR: 3.824808\n",
      "Training Epoch: 39 [12672/50000]\tLoss: 4.6517\tLR: 3.825064\n",
      "Training Epoch: 39 [12800/50000]\tLoss: 4.6836\tLR: 3.825320\n",
      "Training Epoch: 39 [12928/50000]\tLoss: 4.6690\tLR: 3.825575\n",
      "Training Epoch: 39 [13056/50000]\tLoss: 4.6719\tLR: 3.825831\n",
      "Training Epoch: 39 [13184/50000]\tLoss: 4.7031\tLR: 3.826087\n",
      "Training Epoch: 39 [13312/50000]\tLoss: 4.6828\tLR: 3.826343\n",
      "Training Epoch: 39 [13440/50000]\tLoss: 4.7130\tLR: 3.826598\n",
      "Training Epoch: 39 [13568/50000]\tLoss: 4.6679\tLR: 3.826854\n",
      "Training Epoch: 39 [13696/50000]\tLoss: 4.6909\tLR: 3.827110\n",
      "Training Epoch: 39 [13824/50000]\tLoss: 4.7255\tLR: 3.827366\n",
      "Training Epoch: 39 [13952/50000]\tLoss: 4.6687\tLR: 3.827621\n",
      "Training Epoch: 39 [14080/50000]\tLoss: 4.6253\tLR: 3.827877\n",
      "Training Epoch: 39 [14208/50000]\tLoss: 4.6469\tLR: 3.828133\n",
      "Training Epoch: 39 [14336/50000]\tLoss: 4.6785\tLR: 3.828389\n",
      "Training Epoch: 39 [14464/50000]\tLoss: 4.6645\tLR: 3.828645\n",
      "Training Epoch: 39 [14592/50000]\tLoss: 4.7236\tLR: 3.828900\n",
      "Training Epoch: 39 [14720/50000]\tLoss: 4.7168\tLR: 3.829156\n",
      "Training Epoch: 39 [14848/50000]\tLoss: 4.6613\tLR: 3.829412\n",
      "Training Epoch: 39 [14976/50000]\tLoss: 4.6799\tLR: 3.829668\n",
      "Training Epoch: 39 [15104/50000]\tLoss: 4.6502\tLR: 3.829923\n",
      "Training Epoch: 39 [15232/50000]\tLoss: 4.6754\tLR: 3.830179\n",
      "Training Epoch: 39 [15360/50000]\tLoss: 4.6803\tLR: 3.830435\n",
      "Training Epoch: 39 [15488/50000]\tLoss: 4.6855\tLR: 3.830691\n",
      "Training Epoch: 39 [15616/50000]\tLoss: 4.6939\tLR: 3.830946\n",
      "Training Epoch: 39 [15744/50000]\tLoss: 4.6624\tLR: 3.831202\n",
      "Training Epoch: 39 [15872/50000]\tLoss: 4.7309\tLR: 3.831458\n",
      "Training Epoch: 39 [16000/50000]\tLoss: 4.6813\tLR: 3.831714\n",
      "Training Epoch: 39 [16128/50000]\tLoss: 4.7316\tLR: 3.831969\n",
      "Training Epoch: 39 [16256/50000]\tLoss: 4.6848\tLR: 3.832225\n",
      "Training Epoch: 39 [16384/50000]\tLoss: 4.7013\tLR: 3.832481\n",
      "Training Epoch: 39 [16512/50000]\tLoss: 4.6201\tLR: 3.832737\n",
      "Training Epoch: 39 [16640/50000]\tLoss: 4.6919\tLR: 3.832992\n",
      "Training Epoch: 39 [16768/50000]\tLoss: 4.6725\tLR: 3.833248\n",
      "Training Epoch: 39 [16896/50000]\tLoss: 4.6774\tLR: 3.833504\n",
      "Training Epoch: 39 [17024/50000]\tLoss: 4.6761\tLR: 3.833760\n",
      "Training Epoch: 39 [17152/50000]\tLoss: 4.6870\tLR: 3.834015\n",
      "Training Epoch: 39 [17280/50000]\tLoss: 4.7001\tLR: 3.834271\n",
      "Training Epoch: 39 [17408/50000]\tLoss: 4.7200\tLR: 3.834527\n",
      "Training Epoch: 39 [17536/50000]\tLoss: 4.6316\tLR: 3.834783\n",
      "Training Epoch: 39 [17664/50000]\tLoss: 4.6658\tLR: 3.835038\n",
      "Training Epoch: 39 [17792/50000]\tLoss: 4.6924\tLR: 3.835294\n",
      "Training Epoch: 39 [17920/50000]\tLoss: 4.7158\tLR: 3.835550\n",
      "Training Epoch: 39 [18048/50000]\tLoss: 4.6287\tLR: 3.835806\n",
      "Training Epoch: 39 [18176/50000]\tLoss: 4.7286\tLR: 3.836061\n",
      "Training Epoch: 39 [18304/50000]\tLoss: 4.7418\tLR: 3.836317\n",
      "Training Epoch: 39 [18432/50000]\tLoss: 4.6262\tLR: 3.836573\n",
      "Training Epoch: 39 [18560/50000]\tLoss: 4.6647\tLR: 3.836829\n",
      "Training Epoch: 39 [18688/50000]\tLoss: 4.7038\tLR: 3.837084\n",
      "Training Epoch: 39 [18816/50000]\tLoss: 4.6979\tLR: 3.837340\n",
      "Training Epoch: 39 [18944/50000]\tLoss: 4.6085\tLR: 3.837596\n",
      "Training Epoch: 39 [19072/50000]\tLoss: 4.6689\tLR: 3.837852\n",
      "Training Epoch: 39 [19200/50000]\tLoss: 4.6530\tLR: 3.838107\n",
      "Training Epoch: 39 [19328/50000]\tLoss: 4.6536\tLR: 3.838363\n",
      "Training Epoch: 39 [19456/50000]\tLoss: 4.7977\tLR: 3.838619\n",
      "Training Epoch: 39 [19584/50000]\tLoss: 4.6091\tLR: 3.838875\n",
      "Training Epoch: 39 [19712/50000]\tLoss: 4.7260\tLR: 3.839130\n",
      "Training Epoch: 39 [19840/50000]\tLoss: 4.6306\tLR: 3.839386\n",
      "Training Epoch: 39 [19968/50000]\tLoss: 4.7115\tLR: 3.839642\n",
      "Training Epoch: 39 [20096/50000]\tLoss: 4.7267\tLR: 3.839898\n",
      "Training Epoch: 39 [20224/50000]\tLoss: 4.6902\tLR: 3.840153\n",
      "Training Epoch: 39 [20352/50000]\tLoss: 4.6348\tLR: 3.840409\n",
      "Training Epoch: 39 [20480/50000]\tLoss: 4.6751\tLR: 3.840665\n",
      "Training Epoch: 39 [20608/50000]\tLoss: 4.7032\tLR: 3.840921\n",
      "Training Epoch: 39 [20736/50000]\tLoss: 4.6989\tLR: 3.841176\n",
      "Training Epoch: 39 [20864/50000]\tLoss: 4.7165\tLR: 3.841432\n",
      "Training Epoch: 39 [20992/50000]\tLoss: 4.6339\tLR: 3.841688\n",
      "Training Epoch: 39 [21120/50000]\tLoss: 4.6338\tLR: 3.841944\n",
      "Training Epoch: 39 [21248/50000]\tLoss: 4.6786\tLR: 3.842199\n",
      "Training Epoch: 39 [21376/50000]\tLoss: 4.6093\tLR: 3.842455\n",
      "Training Epoch: 39 [21504/50000]\tLoss: 4.6592\tLR: 3.842711\n",
      "Training Epoch: 39 [21632/50000]\tLoss: 4.7014\tLR: 3.842967\n",
      "Training Epoch: 39 [21760/50000]\tLoss: 4.6728\tLR: 3.843223\n",
      "Training Epoch: 39 [21888/50000]\tLoss: 4.7106\tLR: 3.843478\n",
      "Training Epoch: 39 [22016/50000]\tLoss: 4.6871\tLR: 3.843734\n",
      "Training Epoch: 39 [22144/50000]\tLoss: 4.6793\tLR: 3.843990\n",
      "Training Epoch: 39 [22272/50000]\tLoss: 4.6735\tLR: 3.844246\n",
      "Training Epoch: 39 [22400/50000]\tLoss: 4.6395\tLR: 3.844501\n",
      "Training Epoch: 39 [22528/50000]\tLoss: 4.6790\tLR: 3.844757\n",
      "Training Epoch: 39 [22656/50000]\tLoss: 4.6144\tLR: 3.845013\n",
      "Training Epoch: 39 [22784/50000]\tLoss: 4.6765\tLR: 3.845269\n",
      "Training Epoch: 39 [22912/50000]\tLoss: 4.6710\tLR: 3.845524\n",
      "Training Epoch: 39 [23040/50000]\tLoss: 4.6147\tLR: 3.845780\n",
      "Training Epoch: 39 [23168/50000]\tLoss: 4.6597\tLR: 3.846036\n",
      "Training Epoch: 39 [23296/50000]\tLoss: 4.6629\tLR: 3.846292\n",
      "Training Epoch: 39 [23424/50000]\tLoss: 4.6815\tLR: 3.846547\n",
      "Training Epoch: 39 [23552/50000]\tLoss: 4.6116\tLR: 3.846803\n",
      "Training Epoch: 39 [23680/50000]\tLoss: 4.6408\tLR: 3.847059\n",
      "Training Epoch: 39 [23808/50000]\tLoss: 4.7171\tLR: 3.847315\n",
      "Training Epoch: 39 [23936/50000]\tLoss: 4.6947\tLR: 3.847570\n",
      "Training Epoch: 39 [24064/50000]\tLoss: 4.6445\tLR: 3.847826\n",
      "Training Epoch: 39 [24192/50000]\tLoss: 4.6634\tLR: 3.848082\n",
      "Training Epoch: 39 [24320/50000]\tLoss: 4.6364\tLR: 3.848338\n",
      "Training Epoch: 39 [24448/50000]\tLoss: 4.6560\tLR: 3.848593\n",
      "Training Epoch: 39 [24576/50000]\tLoss: 4.6129\tLR: 3.848849\n",
      "Training Epoch: 39 [24704/50000]\tLoss: 4.7423\tLR: 3.849105\n",
      "Training Epoch: 39 [24832/50000]\tLoss: 4.6381\tLR: 3.849361\n",
      "Training Epoch: 39 [24960/50000]\tLoss: 4.6682\tLR: 3.849616\n",
      "Training Epoch: 39 [25088/50000]\tLoss: 4.6730\tLR: 3.849872\n",
      "Training Epoch: 39 [25216/50000]\tLoss: 4.6671\tLR: 3.850128\n",
      "Training Epoch: 39 [25344/50000]\tLoss: 4.7046\tLR: 3.850384\n",
      "Training Epoch: 39 [25472/50000]\tLoss: 4.6421\tLR: 3.850639\n",
      "Training Epoch: 39 [25600/50000]\tLoss: 4.5879\tLR: 3.850895\n",
      "Training Epoch: 39 [25728/50000]\tLoss: 4.6738\tLR: 3.851151\n",
      "Training Epoch: 39 [25856/50000]\tLoss: 4.7013\tLR: 3.851407\n",
      "Training Epoch: 39 [25984/50000]\tLoss: 4.6717\tLR: 3.851662\n",
      "Training Epoch: 39 [26112/50000]\tLoss: 4.7288\tLR: 3.851918\n",
      "Training Epoch: 39 [26240/50000]\tLoss: 4.6595\tLR: 3.852174\n",
      "Training Epoch: 39 [26368/50000]\tLoss: 4.6552\tLR: 3.852430\n",
      "Training Epoch: 39 [26496/50000]\tLoss: 4.7454\tLR: 3.852685\n",
      "Training Epoch: 39 [26624/50000]\tLoss: 4.6433\tLR: 3.852941\n",
      "Training Epoch: 39 [26752/50000]\tLoss: 4.6349\tLR: 3.853197\n",
      "Training Epoch: 39 [26880/50000]\tLoss: 4.6380\tLR: 3.853453\n",
      "Training Epoch: 39 [27008/50000]\tLoss: 4.6228\tLR: 3.853708\n",
      "Training Epoch: 39 [27136/50000]\tLoss: 4.6909\tLR: 3.853964\n",
      "Training Epoch: 39 [27264/50000]\tLoss: 4.7140\tLR: 3.854220\n",
      "Training Epoch: 39 [27392/50000]\tLoss: 4.6146\tLR: 3.854476\n",
      "Training Epoch: 39 [27520/50000]\tLoss: 4.7790\tLR: 3.854731\n",
      "Training Epoch: 39 [27648/50000]\tLoss: 4.6618\tLR: 3.854987\n",
      "Training Epoch: 39 [27776/50000]\tLoss: 4.7562\tLR: 3.855243\n",
      "Training Epoch: 39 [27904/50000]\tLoss: 4.6389\tLR: 3.855499\n",
      "Training Epoch: 39 [28032/50000]\tLoss: 4.7667\tLR: 3.855754\n",
      "Training Epoch: 39 [28160/50000]\tLoss: 4.6620\tLR: 3.856010\n",
      "Training Epoch: 39 [28288/50000]\tLoss: 4.7074\tLR: 3.856266\n",
      "Training Epoch: 39 [28416/50000]\tLoss: 4.6335\tLR: 3.856522\n",
      "Training Epoch: 39 [28544/50000]\tLoss: 4.6641\tLR: 3.856777\n",
      "Training Epoch: 39 [28672/50000]\tLoss: 4.7012\tLR: 3.857033\n",
      "Training Epoch: 39 [28800/50000]\tLoss: 4.6720\tLR: 3.857289\n",
      "Training Epoch: 39 [28928/50000]\tLoss: 4.7348\tLR: 3.857545\n",
      "Training Epoch: 39 [29056/50000]\tLoss: 4.6727\tLR: 3.857801\n",
      "Training Epoch: 39 [29184/50000]\tLoss: 4.6653\tLR: 3.858056\n",
      "Training Epoch: 39 [29312/50000]\tLoss: 4.6766\tLR: 3.858312\n",
      "Training Epoch: 39 [29440/50000]\tLoss: 4.7557\tLR: 3.858568\n",
      "Training Epoch: 39 [29568/50000]\tLoss: 4.7037\tLR: 3.858824\n",
      "Training Epoch: 39 [29696/50000]\tLoss: 4.6490\tLR: 3.859079\n",
      "Training Epoch: 39 [29824/50000]\tLoss: 4.7028\tLR: 3.859335\n",
      "Training Epoch: 39 [29952/50000]\tLoss: 4.7025\tLR: 3.859591\n",
      "Training Epoch: 39 [30080/50000]\tLoss: 4.6866\tLR: 3.859847\n",
      "Training Epoch: 39 [30208/50000]\tLoss: 4.6754\tLR: 3.860102\n",
      "Training Epoch: 39 [30336/50000]\tLoss: 4.6821\tLR: 3.860358\n",
      "Training Epoch: 39 [30464/50000]\tLoss: 4.6622\tLR: 3.860614\n",
      "Training Epoch: 39 [30592/50000]\tLoss: 4.6566\tLR: 3.860870\n",
      "Training Epoch: 39 [30720/50000]\tLoss: 4.7055\tLR: 3.861125\n",
      "Training Epoch: 39 [30848/50000]\tLoss: 4.6367\tLR: 3.861381\n",
      "Training Epoch: 39 [30976/50000]\tLoss: 4.7247\tLR: 3.861637\n",
      "Training Epoch: 39 [31104/50000]\tLoss: 4.6706\tLR: 3.861893\n",
      "Training Epoch: 39 [31232/50000]\tLoss: 4.6674\tLR: 3.862148\n",
      "Training Epoch: 39 [31360/50000]\tLoss: 4.6374\tLR: 3.862404\n",
      "Training Epoch: 39 [31488/50000]\tLoss: 4.7018\tLR: 3.862660\n",
      "Training Epoch: 39 [31616/50000]\tLoss: 4.6997\tLR: 3.862916\n",
      "Training Epoch: 39 [31744/50000]\tLoss: 4.7005\tLR: 3.863171\n",
      "Training Epoch: 39 [31872/50000]\tLoss: 4.7059\tLR: 3.863427\n",
      "Training Epoch: 39 [32000/50000]\tLoss: 4.6403\tLR: 3.863683\n",
      "Training Epoch: 39 [32128/50000]\tLoss: 4.6723\tLR: 3.863939\n",
      "Training Epoch: 39 [32256/50000]\tLoss: 4.6198\tLR: 3.864194\n",
      "Training Epoch: 39 [32384/50000]\tLoss: 4.6667\tLR: 3.864450\n",
      "Training Epoch: 39 [32512/50000]\tLoss: 4.7047\tLR: 3.864706\n",
      "Training Epoch: 39 [32640/50000]\tLoss: 4.6406\tLR: 3.864962\n",
      "Training Epoch: 39 [32768/50000]\tLoss: 4.6887\tLR: 3.865217\n",
      "Training Epoch: 39 [32896/50000]\tLoss: 4.6748\tLR: 3.865473\n",
      "Training Epoch: 39 [33024/50000]\tLoss: 4.6568\tLR: 3.865729\n",
      "Training Epoch: 39 [33152/50000]\tLoss: 4.6783\tLR: 3.865985\n",
      "Training Epoch: 39 [33280/50000]\tLoss: 4.6900\tLR: 3.866240\n",
      "Training Epoch: 39 [33408/50000]\tLoss: 4.6665\tLR: 3.866496\n",
      "Training Epoch: 39 [33536/50000]\tLoss: 4.6374\tLR: 3.866752\n",
      "Training Epoch: 39 [33664/50000]\tLoss: 4.6617\tLR: 3.867008\n",
      "Training Epoch: 39 [33792/50000]\tLoss: 4.6393\tLR: 3.867263\n",
      "Training Epoch: 39 [33920/50000]\tLoss: 4.7141\tLR: 3.867519\n",
      "Training Epoch: 39 [34048/50000]\tLoss: 4.6956\tLR: 3.867775\n",
      "Training Epoch: 39 [34176/50000]\tLoss: 4.6834\tLR: 3.868031\n",
      "Training Epoch: 39 [34304/50000]\tLoss: 4.7090\tLR: 3.868286\n",
      "Training Epoch: 39 [34432/50000]\tLoss: 4.6354\tLR: 3.868542\n",
      "Training Epoch: 39 [34560/50000]\tLoss: 4.6543\tLR: 3.868798\n",
      "Training Epoch: 39 [34688/50000]\tLoss: 4.6690\tLR: 3.869054\n",
      "Training Epoch: 39 [34816/50000]\tLoss: 4.6763\tLR: 3.869309\n",
      "Training Epoch: 39 [34944/50000]\tLoss: 4.6646\tLR: 3.869565\n",
      "Training Epoch: 39 [35072/50000]\tLoss: 4.7154\tLR: 3.869821\n",
      "Training Epoch: 39 [35200/50000]\tLoss: 4.6921\tLR: 3.870077\n",
      "Training Epoch: 39 [35328/50000]\tLoss: 4.6505\tLR: 3.870332\n",
      "Training Epoch: 39 [35456/50000]\tLoss: 4.7311\tLR: 3.870588\n",
      "Training Epoch: 39 [35584/50000]\tLoss: 4.7004\tLR: 3.870844\n",
      "Training Epoch: 39 [35712/50000]\tLoss: 4.6616\tLR: 3.871100\n",
      "Training Epoch: 39 [35840/50000]\tLoss: 4.7010\tLR: 3.871355\n",
      "Training Epoch: 39 [35968/50000]\tLoss: 4.6476\tLR: 3.871611\n",
      "Training Epoch: 39 [36096/50000]\tLoss: 4.6600\tLR: 3.871867\n",
      "Training Epoch: 39 [36224/50000]\tLoss: 4.6385\tLR: 3.872123\n",
      "Training Epoch: 39 [36352/50000]\tLoss: 4.6570\tLR: 3.872379\n",
      "Training Epoch: 39 [36480/50000]\tLoss: 4.6889\tLR: 3.872634\n",
      "Training Epoch: 39 [36608/50000]\tLoss: 4.6913\tLR: 3.872890\n",
      "Training Epoch: 39 [36736/50000]\tLoss: 4.6843\tLR: 3.873146\n",
      "Training Epoch: 39 [36864/50000]\tLoss: 4.6096\tLR: 3.873402\n",
      "Training Epoch: 39 [36992/50000]\tLoss: 4.6642\tLR: 3.873657\n",
      "Training Epoch: 39 [37120/50000]\tLoss: 4.7100\tLR: 3.873913\n",
      "Training Epoch: 39 [37248/50000]\tLoss: 4.6018\tLR: 3.874169\n",
      "Training Epoch: 39 [37376/50000]\tLoss: 4.6915\tLR: 3.874425\n",
      "Training Epoch: 39 [37504/50000]\tLoss: 4.6733\tLR: 3.874680\n",
      "Training Epoch: 39 [37632/50000]\tLoss: 4.7135\tLR: 3.874936\n",
      "Training Epoch: 39 [37760/50000]\tLoss: 4.7214\tLR: 3.875192\n",
      "Training Epoch: 39 [37888/50000]\tLoss: 4.6379\tLR: 3.875448\n",
      "Training Epoch: 39 [38016/50000]\tLoss: 4.6861\tLR: 3.875703\n",
      "Training Epoch: 39 [38144/50000]\tLoss: 4.6282\tLR: 3.875959\n",
      "Training Epoch: 39 [38272/50000]\tLoss: 4.6539\tLR: 3.876215\n",
      "Training Epoch: 39 [38400/50000]\tLoss: 4.6668\tLR: 3.876471\n",
      "Training Epoch: 39 [38528/50000]\tLoss: 4.6879\tLR: 3.876726\n",
      "Training Epoch: 39 [38656/50000]\tLoss: 4.7115\tLR: 3.876982\n",
      "Training Epoch: 39 [38784/50000]\tLoss: 4.7397\tLR: 3.877238\n",
      "Training Epoch: 39 [38912/50000]\tLoss: 4.7293\tLR: 3.877494\n",
      "Training Epoch: 39 [39040/50000]\tLoss: 4.6932\tLR: 3.877749\n",
      "Training Epoch: 39 [39168/50000]\tLoss: 4.6793\tLR: 3.878005\n",
      "Training Epoch: 39 [39296/50000]\tLoss: 4.6922\tLR: 3.878261\n",
      "Training Epoch: 39 [39424/50000]\tLoss: 4.7036\tLR: 3.878517\n",
      "Training Epoch: 39 [39552/50000]\tLoss: 4.6451\tLR: 3.878772\n",
      "Training Epoch: 39 [39680/50000]\tLoss: 4.6438\tLR: 3.879028\n",
      "Training Epoch: 39 [39808/50000]\tLoss: 4.6204\tLR: 3.879284\n",
      "Training Epoch: 39 [39936/50000]\tLoss: 4.6225\tLR: 3.879540\n",
      "Training Epoch: 39 [40064/50000]\tLoss: 4.7488\tLR: 3.879795\n",
      "Training Epoch: 39 [40192/50000]\tLoss: 4.6669\tLR: 3.880051\n",
      "Training Epoch: 39 [40320/50000]\tLoss: 4.7055\tLR: 3.880307\n",
      "Training Epoch: 39 [40448/50000]\tLoss: 4.6983\tLR: 3.880563\n",
      "Training Epoch: 39 [40576/50000]\tLoss: 4.7149\tLR: 3.880818\n",
      "Training Epoch: 39 [40704/50000]\tLoss: 4.6614\tLR: 3.881074\n",
      "Training Epoch: 39 [40832/50000]\tLoss: 4.7185\tLR: 3.881330\n",
      "Training Epoch: 39 [40960/50000]\tLoss: 4.6935\tLR: 3.881586\n",
      "Training Epoch: 39 [41088/50000]\tLoss: 4.6639\tLR: 3.881841\n",
      "Training Epoch: 39 [41216/50000]\tLoss: 4.6999\tLR: 3.882097\n",
      "Training Epoch: 39 [41344/50000]\tLoss: 4.6598\tLR: 3.882353\n",
      "Training Epoch: 39 [41472/50000]\tLoss: 4.7213\tLR: 3.882609\n",
      "Training Epoch: 39 [41600/50000]\tLoss: 4.7183\tLR: 3.882864\n",
      "Training Epoch: 39 [41728/50000]\tLoss: 4.6474\tLR: 3.883120\n",
      "Training Epoch: 39 [41856/50000]\tLoss: 4.6535\tLR: 3.883376\n",
      "Training Epoch: 39 [41984/50000]\tLoss: 4.7427\tLR: 3.883632\n",
      "Training Epoch: 39 [42112/50000]\tLoss: 4.7581\tLR: 3.883887\n",
      "Training Epoch: 39 [42240/50000]\tLoss: 4.6377\tLR: 3.884143\n",
      "Training Epoch: 39 [42368/50000]\tLoss: 4.7682\tLR: 3.884399\n",
      "Training Epoch: 39 [42496/50000]\tLoss: 4.6372\tLR: 3.884655\n",
      "Training Epoch: 39 [42624/50000]\tLoss: 4.6497\tLR: 3.884910\n",
      "Training Epoch: 39 [42752/50000]\tLoss: 4.6548\tLR: 3.885166\n",
      "Training Epoch: 39 [42880/50000]\tLoss: 4.6855\tLR: 3.885422\n",
      "Training Epoch: 39 [43008/50000]\tLoss: 4.6940\tLR: 3.885678\n",
      "Training Epoch: 39 [43136/50000]\tLoss: 4.6146\tLR: 3.885934\n",
      "Training Epoch: 39 [43264/50000]\tLoss: 4.6919\tLR: 3.886189\n",
      "Training Epoch: 39 [43392/50000]\tLoss: 4.7738\tLR: 3.886445\n",
      "Training Epoch: 39 [43520/50000]\tLoss: 4.6726\tLR: 3.886701\n",
      "Training Epoch: 39 [43648/50000]\tLoss: 4.7212\tLR: 3.886957\n",
      "Training Epoch: 39 [43776/50000]\tLoss: 4.6418\tLR: 3.887212\n",
      "Training Epoch: 39 [43904/50000]\tLoss: 4.7486\tLR: 3.887468\n",
      "Training Epoch: 39 [44032/50000]\tLoss: 4.7161\tLR: 3.887724\n",
      "Training Epoch: 39 [44160/50000]\tLoss: 4.6235\tLR: 3.887980\n",
      "Training Epoch: 39 [44288/50000]\tLoss: 4.7107\tLR: 3.888235\n",
      "Training Epoch: 39 [44416/50000]\tLoss: 4.6342\tLR: 3.888491\n",
      "Training Epoch: 39 [44544/50000]\tLoss: 4.6963\tLR: 3.888747\n",
      "Training Epoch: 39 [44672/50000]\tLoss: 4.6833\tLR: 3.889003\n",
      "Training Epoch: 39 [44800/50000]\tLoss: 4.6751\tLR: 3.889258\n",
      "Training Epoch: 39 [44928/50000]\tLoss: 4.6597\tLR: 3.889514\n",
      "Training Epoch: 39 [45056/50000]\tLoss: 4.7208\tLR: 3.889770\n",
      "Training Epoch: 39 [45184/50000]\tLoss: 4.6732\tLR: 3.890026\n",
      "Training Epoch: 39 [45312/50000]\tLoss: 4.6525\tLR: 3.890281\n",
      "Training Epoch: 39 [45440/50000]\tLoss: 4.6501\tLR: 3.890537\n",
      "Training Epoch: 39 [45568/50000]\tLoss: 4.6500\tLR: 3.890793\n",
      "Training Epoch: 39 [45696/50000]\tLoss: 4.6237\tLR: 3.891049\n",
      "Training Epoch: 39 [45824/50000]\tLoss: 4.6035\tLR: 3.891304\n",
      "Training Epoch: 39 [45952/50000]\tLoss: 4.7286\tLR: 3.891560\n",
      "Training Epoch: 39 [46080/50000]\tLoss: 4.7161\tLR: 3.891816\n",
      "Training Epoch: 39 [46208/50000]\tLoss: 4.6569\tLR: 3.892072\n",
      "Training Epoch: 39 [46336/50000]\tLoss: 4.6738\tLR: 3.892327\n",
      "Training Epoch: 39 [46464/50000]\tLoss: 4.6713\tLR: 3.892583\n",
      "Training Epoch: 39 [46592/50000]\tLoss: 4.7012\tLR: 3.892839\n",
      "Training Epoch: 39 [46720/50000]\tLoss: 4.7275\tLR: 3.893095\n",
      "Training Epoch: 39 [46848/50000]\tLoss: 4.6822\tLR: 3.893350\n",
      "Training Epoch: 39 [46976/50000]\tLoss: 4.6518\tLR: 3.893606\n",
      "Training Epoch: 39 [47104/50000]\tLoss: 4.6730\tLR: 3.893862\n",
      "Training Epoch: 39 [47232/50000]\tLoss: 4.6558\tLR: 3.894118\n",
      "Training Epoch: 39 [47360/50000]\tLoss: 4.6425\tLR: 3.894373\n",
      "Training Epoch: 39 [47488/50000]\tLoss: 4.7361\tLR: 3.894629\n",
      "Training Epoch: 39 [47616/50000]\tLoss: 4.7269\tLR: 3.894885\n",
      "Training Epoch: 39 [47744/50000]\tLoss: 4.7356\tLR: 3.895141\n",
      "Training Epoch: 39 [47872/50000]\tLoss: 4.6987\tLR: 3.895396\n",
      "Training Epoch: 39 [48000/50000]\tLoss: 4.6079\tLR: 3.895652\n",
      "Training Epoch: 39 [48128/50000]\tLoss: 4.6649\tLR: 3.895908\n",
      "Training Epoch: 39 [48256/50000]\tLoss: 4.6860\tLR: 3.896164\n",
      "Training Epoch: 39 [48384/50000]\tLoss: 4.6968\tLR: 3.896419\n",
      "Training Epoch: 39 [48512/50000]\tLoss: 4.6339\tLR: 3.896675\n",
      "Training Epoch: 39 [48640/50000]\tLoss: 4.6457\tLR: 3.896931\n",
      "Training Epoch: 39 [48768/50000]\tLoss: 4.7098\tLR: 3.897187\n",
      "Training Epoch: 39 [48896/50000]\tLoss: 4.6489\tLR: 3.897442\n",
      "Training Epoch: 39 [49024/50000]\tLoss: 4.6703\tLR: 3.897698\n",
      "Training Epoch: 39 [49152/50000]\tLoss: 4.6817\tLR: 3.897954\n",
      "Training Epoch: 39 [49280/50000]\tLoss: 4.6599\tLR: 3.898210\n",
      "Training Epoch: 39 [49408/50000]\tLoss: 4.6697\tLR: 3.898465\n",
      "Training Epoch: 39 [49536/50000]\tLoss: 4.6558\tLR: 3.898721\n",
      "Training Epoch: 39 [49664/50000]\tLoss: 4.6463\tLR: 3.898977\n",
      "Training Epoch: 39 [49792/50000]\tLoss: 4.7187\tLR: 3.899233\n",
      "Training Epoch: 39 [49920/50000]\tLoss: 4.6899\tLR: 3.899488\n",
      "Training Epoch: 39 [50000/50000]\tLoss: 4.7173\tLR: 3.899744\n",
      "epoch 39 training time consumed: 489.01s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   54676 GB |   54676 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   54508 GB |   54508 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     167 GB |     167 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   54676 GB |   54676 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   54508 GB |   54508 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     167 GB |     167 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   53905 GB |   53905 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   53738 GB |   53738 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     167 GB |     167 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    5797 K  |    5797 K  |\n",
      "|       from large pool |      24    |      65    |    2471 K  |    2471 K  |\n",
      "|       from small pool |     231    |     274    |    3326 K  |    3326 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    5797 K  |    5797 K  |\n",
      "|       from large pool |      24    |      65    |    2471 K  |    2471 K  |\n",
      "|       from small pool |     231    |     274    |    3326 K  |    3326 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      46    |    3360 K  |    3360 K  |\n",
      "|       from large pool |      10    |      23    |    1187 K  |    1187 K  |\n",
      "|       from small pool |      25    |      35    |    2172 K  |    2172 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 39, Average loss: 0.0369, Accuracy: 0.0100, Time consumed:31.14s\n",
      "\n",
      "Training Epoch: 40 [128/50000]\tLoss: 4.7631\tLR: 0.100000\n",
      "Training Epoch: 40 [256/50000]\tLoss: 4.6831\tLR: 3.900256\n",
      "Training Epoch: 40 [384/50000]\tLoss: 4.6265\tLR: 3.900512\n",
      "Training Epoch: 40 [512/50000]\tLoss: 4.7052\tLR: 3.900767\n",
      "Training Epoch: 40 [640/50000]\tLoss: 4.6988\tLR: 3.901023\n",
      "Training Epoch: 40 [768/50000]\tLoss: 4.6758\tLR: 3.901279\n",
      "Training Epoch: 40 [896/50000]\tLoss: 4.6428\tLR: 3.901535\n",
      "Training Epoch: 40 [1024/50000]\tLoss: 4.6721\tLR: 3.901790\n",
      "Training Epoch: 40 [1152/50000]\tLoss: 4.6596\tLR: 3.902046\n",
      "Training Epoch: 40 [1280/50000]\tLoss: 4.6656\tLR: 3.902302\n",
      "Training Epoch: 40 [1408/50000]\tLoss: 4.6797\tLR: 3.902558\n",
      "Training Epoch: 40 [1536/50000]\tLoss: 4.6429\tLR: 3.902813\n",
      "Training Epoch: 40 [1664/50000]\tLoss: 4.7268\tLR: 3.903069\n",
      "Training Epoch: 40 [1792/50000]\tLoss: 4.7256\tLR: 3.903325\n",
      "Training Epoch: 40 [1920/50000]\tLoss: 4.6636\tLR: 3.903581\n",
      "Training Epoch: 40 [2048/50000]\tLoss: 4.6658\tLR: 3.903836\n",
      "Training Epoch: 40 [2176/50000]\tLoss: 4.7200\tLR: 3.904092\n",
      "Training Epoch: 40 [2304/50000]\tLoss: 4.7114\tLR: 3.904348\n",
      "Training Epoch: 40 [2432/50000]\tLoss: 4.7085\tLR: 3.904604\n",
      "Training Epoch: 40 [2560/50000]\tLoss: 4.7130\tLR: 3.904859\n",
      "Training Epoch: 40 [2688/50000]\tLoss: 4.6710\tLR: 3.905115\n",
      "Training Epoch: 40 [2816/50000]\tLoss: 4.6441\tLR: 3.905371\n",
      "Training Epoch: 40 [2944/50000]\tLoss: 4.7194\tLR: 3.905627\n",
      "Training Epoch: 40 [3072/50000]\tLoss: 4.6512\tLR: 3.905882\n",
      "Training Epoch: 40 [3200/50000]\tLoss: 4.6337\tLR: 3.906138\n",
      "Training Epoch: 40 [3328/50000]\tLoss: 4.6710\tLR: 3.906394\n",
      "Training Epoch: 40 [3456/50000]\tLoss: 4.7015\tLR: 3.906650\n",
      "Training Epoch: 40 [3584/50000]\tLoss: 4.6817\tLR: 3.906905\n",
      "Training Epoch: 40 [3712/50000]\tLoss: 4.6427\tLR: 3.907161\n",
      "Training Epoch: 40 [3840/50000]\tLoss: 4.7354\tLR: 3.907417\n",
      "Training Epoch: 40 [3968/50000]\tLoss: 4.6520\tLR: 3.907673\n",
      "Training Epoch: 40 [4096/50000]\tLoss: 4.7159\tLR: 3.907928\n",
      "Training Epoch: 40 [4224/50000]\tLoss: 4.6387\tLR: 3.908184\n",
      "Training Epoch: 40 [4352/50000]\tLoss: 4.6879\tLR: 3.908440\n",
      "Training Epoch: 40 [4480/50000]\tLoss: 4.6737\tLR: 3.908696\n",
      "Training Epoch: 40 [4608/50000]\tLoss: 4.7226\tLR: 3.908951\n",
      "Training Epoch: 40 [4736/50000]\tLoss: 4.6399\tLR: 3.909207\n",
      "Training Epoch: 40 [4864/50000]\tLoss: 4.6720\tLR: 3.909463\n",
      "Training Epoch: 40 [4992/50000]\tLoss: 4.6855\tLR: 3.909719\n",
      "Training Epoch: 40 [5120/50000]\tLoss: 4.6716\tLR: 3.909974\n",
      "Training Epoch: 40 [5248/50000]\tLoss: 4.6727\tLR: 3.910230\n",
      "Training Epoch: 40 [5376/50000]\tLoss: 4.6796\tLR: 3.910486\n",
      "Training Epoch: 40 [5504/50000]\tLoss: 4.7443\tLR: 3.910742\n",
      "Training Epoch: 40 [5632/50000]\tLoss: 4.6694\tLR: 3.910997\n",
      "Training Epoch: 40 [5760/50000]\tLoss: 4.7152\tLR: 3.911253\n",
      "Training Epoch: 40 [5888/50000]\tLoss: 4.7301\tLR: 3.911509\n",
      "Training Epoch: 40 [6016/50000]\tLoss: 4.6851\tLR: 3.911765\n",
      "Training Epoch: 40 [6144/50000]\tLoss: 4.6391\tLR: 3.912020\n",
      "Training Epoch: 40 [6272/50000]\tLoss: 4.6445\tLR: 3.912276\n",
      "Training Epoch: 40 [6400/50000]\tLoss: 4.6605\tLR: 3.912532\n",
      "Training Epoch: 40 [6528/50000]\tLoss: 4.7235\tLR: 3.912788\n",
      "Training Epoch: 40 [6656/50000]\tLoss: 4.6661\tLR: 3.913043\n",
      "Training Epoch: 40 [6784/50000]\tLoss: 4.6544\tLR: 3.913299\n",
      "Training Epoch: 40 [6912/50000]\tLoss: 4.7004\tLR: 3.913555\n",
      "Training Epoch: 40 [7040/50000]\tLoss: 4.6984\tLR: 3.913811\n",
      "Training Epoch: 40 [7168/50000]\tLoss: 4.6800\tLR: 3.914066\n",
      "Training Epoch: 40 [7296/50000]\tLoss: 4.6680\tLR: 3.914322\n",
      "Training Epoch: 40 [7424/50000]\tLoss: 4.6911\tLR: 3.914578\n",
      "Training Epoch: 40 [7552/50000]\tLoss: 4.6890\tLR: 3.914834\n",
      "Training Epoch: 40 [7680/50000]\tLoss: 4.6608\tLR: 3.915090\n",
      "Training Epoch: 40 [7808/50000]\tLoss: 4.7248\tLR: 3.915345\n",
      "Training Epoch: 40 [7936/50000]\tLoss: 4.6192\tLR: 3.915601\n",
      "Training Epoch: 40 [8064/50000]\tLoss: 4.6109\tLR: 3.915857\n",
      "Training Epoch: 40 [8192/50000]\tLoss: 4.7579\tLR: 3.916113\n",
      "Training Epoch: 40 [8320/50000]\tLoss: 4.6989\tLR: 3.916368\n",
      "Training Epoch: 40 [8448/50000]\tLoss: 4.6572\tLR: 3.916624\n",
      "Training Epoch: 40 [8576/50000]\tLoss: 4.6984\tLR: 3.916880\n",
      "Training Epoch: 40 [8704/50000]\tLoss: 4.6281\tLR: 3.917136\n",
      "Training Epoch: 40 [8832/50000]\tLoss: 4.6934\tLR: 3.917391\n",
      "Training Epoch: 40 [8960/50000]\tLoss: 4.6620\tLR: 3.917647\n",
      "Training Epoch: 40 [9088/50000]\tLoss: 4.7116\tLR: 3.917903\n",
      "Training Epoch: 40 [9216/50000]\tLoss: 4.7476\tLR: 3.918159\n",
      "Training Epoch: 40 [9344/50000]\tLoss: 4.6586\tLR: 3.918414\n",
      "Training Epoch: 40 [9472/50000]\tLoss: 4.6781\tLR: 3.918670\n",
      "Training Epoch: 40 [9600/50000]\tLoss: 4.6310\tLR: 3.918926\n",
      "Training Epoch: 40 [9728/50000]\tLoss: 4.6526\tLR: 3.919182\n",
      "Training Epoch: 40 [9856/50000]\tLoss: 4.7041\tLR: 3.919437\n",
      "Training Epoch: 40 [9984/50000]\tLoss: 4.6483\tLR: 3.919693\n",
      "Training Epoch: 40 [10112/50000]\tLoss: 4.6982\tLR: 3.919949\n",
      "Training Epoch: 40 [10240/50000]\tLoss: 4.7090\tLR: 3.920205\n",
      "Training Epoch: 40 [10368/50000]\tLoss: 4.6261\tLR: 3.920460\n",
      "Training Epoch: 40 [10496/50000]\tLoss: 4.7743\tLR: 3.920716\n",
      "Training Epoch: 40 [10624/50000]\tLoss: 4.7052\tLR: 3.920972\n",
      "Training Epoch: 40 [10752/50000]\tLoss: 4.6675\tLR: 3.921228\n",
      "Training Epoch: 40 [10880/50000]\tLoss: 4.7490\tLR: 3.921483\n",
      "Training Epoch: 40 [11008/50000]\tLoss: 4.7167\tLR: 3.921739\n",
      "Training Epoch: 40 [11136/50000]\tLoss: 4.6987\tLR: 3.921995\n",
      "Training Epoch: 40 [11264/50000]\tLoss: 4.6952\tLR: 3.922251\n",
      "Training Epoch: 40 [11392/50000]\tLoss: 4.6150\tLR: 3.922506\n",
      "Training Epoch: 40 [11520/50000]\tLoss: 4.6892\tLR: 3.922762\n",
      "Training Epoch: 40 [11648/50000]\tLoss: 4.6732\tLR: 3.923018\n",
      "Training Epoch: 40 [11776/50000]\tLoss: 4.7333\tLR: 3.923274\n",
      "Training Epoch: 40 [11904/50000]\tLoss: 4.6634\tLR: 3.923529\n",
      "Training Epoch: 40 [12032/50000]\tLoss: 4.6854\tLR: 3.923785\n",
      "Training Epoch: 40 [12160/50000]\tLoss: 4.6576\tLR: 3.924041\n",
      "Training Epoch: 40 [12288/50000]\tLoss: 4.6484\tLR: 3.924297\n",
      "Training Epoch: 40 [12416/50000]\tLoss: 4.6607\tLR: 3.924552\n",
      "Training Epoch: 40 [12544/50000]\tLoss: 4.6816\tLR: 3.924808\n",
      "Training Epoch: 40 [12672/50000]\tLoss: 4.6708\tLR: 3.925064\n",
      "Training Epoch: 40 [12800/50000]\tLoss: 4.6890\tLR: 3.925320\n",
      "Training Epoch: 40 [12928/50000]\tLoss: 4.5898\tLR: 3.925575\n",
      "Training Epoch: 40 [13056/50000]\tLoss: 4.7042\tLR: 3.925831\n",
      "Training Epoch: 40 [13184/50000]\tLoss: 4.7473\tLR: 3.926087\n",
      "Training Epoch: 40 [13312/50000]\tLoss: 4.7061\tLR: 3.926343\n",
      "Training Epoch: 40 [13440/50000]\tLoss: 4.6182\tLR: 3.926598\n",
      "Training Epoch: 40 [13568/50000]\tLoss: 4.6712\tLR: 3.926854\n",
      "Training Epoch: 40 [13696/50000]\tLoss: 4.7067\tLR: 3.927110\n",
      "Training Epoch: 40 [13824/50000]\tLoss: 4.6657\tLR: 3.927366\n",
      "Training Epoch: 40 [13952/50000]\tLoss: 4.6383\tLR: 3.927621\n",
      "Training Epoch: 40 [14080/50000]\tLoss: 4.7296\tLR: 3.927877\n",
      "Training Epoch: 40 [14208/50000]\tLoss: 4.6926\tLR: 3.928133\n",
      "Training Epoch: 40 [14336/50000]\tLoss: 4.6680\tLR: 3.928389\n",
      "Training Epoch: 40 [14464/50000]\tLoss: 4.6551\tLR: 3.928645\n",
      "Training Epoch: 40 [14592/50000]\tLoss: 4.5758\tLR: 3.928900\n",
      "Training Epoch: 40 [14720/50000]\tLoss: 4.6136\tLR: 3.929156\n",
      "Training Epoch: 40 [14848/50000]\tLoss: 4.6912\tLR: 3.929412\n",
      "Training Epoch: 40 [14976/50000]\tLoss: 4.6882\tLR: 3.929668\n",
      "Training Epoch: 40 [15104/50000]\tLoss: 4.6875\tLR: 3.929923\n",
      "Training Epoch: 40 [15232/50000]\tLoss: 4.7025\tLR: 3.930179\n",
      "Training Epoch: 40 [15360/50000]\tLoss: 4.5826\tLR: 3.930435\n",
      "Training Epoch: 40 [15488/50000]\tLoss: 4.6463\tLR: 3.930691\n",
      "Training Epoch: 40 [15616/50000]\tLoss: 4.6605\tLR: 3.930946\n",
      "Training Epoch: 40 [15744/50000]\tLoss: 4.7066\tLR: 3.931202\n",
      "Training Epoch: 40 [15872/50000]\tLoss: 4.6438\tLR: 3.931458\n",
      "Training Epoch: 40 [16000/50000]\tLoss: 4.7516\tLR: 3.931714\n",
      "Training Epoch: 40 [16128/50000]\tLoss: 4.6528\tLR: 3.931969\n",
      "Training Epoch: 40 [16256/50000]\tLoss: 4.7545\tLR: 3.932225\n",
      "Training Epoch: 40 [16384/50000]\tLoss: 4.6807\tLR: 3.932481\n",
      "Training Epoch: 40 [16512/50000]\tLoss: 4.6544\tLR: 3.932737\n",
      "Training Epoch: 40 [16640/50000]\tLoss: 4.6847\tLR: 3.932992\n",
      "Training Epoch: 40 [16768/50000]\tLoss: 4.6346\tLR: 3.933248\n",
      "Training Epoch: 40 [16896/50000]\tLoss: 4.6684\tLR: 3.933504\n",
      "Training Epoch: 40 [17024/50000]\tLoss: 4.6951\tLR: 3.933760\n",
      "Training Epoch: 40 [17152/50000]\tLoss: 4.7610\tLR: 3.934015\n",
      "Training Epoch: 40 [17280/50000]\tLoss: 4.6602\tLR: 3.934271\n",
      "Training Epoch: 40 [17408/50000]\tLoss: 4.6745\tLR: 3.934527\n",
      "Training Epoch: 40 [17536/50000]\tLoss: 4.6815\tLR: 3.934783\n",
      "Training Epoch: 40 [17664/50000]\tLoss: 4.6702\tLR: 3.935038\n",
      "Training Epoch: 40 [17792/50000]\tLoss: 4.6403\tLR: 3.935294\n",
      "Training Epoch: 40 [17920/50000]\tLoss: 4.6314\tLR: 3.935550\n",
      "Training Epoch: 40 [18048/50000]\tLoss: 4.6866\tLR: 3.935806\n",
      "Training Epoch: 40 [18176/50000]\tLoss: 4.6609\tLR: 3.936061\n",
      "Training Epoch: 40 [18304/50000]\tLoss: 4.6874\tLR: 3.936317\n",
      "Training Epoch: 40 [18432/50000]\tLoss: 4.6374\tLR: 3.936573\n",
      "Training Epoch: 40 [18560/50000]\tLoss: 4.7086\tLR: 3.936829\n",
      "Training Epoch: 40 [18688/50000]\tLoss: 4.7054\tLR: 3.937084\n",
      "Training Epoch: 40 [18816/50000]\tLoss: 4.6942\tLR: 3.937340\n",
      "Training Epoch: 40 [18944/50000]\tLoss: 4.6868\tLR: 3.937596\n",
      "Training Epoch: 40 [19072/50000]\tLoss: 4.6005\tLR: 3.937852\n",
      "Training Epoch: 40 [19200/50000]\tLoss: 4.7399\tLR: 3.938107\n",
      "Training Epoch: 40 [19328/50000]\tLoss: 4.7179\tLR: 3.938363\n",
      "Training Epoch: 40 [19456/50000]\tLoss: 4.6895\tLR: 3.938619\n",
      "Training Epoch: 40 [19584/50000]\tLoss: 4.6834\tLR: 3.938875\n",
      "Training Epoch: 40 [19712/50000]\tLoss: 4.6716\tLR: 3.939130\n",
      "Training Epoch: 40 [19840/50000]\tLoss: 4.6751\tLR: 3.939386\n",
      "Training Epoch: 40 [19968/50000]\tLoss: 4.6614\tLR: 3.939642\n",
      "Training Epoch: 40 [20096/50000]\tLoss: 4.6840\tLR: 3.939898\n",
      "Training Epoch: 40 [20224/50000]\tLoss: 4.6214\tLR: 3.940153\n",
      "Training Epoch: 40 [20352/50000]\tLoss: 4.6820\tLR: 3.940409\n",
      "Training Epoch: 40 [20480/50000]\tLoss: 4.6862\tLR: 3.940665\n",
      "Training Epoch: 40 [20608/50000]\tLoss: 4.6585\tLR: 3.940921\n",
      "Training Epoch: 40 [20736/50000]\tLoss: 4.6472\tLR: 3.941176\n",
      "Training Epoch: 40 [20864/50000]\tLoss: 4.7130\tLR: 3.941432\n",
      "Training Epoch: 40 [20992/50000]\tLoss: 4.7711\tLR: 3.941688\n",
      "Training Epoch: 40 [21120/50000]\tLoss: 4.6580\tLR: 3.941944\n",
      "Training Epoch: 40 [21248/50000]\tLoss: 4.6385\tLR: 3.942199\n",
      "Training Epoch: 40 [21376/50000]\tLoss: 4.6450\tLR: 3.942455\n",
      "Training Epoch: 40 [21504/50000]\tLoss: 4.7193\tLR: 3.942711\n",
      "Training Epoch: 40 [21632/50000]\tLoss: 4.6692\tLR: 3.942967\n",
      "Training Epoch: 40 [21760/50000]\tLoss: 4.6955\tLR: 3.943223\n",
      "Training Epoch: 40 [21888/50000]\tLoss: 4.6351\tLR: 3.943478\n",
      "Training Epoch: 40 [22016/50000]\tLoss: 4.6777\tLR: 3.943734\n",
      "Training Epoch: 40 [22144/50000]\tLoss: 4.6584\tLR: 3.943990\n",
      "Training Epoch: 40 [22272/50000]\tLoss: 4.6535\tLR: 3.944246\n",
      "Training Epoch: 40 [22400/50000]\tLoss: 4.6526\tLR: 3.944501\n",
      "Training Epoch: 40 [22528/50000]\tLoss: 4.6829\tLR: 3.944757\n",
      "Training Epoch: 40 [22656/50000]\tLoss: 4.6458\tLR: 3.945013\n",
      "Training Epoch: 40 [22784/50000]\tLoss: 4.6474\tLR: 3.945269\n",
      "Training Epoch: 40 [22912/50000]\tLoss: 4.6725\tLR: 3.945524\n",
      "Training Epoch: 40 [23040/50000]\tLoss: 4.6622\tLR: 3.945780\n",
      "Training Epoch: 40 [23168/50000]\tLoss: 4.6823\tLR: 3.946036\n",
      "Training Epoch: 40 [23296/50000]\tLoss: 4.6798\tLR: 3.946292\n",
      "Training Epoch: 40 [23424/50000]\tLoss: 4.6643\tLR: 3.946547\n",
      "Training Epoch: 40 [23552/50000]\tLoss: 4.6846\tLR: 3.946803\n",
      "Training Epoch: 40 [23680/50000]\tLoss: 4.6276\tLR: 3.947059\n",
      "Training Epoch: 40 [23808/50000]\tLoss: 4.6448\tLR: 3.947315\n",
      "Training Epoch: 40 [23936/50000]\tLoss: 4.6807\tLR: 3.947570\n",
      "Training Epoch: 40 [24064/50000]\tLoss: 4.6240\tLR: 3.947826\n",
      "Training Epoch: 40 [24192/50000]\tLoss: 4.6688\tLR: 3.948082\n",
      "Training Epoch: 40 [24320/50000]\tLoss: 4.6020\tLR: 3.948338\n",
      "Training Epoch: 40 [24448/50000]\tLoss: 4.6694\tLR: 3.948593\n",
      "Training Epoch: 40 [24576/50000]\tLoss: 4.7294\tLR: 3.948849\n",
      "Training Epoch: 40 [24704/50000]\tLoss: 4.6360\tLR: 3.949105\n",
      "Training Epoch: 40 [24832/50000]\tLoss: 4.6558\tLR: 3.949361\n",
      "Training Epoch: 40 [24960/50000]\tLoss: 4.6927\tLR: 3.949616\n",
      "Training Epoch: 40 [25088/50000]\tLoss: 4.7494\tLR: 3.949872\n",
      "Training Epoch: 40 [25216/50000]\tLoss: 4.6931\tLR: 3.950128\n",
      "Training Epoch: 40 [25344/50000]\tLoss: 4.7284\tLR: 3.950384\n",
      "Training Epoch: 40 [25472/50000]\tLoss: 4.6786\tLR: 3.950639\n",
      "Training Epoch: 40 [25600/50000]\tLoss: 4.7318\tLR: 3.950895\n",
      "Training Epoch: 40 [25728/50000]\tLoss: 4.7022\tLR: 3.951151\n",
      "Training Epoch: 40 [25856/50000]\tLoss: 4.7015\tLR: 3.951407\n",
      "Training Epoch: 40 [25984/50000]\tLoss: 4.6673\tLR: 3.951662\n",
      "Training Epoch: 40 [26112/50000]\tLoss: 4.6453\tLR: 3.951918\n",
      "Training Epoch: 40 [26240/50000]\tLoss: 4.6585\tLR: 3.952174\n",
      "Training Epoch: 40 [26368/50000]\tLoss: 4.6520\tLR: 3.952430\n",
      "Training Epoch: 40 [26496/50000]\tLoss: 4.6766\tLR: 3.952685\n",
      "Training Epoch: 40 [26624/50000]\tLoss: 4.6805\tLR: 3.952941\n",
      "Training Epoch: 40 [26752/50000]\tLoss: 4.7263\tLR: 3.953197\n",
      "Training Epoch: 40 [26880/50000]\tLoss: 4.6727\tLR: 3.953453\n",
      "Training Epoch: 40 [27008/50000]\tLoss: 4.6124\tLR: 3.953708\n",
      "Training Epoch: 40 [27136/50000]\tLoss: 4.6722\tLR: 3.953964\n",
      "Training Epoch: 40 [27264/50000]\tLoss: 4.6344\tLR: 3.954220\n",
      "Training Epoch: 40 [27392/50000]\tLoss: 4.6657\tLR: 3.954476\n",
      "Training Epoch: 40 [27520/50000]\tLoss: 4.6524\tLR: 3.954731\n",
      "Training Epoch: 40 [27648/50000]\tLoss: 4.6951\tLR: 3.954987\n",
      "Training Epoch: 40 [27776/50000]\tLoss: 4.6362\tLR: 3.955243\n",
      "Training Epoch: 40 [27904/50000]\tLoss: 4.7269\tLR: 3.955499\n",
      "Training Epoch: 40 [28032/50000]\tLoss: 4.7133\tLR: 3.955754\n",
      "Training Epoch: 40 [28160/50000]\tLoss: 4.6833\tLR: 3.956010\n",
      "Training Epoch: 40 [28288/50000]\tLoss: 4.6930\tLR: 3.956266\n",
      "Training Epoch: 40 [28416/50000]\tLoss: 4.6961\tLR: 3.956522\n",
      "Training Epoch: 40 [28544/50000]\tLoss: 4.7547\tLR: 3.956777\n",
      "Training Epoch: 40 [28672/50000]\tLoss: 4.6882\tLR: 3.957033\n",
      "Training Epoch: 40 [28800/50000]\tLoss: 4.6243\tLR: 3.957289\n",
      "Training Epoch: 40 [28928/50000]\tLoss: 4.6154\tLR: 3.957545\n",
      "Training Epoch: 40 [29056/50000]\tLoss: 4.6251\tLR: 3.957801\n",
      "Training Epoch: 40 [29184/50000]\tLoss: 4.6600\tLR: 3.958056\n",
      "Training Epoch: 40 [29312/50000]\tLoss: 4.7532\tLR: 3.958312\n",
      "Training Epoch: 40 [29440/50000]\tLoss: 4.7227\tLR: 3.958568\n",
      "Training Epoch: 40 [29568/50000]\tLoss: 4.7557\tLR: 3.958824\n",
      "Training Epoch: 40 [29696/50000]\tLoss: 4.6893\tLR: 3.959079\n",
      "Training Epoch: 40 [29824/50000]\tLoss: 4.7160\tLR: 3.959335\n",
      "Training Epoch: 40 [29952/50000]\tLoss: 4.7379\tLR: 3.959591\n",
      "Training Epoch: 40 [30080/50000]\tLoss: 4.6396\tLR: 3.959847\n",
      "Training Epoch: 40 [30208/50000]\tLoss: 4.6779\tLR: 3.960102\n",
      "Training Epoch: 40 [30336/50000]\tLoss: 4.6685\tLR: 3.960358\n",
      "Training Epoch: 40 [30464/50000]\tLoss: 4.7235\tLR: 3.960614\n",
      "Training Epoch: 40 [30592/50000]\tLoss: 4.6573\tLR: 3.960870\n",
      "Training Epoch: 40 [30720/50000]\tLoss: 4.5920\tLR: 3.961125\n",
      "Training Epoch: 40 [30848/50000]\tLoss: 4.6871\tLR: 3.961381\n",
      "Training Epoch: 40 [30976/50000]\tLoss: 4.7223\tLR: 3.961637\n",
      "Training Epoch: 40 [31104/50000]\tLoss: 4.6678\tLR: 3.961893\n",
      "Training Epoch: 40 [31232/50000]\tLoss: 4.6468\tLR: 3.962148\n",
      "Training Epoch: 40 [31360/50000]\tLoss: 4.6801\tLR: 3.962404\n",
      "Training Epoch: 40 [31488/50000]\tLoss: 4.6473\tLR: 3.962660\n",
      "Training Epoch: 40 [31616/50000]\tLoss: 4.6438\tLR: 3.962916\n",
      "Training Epoch: 40 [31744/50000]\tLoss: 4.7242\tLR: 3.963171\n",
      "Training Epoch: 40 [31872/50000]\tLoss: 4.7017\tLR: 3.963427\n",
      "Training Epoch: 40 [32000/50000]\tLoss: 4.6970\tLR: 3.963683\n",
      "Training Epoch: 40 [32128/50000]\tLoss: 4.7163\tLR: 3.963939\n",
      "Training Epoch: 40 [32256/50000]\tLoss: 4.7049\tLR: 3.964194\n",
      "Training Epoch: 40 [32384/50000]\tLoss: 4.7172\tLR: 3.964450\n",
      "Training Epoch: 40 [32512/50000]\tLoss: 4.6758\tLR: 3.964706\n",
      "Training Epoch: 40 [32640/50000]\tLoss: 4.7152\tLR: 3.964962\n",
      "Training Epoch: 40 [32768/50000]\tLoss: 4.7314\tLR: 3.965217\n",
      "Training Epoch: 40 [32896/50000]\tLoss: 4.7256\tLR: 3.965473\n",
      "Training Epoch: 40 [33024/50000]\tLoss: 4.7079\tLR: 3.965729\n",
      "Training Epoch: 40 [33152/50000]\tLoss: 4.7055\tLR: 3.965985\n",
      "Training Epoch: 40 [33280/50000]\tLoss: 4.6919\tLR: 3.966240\n",
      "Training Epoch: 40 [33408/50000]\tLoss: 4.6776\tLR: 3.966496\n",
      "Training Epoch: 40 [33536/50000]\tLoss: 4.7005\tLR: 3.966752\n",
      "Training Epoch: 40 [33664/50000]\tLoss: 4.6613\tLR: 3.967008\n",
      "Training Epoch: 40 [33792/50000]\tLoss: 4.6787\tLR: 3.967263\n",
      "Training Epoch: 40 [33920/50000]\tLoss: 4.7047\tLR: 3.967519\n",
      "Training Epoch: 40 [34048/50000]\tLoss: 4.6839\tLR: 3.967775\n",
      "Training Epoch: 40 [34176/50000]\tLoss: 4.7234\tLR: 3.968031\n",
      "Training Epoch: 40 [34304/50000]\tLoss: 4.7401\tLR: 3.968286\n",
      "Training Epoch: 40 [34432/50000]\tLoss: 4.6353\tLR: 3.968542\n",
      "Training Epoch: 40 [34560/50000]\tLoss: 4.6650\tLR: 3.968798\n",
      "Training Epoch: 40 [34688/50000]\tLoss: 4.7076\tLR: 3.969054\n",
      "Training Epoch: 40 [34816/50000]\tLoss: 4.7209\tLR: 3.969309\n",
      "Training Epoch: 40 [34944/50000]\tLoss: 4.6645\tLR: 3.969565\n",
      "Training Epoch: 40 [35072/50000]\tLoss: 4.6761\tLR: 3.969821\n",
      "Training Epoch: 40 [35200/50000]\tLoss: 4.6526\tLR: 3.970077\n",
      "Training Epoch: 40 [35328/50000]\tLoss: 4.5981\tLR: 3.970332\n",
      "Training Epoch: 40 [35456/50000]\tLoss: 4.6008\tLR: 3.970588\n",
      "Training Epoch: 40 [35584/50000]\tLoss: 4.6359\tLR: 3.970844\n",
      "Training Epoch: 40 [35712/50000]\tLoss: 4.7012\tLR: 3.971100\n",
      "Training Epoch: 40 [35840/50000]\tLoss: 4.6996\tLR: 3.971355\n",
      "Training Epoch: 40 [35968/50000]\tLoss: 4.6295\tLR: 3.971611\n",
      "Training Epoch: 40 [36096/50000]\tLoss: 4.7233\tLR: 3.971867\n",
      "Training Epoch: 40 [36224/50000]\tLoss: 4.6837\tLR: 3.972123\n",
      "Training Epoch: 40 [36352/50000]\tLoss: 4.6764\tLR: 3.972379\n",
      "Training Epoch: 40 [36480/50000]\tLoss: 4.6288\tLR: 3.972634\n",
      "Training Epoch: 40 [36608/50000]\tLoss: 4.6609\tLR: 3.972890\n",
      "Training Epoch: 40 [36736/50000]\tLoss: 4.6183\tLR: 3.973146\n",
      "Training Epoch: 40 [36864/50000]\tLoss: 4.7148\tLR: 3.973402\n",
      "Training Epoch: 40 [36992/50000]\tLoss: 4.7031\tLR: 3.973657\n",
      "Training Epoch: 40 [37120/50000]\tLoss: 4.6244\tLR: 3.973913\n",
      "Training Epoch: 40 [37248/50000]\tLoss: 4.6703\tLR: 3.974169\n",
      "Training Epoch: 40 [37376/50000]\tLoss: 4.6551\tLR: 3.974425\n",
      "Training Epoch: 40 [37504/50000]\tLoss: 4.6737\tLR: 3.974680\n",
      "Training Epoch: 40 [37632/50000]\tLoss: 4.6231\tLR: 3.974936\n",
      "Training Epoch: 40 [37760/50000]\tLoss: 4.6617\tLR: 3.975192\n",
      "Training Epoch: 40 [37888/50000]\tLoss: 4.7384\tLR: 3.975448\n",
      "Training Epoch: 40 [38016/50000]\tLoss: 4.6430\tLR: 3.975703\n",
      "Training Epoch: 40 [38144/50000]\tLoss: 4.7280\tLR: 3.975959\n",
      "Training Epoch: 40 [38272/50000]\tLoss: 4.6207\tLR: 3.976215\n",
      "Training Epoch: 40 [38400/50000]\tLoss: 4.6574\tLR: 3.976471\n",
      "Training Epoch: 40 [38528/50000]\tLoss: 4.6556\tLR: 3.976726\n",
      "Training Epoch: 40 [38656/50000]\tLoss: 4.6638\tLR: 3.976982\n",
      "Training Epoch: 40 [38784/50000]\tLoss: 4.7161\tLR: 3.977238\n",
      "Training Epoch: 40 [38912/50000]\tLoss: 4.6367\tLR: 3.977494\n",
      "Training Epoch: 40 [39040/50000]\tLoss: 4.6814\tLR: 3.977749\n",
      "Training Epoch: 40 [39168/50000]\tLoss: 4.6966\tLR: 3.978005\n",
      "Training Epoch: 40 [39296/50000]\tLoss: 4.7465\tLR: 3.978261\n",
      "Training Epoch: 40 [39424/50000]\tLoss: 4.5832\tLR: 3.978517\n",
      "Training Epoch: 40 [39552/50000]\tLoss: 4.6340\tLR: 3.978772\n",
      "Training Epoch: 40 [39680/50000]\tLoss: 4.6906\tLR: 3.979028\n",
      "Training Epoch: 40 [39808/50000]\tLoss: 4.6501\tLR: 3.979284\n",
      "Training Epoch: 40 [39936/50000]\tLoss: 4.7250\tLR: 3.979540\n",
      "Training Epoch: 40 [40064/50000]\tLoss: 4.6634\tLR: 3.979795\n",
      "Training Epoch: 40 [40192/50000]\tLoss: 4.6541\tLR: 3.980051\n",
      "Training Epoch: 40 [40320/50000]\tLoss: 4.6716\tLR: 3.980307\n",
      "Training Epoch: 40 [40448/50000]\tLoss: 4.7160\tLR: 3.980563\n",
      "Training Epoch: 40 [40576/50000]\tLoss: 4.6480\tLR: 3.980818\n",
      "Training Epoch: 40 [40704/50000]\tLoss: 4.7210\tLR: 3.981074\n",
      "Training Epoch: 40 [40832/50000]\tLoss: 4.6928\tLR: 3.981330\n",
      "Training Epoch: 40 [40960/50000]\tLoss: 4.7542\tLR: 3.981586\n",
      "Training Epoch: 40 [41088/50000]\tLoss: 4.7293\tLR: 3.981841\n",
      "Training Epoch: 40 [41216/50000]\tLoss: 4.6305\tLR: 3.982097\n",
      "Training Epoch: 40 [41344/50000]\tLoss: 4.6836\tLR: 3.982353\n",
      "Training Epoch: 40 [41472/50000]\tLoss: 4.7288\tLR: 3.982609\n",
      "Training Epoch: 40 [41600/50000]\tLoss: 4.6483\tLR: 3.982864\n",
      "Training Epoch: 40 [41728/50000]\tLoss: 4.6469\tLR: 3.983120\n",
      "Training Epoch: 40 [41856/50000]\tLoss: 4.7068\tLR: 3.983376\n",
      "Training Epoch: 40 [41984/50000]\tLoss: 4.7092\tLR: 3.983632\n",
      "Training Epoch: 40 [42112/50000]\tLoss: 4.6219\tLR: 3.983887\n",
      "Training Epoch: 40 [42240/50000]\tLoss: 4.6295\tLR: 3.984143\n",
      "Training Epoch: 40 [42368/50000]\tLoss: 4.6918\tLR: 3.984399\n",
      "Training Epoch: 40 [42496/50000]\tLoss: 4.7275\tLR: 3.984655\n",
      "Training Epoch: 40 [42624/50000]\tLoss: 4.6762\tLR: 3.984910\n",
      "Training Epoch: 40 [42752/50000]\tLoss: 4.6837\tLR: 3.985166\n",
      "Training Epoch: 40 [42880/50000]\tLoss: 4.6578\tLR: 3.985422\n",
      "Training Epoch: 40 [43008/50000]\tLoss: 4.7213\tLR: 3.985678\n",
      "Training Epoch: 40 [43136/50000]\tLoss: 4.6803\tLR: 3.985934\n",
      "Training Epoch: 40 [43264/50000]\tLoss: 4.6649\tLR: 3.986189\n",
      "Training Epoch: 40 [43392/50000]\tLoss: 4.6497\tLR: 3.986445\n",
      "Training Epoch: 40 [43520/50000]\tLoss: 4.6080\tLR: 3.986701\n",
      "Training Epoch: 40 [43648/50000]\tLoss: 4.6884\tLR: 3.986957\n",
      "Training Epoch: 40 [43776/50000]\tLoss: 4.6997\tLR: 3.987212\n",
      "Training Epoch: 40 [43904/50000]\tLoss: 4.6506\tLR: 3.987468\n",
      "Training Epoch: 40 [44032/50000]\tLoss: 4.6759\tLR: 3.987724\n",
      "Training Epoch: 40 [44160/50000]\tLoss: 4.6575\tLR: 3.987980\n",
      "Training Epoch: 40 [44288/50000]\tLoss: 4.6942\tLR: 3.988235\n",
      "Training Epoch: 40 [44416/50000]\tLoss: 4.7213\tLR: 3.988491\n",
      "Training Epoch: 40 [44544/50000]\tLoss: 4.6787\tLR: 3.988747\n",
      "Training Epoch: 40 [44672/50000]\tLoss: 4.6973\tLR: 3.989003\n",
      "Training Epoch: 40 [44800/50000]\tLoss: 4.6995\tLR: 3.989258\n",
      "Training Epoch: 40 [44928/50000]\tLoss: 4.6813\tLR: 3.989514\n",
      "Training Epoch: 40 [45056/50000]\tLoss: 4.6824\tLR: 3.989770\n",
      "Training Epoch: 40 [45184/50000]\tLoss: 4.6476\tLR: 3.990026\n",
      "Training Epoch: 40 [45312/50000]\tLoss: 4.6742\tLR: 3.990281\n",
      "Training Epoch: 40 [45440/50000]\tLoss: 4.7245\tLR: 3.990537\n",
      "Training Epoch: 40 [45568/50000]\tLoss: 4.7582\tLR: 3.990793\n",
      "Training Epoch: 40 [45696/50000]\tLoss: 4.6450\tLR: 3.991049\n",
      "Training Epoch: 40 [45824/50000]\tLoss: 4.7942\tLR: 3.991304\n",
      "Training Epoch: 40 [45952/50000]\tLoss: 4.7316\tLR: 3.991560\n",
      "Training Epoch: 40 [46080/50000]\tLoss: 4.6713\tLR: 3.991816\n",
      "Training Epoch: 40 [46208/50000]\tLoss: 4.6859\tLR: 3.992072\n",
      "Training Epoch: 40 [46336/50000]\tLoss: 4.6599\tLR: 3.992327\n",
      "Training Epoch: 40 [46464/50000]\tLoss: 4.6460\tLR: 3.992583\n",
      "Training Epoch: 40 [46592/50000]\tLoss: 4.6824\tLR: 3.992839\n",
      "Training Epoch: 40 [46720/50000]\tLoss: 4.6740\tLR: 3.993095\n",
      "Training Epoch: 40 [46848/50000]\tLoss: 4.6591\tLR: 3.993350\n",
      "Training Epoch: 40 [46976/50000]\tLoss: 4.6462\tLR: 3.993606\n",
      "Training Epoch: 40 [47104/50000]\tLoss: 4.6646\tLR: 3.993862\n",
      "Training Epoch: 40 [47232/50000]\tLoss: 4.7298\tLR: 3.994118\n",
      "Training Epoch: 40 [47360/50000]\tLoss: 4.6701\tLR: 3.994373\n",
      "Training Epoch: 40 [47488/50000]\tLoss: 4.6454\tLR: 3.994629\n",
      "Training Epoch: 40 [47616/50000]\tLoss: 4.6788\tLR: 3.994885\n",
      "Training Epoch: 40 [47744/50000]\tLoss: 4.6781\tLR: 3.995141\n",
      "Training Epoch: 40 [47872/50000]\tLoss: 4.6657\tLR: 3.995396\n",
      "Training Epoch: 40 [48000/50000]\tLoss: 4.6285\tLR: 3.995652\n",
      "Training Epoch: 40 [48128/50000]\tLoss: 4.6379\tLR: 3.995908\n",
      "Training Epoch: 40 [48256/50000]\tLoss: 4.6694\tLR: 3.996164\n",
      "Training Epoch: 40 [48384/50000]\tLoss: 4.6821\tLR: 3.996419\n",
      "Training Epoch: 40 [48512/50000]\tLoss: 4.7057\tLR: 3.996675\n",
      "Training Epoch: 40 [48640/50000]\tLoss: 4.6561\tLR: 3.996931\n",
      "Training Epoch: 40 [48768/50000]\tLoss: 4.6998\tLR: 3.997187\n",
      "Training Epoch: 40 [48896/50000]\tLoss: 4.6729\tLR: 3.997442\n",
      "Training Epoch: 40 [49024/50000]\tLoss: 4.7075\tLR: 3.997698\n",
      "Training Epoch: 40 [49152/50000]\tLoss: 4.6892\tLR: 3.997954\n",
      "Training Epoch: 40 [49280/50000]\tLoss: 4.6824\tLR: 3.998210\n",
      "Training Epoch: 40 [49408/50000]\tLoss: 4.6672\tLR: 3.998465\n",
      "Training Epoch: 40 [49536/50000]\tLoss: 4.7309\tLR: 3.998721\n",
      "Training Epoch: 40 [49664/50000]\tLoss: 4.6904\tLR: 3.998977\n",
      "Training Epoch: 40 [49792/50000]\tLoss: 4.6583\tLR: 3.999233\n",
      "Training Epoch: 40 [49920/50000]\tLoss: 4.6578\tLR: 3.999488\n",
      "Training Epoch: 40 [50000/50000]\tLoss: 4.6132\tLR: 3.999744\n",
      "epoch 40 training time consumed: 488.83s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   56078 GB |   56078 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   55906 GB |   55905 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     172 GB |     172 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   56078 GB |   56078 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   55906 GB |   55905 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     172 GB |     172 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   55288 GB |   55288 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   55116 GB |   55115 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     172 GB |     172 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    5946 K  |    5946 K  |\n",
      "|       from large pool |      24    |      65    |    2534 K  |    2534 K  |\n",
      "|       from small pool |     231    |     274    |    3411 K  |    3411 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    5946 K  |    5946 K  |\n",
      "|       from large pool |      24    |      65    |    2534 K  |    2534 K  |\n",
      "|       from small pool |     231    |     274    |    3411 K  |    3411 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      38    |      46    |    3446 K  |    3446 K  |\n",
      "|       from large pool |      10    |      23    |    1218 K  |    1218 K  |\n",
      "|       from small pool |      28    |      35    |    2228 K  |    2228 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 40, Average loss: 0.0369, Accuracy: 0.0100, Time consumed:31.14s\n",
      "\n",
      "saving weights file to checkpoint\\resnet18\\Monday_25_July_2022_16h_56m_47s\\resnet18-40-regular.pth\n",
      "Training Epoch: 41 [128/50000]\tLoss: 4.6429\tLR: 0.100000\n",
      "Training Epoch: 41 [256/50000]\tLoss: 4.6924\tLR: 4.000256\n",
      "Training Epoch: 41 [384/50000]\tLoss: 4.6820\tLR: 4.000512\n",
      "Training Epoch: 41 [512/50000]\tLoss: 4.6759\tLR: 4.000767\n",
      "Training Epoch: 41 [640/50000]\tLoss: 4.6663\tLR: 4.001023\n",
      "Training Epoch: 41 [768/50000]\tLoss: 4.6554\tLR: 4.001279\n",
      "Training Epoch: 41 [896/50000]\tLoss: 4.7065\tLR: 4.001535\n",
      "Training Epoch: 41 [1024/50000]\tLoss: 4.6673\tLR: 4.001790\n",
      "Training Epoch: 41 [1152/50000]\tLoss: 4.7107\tLR: 4.002046\n",
      "Training Epoch: 41 [1280/50000]\tLoss: 4.7735\tLR: 4.002302\n",
      "Training Epoch: 41 [1408/50000]\tLoss: 4.7206\tLR: 4.002558\n",
      "Training Epoch: 41 [1536/50000]\tLoss: 4.6875\tLR: 4.002813\n",
      "Training Epoch: 41 [1664/50000]\tLoss: 4.6735\tLR: 4.003069\n",
      "Training Epoch: 41 [1792/50000]\tLoss: 4.6800\tLR: 4.003325\n",
      "Training Epoch: 41 [1920/50000]\tLoss: 4.7114\tLR: 4.003581\n",
      "Training Epoch: 41 [2048/50000]\tLoss: 4.6567\tLR: 4.003836\n",
      "Training Epoch: 41 [2176/50000]\tLoss: 4.6481\tLR: 4.004092\n",
      "Training Epoch: 41 [2304/50000]\tLoss: 4.6501\tLR: 4.004348\n",
      "Training Epoch: 41 [2432/50000]\tLoss: 4.6702\tLR: 4.004604\n",
      "Training Epoch: 41 [2560/50000]\tLoss: 4.7041\tLR: 4.004859\n",
      "Training Epoch: 41 [2688/50000]\tLoss: 4.7363\tLR: 4.005115\n",
      "Training Epoch: 41 [2816/50000]\tLoss: 4.6230\tLR: 4.005371\n",
      "Training Epoch: 41 [2944/50000]\tLoss: 4.6904\tLR: 4.005627\n",
      "Training Epoch: 41 [3072/50000]\tLoss: 4.7569\tLR: 4.005882\n",
      "Training Epoch: 41 [3200/50000]\tLoss: 4.7075\tLR: 4.006138\n",
      "Training Epoch: 41 [3328/50000]\tLoss: 4.7113\tLR: 4.006394\n",
      "Training Epoch: 41 [3456/50000]\tLoss: 4.6188\tLR: 4.006650\n",
      "Training Epoch: 41 [3584/50000]\tLoss: 4.7079\tLR: 4.006905\n",
      "Training Epoch: 41 [3712/50000]\tLoss: 4.6310\tLR: 4.007161\n",
      "Training Epoch: 41 [3840/50000]\tLoss: 4.7461\tLR: 4.007417\n",
      "Training Epoch: 41 [3968/50000]\tLoss: 4.7085\tLR: 4.007673\n",
      "Training Epoch: 41 [4096/50000]\tLoss: 4.6899\tLR: 4.007928\n",
      "Training Epoch: 41 [4224/50000]\tLoss: 4.6865\tLR: 4.008184\n",
      "Training Epoch: 41 [4352/50000]\tLoss: 4.7066\tLR: 4.008440\n",
      "Training Epoch: 41 [4480/50000]\tLoss: 4.7451\tLR: 4.008696\n",
      "Training Epoch: 41 [4608/50000]\tLoss: 4.6616\tLR: 4.008951\n",
      "Training Epoch: 41 [4736/50000]\tLoss: 4.6462\tLR: 4.009207\n",
      "Training Epoch: 41 [4864/50000]\tLoss: 4.6712\tLR: 4.009463\n",
      "Training Epoch: 41 [4992/50000]\tLoss: 4.6586\tLR: 4.009719\n",
      "Training Epoch: 41 [5120/50000]\tLoss: 4.7303\tLR: 4.009974\n",
      "Training Epoch: 41 [5248/50000]\tLoss: 4.6124\tLR: 4.010230\n",
      "Training Epoch: 41 [5376/50000]\tLoss: 4.6592\tLR: 4.010486\n",
      "Training Epoch: 41 [5504/50000]\tLoss: 4.7270\tLR: 4.010742\n",
      "Training Epoch: 41 [5632/50000]\tLoss: 4.6896\tLR: 4.010997\n",
      "Training Epoch: 41 [5760/50000]\tLoss: 4.7704\tLR: 4.011253\n",
      "Training Epoch: 41 [5888/50000]\tLoss: 4.7150\tLR: 4.011509\n",
      "Training Epoch: 41 [6016/50000]\tLoss: 4.7558\tLR: 4.011765\n",
      "Training Epoch: 41 [6144/50000]\tLoss: 4.7563\tLR: 4.012020\n",
      "Training Epoch: 41 [6272/50000]\tLoss: 4.7043\tLR: 4.012276\n",
      "Training Epoch: 41 [6400/50000]\tLoss: 4.6761\tLR: 4.012532\n",
      "Training Epoch: 41 [6528/50000]\tLoss: 4.6683\tLR: 4.012788\n",
      "Training Epoch: 41 [6656/50000]\tLoss: 4.6332\tLR: 4.013043\n",
      "Training Epoch: 41 [6784/50000]\tLoss: 4.6693\tLR: 4.013299\n",
      "Training Epoch: 41 [6912/50000]\tLoss: 4.6500\tLR: 4.013555\n",
      "Training Epoch: 41 [7040/50000]\tLoss: 4.6927\tLR: 4.013811\n",
      "Training Epoch: 41 [7168/50000]\tLoss: 4.7107\tLR: 4.014066\n",
      "Training Epoch: 41 [7296/50000]\tLoss: 4.6877\tLR: 4.014322\n",
      "Training Epoch: 41 [7424/50000]\tLoss: 4.7205\tLR: 4.014578\n",
      "Training Epoch: 41 [7552/50000]\tLoss: 4.6809\tLR: 4.014834\n",
      "Training Epoch: 41 [7680/50000]\tLoss: 4.6943\tLR: 4.015090\n",
      "Training Epoch: 41 [7808/50000]\tLoss: 4.7262\tLR: 4.015345\n",
      "Training Epoch: 41 [7936/50000]\tLoss: 4.6628\tLR: 4.015601\n",
      "Training Epoch: 41 [8064/50000]\tLoss: 4.6738\tLR: 4.015857\n",
      "Training Epoch: 41 [8192/50000]\tLoss: 4.7068\tLR: 4.016113\n",
      "Training Epoch: 41 [8320/50000]\tLoss: 4.6905\tLR: 4.016368\n",
      "Training Epoch: 41 [8448/50000]\tLoss: 4.6492\tLR: 4.016624\n",
      "Training Epoch: 41 [8576/50000]\tLoss: 4.6272\tLR: 4.016880\n",
      "Training Epoch: 41 [8704/50000]\tLoss: 4.7311\tLR: 4.017136\n",
      "Training Epoch: 41 [8832/50000]\tLoss: 4.6901\tLR: 4.017391\n",
      "Training Epoch: 41 [8960/50000]\tLoss: 4.6797\tLR: 4.017647\n",
      "Training Epoch: 41 [9088/50000]\tLoss: 4.6963\tLR: 4.017903\n",
      "Training Epoch: 41 [9216/50000]\tLoss: 4.6377\tLR: 4.018159\n",
      "Training Epoch: 41 [9344/50000]\tLoss: 4.7506\tLR: 4.018414\n",
      "Training Epoch: 41 [9472/50000]\tLoss: 4.7171\tLR: 4.018670\n",
      "Training Epoch: 41 [9600/50000]\tLoss: 4.7282\tLR: 4.018926\n",
      "Training Epoch: 41 [9728/50000]\tLoss: 4.6468\tLR: 4.019182\n",
      "Training Epoch: 41 [9856/50000]\tLoss: 4.6632\tLR: 4.019437\n",
      "Training Epoch: 41 [9984/50000]\tLoss: 4.6847\tLR: 4.019693\n",
      "Training Epoch: 41 [10112/50000]\tLoss: 4.6462\tLR: 4.019949\n",
      "Training Epoch: 41 [10240/50000]\tLoss: 4.6370\tLR: 4.020205\n",
      "Training Epoch: 41 [10368/50000]\tLoss: 4.6123\tLR: 4.020460\n",
      "Training Epoch: 41 [10496/50000]\tLoss: 4.6728\tLR: 4.020716\n",
      "Training Epoch: 41 [10624/50000]\tLoss: 4.7178\tLR: 4.020972\n",
      "Training Epoch: 41 [10752/50000]\tLoss: 4.7082\tLR: 4.021228\n",
      "Training Epoch: 41 [10880/50000]\tLoss: 4.6817\tLR: 4.021483\n",
      "Training Epoch: 41 [11008/50000]\tLoss: 4.6925\tLR: 4.021739\n",
      "Training Epoch: 41 [11136/50000]\tLoss: 4.7152\tLR: 4.021995\n",
      "Training Epoch: 41 [11264/50000]\tLoss: 4.6636\tLR: 4.022251\n",
      "Training Epoch: 41 [11392/50000]\tLoss: 4.6850\tLR: 4.022506\n",
      "Training Epoch: 41 [11520/50000]\tLoss: 4.6725\tLR: 4.022762\n",
      "Training Epoch: 41 [11648/50000]\tLoss: 4.6392\tLR: 4.023018\n",
      "Training Epoch: 41 [11776/50000]\tLoss: 4.6581\tLR: 4.023274\n",
      "Training Epoch: 41 [11904/50000]\tLoss: 4.6393\tLR: 4.023529\n",
      "Training Epoch: 41 [12032/50000]\tLoss: 4.6629\tLR: 4.023785\n",
      "Training Epoch: 41 [12160/50000]\tLoss: 4.6638\tLR: 4.024041\n",
      "Training Epoch: 41 [12288/50000]\tLoss: 4.6881\tLR: 4.024297\n",
      "Training Epoch: 41 [12416/50000]\tLoss: 4.6496\tLR: 4.024552\n",
      "Training Epoch: 41 [12544/50000]\tLoss: 4.6665\tLR: 4.024808\n",
      "Training Epoch: 41 [12672/50000]\tLoss: 4.6309\tLR: 4.025064\n",
      "Training Epoch: 41 [12800/50000]\tLoss: 4.6611\tLR: 4.025320\n",
      "Training Epoch: 41 [12928/50000]\tLoss: 4.7306\tLR: 4.025575\n",
      "Training Epoch: 41 [13056/50000]\tLoss: 4.6968\tLR: 4.025831\n",
      "Training Epoch: 41 [13184/50000]\tLoss: 4.7813\tLR: 4.026087\n",
      "Training Epoch: 41 [13312/50000]\tLoss: 4.6431\tLR: 4.026343\n",
      "Training Epoch: 41 [13440/50000]\tLoss: 4.6982\tLR: 4.026598\n",
      "Training Epoch: 41 [13568/50000]\tLoss: 4.7218\tLR: 4.026854\n",
      "Training Epoch: 41 [13696/50000]\tLoss: 4.6573\tLR: 4.027110\n",
      "Training Epoch: 41 [13824/50000]\tLoss: 4.7091\tLR: 4.027366\n",
      "Training Epoch: 41 [13952/50000]\tLoss: 4.6752\tLR: 4.027621\n",
      "Training Epoch: 41 [14080/50000]\tLoss: 4.6389\tLR: 4.027877\n",
      "Training Epoch: 41 [14208/50000]\tLoss: 4.6722\tLR: 4.028133\n",
      "Training Epoch: 41 [14336/50000]\tLoss: 4.6492\tLR: 4.028389\n",
      "Training Epoch: 41 [14464/50000]\tLoss: 4.6790\tLR: 4.028645\n",
      "Training Epoch: 41 [14592/50000]\tLoss: 4.7138\tLR: 4.028900\n",
      "Training Epoch: 41 [14720/50000]\tLoss: 4.6891\tLR: 4.029156\n",
      "Training Epoch: 41 [14848/50000]\tLoss: 4.6667\tLR: 4.029412\n",
      "Training Epoch: 41 [14976/50000]\tLoss: 4.7450\tLR: 4.029668\n",
      "Training Epoch: 41 [15104/50000]\tLoss: 4.6716\tLR: 4.029923\n",
      "Training Epoch: 41 [15232/50000]\tLoss: 4.7288\tLR: 4.030179\n",
      "Training Epoch: 41 [15360/50000]\tLoss: 4.6944\tLR: 4.030435\n",
      "Training Epoch: 41 [15488/50000]\tLoss: 4.6296\tLR: 4.030691\n",
      "Training Epoch: 41 [15616/50000]\tLoss: 4.6838\tLR: 4.030946\n",
      "Training Epoch: 41 [15744/50000]\tLoss: 4.6581\tLR: 4.031202\n",
      "Training Epoch: 41 [15872/50000]\tLoss: 4.6272\tLR: 4.031458\n",
      "Training Epoch: 41 [16000/50000]\tLoss: 4.6660\tLR: 4.031714\n",
      "Training Epoch: 41 [16128/50000]\tLoss: 4.6547\tLR: 4.031969\n",
      "Training Epoch: 41 [16256/50000]\tLoss: 4.6192\tLR: 4.032225\n",
      "Training Epoch: 41 [16384/50000]\tLoss: 4.6315\tLR: 4.032481\n",
      "Training Epoch: 41 [16512/50000]\tLoss: 4.6919\tLR: 4.032737\n",
      "Training Epoch: 41 [16640/50000]\tLoss: 4.6821\tLR: 4.032992\n",
      "Training Epoch: 41 [16768/50000]\tLoss: 4.6347\tLR: 4.033248\n",
      "Training Epoch: 41 [16896/50000]\tLoss: 4.7047\tLR: 4.033504\n",
      "Training Epoch: 41 [17024/50000]\tLoss: 4.6873\tLR: 4.033760\n",
      "Training Epoch: 41 [17152/50000]\tLoss: 4.6819\tLR: 4.034015\n",
      "Training Epoch: 41 [17280/50000]\tLoss: 4.7212\tLR: 4.034271\n",
      "Training Epoch: 41 [17408/50000]\tLoss: 4.7138\tLR: 4.034527\n",
      "Training Epoch: 41 [17536/50000]\tLoss: 4.7010\tLR: 4.034783\n",
      "Training Epoch: 41 [17664/50000]\tLoss: 4.7028\tLR: 4.035038\n",
      "Training Epoch: 41 [17792/50000]\tLoss: 4.7369\tLR: 4.035294\n",
      "Training Epoch: 41 [17920/50000]\tLoss: 4.7002\tLR: 4.035550\n",
      "Training Epoch: 41 [18048/50000]\tLoss: 4.6808\tLR: 4.035806\n",
      "Training Epoch: 41 [18176/50000]\tLoss: 4.6706\tLR: 4.036061\n",
      "Training Epoch: 41 [18304/50000]\tLoss: 4.6600\tLR: 4.036317\n",
      "Training Epoch: 41 [18432/50000]\tLoss: 4.6461\tLR: 4.036573\n",
      "Training Epoch: 41 [18560/50000]\tLoss: 4.6658\tLR: 4.036829\n",
      "Training Epoch: 41 [18688/50000]\tLoss: 4.6243\tLR: 4.037084\n",
      "Training Epoch: 41 [18816/50000]\tLoss: 4.6940\tLR: 4.037340\n",
      "Training Epoch: 41 [18944/50000]\tLoss: 4.7075\tLR: 4.037596\n",
      "Training Epoch: 41 [19072/50000]\tLoss: 4.6738\tLR: 4.037852\n",
      "Training Epoch: 41 [19200/50000]\tLoss: 4.6560\tLR: 4.038107\n",
      "Training Epoch: 41 [19328/50000]\tLoss: 4.6699\tLR: 4.038363\n",
      "Training Epoch: 41 [19456/50000]\tLoss: 4.7202\tLR: 4.038619\n",
      "Training Epoch: 41 [19584/50000]\tLoss: 4.7595\tLR: 4.038875\n",
      "Training Epoch: 41 [19712/50000]\tLoss: 4.6971\tLR: 4.039130\n",
      "Training Epoch: 41 [19840/50000]\tLoss: 4.6370\tLR: 4.039386\n",
      "Training Epoch: 41 [19968/50000]\tLoss: 4.7440\tLR: 4.039642\n",
      "Training Epoch: 41 [20096/50000]\tLoss: 4.6878\tLR: 4.039898\n",
      "Training Epoch: 41 [20224/50000]\tLoss: 4.6611\tLR: 4.040153\n",
      "Training Epoch: 41 [20352/50000]\tLoss: 4.6438\tLR: 4.040409\n",
      "Training Epoch: 41 [20480/50000]\tLoss: 4.6855\tLR: 4.040665\n",
      "Training Epoch: 41 [20608/50000]\tLoss: 4.6775\tLR: 4.040921\n",
      "Training Epoch: 41 [20736/50000]\tLoss: 4.7421\tLR: 4.041176\n",
      "Training Epoch: 41 [20864/50000]\tLoss: 4.6592\tLR: 4.041432\n",
      "Training Epoch: 41 [20992/50000]\tLoss: 4.7408\tLR: 4.041688\n",
      "Training Epoch: 41 [21120/50000]\tLoss: 4.7312\tLR: 4.041944\n",
      "Training Epoch: 41 [21248/50000]\tLoss: 4.6521\tLR: 4.042199\n",
      "Training Epoch: 41 [21376/50000]\tLoss: 4.7494\tLR: 4.042455\n",
      "Training Epoch: 41 [21504/50000]\tLoss: 4.6592\tLR: 4.042711\n",
      "Training Epoch: 41 [21632/50000]\tLoss: 4.7088\tLR: 4.042967\n",
      "Training Epoch: 41 [21760/50000]\tLoss: 4.7126\tLR: 4.043223\n",
      "Training Epoch: 41 [21888/50000]\tLoss: 4.6469\tLR: 4.043478\n",
      "Training Epoch: 41 [22016/50000]\tLoss: 4.6420\tLR: 4.043734\n",
      "Training Epoch: 41 [22144/50000]\tLoss: 4.6542\tLR: 4.043990\n",
      "Training Epoch: 41 [22272/50000]\tLoss: 4.6461\tLR: 4.044246\n",
      "Training Epoch: 41 [22400/50000]\tLoss: 4.7012\tLR: 4.044501\n",
      "Training Epoch: 41 [22528/50000]\tLoss: 4.7203\tLR: 4.044757\n",
      "Training Epoch: 41 [22656/50000]\tLoss: 4.7276\tLR: 4.045013\n",
      "Training Epoch: 41 [22784/50000]\tLoss: 4.6786\tLR: 4.045269\n",
      "Training Epoch: 41 [22912/50000]\tLoss: 4.6534\tLR: 4.045524\n",
      "Training Epoch: 41 [23040/50000]\tLoss: 4.7119\tLR: 4.045780\n",
      "Training Epoch: 41 [23168/50000]\tLoss: 4.7279\tLR: 4.046036\n",
      "Training Epoch: 41 [23296/50000]\tLoss: 4.6384\tLR: 4.046292\n",
      "Training Epoch: 41 [23424/50000]\tLoss: 4.6617\tLR: 4.046547\n",
      "Training Epoch: 41 [23552/50000]\tLoss: 4.6552\tLR: 4.046803\n",
      "Training Epoch: 41 [23680/50000]\tLoss: 4.6932\tLR: 4.047059\n",
      "Training Epoch: 41 [23808/50000]\tLoss: 4.6678\tLR: 4.047315\n",
      "Training Epoch: 41 [23936/50000]\tLoss: 4.7097\tLR: 4.047570\n",
      "Training Epoch: 41 [24064/50000]\tLoss: 4.7505\tLR: 4.047826\n",
      "Training Epoch: 41 [24192/50000]\tLoss: 4.6782\tLR: 4.048082\n",
      "Training Epoch: 41 [24320/50000]\tLoss: 4.6786\tLR: 4.048338\n",
      "Training Epoch: 41 [24448/50000]\tLoss: 4.6570\tLR: 4.048593\n",
      "Training Epoch: 41 [24576/50000]\tLoss: 4.6511\tLR: 4.048849\n",
      "Training Epoch: 41 [24704/50000]\tLoss: 4.6721\tLR: 4.049105\n",
      "Training Epoch: 41 [24832/50000]\tLoss: 4.6844\tLR: 4.049361\n",
      "Training Epoch: 41 [24960/50000]\tLoss: 4.6892\tLR: 4.049616\n",
      "Training Epoch: 41 [25088/50000]\tLoss: 4.6370\tLR: 4.049872\n",
      "Training Epoch: 41 [25216/50000]\tLoss: 4.6436\tLR: 4.050128\n",
      "Training Epoch: 41 [25344/50000]\tLoss: 4.6871\tLR: 4.050384\n",
      "Training Epoch: 41 [25472/50000]\tLoss: 4.6569\tLR: 4.050639\n",
      "Training Epoch: 41 [25600/50000]\tLoss: 4.7017\tLR: 4.050895\n",
      "Training Epoch: 41 [25728/50000]\tLoss: 4.7005\tLR: 4.051151\n",
      "Training Epoch: 41 [25856/50000]\tLoss: 4.6601\tLR: 4.051407\n",
      "Training Epoch: 41 [25984/50000]\tLoss: 4.6507\tLR: 4.051662\n",
      "Training Epoch: 41 [26112/50000]\tLoss: 4.6876\tLR: 4.051918\n",
      "Training Epoch: 41 [26240/50000]\tLoss: 4.6726\tLR: 4.052174\n",
      "Training Epoch: 41 [26368/50000]\tLoss: 4.6828\tLR: 4.052430\n",
      "Training Epoch: 41 [26496/50000]\tLoss: 4.6247\tLR: 4.052685\n",
      "Training Epoch: 41 [26624/50000]\tLoss: 4.7190\tLR: 4.052941\n",
      "Training Epoch: 41 [26752/50000]\tLoss: 4.7656\tLR: 4.053197\n",
      "Training Epoch: 41 [26880/50000]\tLoss: 4.6749\tLR: 4.053453\n",
      "Training Epoch: 41 [27008/50000]\tLoss: 4.6608\tLR: 4.053708\n",
      "Training Epoch: 41 [27136/50000]\tLoss: 4.6702\tLR: 4.053964\n",
      "Training Epoch: 41 [27264/50000]\tLoss: 4.6632\tLR: 4.054220\n",
      "Training Epoch: 41 [27392/50000]\tLoss: 4.6674\tLR: 4.054476\n",
      "Training Epoch: 41 [27520/50000]\tLoss: 4.6741\tLR: 4.054731\n",
      "Training Epoch: 41 [27648/50000]\tLoss: 4.6635\tLR: 4.054987\n",
      "Training Epoch: 41 [27776/50000]\tLoss: 4.6956\tLR: 4.055243\n",
      "Training Epoch: 41 [27904/50000]\tLoss: 4.6931\tLR: 4.055499\n",
      "Training Epoch: 41 [28032/50000]\tLoss: 4.7391\tLR: 4.055754\n",
      "Training Epoch: 41 [28160/50000]\tLoss: 4.6876\tLR: 4.056010\n",
      "Training Epoch: 41 [28288/50000]\tLoss: 4.6853\tLR: 4.056266\n",
      "Training Epoch: 41 [28416/50000]\tLoss: 4.6552\tLR: 4.056522\n",
      "Training Epoch: 41 [28544/50000]\tLoss: 4.7360\tLR: 4.056777\n",
      "Training Epoch: 41 [28672/50000]\tLoss: 4.6495\tLR: 4.057033\n",
      "Training Epoch: 41 [28800/50000]\tLoss: 4.6490\tLR: 4.057289\n",
      "Training Epoch: 41 [28928/50000]\tLoss: 4.6905\tLR: 4.057545\n",
      "Training Epoch: 41 [29056/50000]\tLoss: 4.7045\tLR: 4.057801\n",
      "Training Epoch: 41 [29184/50000]\tLoss: 4.7214\tLR: 4.058056\n",
      "Training Epoch: 41 [29312/50000]\tLoss: 4.6802\tLR: 4.058312\n",
      "Training Epoch: 41 [29440/50000]\tLoss: 4.7025\tLR: 4.058568\n",
      "Training Epoch: 41 [29568/50000]\tLoss: 4.6713\tLR: 4.058824\n",
      "Training Epoch: 41 [29696/50000]\tLoss: 4.6769\tLR: 4.059079\n",
      "Training Epoch: 41 [29824/50000]\tLoss: 4.6788\tLR: 4.059335\n",
      "Training Epoch: 41 [29952/50000]\tLoss: 4.7019\tLR: 4.059591\n",
      "Training Epoch: 41 [30080/50000]\tLoss: 4.7416\tLR: 4.059847\n",
      "Training Epoch: 41 [30208/50000]\tLoss: 4.6271\tLR: 4.060102\n",
      "Training Epoch: 41 [30336/50000]\tLoss: 4.7382\tLR: 4.060358\n",
      "Training Epoch: 41 [30464/50000]\tLoss: 4.6410\tLR: 4.060614\n",
      "Training Epoch: 41 [30592/50000]\tLoss: 4.6302\tLR: 4.060870\n",
      "Training Epoch: 41 [30720/50000]\tLoss: 4.7099\tLR: 4.061125\n",
      "Training Epoch: 41 [30848/50000]\tLoss: 4.7295\tLR: 4.061381\n",
      "Training Epoch: 41 [30976/50000]\tLoss: 4.6481\tLR: 4.061637\n",
      "Training Epoch: 41 [31104/50000]\tLoss: 4.6818\tLR: 4.061893\n",
      "Training Epoch: 41 [31232/50000]\tLoss: 4.6879\tLR: 4.062148\n",
      "Training Epoch: 41 [31360/50000]\tLoss: 4.6669\tLR: 4.062404\n",
      "Training Epoch: 41 [31488/50000]\tLoss: 4.6557\tLR: 4.062660\n",
      "Training Epoch: 41 [31616/50000]\tLoss: 4.7358\tLR: 4.062916\n",
      "Training Epoch: 41 [31744/50000]\tLoss: 4.7075\tLR: 4.063171\n",
      "Training Epoch: 41 [31872/50000]\tLoss: 4.6265\tLR: 4.063427\n",
      "Training Epoch: 41 [32000/50000]\tLoss: 4.6830\tLR: 4.063683\n",
      "Training Epoch: 41 [32128/50000]\tLoss: 4.6799\tLR: 4.063939\n",
      "Training Epoch: 41 [32256/50000]\tLoss: 4.6491\tLR: 4.064194\n",
      "Training Epoch: 41 [32384/50000]\tLoss: 4.6277\tLR: 4.064450\n",
      "Training Epoch: 41 [32512/50000]\tLoss: 4.5929\tLR: 4.064706\n",
      "Training Epoch: 41 [32640/50000]\tLoss: 4.6726\tLR: 4.064962\n",
      "Training Epoch: 41 [32768/50000]\tLoss: 4.6949\tLR: 4.065217\n",
      "Training Epoch: 41 [32896/50000]\tLoss: 4.7016\tLR: 4.065473\n",
      "Training Epoch: 41 [33024/50000]\tLoss: 4.7063\tLR: 4.065729\n",
      "Training Epoch: 41 [33152/50000]\tLoss: 4.7155\tLR: 4.065985\n",
      "Training Epoch: 41 [33280/50000]\tLoss: 4.6643\tLR: 4.066240\n",
      "Training Epoch: 41 [33408/50000]\tLoss: 4.6736\tLR: 4.066496\n",
      "Training Epoch: 41 [33536/50000]\tLoss: 4.6950\tLR: 4.066752\n",
      "Training Epoch: 41 [33664/50000]\tLoss: 4.6734\tLR: 4.067008\n",
      "Training Epoch: 41 [33792/50000]\tLoss: 4.7554\tLR: 4.067263\n",
      "Training Epoch: 41 [33920/50000]\tLoss: 4.6536\tLR: 4.067519\n",
      "Training Epoch: 41 [34048/50000]\tLoss: 4.7457\tLR: 4.067775\n",
      "Training Epoch: 41 [34176/50000]\tLoss: 4.6855\tLR: 4.068031\n",
      "Training Epoch: 41 [34304/50000]\tLoss: 4.7594\tLR: 4.068286\n",
      "Training Epoch: 41 [34432/50000]\tLoss: 4.6644\tLR: 4.068542\n",
      "Training Epoch: 41 [34560/50000]\tLoss: 4.6375\tLR: 4.068798\n",
      "Training Epoch: 41 [34688/50000]\tLoss: 4.6365\tLR: 4.069054\n",
      "Training Epoch: 41 [34816/50000]\tLoss: 4.7010\tLR: 4.069309\n",
      "Training Epoch: 41 [34944/50000]\tLoss: 4.6603\tLR: 4.069565\n",
      "Training Epoch: 41 [35072/50000]\tLoss: 4.6112\tLR: 4.069821\n",
      "Training Epoch: 41 [35200/50000]\tLoss: 4.7352\tLR: 4.070077\n",
      "Training Epoch: 41 [35328/50000]\tLoss: 4.7131\tLR: 4.070332\n",
      "Training Epoch: 41 [35456/50000]\tLoss: 4.6873\tLR: 4.070588\n",
      "Training Epoch: 41 [35584/50000]\tLoss: 4.6636\tLR: 4.070844\n",
      "Training Epoch: 41 [35712/50000]\tLoss: 4.6856\tLR: 4.071100\n",
      "Training Epoch: 41 [35840/50000]\tLoss: 4.6833\tLR: 4.071355\n",
      "Training Epoch: 41 [35968/50000]\tLoss: 4.6751\tLR: 4.071611\n",
      "Training Epoch: 41 [36096/50000]\tLoss: 4.6791\tLR: 4.071867\n",
      "Training Epoch: 41 [36224/50000]\tLoss: 4.6768\tLR: 4.072123\n",
      "Training Epoch: 41 [36352/50000]\tLoss: 4.7446\tLR: 4.072379\n",
      "Training Epoch: 41 [36480/50000]\tLoss: 4.6825\tLR: 4.072634\n",
      "Training Epoch: 41 [36608/50000]\tLoss: 4.7107\tLR: 4.072890\n",
      "Training Epoch: 41 [36736/50000]\tLoss: 4.6857\tLR: 4.073146\n",
      "Training Epoch: 41 [36864/50000]\tLoss: 4.7098\tLR: 4.073402\n",
      "Training Epoch: 41 [36992/50000]\tLoss: 4.6640\tLR: 4.073657\n",
      "Training Epoch: 41 [37120/50000]\tLoss: 4.6730\tLR: 4.073913\n",
      "Training Epoch: 41 [37248/50000]\tLoss: 4.6372\tLR: 4.074169\n",
      "Training Epoch: 41 [37376/50000]\tLoss: 4.6602\tLR: 4.074425\n",
      "Training Epoch: 41 [37504/50000]\tLoss: 4.6860\tLR: 4.074680\n",
      "Training Epoch: 41 [37632/50000]\tLoss: 4.6757\tLR: 4.074936\n",
      "Training Epoch: 41 [37760/50000]\tLoss: 4.6839\tLR: 4.075192\n",
      "Training Epoch: 41 [37888/50000]\tLoss: 4.7321\tLR: 4.075448\n",
      "Training Epoch: 41 [38016/50000]\tLoss: 4.6972\tLR: 4.075703\n",
      "Training Epoch: 41 [38144/50000]\tLoss: 4.6510\tLR: 4.075959\n",
      "Training Epoch: 41 [38272/50000]\tLoss: 4.6334\tLR: 4.076215\n",
      "Training Epoch: 41 [38400/50000]\tLoss: 4.6842\tLR: 4.076471\n",
      "Training Epoch: 41 [38528/50000]\tLoss: 4.7369\tLR: 4.076726\n",
      "Training Epoch: 41 [38656/50000]\tLoss: 4.8109\tLR: 4.076982\n",
      "Training Epoch: 41 [38784/50000]\tLoss: 4.6762\tLR: 4.077238\n",
      "Training Epoch: 41 [38912/50000]\tLoss: 4.7125\tLR: 4.077494\n",
      "Training Epoch: 41 [39040/50000]\tLoss: 4.6455\tLR: 4.077749\n",
      "Training Epoch: 41 [39168/50000]\tLoss: 4.7186\tLR: 4.078005\n",
      "Training Epoch: 41 [39296/50000]\tLoss: 4.6367\tLR: 4.078261\n",
      "Training Epoch: 41 [39424/50000]\tLoss: 4.7049\tLR: 4.078517\n",
      "Training Epoch: 41 [39552/50000]\tLoss: 4.6979\tLR: 4.078772\n",
      "Training Epoch: 41 [39680/50000]\tLoss: 4.6621\tLR: 4.079028\n",
      "Training Epoch: 41 [39808/50000]\tLoss: 4.6426\tLR: 4.079284\n",
      "Training Epoch: 41 [39936/50000]\tLoss: 4.6875\tLR: 4.079540\n",
      "Training Epoch: 41 [40064/50000]\tLoss: 4.7284\tLR: 4.079795\n",
      "Training Epoch: 41 [40192/50000]\tLoss: 4.6591\tLR: 4.080051\n",
      "Training Epoch: 41 [40320/50000]\tLoss: 4.6316\tLR: 4.080307\n",
      "Training Epoch: 41 [40448/50000]\tLoss: 4.7199\tLR: 4.080563\n",
      "Training Epoch: 41 [40576/50000]\tLoss: 4.7008\tLR: 4.080818\n",
      "Training Epoch: 41 [40704/50000]\tLoss: 4.7206\tLR: 4.081074\n",
      "Training Epoch: 41 [40832/50000]\tLoss: 4.7223\tLR: 4.081330\n",
      "Training Epoch: 41 [40960/50000]\tLoss: 4.6771\tLR: 4.081586\n",
      "Training Epoch: 41 [41088/50000]\tLoss: 4.6864\tLR: 4.081841\n",
      "Training Epoch: 41 [41216/50000]\tLoss: 4.6825\tLR: 4.082097\n",
      "Training Epoch: 41 [41344/50000]\tLoss: 4.6605\tLR: 4.082353\n",
      "Training Epoch: 41 [41472/50000]\tLoss: 4.7008\tLR: 4.082609\n",
      "Training Epoch: 41 [41600/50000]\tLoss: 4.6514\tLR: 4.082864\n",
      "Training Epoch: 41 [41728/50000]\tLoss: 4.7355\tLR: 4.083120\n",
      "Training Epoch: 41 [41856/50000]\tLoss: 4.6992\tLR: 4.083376\n",
      "Training Epoch: 41 [41984/50000]\tLoss: 4.6222\tLR: 4.083632\n",
      "Training Epoch: 41 [42112/50000]\tLoss: 4.6489\tLR: 4.083887\n",
      "Training Epoch: 41 [42240/50000]\tLoss: 4.6352\tLR: 4.084143\n",
      "Training Epoch: 41 [42368/50000]\tLoss: 4.6714\tLR: 4.084399\n",
      "Training Epoch: 41 [42496/50000]\tLoss: 4.7114\tLR: 4.084655\n",
      "Training Epoch: 41 [42624/50000]\tLoss: 4.6623\tLR: 4.084910\n",
      "Training Epoch: 41 [42752/50000]\tLoss: 4.6818\tLR: 4.085166\n",
      "Training Epoch: 41 [42880/50000]\tLoss: 4.7039\tLR: 4.085422\n",
      "Training Epoch: 41 [43008/50000]\tLoss: 4.6843\tLR: 4.085678\n",
      "Training Epoch: 41 [43136/50000]\tLoss: 4.6859\tLR: 4.085934\n",
      "Training Epoch: 41 [43264/50000]\tLoss: 4.6761\tLR: 4.086189\n",
      "Training Epoch: 41 [43392/50000]\tLoss: 4.6475\tLR: 4.086445\n",
      "Training Epoch: 41 [43520/50000]\tLoss: 4.6697\tLR: 4.086701\n",
      "Training Epoch: 41 [43648/50000]\tLoss: 4.6750\tLR: 4.086957\n",
      "Training Epoch: 41 [43776/50000]\tLoss: 4.7341\tLR: 4.087212\n",
      "Training Epoch: 41 [43904/50000]\tLoss: 4.6583\tLR: 4.087468\n",
      "Training Epoch: 41 [44032/50000]\tLoss: 4.7454\tLR: 4.087724\n",
      "Training Epoch: 41 [44160/50000]\tLoss: 4.6580\tLR: 4.087980\n",
      "Training Epoch: 41 [44288/50000]\tLoss: 4.6697\tLR: 4.088235\n",
      "Training Epoch: 41 [44416/50000]\tLoss: 4.6104\tLR: 4.088491\n",
      "Training Epoch: 41 [44544/50000]\tLoss: 4.6782\tLR: 4.088747\n",
      "Training Epoch: 41 [44672/50000]\tLoss: 4.6620\tLR: 4.089003\n",
      "Training Epoch: 41 [44800/50000]\tLoss: 4.6250\tLR: 4.089258\n",
      "Training Epoch: 41 [44928/50000]\tLoss: 4.7101\tLR: 4.089514\n",
      "Training Epoch: 41 [45056/50000]\tLoss: 4.6686\tLR: 4.089770\n",
      "Training Epoch: 41 [45184/50000]\tLoss: 4.7037\tLR: 4.090026\n",
      "Training Epoch: 41 [45312/50000]\tLoss: 4.6483\tLR: 4.090281\n",
      "Training Epoch: 41 [45440/50000]\tLoss: 4.6900\tLR: 4.090537\n",
      "Training Epoch: 41 [45568/50000]\tLoss: 4.6263\tLR: 4.090793\n",
      "Training Epoch: 41 [45696/50000]\tLoss: 4.6979\tLR: 4.091049\n",
      "Training Epoch: 41 [45824/50000]\tLoss: 4.6396\tLR: 4.091304\n",
      "Training Epoch: 41 [45952/50000]\tLoss: 4.7030\tLR: 4.091560\n",
      "Training Epoch: 41 [46080/50000]\tLoss: 4.7366\tLR: 4.091816\n",
      "Training Epoch: 41 [46208/50000]\tLoss: 4.7306\tLR: 4.092072\n",
      "Training Epoch: 41 [46336/50000]\tLoss: 4.6341\tLR: 4.092327\n",
      "Training Epoch: 41 [46464/50000]\tLoss: 4.7078\tLR: 4.092583\n",
      "Training Epoch: 41 [46592/50000]\tLoss: 4.7138\tLR: 4.092839\n",
      "Training Epoch: 41 [46720/50000]\tLoss: 4.6761\tLR: 4.093095\n",
      "Training Epoch: 41 [46848/50000]\tLoss: 4.6593\tLR: 4.093350\n",
      "Training Epoch: 41 [46976/50000]\tLoss: 4.7070\tLR: 4.093606\n",
      "Training Epoch: 41 [47104/50000]\tLoss: 4.6794\tLR: 4.093862\n",
      "Training Epoch: 41 [47232/50000]\tLoss: 4.7571\tLR: 4.094118\n",
      "Training Epoch: 41 [47360/50000]\tLoss: 4.6934\tLR: 4.094373\n",
      "Training Epoch: 41 [47488/50000]\tLoss: 4.6907\tLR: 4.094629\n",
      "Training Epoch: 41 [47616/50000]\tLoss: 4.6890\tLR: 4.094885\n",
      "Training Epoch: 41 [47744/50000]\tLoss: 4.6910\tLR: 4.095141\n",
      "Training Epoch: 41 [47872/50000]\tLoss: 4.7036\tLR: 4.095396\n",
      "Training Epoch: 41 [48000/50000]\tLoss: 4.6524\tLR: 4.095652\n",
      "Training Epoch: 41 [48128/50000]\tLoss: 4.6380\tLR: 4.095908\n",
      "Training Epoch: 41 [48256/50000]\tLoss: 4.6199\tLR: 4.096164\n",
      "Training Epoch: 41 [48384/50000]\tLoss: 4.6783\tLR: 4.096419\n",
      "Training Epoch: 41 [48512/50000]\tLoss: 4.6658\tLR: 4.096675\n",
      "Training Epoch: 41 [48640/50000]\tLoss: 4.7541\tLR: 4.096931\n",
      "Training Epoch: 41 [48768/50000]\tLoss: 4.6592\tLR: 4.097187\n",
      "Training Epoch: 41 [48896/50000]\tLoss: 4.6796\tLR: 4.097442\n",
      "Training Epoch: 41 [49024/50000]\tLoss: 4.7544\tLR: 4.097698\n",
      "Training Epoch: 41 [49152/50000]\tLoss: 4.6636\tLR: 4.097954\n",
      "Training Epoch: 41 [49280/50000]\tLoss: 4.6120\tLR: 4.098210\n",
      "Training Epoch: 41 [49408/50000]\tLoss: 4.6848\tLR: 4.098465\n",
      "Training Epoch: 41 [49536/50000]\tLoss: 4.6525\tLR: 4.098721\n",
      "Training Epoch: 41 [49664/50000]\tLoss: 4.7034\tLR: 4.098977\n",
      "Training Epoch: 41 [49792/50000]\tLoss: 4.6690\tLR: 4.099233\n",
      "Training Epoch: 41 [49920/50000]\tLoss: 4.6692\tLR: 4.099488\n",
      "Training Epoch: 41 [50000/50000]\tLoss: 4.6783\tLR: 4.099744\n",
      "epoch 41 training time consumed: 488.77s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   57480 GB |   57480 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   57303 GB |   57303 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     176 GB |     176 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   57480 GB |   57480 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   57303 GB |   57303 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     176 GB |     176 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   56670 GB |   56670 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   56493 GB |   56493 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     176 GB |     176 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    6095 K  |    6094 K  |\n",
      "|       from large pool |      24    |      65    |    2598 K  |    2598 K  |\n",
      "|       from small pool |     231    |     274    |    3496 K  |    3496 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    6095 K  |    6094 K  |\n",
      "|       from large pool |      24    |      65    |    2598 K  |    2598 K  |\n",
      "|       from small pool |     231    |     274    |    3496 K  |    3496 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      46    |    3533 K  |    3533 K  |\n",
      "|       from large pool |      10    |      23    |    1248 K  |    1248 K  |\n",
      "|       from small pool |      25    |      35    |    2284 K  |    2284 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 41, Average loss: 0.0370, Accuracy: 0.0100, Time consumed:31.07s\n",
      "\n",
      "Training Epoch: 42 [128/50000]\tLoss: 4.6824\tLR: 0.100000\n",
      "Training Epoch: 42 [256/50000]\tLoss: 4.6563\tLR: 4.100256\n",
      "Training Epoch: 42 [384/50000]\tLoss: 4.6805\tLR: 4.100512\n",
      "Training Epoch: 42 [512/50000]\tLoss: 4.6616\tLR: 4.100767\n",
      "Training Epoch: 42 [640/50000]\tLoss: 4.6634\tLR: 4.101023\n",
      "Training Epoch: 42 [768/50000]\tLoss: 4.6499\tLR: 4.101279\n",
      "Training Epoch: 42 [896/50000]\tLoss: 4.6733\tLR: 4.101535\n",
      "Training Epoch: 42 [1024/50000]\tLoss: 4.6823\tLR: 4.101790\n",
      "Training Epoch: 42 [1152/50000]\tLoss: 4.6543\tLR: 4.102046\n",
      "Training Epoch: 42 [1280/50000]\tLoss: 4.6962\tLR: 4.102302\n",
      "Training Epoch: 42 [1408/50000]\tLoss: 4.6382\tLR: 4.102558\n",
      "Training Epoch: 42 [1536/50000]\tLoss: 4.6880\tLR: 4.102813\n",
      "Training Epoch: 42 [1664/50000]\tLoss: 4.6584\tLR: 4.103069\n",
      "Training Epoch: 42 [1792/50000]\tLoss: 4.7132\tLR: 4.103325\n",
      "Training Epoch: 42 [1920/50000]\tLoss: 4.7060\tLR: 4.103581\n",
      "Training Epoch: 42 [2048/50000]\tLoss: 4.6944\tLR: 4.103836\n",
      "Training Epoch: 42 [2176/50000]\tLoss: 4.7445\tLR: 4.104092\n",
      "Training Epoch: 42 [2304/50000]\tLoss: 4.6762\tLR: 4.104348\n",
      "Training Epoch: 42 [2432/50000]\tLoss: 4.7064\tLR: 4.104604\n",
      "Training Epoch: 42 [2560/50000]\tLoss: 4.6434\tLR: 4.104859\n",
      "Training Epoch: 42 [2688/50000]\tLoss: 4.6955\tLR: 4.105115\n",
      "Training Epoch: 42 [2816/50000]\tLoss: 4.6579\tLR: 4.105371\n",
      "Training Epoch: 42 [2944/50000]\tLoss: 4.6497\tLR: 4.105627\n",
      "Training Epoch: 42 [3072/50000]\tLoss: 4.6703\tLR: 4.105882\n",
      "Training Epoch: 42 [3200/50000]\tLoss: 4.6587\tLR: 4.106138\n",
      "Training Epoch: 42 [3328/50000]\tLoss: 4.7081\tLR: 4.106394\n",
      "Training Epoch: 42 [3456/50000]\tLoss: 4.6305\tLR: 4.106650\n",
      "Training Epoch: 42 [3584/50000]\tLoss: 4.6416\tLR: 4.106905\n",
      "Training Epoch: 42 [3712/50000]\tLoss: 4.6366\tLR: 4.107161\n",
      "Training Epoch: 42 [3840/50000]\tLoss: 4.7222\tLR: 4.107417\n",
      "Training Epoch: 42 [3968/50000]\tLoss: 4.7049\tLR: 4.107673\n",
      "Training Epoch: 42 [4096/50000]\tLoss: 4.7273\tLR: 4.107928\n",
      "Training Epoch: 42 [4224/50000]\tLoss: 4.7607\tLR: 4.108184\n",
      "Training Epoch: 42 [4352/50000]\tLoss: 4.6815\tLR: 4.108440\n",
      "Training Epoch: 42 [4480/50000]\tLoss: 4.6776\tLR: 4.108696\n",
      "Training Epoch: 42 [4608/50000]\tLoss: 4.7273\tLR: 4.108951\n",
      "Training Epoch: 42 [4736/50000]\tLoss: 4.6659\tLR: 4.109207\n",
      "Training Epoch: 42 [4864/50000]\tLoss: 4.6513\tLR: 4.109463\n",
      "Training Epoch: 42 [4992/50000]\tLoss: 4.6940\tLR: 4.109719\n",
      "Training Epoch: 42 [5120/50000]\tLoss: 4.6779\tLR: 4.109974\n",
      "Training Epoch: 42 [5248/50000]\tLoss: 4.6689\tLR: 4.110230\n",
      "Training Epoch: 42 [5376/50000]\tLoss: 4.7140\tLR: 4.110486\n",
      "Training Epoch: 42 [5504/50000]\tLoss: 4.7135\tLR: 4.110742\n",
      "Training Epoch: 42 [5632/50000]\tLoss: 4.7073\tLR: 4.110997\n",
      "Training Epoch: 42 [5760/50000]\tLoss: 4.6717\tLR: 4.111253\n",
      "Training Epoch: 42 [5888/50000]\tLoss: 4.6585\tLR: 4.111509\n",
      "Training Epoch: 42 [6016/50000]\tLoss: 4.7689\tLR: 4.111765\n",
      "Training Epoch: 42 [6144/50000]\tLoss: 4.6881\tLR: 4.112020\n",
      "Training Epoch: 42 [6272/50000]\tLoss: 4.6504\tLR: 4.112276\n",
      "Training Epoch: 42 [6400/50000]\tLoss: 4.7221\tLR: 4.112532\n",
      "Training Epoch: 42 [6528/50000]\tLoss: 4.6757\tLR: 4.112788\n",
      "Training Epoch: 42 [6656/50000]\tLoss: 4.7589\tLR: 4.113043\n",
      "Training Epoch: 42 [6784/50000]\tLoss: 4.7367\tLR: 4.113299\n",
      "Training Epoch: 42 [6912/50000]\tLoss: 4.6636\tLR: 4.113555\n",
      "Training Epoch: 42 [7040/50000]\tLoss: 4.6520\tLR: 4.113811\n",
      "Training Epoch: 42 [7168/50000]\tLoss: 4.6956\tLR: 4.114066\n",
      "Training Epoch: 42 [7296/50000]\tLoss: 4.6567\tLR: 4.114322\n",
      "Training Epoch: 42 [7424/50000]\tLoss: 4.6788\tLR: 4.114578\n",
      "Training Epoch: 42 [7552/50000]\tLoss: 4.6725\tLR: 4.114834\n",
      "Training Epoch: 42 [7680/50000]\tLoss: 4.6762\tLR: 4.115090\n",
      "Training Epoch: 42 [7808/50000]\tLoss: 4.6388\tLR: 4.115345\n",
      "Training Epoch: 42 [7936/50000]\tLoss: 4.6632\tLR: 4.115601\n",
      "Training Epoch: 42 [8064/50000]\tLoss: 4.6257\tLR: 4.115857\n",
      "Training Epoch: 42 [8192/50000]\tLoss: 4.6504\tLR: 4.116113\n",
      "Training Epoch: 42 [8320/50000]\tLoss: 4.6345\tLR: 4.116368\n",
      "Training Epoch: 42 [8448/50000]\tLoss: 4.8054\tLR: 4.116624\n",
      "Training Epoch: 42 [8576/50000]\tLoss: 4.7186\tLR: 4.116880\n",
      "Training Epoch: 42 [8704/50000]\tLoss: 4.6577\tLR: 4.117136\n",
      "Training Epoch: 42 [8832/50000]\tLoss: 4.7186\tLR: 4.117391\n",
      "Training Epoch: 42 [8960/50000]\tLoss: 4.7287\tLR: 4.117647\n",
      "Training Epoch: 42 [9088/50000]\tLoss: 4.6993\tLR: 4.117903\n",
      "Training Epoch: 42 [9216/50000]\tLoss: 4.6895\tLR: 4.118159\n",
      "Training Epoch: 42 [9344/50000]\tLoss: 4.6215\tLR: 4.118414\n",
      "Training Epoch: 42 [9472/50000]\tLoss: 4.6247\tLR: 4.118670\n",
      "Training Epoch: 42 [9600/50000]\tLoss: 4.6954\tLR: 4.118926\n",
      "Training Epoch: 42 [9728/50000]\tLoss: 4.6486\tLR: 4.119182\n",
      "Training Epoch: 42 [9856/50000]\tLoss: 4.7053\tLR: 4.119437\n",
      "Training Epoch: 42 [9984/50000]\tLoss: 4.6966\tLR: 4.119693\n",
      "Training Epoch: 42 [10112/50000]\tLoss: 4.6872\tLR: 4.119949\n",
      "Training Epoch: 42 [10240/50000]\tLoss: 4.6356\tLR: 4.120205\n",
      "Training Epoch: 42 [10368/50000]\tLoss: 4.7417\tLR: 4.120460\n",
      "Training Epoch: 42 [10496/50000]\tLoss: 4.7283\tLR: 4.120716\n",
      "Training Epoch: 42 [10624/50000]\tLoss: 4.7181\tLR: 4.120972\n",
      "Training Epoch: 42 [10752/50000]\tLoss: 4.6651\tLR: 4.121228\n",
      "Training Epoch: 42 [10880/50000]\tLoss: 4.6837\tLR: 4.121483\n",
      "Training Epoch: 42 [11008/50000]\tLoss: 4.6495\tLR: 4.121739\n",
      "Training Epoch: 42 [11136/50000]\tLoss: 4.7087\tLR: 4.121995\n",
      "Training Epoch: 42 [11264/50000]\tLoss: 4.6981\tLR: 4.122251\n",
      "Training Epoch: 42 [11392/50000]\tLoss: 4.7181\tLR: 4.122506\n",
      "Training Epoch: 42 [11520/50000]\tLoss: 4.6578\tLR: 4.122762\n",
      "Training Epoch: 42 [11648/50000]\tLoss: 4.6521\tLR: 4.123018\n",
      "Training Epoch: 42 [11776/50000]\tLoss: 4.7228\tLR: 4.123274\n",
      "Training Epoch: 42 [11904/50000]\tLoss: 4.6647\tLR: 4.123529\n",
      "Training Epoch: 42 [12032/50000]\tLoss: 4.6774\tLR: 4.123785\n",
      "Training Epoch: 42 [12160/50000]\tLoss: 4.7318\tLR: 4.124041\n",
      "Training Epoch: 42 [12288/50000]\tLoss: 4.7004\tLR: 4.124297\n",
      "Training Epoch: 42 [12416/50000]\tLoss: 4.6915\tLR: 4.124552\n",
      "Training Epoch: 42 [12544/50000]\tLoss: 4.6559\tLR: 4.124808\n",
      "Training Epoch: 42 [12672/50000]\tLoss: 4.7058\tLR: 4.125064\n",
      "Training Epoch: 42 [12800/50000]\tLoss: 4.6925\tLR: 4.125320\n",
      "Training Epoch: 42 [12928/50000]\tLoss: 4.6021\tLR: 4.125575\n",
      "Training Epoch: 42 [13056/50000]\tLoss: 4.6903\tLR: 4.125831\n",
      "Training Epoch: 42 [13184/50000]\tLoss: 4.6563\tLR: 4.126087\n",
      "Training Epoch: 42 [13312/50000]\tLoss: 4.6627\tLR: 4.126343\n",
      "Training Epoch: 42 [13440/50000]\tLoss: 4.7036\tLR: 4.126598\n",
      "Training Epoch: 42 [13568/50000]\tLoss: 4.7293\tLR: 4.126854\n",
      "Training Epoch: 42 [13696/50000]\tLoss: 4.6818\tLR: 4.127110\n",
      "Training Epoch: 42 [13824/50000]\tLoss: 4.6391\tLR: 4.127366\n",
      "Training Epoch: 42 [13952/50000]\tLoss: 4.6378\tLR: 4.127621\n",
      "Training Epoch: 42 [14080/50000]\tLoss: 4.6808\tLR: 4.127877\n",
      "Training Epoch: 42 [14208/50000]\tLoss: 4.6985\tLR: 4.128133\n",
      "Training Epoch: 42 [14336/50000]\tLoss: 4.6332\tLR: 4.128389\n",
      "Training Epoch: 42 [14464/50000]\tLoss: 4.6793\tLR: 4.128645\n",
      "Training Epoch: 42 [14592/50000]\tLoss: 4.6857\tLR: 4.128900\n",
      "Training Epoch: 42 [14720/50000]\tLoss: 4.7696\tLR: 4.129156\n",
      "Training Epoch: 42 [14848/50000]\tLoss: 4.6716\tLR: 4.129412\n",
      "Training Epoch: 42 [14976/50000]\tLoss: 4.5992\tLR: 4.129668\n",
      "Training Epoch: 42 [15104/50000]\tLoss: 4.7579\tLR: 4.129923\n",
      "Training Epoch: 42 [15232/50000]\tLoss: 4.6276\tLR: 4.130179\n",
      "Training Epoch: 42 [15360/50000]\tLoss: 4.6408\tLR: 4.130435\n",
      "Training Epoch: 42 [15488/50000]\tLoss: 4.6764\tLR: 4.130691\n",
      "Training Epoch: 42 [15616/50000]\tLoss: 4.6087\tLR: 4.130946\n",
      "Training Epoch: 42 [15744/50000]\tLoss: 4.6582\tLR: 4.131202\n",
      "Training Epoch: 42 [15872/50000]\tLoss: 4.6777\tLR: 4.131458\n",
      "Training Epoch: 42 [16000/50000]\tLoss: 4.6852\tLR: 4.131714\n",
      "Training Epoch: 42 [16128/50000]\tLoss: 4.6396\tLR: 4.131969\n",
      "Training Epoch: 42 [16256/50000]\tLoss: 4.7165\tLR: 4.132225\n",
      "Training Epoch: 42 [16384/50000]\tLoss: 4.6398\tLR: 4.132481\n",
      "Training Epoch: 42 [16512/50000]\tLoss: 4.6347\tLR: 4.132737\n",
      "Training Epoch: 42 [16640/50000]\tLoss: 4.6702\tLR: 4.132992\n",
      "Training Epoch: 42 [16768/50000]\tLoss: 4.6331\tLR: 4.133248\n",
      "Training Epoch: 42 [16896/50000]\tLoss: 4.7116\tLR: 4.133504\n",
      "Training Epoch: 42 [17024/50000]\tLoss: 4.7343\tLR: 4.133760\n",
      "Training Epoch: 42 [17152/50000]\tLoss: 4.6750\tLR: 4.134015\n",
      "Training Epoch: 42 [17280/50000]\tLoss: 4.7064\tLR: 4.134271\n",
      "Training Epoch: 42 [17408/50000]\tLoss: 4.6544\tLR: 4.134527\n",
      "Training Epoch: 42 [17536/50000]\tLoss: 4.6678\tLR: 4.134783\n",
      "Training Epoch: 42 [17664/50000]\tLoss: 4.6717\tLR: 4.135038\n",
      "Training Epoch: 42 [17792/50000]\tLoss: 4.6423\tLR: 4.135294\n",
      "Training Epoch: 42 [17920/50000]\tLoss: 4.7384\tLR: 4.135550\n",
      "Training Epoch: 42 [18048/50000]\tLoss: 4.6575\tLR: 4.135806\n",
      "Training Epoch: 42 [18176/50000]\tLoss: 4.6683\tLR: 4.136061\n",
      "Training Epoch: 42 [18304/50000]\tLoss: 4.6323\tLR: 4.136317\n",
      "Training Epoch: 42 [18432/50000]\tLoss: 4.6517\tLR: 4.136573\n",
      "Training Epoch: 42 [18560/50000]\tLoss: 4.7153\tLR: 4.136829\n",
      "Training Epoch: 42 [18688/50000]\tLoss: 4.6302\tLR: 4.137084\n",
      "Training Epoch: 42 [18816/50000]\tLoss: 4.6394\tLR: 4.137340\n",
      "Training Epoch: 42 [18944/50000]\tLoss: 4.7018\tLR: 4.137596\n",
      "Training Epoch: 42 [19072/50000]\tLoss: 4.6368\tLR: 4.137852\n",
      "Training Epoch: 42 [19200/50000]\tLoss: 4.7124\tLR: 4.138107\n",
      "Training Epoch: 42 [19328/50000]\tLoss: 4.6864\tLR: 4.138363\n",
      "Training Epoch: 42 [19456/50000]\tLoss: 4.7223\tLR: 4.138619\n",
      "Training Epoch: 42 [19584/50000]\tLoss: 4.7509\tLR: 4.138875\n",
      "Training Epoch: 42 [19712/50000]\tLoss: 4.7242\tLR: 4.139130\n",
      "Training Epoch: 42 [19840/50000]\tLoss: 4.6693\tLR: 4.139386\n",
      "Training Epoch: 42 [19968/50000]\tLoss: 4.6728\tLR: 4.139642\n",
      "Training Epoch: 42 [20096/50000]\tLoss: 4.6988\tLR: 4.139898\n",
      "Training Epoch: 42 [20224/50000]\tLoss: 4.6315\tLR: 4.140153\n",
      "Training Epoch: 42 [20352/50000]\tLoss: 4.7011\tLR: 4.140409\n",
      "Training Epoch: 42 [20480/50000]\tLoss: 4.6935\tLR: 4.140665\n",
      "Training Epoch: 42 [20608/50000]\tLoss: 4.6571\tLR: 4.140921\n",
      "Training Epoch: 42 [20736/50000]\tLoss: 4.6169\tLR: 4.141176\n",
      "Training Epoch: 42 [20864/50000]\tLoss: 4.6663\tLR: 4.141432\n",
      "Training Epoch: 42 [20992/50000]\tLoss: 4.6493\tLR: 4.141688\n",
      "Training Epoch: 42 [21120/50000]\tLoss: 4.6484\tLR: 4.141944\n",
      "Training Epoch: 42 [21248/50000]\tLoss: 4.6589\tLR: 4.142199\n",
      "Training Epoch: 42 [21376/50000]\tLoss: 4.7180\tLR: 4.142455\n",
      "Training Epoch: 42 [21504/50000]\tLoss: 4.6758\tLR: 4.142711\n",
      "Training Epoch: 42 [21632/50000]\tLoss: 4.7024\tLR: 4.142967\n",
      "Training Epoch: 42 [21760/50000]\tLoss: 4.6352\tLR: 4.143223\n",
      "Training Epoch: 42 [21888/50000]\tLoss: 4.7057\tLR: 4.143478\n",
      "Training Epoch: 42 [22016/50000]\tLoss: 4.6245\tLR: 4.143734\n",
      "Training Epoch: 42 [22144/50000]\tLoss: 4.7034\tLR: 4.143990\n",
      "Training Epoch: 42 [22272/50000]\tLoss: 4.7593\tLR: 4.144246\n",
      "Training Epoch: 42 [22400/50000]\tLoss: 4.6847\tLR: 4.144501\n",
      "Training Epoch: 42 [22528/50000]\tLoss: 4.6773\tLR: 4.144757\n",
      "Training Epoch: 42 [22656/50000]\tLoss: 4.6944\tLR: 4.145013\n",
      "Training Epoch: 42 [22784/50000]\tLoss: 4.6980\tLR: 4.145269\n",
      "Training Epoch: 42 [22912/50000]\tLoss: 4.6961\tLR: 4.145524\n",
      "Training Epoch: 42 [23040/50000]\tLoss: 4.6920\tLR: 4.145780\n",
      "Training Epoch: 42 [23168/50000]\tLoss: 4.6996\tLR: 4.146036\n",
      "Training Epoch: 42 [23296/50000]\tLoss: 4.6877\tLR: 4.146292\n",
      "Training Epoch: 42 [23424/50000]\tLoss: 4.6970\tLR: 4.146547\n",
      "Training Epoch: 42 [23552/50000]\tLoss: 4.5916\tLR: 4.146803\n",
      "Training Epoch: 42 [23680/50000]\tLoss: 4.6257\tLR: 4.147059\n",
      "Training Epoch: 42 [23808/50000]\tLoss: 4.6198\tLR: 4.147315\n",
      "Training Epoch: 42 [23936/50000]\tLoss: 4.7316\tLR: 4.147570\n",
      "Training Epoch: 42 [24064/50000]\tLoss: 4.7332\tLR: 4.147826\n",
      "Training Epoch: 42 [24192/50000]\tLoss: 4.7127\tLR: 4.148082\n",
      "Training Epoch: 42 [24320/50000]\tLoss: 4.6450\tLR: 4.148338\n",
      "Training Epoch: 42 [24448/50000]\tLoss: 4.7259\tLR: 4.148593\n",
      "Training Epoch: 42 [24576/50000]\tLoss: 4.6174\tLR: 4.148849\n",
      "Training Epoch: 42 [24704/50000]\tLoss: 4.6606\tLR: 4.149105\n",
      "Training Epoch: 42 [24832/50000]\tLoss: 4.6840\tLR: 4.149361\n",
      "Training Epoch: 42 [24960/50000]\tLoss: 4.7236\tLR: 4.149616\n",
      "Training Epoch: 42 [25088/50000]\tLoss: 4.7114\tLR: 4.149872\n",
      "Training Epoch: 42 [25216/50000]\tLoss: 4.6816\tLR: 4.150128\n",
      "Training Epoch: 42 [25344/50000]\tLoss: 4.6779\tLR: 4.150384\n",
      "Training Epoch: 42 [25472/50000]\tLoss: 4.6230\tLR: 4.150639\n",
      "Training Epoch: 42 [25600/50000]\tLoss: 4.6691\tLR: 4.150895\n",
      "Training Epoch: 42 [25728/50000]\tLoss: 4.6454\tLR: 4.151151\n",
      "Training Epoch: 42 [25856/50000]\tLoss: 4.6407\tLR: 4.151407\n",
      "Training Epoch: 42 [25984/50000]\tLoss: 4.7037\tLR: 4.151662\n",
      "Training Epoch: 42 [26112/50000]\tLoss: 4.7039\tLR: 4.151918\n",
      "Training Epoch: 42 [26240/50000]\tLoss: 4.6871\tLR: 4.152174\n",
      "Training Epoch: 42 [26368/50000]\tLoss: 4.6704\tLR: 4.152430\n",
      "Training Epoch: 42 [26496/50000]\tLoss: 4.6913\tLR: 4.152685\n",
      "Training Epoch: 42 [26624/50000]\tLoss: 4.6918\tLR: 4.152941\n",
      "Training Epoch: 42 [26752/50000]\tLoss: 4.6726\tLR: 4.153197\n",
      "Training Epoch: 42 [26880/50000]\tLoss: 4.5904\tLR: 4.153453\n",
      "Training Epoch: 42 [27008/50000]\tLoss: 4.7540\tLR: 4.153708\n",
      "Training Epoch: 42 [27136/50000]\tLoss: 4.6597\tLR: 4.153964\n",
      "Training Epoch: 42 [27264/50000]\tLoss: 4.7017\tLR: 4.154220\n",
      "Training Epoch: 42 [27392/50000]\tLoss: 4.6468\tLR: 4.154476\n",
      "Training Epoch: 42 [27520/50000]\tLoss: 4.6318\tLR: 4.154731\n",
      "Training Epoch: 42 [27648/50000]\tLoss: 4.7192\tLR: 4.154987\n",
      "Training Epoch: 42 [27776/50000]\tLoss: 4.6625\tLR: 4.155243\n",
      "Training Epoch: 42 [27904/50000]\tLoss: 4.6936\tLR: 4.155499\n",
      "Training Epoch: 42 [28032/50000]\tLoss: 4.6709\tLR: 4.155754\n",
      "Training Epoch: 42 [28160/50000]\tLoss: 4.6692\tLR: 4.156010\n",
      "Training Epoch: 42 [28288/50000]\tLoss: 4.6783\tLR: 4.156266\n",
      "Training Epoch: 42 [28416/50000]\tLoss: 4.7532\tLR: 4.156522\n",
      "Training Epoch: 42 [28544/50000]\tLoss: 4.6746\tLR: 4.156777\n",
      "Training Epoch: 42 [28672/50000]\tLoss: 4.6733\tLR: 4.157033\n",
      "Training Epoch: 42 [28800/50000]\tLoss: 4.7312\tLR: 4.157289\n",
      "Training Epoch: 42 [28928/50000]\tLoss: 4.5981\tLR: 4.157545\n",
      "Training Epoch: 42 [29056/50000]\tLoss: 4.6777\tLR: 4.157801\n",
      "Training Epoch: 42 [29184/50000]\tLoss: 4.6488\tLR: 4.158056\n",
      "Training Epoch: 42 [29312/50000]\tLoss: 4.7277\tLR: 4.158312\n",
      "Training Epoch: 42 [29440/50000]\tLoss: 4.6924\tLR: 4.158568\n",
      "Training Epoch: 42 [29568/50000]\tLoss: 4.7345\tLR: 4.158824\n",
      "Training Epoch: 42 [29696/50000]\tLoss: 4.6108\tLR: 4.159079\n",
      "Training Epoch: 42 [29824/50000]\tLoss: 4.7326\tLR: 4.159335\n",
      "Training Epoch: 42 [29952/50000]\tLoss: 4.7098\tLR: 4.159591\n",
      "Training Epoch: 42 [30080/50000]\tLoss: 4.7613\tLR: 4.159847\n",
      "Training Epoch: 42 [30208/50000]\tLoss: 4.6743\tLR: 4.160102\n",
      "Training Epoch: 42 [30336/50000]\tLoss: 4.6223\tLR: 4.160358\n",
      "Training Epoch: 42 [30464/50000]\tLoss: 4.7048\tLR: 4.160614\n",
      "Training Epoch: 42 [30592/50000]\tLoss: 4.6375\tLR: 4.160870\n",
      "Training Epoch: 42 [30720/50000]\tLoss: 4.6228\tLR: 4.161125\n",
      "Training Epoch: 42 [30848/50000]\tLoss: 4.6992\tLR: 4.161381\n",
      "Training Epoch: 42 [30976/50000]\tLoss: 4.6853\tLR: 4.161637\n",
      "Training Epoch: 42 [31104/50000]\tLoss: 4.6555\tLR: 4.161893\n",
      "Training Epoch: 42 [31232/50000]\tLoss: 4.6571\tLR: 4.162148\n",
      "Training Epoch: 42 [31360/50000]\tLoss: 4.6256\tLR: 4.162404\n",
      "Training Epoch: 42 [31488/50000]\tLoss: 4.7251\tLR: 4.162660\n",
      "Training Epoch: 42 [31616/50000]\tLoss: 4.7532\tLR: 4.162916\n",
      "Training Epoch: 42 [31744/50000]\tLoss: 4.7308\tLR: 4.163171\n",
      "Training Epoch: 42 [31872/50000]\tLoss: 4.6786\tLR: 4.163427\n",
      "Training Epoch: 42 [32000/50000]\tLoss: 4.7106\tLR: 4.163683\n",
      "Training Epoch: 42 [32128/50000]\tLoss: 4.7233\tLR: 4.163939\n",
      "Training Epoch: 42 [32256/50000]\tLoss: 4.7048\tLR: 4.164194\n",
      "Training Epoch: 42 [32384/50000]\tLoss: 4.6470\tLR: 4.164450\n",
      "Training Epoch: 42 [32512/50000]\tLoss: 4.6838\tLR: 4.164706\n",
      "Training Epoch: 42 [32640/50000]\tLoss: 4.6699\tLR: 4.164962\n",
      "Training Epoch: 42 [32768/50000]\tLoss: 4.5883\tLR: 4.165217\n",
      "Training Epoch: 42 [32896/50000]\tLoss: 4.6992\tLR: 4.165473\n",
      "Training Epoch: 42 [33024/50000]\tLoss: 4.7531\tLR: 4.165729\n",
      "Training Epoch: 42 [33152/50000]\tLoss: 4.7903\tLR: 4.165985\n",
      "Training Epoch: 42 [33280/50000]\tLoss: 4.6715\tLR: 4.166240\n",
      "Training Epoch: 42 [33408/50000]\tLoss: 4.6258\tLR: 4.166496\n",
      "Training Epoch: 42 [33536/50000]\tLoss: 4.7325\tLR: 4.166752\n",
      "Training Epoch: 42 [33664/50000]\tLoss: 4.6967\tLR: 4.167008\n",
      "Training Epoch: 42 [33792/50000]\tLoss: 4.6377\tLR: 4.167263\n",
      "Training Epoch: 42 [33920/50000]\tLoss: 4.7354\tLR: 4.167519\n",
      "Training Epoch: 42 [34048/50000]\tLoss: 4.6857\tLR: 4.167775\n",
      "Training Epoch: 42 [34176/50000]\tLoss: 4.7424\tLR: 4.168031\n",
      "Training Epoch: 42 [34304/50000]\tLoss: 4.6402\tLR: 4.168286\n",
      "Training Epoch: 42 [34432/50000]\tLoss: 4.6668\tLR: 4.168542\n",
      "Training Epoch: 42 [34560/50000]\tLoss: 4.6767\tLR: 4.168798\n",
      "Training Epoch: 42 [34688/50000]\tLoss: 4.7094\tLR: 4.169054\n",
      "Training Epoch: 42 [34816/50000]\tLoss: 4.7063\tLR: 4.169309\n",
      "Training Epoch: 42 [34944/50000]\tLoss: 4.6904\tLR: 4.169565\n",
      "Training Epoch: 42 [35072/50000]\tLoss: 4.6805\tLR: 4.169821\n",
      "Training Epoch: 42 [35200/50000]\tLoss: 4.6966\tLR: 4.170077\n",
      "Training Epoch: 42 [35328/50000]\tLoss: 4.7061\tLR: 4.170332\n",
      "Training Epoch: 42 [35456/50000]\tLoss: 4.6190\tLR: 4.170588\n",
      "Training Epoch: 42 [35584/50000]\tLoss: 4.6799\tLR: 4.170844\n",
      "Training Epoch: 42 [35712/50000]\tLoss: 4.6436\tLR: 4.171100\n",
      "Training Epoch: 42 [35840/50000]\tLoss: 4.7170\tLR: 4.171355\n",
      "Training Epoch: 42 [35968/50000]\tLoss: 4.7825\tLR: 4.171611\n",
      "Training Epoch: 42 [36096/50000]\tLoss: 4.6899\tLR: 4.171867\n",
      "Training Epoch: 42 [36224/50000]\tLoss: 4.6170\tLR: 4.172123\n",
      "Training Epoch: 42 [36352/50000]\tLoss: 4.7350\tLR: 4.172379\n",
      "Training Epoch: 42 [36480/50000]\tLoss: 4.7446\tLR: 4.172634\n",
      "Training Epoch: 42 [36608/50000]\tLoss: 4.7752\tLR: 4.172890\n",
      "Training Epoch: 42 [36736/50000]\tLoss: 4.7109\tLR: 4.173146\n",
      "Training Epoch: 42 [36864/50000]\tLoss: 4.7070\tLR: 4.173402\n",
      "Training Epoch: 42 [36992/50000]\tLoss: 4.6841\tLR: 4.173657\n",
      "Training Epoch: 42 [37120/50000]\tLoss: 4.7684\tLR: 4.173913\n",
      "Training Epoch: 42 [37248/50000]\tLoss: 4.6792\tLR: 4.174169\n",
      "Training Epoch: 42 [37376/50000]\tLoss: 4.6301\tLR: 4.174425\n",
      "Training Epoch: 42 [37504/50000]\tLoss: 4.6896\tLR: 4.174680\n",
      "Training Epoch: 42 [37632/50000]\tLoss: 4.6953\tLR: 4.174936\n",
      "Training Epoch: 42 [37760/50000]\tLoss: 4.7838\tLR: 4.175192\n",
      "Training Epoch: 42 [37888/50000]\tLoss: 4.7249\tLR: 4.175448\n",
      "Training Epoch: 42 [38016/50000]\tLoss: 4.7600\tLR: 4.175703\n",
      "Training Epoch: 42 [38144/50000]\tLoss: 4.7410\tLR: 4.175959\n",
      "Training Epoch: 42 [38272/50000]\tLoss: 4.6211\tLR: 4.176215\n",
      "Training Epoch: 42 [38400/50000]\tLoss: 4.6372\tLR: 4.176471\n",
      "Training Epoch: 42 [38528/50000]\tLoss: 4.6957\tLR: 4.176726\n",
      "Training Epoch: 42 [38656/50000]\tLoss: 4.7054\tLR: 4.176982\n",
      "Training Epoch: 42 [38784/50000]\tLoss: 4.7013\tLR: 4.177238\n",
      "Training Epoch: 42 [38912/50000]\tLoss: 4.7350\tLR: 4.177494\n",
      "Training Epoch: 42 [39040/50000]\tLoss: 4.6969\tLR: 4.177749\n",
      "Training Epoch: 42 [39168/50000]\tLoss: 4.6968\tLR: 4.178005\n",
      "Training Epoch: 42 [39296/50000]\tLoss: 4.7035\tLR: 4.178261\n",
      "Training Epoch: 42 [39424/50000]\tLoss: 4.7179\tLR: 4.178517\n",
      "Training Epoch: 42 [39552/50000]\tLoss: 4.6694\tLR: 4.178772\n",
      "Training Epoch: 42 [39680/50000]\tLoss: 4.6754\tLR: 4.179028\n",
      "Training Epoch: 42 [39808/50000]\tLoss: 4.7206\tLR: 4.179284\n",
      "Training Epoch: 42 [39936/50000]\tLoss: 4.7259\tLR: 4.179540\n",
      "Training Epoch: 42 [40064/50000]\tLoss: 4.6824\tLR: 4.179795\n",
      "Training Epoch: 42 [40192/50000]\tLoss: 4.6483\tLR: 4.180051\n",
      "Training Epoch: 42 [40320/50000]\tLoss: 4.7085\tLR: 4.180307\n",
      "Training Epoch: 42 [40448/50000]\tLoss: 4.6859\tLR: 4.180563\n",
      "Training Epoch: 42 [40576/50000]\tLoss: 4.7163\tLR: 4.180818\n",
      "Training Epoch: 42 [40704/50000]\tLoss: 4.7787\tLR: 4.181074\n",
      "Training Epoch: 42 [40832/50000]\tLoss: 4.6792\tLR: 4.181330\n",
      "Training Epoch: 42 [40960/50000]\tLoss: 4.7058\tLR: 4.181586\n",
      "Training Epoch: 42 [41088/50000]\tLoss: 4.6991\tLR: 4.181841\n",
      "Training Epoch: 42 [41216/50000]\tLoss: 4.6360\tLR: 4.182097\n",
      "Training Epoch: 42 [41344/50000]\tLoss: 4.6119\tLR: 4.182353\n",
      "Training Epoch: 42 [41472/50000]\tLoss: 4.6993\tLR: 4.182609\n",
      "Training Epoch: 42 [41600/50000]\tLoss: 4.7016\tLR: 4.182864\n",
      "Training Epoch: 42 [41728/50000]\tLoss: 4.6375\tLR: 4.183120\n",
      "Training Epoch: 42 [41856/50000]\tLoss: 4.6930\tLR: 4.183376\n",
      "Training Epoch: 42 [41984/50000]\tLoss: 4.6332\tLR: 4.183632\n",
      "Training Epoch: 42 [42112/50000]\tLoss: 4.7257\tLR: 4.183887\n",
      "Training Epoch: 42 [42240/50000]\tLoss: 4.6978\tLR: 4.184143\n",
      "Training Epoch: 42 [42368/50000]\tLoss: 4.6677\tLR: 4.184399\n",
      "Training Epoch: 42 [42496/50000]\tLoss: 4.6950\tLR: 4.184655\n",
      "Training Epoch: 42 [42624/50000]\tLoss: 4.6239\tLR: 4.184910\n",
      "Training Epoch: 42 [42752/50000]\tLoss: 4.6809\tLR: 4.185166\n",
      "Training Epoch: 42 [42880/50000]\tLoss: 4.6838\tLR: 4.185422\n",
      "Training Epoch: 42 [43008/50000]\tLoss: 4.6549\tLR: 4.185678\n",
      "Training Epoch: 42 [43136/50000]\tLoss: 4.7036\tLR: 4.185934\n",
      "Training Epoch: 42 [43264/50000]\tLoss: 4.6787\tLR: 4.186189\n",
      "Training Epoch: 42 [43392/50000]\tLoss: 4.7236\tLR: 4.186445\n",
      "Training Epoch: 42 [43520/50000]\tLoss: 4.6440\tLR: 4.186701\n",
      "Training Epoch: 42 [43648/50000]\tLoss: 4.7042\tLR: 4.186957\n",
      "Training Epoch: 42 [43776/50000]\tLoss: 4.7410\tLR: 4.187212\n",
      "Training Epoch: 42 [43904/50000]\tLoss: 4.7151\tLR: 4.187468\n",
      "Training Epoch: 42 [44032/50000]\tLoss: 4.6662\tLR: 4.187724\n",
      "Training Epoch: 42 [44160/50000]\tLoss: 4.7209\tLR: 4.187980\n",
      "Training Epoch: 42 [44288/50000]\tLoss: 4.7118\tLR: 4.188235\n",
      "Training Epoch: 42 [44416/50000]\tLoss: 4.7046\tLR: 4.188491\n",
      "Training Epoch: 42 [44544/50000]\tLoss: 4.7349\tLR: 4.188747\n",
      "Training Epoch: 42 [44672/50000]\tLoss: 4.7119\tLR: 4.189003\n",
      "Training Epoch: 42 [44800/50000]\tLoss: 4.6332\tLR: 4.189258\n",
      "Training Epoch: 42 [44928/50000]\tLoss: 4.7171\tLR: 4.189514\n",
      "Training Epoch: 42 [45056/50000]\tLoss: 4.6940\tLR: 4.189770\n",
      "Training Epoch: 42 [45184/50000]\tLoss: 4.6694\tLR: 4.190026\n",
      "Training Epoch: 42 [45312/50000]\tLoss: 4.6201\tLR: 4.190281\n",
      "Training Epoch: 42 [45440/50000]\tLoss: 4.6850\tLR: 4.190537\n",
      "Training Epoch: 42 [45568/50000]\tLoss: 4.7272\tLR: 4.190793\n",
      "Training Epoch: 42 [45696/50000]\tLoss: 4.7841\tLR: 4.191049\n",
      "Training Epoch: 42 [45824/50000]\tLoss: 4.7568\tLR: 4.191304\n",
      "Training Epoch: 42 [45952/50000]\tLoss: 4.6638\tLR: 4.191560\n",
      "Training Epoch: 42 [46080/50000]\tLoss: 4.6669\tLR: 4.191816\n",
      "Training Epoch: 42 [46208/50000]\tLoss: 4.6698\tLR: 4.192072\n",
      "Training Epoch: 42 [46336/50000]\tLoss: 4.6756\tLR: 4.192327\n",
      "Training Epoch: 42 [46464/50000]\tLoss: 4.6274\tLR: 4.192583\n",
      "Training Epoch: 42 [46592/50000]\tLoss: 4.7249\tLR: 4.192839\n",
      "Training Epoch: 42 [46720/50000]\tLoss: 4.6524\tLR: 4.193095\n",
      "Training Epoch: 42 [46848/50000]\tLoss: 4.6872\tLR: 4.193350\n",
      "Training Epoch: 42 [46976/50000]\tLoss: 4.7140\tLR: 4.193606\n",
      "Training Epoch: 42 [47104/50000]\tLoss: 4.7032\tLR: 4.193862\n",
      "Training Epoch: 42 [47232/50000]\tLoss: 4.6628\tLR: 4.194118\n",
      "Training Epoch: 42 [47360/50000]\tLoss: 4.6829\tLR: 4.194373\n",
      "Training Epoch: 42 [47488/50000]\tLoss: 4.7933\tLR: 4.194629\n",
      "Training Epoch: 42 [47616/50000]\tLoss: 4.6653\tLR: 4.194885\n",
      "Training Epoch: 42 [47744/50000]\tLoss: 4.7080\tLR: 4.195141\n",
      "Training Epoch: 42 [47872/50000]\tLoss: 4.6685\tLR: 4.195396\n",
      "Training Epoch: 42 [48000/50000]\tLoss: 4.5972\tLR: 4.195652\n",
      "Training Epoch: 42 [48128/50000]\tLoss: 4.6575\tLR: 4.195908\n",
      "Training Epoch: 42 [48256/50000]\tLoss: 4.7364\tLR: 4.196164\n",
      "Training Epoch: 42 [48384/50000]\tLoss: 4.6373\tLR: 4.196419\n",
      "Training Epoch: 42 [48512/50000]\tLoss: 4.7095\tLR: 4.196675\n",
      "Training Epoch: 42 [48640/50000]\tLoss: 4.6372\tLR: 4.196931\n",
      "Training Epoch: 42 [48768/50000]\tLoss: 4.6754\tLR: 4.197187\n",
      "Training Epoch: 42 [48896/50000]\tLoss: 4.7060\tLR: 4.197442\n",
      "Training Epoch: 42 [49024/50000]\tLoss: 4.6824\tLR: 4.197698\n",
      "Training Epoch: 42 [49152/50000]\tLoss: 4.6726\tLR: 4.197954\n",
      "Training Epoch: 42 [49280/50000]\tLoss: 4.6866\tLR: 4.198210\n",
      "Training Epoch: 42 [49408/50000]\tLoss: 4.6675\tLR: 4.198465\n",
      "Training Epoch: 42 [49536/50000]\tLoss: 4.6505\tLR: 4.198721\n",
      "Training Epoch: 42 [49664/50000]\tLoss: 4.6674\tLR: 4.198977\n",
      "Training Epoch: 42 [49792/50000]\tLoss: 4.6955\tLR: 4.199233\n",
      "Training Epoch: 42 [49920/50000]\tLoss: 4.6949\tLR: 4.199488\n",
      "Training Epoch: 42 [50000/50000]\tLoss: 4.6785\tLR: 4.199744\n",
      "epoch 42 training time consumed: 488.84s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   58882 GB |   58881 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   58701 GB |   58701 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     180 GB |     180 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   58882 GB |   58881 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   58701 GB |   58701 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     180 GB |     180 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   58052 GB |   58052 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   57871 GB |   57871 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     180 GB |     180 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    6243 K  |    6243 K  |\n",
      "|       from large pool |      24    |      65    |    2661 K  |    2661 K  |\n",
      "|       from small pool |     231    |     274    |    3582 K  |    3581 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    6243 K  |    6243 K  |\n",
      "|       from large pool |      24    |      65    |    2661 K  |    2661 K  |\n",
      "|       from small pool |     231    |     274    |    3582 K  |    3581 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      34    |      46    |    3619 K  |    3619 K  |\n",
      "|       from large pool |      10    |      23    |    1279 K  |    1279 K  |\n",
      "|       from small pool |      24    |      35    |    2339 K  |    2339 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 42, Average loss: 0.0371, Accuracy: 0.0100, Time consumed:31.04s\n",
      "\n",
      "Training Epoch: 43 [128/50000]\tLoss: 4.6838\tLR: 0.100000\n",
      "Training Epoch: 43 [256/50000]\tLoss: 4.6764\tLR: 4.200256\n",
      "Training Epoch: 43 [384/50000]\tLoss: 4.7248\tLR: 4.200512\n",
      "Training Epoch: 43 [512/50000]\tLoss: 4.6593\tLR: 4.200767\n",
      "Training Epoch: 43 [640/50000]\tLoss: 4.7375\tLR: 4.201023\n",
      "Training Epoch: 43 [768/50000]\tLoss: 4.7068\tLR: 4.201279\n",
      "Training Epoch: 43 [896/50000]\tLoss: 4.7032\tLR: 4.201535\n",
      "Training Epoch: 43 [1024/50000]\tLoss: 4.6534\tLR: 4.201790\n",
      "Training Epoch: 43 [1152/50000]\tLoss: 4.6642\tLR: 4.202046\n",
      "Training Epoch: 43 [1280/50000]\tLoss: 4.6441\tLR: 4.202302\n",
      "Training Epoch: 43 [1408/50000]\tLoss: 4.7243\tLR: 4.202558\n",
      "Training Epoch: 43 [1536/50000]\tLoss: 4.7060\tLR: 4.202813\n",
      "Training Epoch: 43 [1664/50000]\tLoss: 4.6985\tLR: 4.203069\n",
      "Training Epoch: 43 [1792/50000]\tLoss: 4.6611\tLR: 4.203325\n",
      "Training Epoch: 43 [1920/50000]\tLoss: 4.6549\tLR: 4.203581\n",
      "Training Epoch: 43 [2048/50000]\tLoss: 4.6705\tLR: 4.203836\n",
      "Training Epoch: 43 [2176/50000]\tLoss: 4.7028\tLR: 4.204092\n",
      "Training Epoch: 43 [2304/50000]\tLoss: 4.7154\tLR: 4.204348\n",
      "Training Epoch: 43 [2432/50000]\tLoss: 4.7294\tLR: 4.204604\n",
      "Training Epoch: 43 [2560/50000]\tLoss: 4.7458\tLR: 4.204859\n",
      "Training Epoch: 43 [2688/50000]\tLoss: 4.6879\tLR: 4.205115\n",
      "Training Epoch: 43 [2816/50000]\tLoss: 4.6477\tLR: 4.205371\n",
      "Training Epoch: 43 [2944/50000]\tLoss: 4.7383\tLR: 4.205627\n",
      "Training Epoch: 43 [3072/50000]\tLoss: 4.6420\tLR: 4.205882\n",
      "Training Epoch: 43 [3200/50000]\tLoss: 4.6238\tLR: 4.206138\n",
      "Training Epoch: 43 [3328/50000]\tLoss: 4.6277\tLR: 4.206394\n",
      "Training Epoch: 43 [3456/50000]\tLoss: 4.6967\tLR: 4.206650\n",
      "Training Epoch: 43 [3584/50000]\tLoss: 4.7274\tLR: 4.206905\n",
      "Training Epoch: 43 [3712/50000]\tLoss: 4.7163\tLR: 4.207161\n",
      "Training Epoch: 43 [3840/50000]\tLoss: 4.6343\tLR: 4.207417\n",
      "Training Epoch: 43 [3968/50000]\tLoss: 4.6579\tLR: 4.207673\n",
      "Training Epoch: 43 [4096/50000]\tLoss: 4.6774\tLR: 4.207928\n",
      "Training Epoch: 43 [4224/50000]\tLoss: 4.7184\tLR: 4.208184\n",
      "Training Epoch: 43 [4352/50000]\tLoss: 4.7003\tLR: 4.208440\n",
      "Training Epoch: 43 [4480/50000]\tLoss: 4.7401\tLR: 4.208696\n",
      "Training Epoch: 43 [4608/50000]\tLoss: 4.6550\tLR: 4.208951\n",
      "Training Epoch: 43 [4736/50000]\tLoss: 4.7230\tLR: 4.209207\n",
      "Training Epoch: 43 [4864/50000]\tLoss: 4.7000\tLR: 4.209463\n",
      "Training Epoch: 43 [4992/50000]\tLoss: 4.7352\tLR: 4.209719\n",
      "Training Epoch: 43 [5120/50000]\tLoss: 4.6702\tLR: 4.209974\n",
      "Training Epoch: 43 [5248/50000]\tLoss: 4.7316\tLR: 4.210230\n",
      "Training Epoch: 43 [5376/50000]\tLoss: 4.6609\tLR: 4.210486\n",
      "Training Epoch: 43 [5504/50000]\tLoss: 4.6035\tLR: 4.210742\n",
      "Training Epoch: 43 [5632/50000]\tLoss: 4.7006\tLR: 4.210997\n",
      "Training Epoch: 43 [5760/50000]\tLoss: 4.6858\tLR: 4.211253\n",
      "Training Epoch: 43 [5888/50000]\tLoss: 4.6402\tLR: 4.211509\n",
      "Training Epoch: 43 [6016/50000]\tLoss: 4.7168\tLR: 4.211765\n",
      "Training Epoch: 43 [6144/50000]\tLoss: 4.6623\tLR: 4.212020\n",
      "Training Epoch: 43 [6272/50000]\tLoss: 4.6667\tLR: 4.212276\n",
      "Training Epoch: 43 [6400/50000]\tLoss: 4.6885\tLR: 4.212532\n",
      "Training Epoch: 43 [6528/50000]\tLoss: 4.7103\tLR: 4.212788\n",
      "Training Epoch: 43 [6656/50000]\tLoss: 4.7114\tLR: 4.213043\n",
      "Training Epoch: 43 [6784/50000]\tLoss: 4.7359\tLR: 4.213299\n",
      "Training Epoch: 43 [6912/50000]\tLoss: 4.7277\tLR: 4.213555\n",
      "Training Epoch: 43 [7040/50000]\tLoss: 4.6879\tLR: 4.213811\n",
      "Training Epoch: 43 [7168/50000]\tLoss: 4.6844\tLR: 4.214066\n",
      "Training Epoch: 43 [7296/50000]\tLoss: 4.6052\tLR: 4.214322\n",
      "Training Epoch: 43 [7424/50000]\tLoss: 4.6474\tLR: 4.214578\n",
      "Training Epoch: 43 [7552/50000]\tLoss: 4.6890\tLR: 4.214834\n",
      "Training Epoch: 43 [7680/50000]\tLoss: 4.6660\tLR: 4.215090\n",
      "Training Epoch: 43 [7808/50000]\tLoss: 4.6646\tLR: 4.215345\n",
      "Training Epoch: 43 [7936/50000]\tLoss: 4.7288\tLR: 4.215601\n",
      "Training Epoch: 43 [8064/50000]\tLoss: 4.6473\tLR: 4.215857\n",
      "Training Epoch: 43 [8192/50000]\tLoss: 4.6680\tLR: 4.216113\n",
      "Training Epoch: 43 [8320/50000]\tLoss: 4.7078\tLR: 4.216368\n",
      "Training Epoch: 43 [8448/50000]\tLoss: 4.7239\tLR: 4.216624\n",
      "Training Epoch: 43 [8576/50000]\tLoss: 4.6861\tLR: 4.216880\n",
      "Training Epoch: 43 [8704/50000]\tLoss: 4.6546\tLR: 4.217136\n",
      "Training Epoch: 43 [8832/50000]\tLoss: 4.6867\tLR: 4.217391\n",
      "Training Epoch: 43 [8960/50000]\tLoss: 4.7130\tLR: 4.217647\n",
      "Training Epoch: 43 [9088/50000]\tLoss: 4.7151\tLR: 4.217903\n",
      "Training Epoch: 43 [9216/50000]\tLoss: 4.7298\tLR: 4.218159\n",
      "Training Epoch: 43 [9344/50000]\tLoss: 4.6735\tLR: 4.218414\n",
      "Training Epoch: 43 [9472/50000]\tLoss: 4.6961\tLR: 4.218670\n",
      "Training Epoch: 43 [9600/50000]\tLoss: 4.7142\tLR: 4.218926\n",
      "Training Epoch: 43 [9728/50000]\tLoss: 4.6518\tLR: 4.219182\n",
      "Training Epoch: 43 [9856/50000]\tLoss: 4.7638\tLR: 4.219437\n",
      "Training Epoch: 43 [9984/50000]\tLoss: 4.6741\tLR: 4.219693\n",
      "Training Epoch: 43 [10112/50000]\tLoss: 4.6619\tLR: 4.219949\n",
      "Training Epoch: 43 [10240/50000]\tLoss: 4.6734\tLR: 4.220205\n",
      "Training Epoch: 43 [10368/50000]\tLoss: 4.6639\tLR: 4.220460\n",
      "Training Epoch: 43 [10496/50000]\tLoss: 4.7237\tLR: 4.220716\n",
      "Training Epoch: 43 [10624/50000]\tLoss: 4.6990\tLR: 4.220972\n",
      "Training Epoch: 43 [10752/50000]\tLoss: 4.7195\tLR: 4.221228\n",
      "Training Epoch: 43 [10880/50000]\tLoss: 4.7031\tLR: 4.221483\n",
      "Training Epoch: 43 [11008/50000]\tLoss: 4.6664\tLR: 4.221739\n",
      "Training Epoch: 43 [11136/50000]\tLoss: 4.6384\tLR: 4.221995\n",
      "Training Epoch: 43 [11264/50000]\tLoss: 4.6691\tLR: 4.222251\n",
      "Training Epoch: 43 [11392/50000]\tLoss: 4.6628\tLR: 4.222506\n",
      "Training Epoch: 43 [11520/50000]\tLoss: 4.6139\tLR: 4.222762\n",
      "Training Epoch: 43 [11648/50000]\tLoss: 4.7271\tLR: 4.223018\n",
      "Training Epoch: 43 [11776/50000]\tLoss: 4.5999\tLR: 4.223274\n",
      "Training Epoch: 43 [11904/50000]\tLoss: 4.7143\tLR: 4.223529\n",
      "Training Epoch: 43 [12032/50000]\tLoss: 4.7545\tLR: 4.223785\n",
      "Training Epoch: 43 [12160/50000]\tLoss: 4.6527\tLR: 4.224041\n",
      "Training Epoch: 43 [12288/50000]\tLoss: 4.6039\tLR: 4.224297\n",
      "Training Epoch: 43 [12416/50000]\tLoss: 4.6624\tLR: 4.224552\n",
      "Training Epoch: 43 [12544/50000]\tLoss: 4.7001\tLR: 4.224808\n",
      "Training Epoch: 43 [12672/50000]\tLoss: 4.7458\tLR: 4.225064\n",
      "Training Epoch: 43 [12800/50000]\tLoss: 4.6638\tLR: 4.225320\n",
      "Training Epoch: 43 [12928/50000]\tLoss: 4.7044\tLR: 4.225575\n",
      "Training Epoch: 43 [13056/50000]\tLoss: 4.7224\tLR: 4.225831\n",
      "Training Epoch: 43 [13184/50000]\tLoss: 4.6938\tLR: 4.226087\n",
      "Training Epoch: 43 [13312/50000]\tLoss: 4.7408\tLR: 4.226343\n",
      "Training Epoch: 43 [13440/50000]\tLoss: 4.7052\tLR: 4.226598\n",
      "Training Epoch: 43 [13568/50000]\tLoss: 4.6220\tLR: 4.226854\n",
      "Training Epoch: 43 [13696/50000]\tLoss: 4.6878\tLR: 4.227110\n",
      "Training Epoch: 43 [13824/50000]\tLoss: 4.6530\tLR: 4.227366\n",
      "Training Epoch: 43 [13952/50000]\tLoss: 4.6814\tLR: 4.227621\n",
      "Training Epoch: 43 [14080/50000]\tLoss: 4.6028\tLR: 4.227877\n",
      "Training Epoch: 43 [14208/50000]\tLoss: 4.6637\tLR: 4.228133\n",
      "Training Epoch: 43 [14336/50000]\tLoss: 4.6510\tLR: 4.228389\n",
      "Training Epoch: 43 [14464/50000]\tLoss: 4.6413\tLR: 4.228645\n",
      "Training Epoch: 43 [14592/50000]\tLoss: 4.6756\tLR: 4.228900\n",
      "Training Epoch: 43 [14720/50000]\tLoss: 4.6300\tLR: 4.229156\n",
      "Training Epoch: 43 [14848/50000]\tLoss: 4.6257\tLR: 4.229412\n",
      "Training Epoch: 43 [14976/50000]\tLoss: 4.6884\tLR: 4.229668\n",
      "Training Epoch: 43 [15104/50000]\tLoss: 4.6620\tLR: 4.229923\n",
      "Training Epoch: 43 [15232/50000]\tLoss: 4.6593\tLR: 4.230179\n",
      "Training Epoch: 43 [15360/50000]\tLoss: 4.6825\tLR: 4.230435\n",
      "Training Epoch: 43 [15488/50000]\tLoss: 4.7709\tLR: 4.230691\n",
      "Training Epoch: 43 [15616/50000]\tLoss: 4.6154\tLR: 4.230946\n",
      "Training Epoch: 43 [15744/50000]\tLoss: 4.6904\tLR: 4.231202\n",
      "Training Epoch: 43 [15872/50000]\tLoss: 4.7149\tLR: 4.231458\n",
      "Training Epoch: 43 [16000/50000]\tLoss: 4.6926\tLR: 4.231714\n",
      "Training Epoch: 43 [16128/50000]\tLoss: 4.6674\tLR: 4.231969\n",
      "Training Epoch: 43 [16256/50000]\tLoss: 4.6465\tLR: 4.232225\n",
      "Training Epoch: 43 [16384/50000]\tLoss: 4.7066\tLR: 4.232481\n",
      "Training Epoch: 43 [16512/50000]\tLoss: 4.7417\tLR: 4.232737\n",
      "Training Epoch: 43 [16640/50000]\tLoss: 4.6855\tLR: 4.232992\n",
      "Training Epoch: 43 [16768/50000]\tLoss: 4.6275\tLR: 4.233248\n",
      "Training Epoch: 43 [16896/50000]\tLoss: 4.6828\tLR: 4.233504\n",
      "Training Epoch: 43 [17024/50000]\tLoss: 4.7031\tLR: 4.233760\n",
      "Training Epoch: 43 [17152/50000]\tLoss: 4.6631\tLR: 4.234015\n",
      "Training Epoch: 43 [17280/50000]\tLoss: 4.6836\tLR: 4.234271\n",
      "Training Epoch: 43 [17408/50000]\tLoss: 4.6467\tLR: 4.234527\n",
      "Training Epoch: 43 [17536/50000]\tLoss: 4.6669\tLR: 4.234783\n",
      "Training Epoch: 43 [17664/50000]\tLoss: 4.7272\tLR: 4.235038\n",
      "Training Epoch: 43 [17792/50000]\tLoss: 4.7671\tLR: 4.235294\n",
      "Training Epoch: 43 [17920/50000]\tLoss: 4.7117\tLR: 4.235550\n",
      "Training Epoch: 43 [18048/50000]\tLoss: 4.7068\tLR: 4.235806\n",
      "Training Epoch: 43 [18176/50000]\tLoss: 4.7019\tLR: 4.236061\n",
      "Training Epoch: 43 [18304/50000]\tLoss: 4.6576\tLR: 4.236317\n",
      "Training Epoch: 43 [18432/50000]\tLoss: 4.6742\tLR: 4.236573\n",
      "Training Epoch: 43 [18560/50000]\tLoss: 4.6914\tLR: 4.236829\n",
      "Training Epoch: 43 [18688/50000]\tLoss: 4.7340\tLR: 4.237084\n",
      "Training Epoch: 43 [18816/50000]\tLoss: 4.6656\tLR: 4.237340\n",
      "Training Epoch: 43 [18944/50000]\tLoss: 4.6329\tLR: 4.237596\n",
      "Training Epoch: 43 [19072/50000]\tLoss: 4.7019\tLR: 4.237852\n",
      "Training Epoch: 43 [19200/50000]\tLoss: 4.6878\tLR: 4.238107\n",
      "Training Epoch: 43 [19328/50000]\tLoss: 4.6868\tLR: 4.238363\n",
      "Training Epoch: 43 [19456/50000]\tLoss: 4.7001\tLR: 4.238619\n",
      "Training Epoch: 43 [19584/50000]\tLoss: 4.6824\tLR: 4.238875\n",
      "Training Epoch: 43 [19712/50000]\tLoss: 4.7680\tLR: 4.239130\n",
      "Training Epoch: 43 [19840/50000]\tLoss: 4.6643\tLR: 4.239386\n",
      "Training Epoch: 43 [19968/50000]\tLoss: 4.6874\tLR: 4.239642\n",
      "Training Epoch: 43 [20096/50000]\tLoss: 4.6939\tLR: 4.239898\n",
      "Training Epoch: 43 [20224/50000]\tLoss: 4.6894\tLR: 4.240153\n",
      "Training Epoch: 43 [20352/50000]\tLoss: 4.6419\tLR: 4.240409\n",
      "Training Epoch: 43 [20480/50000]\tLoss: 4.6393\tLR: 4.240665\n",
      "Training Epoch: 43 [20608/50000]\tLoss: 4.6869\tLR: 4.240921\n",
      "Training Epoch: 43 [20736/50000]\tLoss: 4.6469\tLR: 4.241176\n",
      "Training Epoch: 43 [20864/50000]\tLoss: 4.7670\tLR: 4.241432\n",
      "Training Epoch: 43 [20992/50000]\tLoss: 4.7069\tLR: 4.241688\n",
      "Training Epoch: 43 [21120/50000]\tLoss: 4.6585\tLR: 4.241944\n",
      "Training Epoch: 43 [21248/50000]\tLoss: 4.6917\tLR: 4.242199\n",
      "Training Epoch: 43 [21376/50000]\tLoss: 4.7433\tLR: 4.242455\n",
      "Training Epoch: 43 [21504/50000]\tLoss: 4.7171\tLR: 4.242711\n",
      "Training Epoch: 43 [21632/50000]\tLoss: 4.6969\tLR: 4.242967\n",
      "Training Epoch: 43 [21760/50000]\tLoss: 4.7264\tLR: 4.243223\n",
      "Training Epoch: 43 [21888/50000]\tLoss: 4.7249\tLR: 4.243478\n",
      "Training Epoch: 43 [22016/50000]\tLoss: 4.6276\tLR: 4.243734\n",
      "Training Epoch: 43 [22144/50000]\tLoss: 4.5922\tLR: 4.243990\n",
      "Training Epoch: 43 [22272/50000]\tLoss: 4.6842\tLR: 4.244246\n",
      "Training Epoch: 43 [22400/50000]\tLoss: 4.6589\tLR: 4.244501\n",
      "Training Epoch: 43 [22528/50000]\tLoss: 4.7530\tLR: 4.244757\n",
      "Training Epoch: 43 [22656/50000]\tLoss: 4.6743\tLR: 4.245013\n",
      "Training Epoch: 43 [22784/50000]\tLoss: 4.6707\tLR: 4.245269\n",
      "Training Epoch: 43 [22912/50000]\tLoss: 4.6748\tLR: 4.245524\n",
      "Training Epoch: 43 [23040/50000]\tLoss: 4.6359\tLR: 4.245780\n",
      "Training Epoch: 43 [23168/50000]\tLoss: 4.7102\tLR: 4.246036\n",
      "Training Epoch: 43 [23296/50000]\tLoss: 4.7155\tLR: 4.246292\n",
      "Training Epoch: 43 [23424/50000]\tLoss: 4.6873\tLR: 4.246547\n",
      "Training Epoch: 43 [23552/50000]\tLoss: 4.6992\tLR: 4.246803\n",
      "Training Epoch: 43 [23680/50000]\tLoss: 4.7283\tLR: 4.247059\n",
      "Training Epoch: 43 [23808/50000]\tLoss: 4.6809\tLR: 4.247315\n",
      "Training Epoch: 43 [23936/50000]\tLoss: 4.6334\tLR: 4.247570\n",
      "Training Epoch: 43 [24064/50000]\tLoss: 4.6490\tLR: 4.247826\n",
      "Training Epoch: 43 [24192/50000]\tLoss: 4.6393\tLR: 4.248082\n",
      "Training Epoch: 43 [24320/50000]\tLoss: 4.6342\tLR: 4.248338\n",
      "Training Epoch: 43 [24448/50000]\tLoss: 4.7151\tLR: 4.248593\n",
      "Training Epoch: 43 [24576/50000]\tLoss: 4.6605\tLR: 4.248849\n",
      "Training Epoch: 43 [24704/50000]\tLoss: 4.7223\tLR: 4.249105\n",
      "Training Epoch: 43 [24832/50000]\tLoss: 4.6914\tLR: 4.249361\n",
      "Training Epoch: 43 [24960/50000]\tLoss: 4.6904\tLR: 4.249616\n",
      "Training Epoch: 43 [25088/50000]\tLoss: 4.7478\tLR: 4.249872\n",
      "Training Epoch: 43 [25216/50000]\tLoss: 4.6863\tLR: 4.250128\n",
      "Training Epoch: 43 [25344/50000]\tLoss: 4.7143\tLR: 4.250384\n",
      "Training Epoch: 43 [25472/50000]\tLoss: 4.6674\tLR: 4.250639\n",
      "Training Epoch: 43 [25600/50000]\tLoss: 4.5900\tLR: 4.250895\n",
      "Training Epoch: 43 [25728/50000]\tLoss: 4.6606\tLR: 4.251151\n",
      "Training Epoch: 43 [25856/50000]\tLoss: 4.7091\tLR: 4.251407\n",
      "Training Epoch: 43 [25984/50000]\tLoss: 4.6323\tLR: 4.251662\n",
      "Training Epoch: 43 [26112/50000]\tLoss: 4.7047\tLR: 4.251918\n",
      "Training Epoch: 43 [26240/50000]\tLoss: 4.7055\tLR: 4.252174\n",
      "Training Epoch: 43 [26368/50000]\tLoss: 4.7064\tLR: 4.252430\n",
      "Training Epoch: 43 [26496/50000]\tLoss: 4.7423\tLR: 4.252685\n",
      "Training Epoch: 43 [26624/50000]\tLoss: 4.7168\tLR: 4.252941\n",
      "Training Epoch: 43 [26752/50000]\tLoss: 4.6769\tLR: 4.253197\n",
      "Training Epoch: 43 [26880/50000]\tLoss: 4.6536\tLR: 4.253453\n",
      "Training Epoch: 43 [27008/50000]\tLoss: 4.7140\tLR: 4.253708\n",
      "Training Epoch: 43 [27136/50000]\tLoss: 4.6744\tLR: 4.253964\n",
      "Training Epoch: 43 [27264/50000]\tLoss: 4.6645\tLR: 4.254220\n",
      "Training Epoch: 43 [27392/50000]\tLoss: 4.7139\tLR: 4.254476\n",
      "Training Epoch: 43 [27520/50000]\tLoss: 4.6843\tLR: 4.254731\n",
      "Training Epoch: 43 [27648/50000]\tLoss: 4.7192\tLR: 4.254987\n",
      "Training Epoch: 43 [27776/50000]\tLoss: 4.6963\tLR: 4.255243\n",
      "Training Epoch: 43 [27904/50000]\tLoss: 4.6644\tLR: 4.255499\n",
      "Training Epoch: 43 [28032/50000]\tLoss: 4.7512\tLR: 4.255754\n",
      "Training Epoch: 43 [28160/50000]\tLoss: 4.7218\tLR: 4.256010\n",
      "Training Epoch: 43 [28288/50000]\tLoss: 4.7011\tLR: 4.256266\n",
      "Training Epoch: 43 [28416/50000]\tLoss: 4.6566\tLR: 4.256522\n",
      "Training Epoch: 43 [28544/50000]\tLoss: 4.7118\tLR: 4.256777\n",
      "Training Epoch: 43 [28672/50000]\tLoss: 4.6785\tLR: 4.257033\n",
      "Training Epoch: 43 [28800/50000]\tLoss: 4.7234\tLR: 4.257289\n",
      "Training Epoch: 43 [28928/50000]\tLoss: 4.6791\tLR: 4.257545\n",
      "Training Epoch: 43 [29056/50000]\tLoss: 4.7109\tLR: 4.257801\n",
      "Training Epoch: 43 [29184/50000]\tLoss: 4.7122\tLR: 4.258056\n",
      "Training Epoch: 43 [29312/50000]\tLoss: 4.6501\tLR: 4.258312\n",
      "Training Epoch: 43 [29440/50000]\tLoss: 4.6170\tLR: 4.258568\n",
      "Training Epoch: 43 [29568/50000]\tLoss: 4.7192\tLR: 4.258824\n",
      "Training Epoch: 43 [29696/50000]\tLoss: 4.7002\tLR: 4.259079\n",
      "Training Epoch: 43 [29824/50000]\tLoss: 4.6502\tLR: 4.259335\n",
      "Training Epoch: 43 [29952/50000]\tLoss: 4.7141\tLR: 4.259591\n",
      "Training Epoch: 43 [30080/50000]\tLoss: 4.6837\tLR: 4.259847\n",
      "Training Epoch: 43 [30208/50000]\tLoss: 4.6382\tLR: 4.260102\n",
      "Training Epoch: 43 [30336/50000]\tLoss: 4.7151\tLR: 4.260358\n",
      "Training Epoch: 43 [30464/50000]\tLoss: 4.6766\tLR: 4.260614\n",
      "Training Epoch: 43 [30592/50000]\tLoss: 4.6821\tLR: 4.260870\n",
      "Training Epoch: 43 [30720/50000]\tLoss: 4.7127\tLR: 4.261125\n",
      "Training Epoch: 43 [30848/50000]\tLoss: 4.6765\tLR: 4.261381\n",
      "Training Epoch: 43 [30976/50000]\tLoss: 4.6392\tLR: 4.261637\n",
      "Training Epoch: 43 [31104/50000]\tLoss: 4.6740\tLR: 4.261893\n",
      "Training Epoch: 43 [31232/50000]\tLoss: 4.7109\tLR: 4.262148\n",
      "Training Epoch: 43 [31360/50000]\tLoss: 4.6806\tLR: 4.262404\n",
      "Training Epoch: 43 [31488/50000]\tLoss: 4.7157\tLR: 4.262660\n",
      "Training Epoch: 43 [31616/50000]\tLoss: 4.6669\tLR: 4.262916\n",
      "Training Epoch: 43 [31744/50000]\tLoss: 4.6365\tLR: 4.263171\n",
      "Training Epoch: 43 [31872/50000]\tLoss: 4.7039\tLR: 4.263427\n",
      "Training Epoch: 43 [32000/50000]\tLoss: 4.6355\tLR: 4.263683\n",
      "Training Epoch: 43 [32128/50000]\tLoss: 4.6617\tLR: 4.263939\n",
      "Training Epoch: 43 [32256/50000]\tLoss: 4.7232\tLR: 4.264194\n",
      "Training Epoch: 43 [32384/50000]\tLoss: 4.6841\tLR: 4.264450\n",
      "Training Epoch: 43 [32512/50000]\tLoss: 4.7617\tLR: 4.264706\n",
      "Training Epoch: 43 [32640/50000]\tLoss: 4.7191\tLR: 4.264962\n",
      "Training Epoch: 43 [32768/50000]\tLoss: 4.6911\tLR: 4.265217\n",
      "Training Epoch: 43 [32896/50000]\tLoss: 4.7118\tLR: 4.265473\n",
      "Training Epoch: 43 [33024/50000]\tLoss: 4.7636\tLR: 4.265729\n",
      "Training Epoch: 43 [33152/50000]\tLoss: 4.6687\tLR: 4.265985\n",
      "Training Epoch: 43 [33280/50000]\tLoss: 4.6692\tLR: 4.266240\n",
      "Training Epoch: 43 [33408/50000]\tLoss: 4.6862\tLR: 4.266496\n",
      "Training Epoch: 43 [33536/50000]\tLoss: 4.6615\tLR: 4.266752\n",
      "Training Epoch: 43 [33664/50000]\tLoss: 4.6676\tLR: 4.267008\n",
      "Training Epoch: 43 [33792/50000]\tLoss: 4.6600\tLR: 4.267263\n",
      "Training Epoch: 43 [33920/50000]\tLoss: 4.7319\tLR: 4.267519\n",
      "Training Epoch: 43 [34048/50000]\tLoss: 4.6781\tLR: 4.267775\n",
      "Training Epoch: 43 [34176/50000]\tLoss: 4.6401\tLR: 4.268031\n",
      "Training Epoch: 43 [34304/50000]\tLoss: 4.6432\tLR: 4.268286\n",
      "Training Epoch: 43 [34432/50000]\tLoss: 4.7349\tLR: 4.268542\n",
      "Training Epoch: 43 [34560/50000]\tLoss: 4.7445\tLR: 4.268798\n",
      "Training Epoch: 43 [34688/50000]\tLoss: 4.6699\tLR: 4.269054\n",
      "Training Epoch: 43 [34816/50000]\tLoss: 4.7203\tLR: 4.269309\n",
      "Training Epoch: 43 [34944/50000]\tLoss: 4.7272\tLR: 4.269565\n",
      "Training Epoch: 43 [35072/50000]\tLoss: 4.7618\tLR: 4.269821\n",
      "Training Epoch: 43 [35200/50000]\tLoss: 4.6192\tLR: 4.270077\n",
      "Training Epoch: 43 [35328/50000]\tLoss: 4.7012\tLR: 4.270332\n",
      "Training Epoch: 43 [35456/50000]\tLoss: 4.6808\tLR: 4.270588\n",
      "Training Epoch: 43 [35584/50000]\tLoss: 4.6893\tLR: 4.270844\n",
      "Training Epoch: 43 [35712/50000]\tLoss: 4.7363\tLR: 4.271100\n",
      "Training Epoch: 43 [35840/50000]\tLoss: 4.6514\tLR: 4.271355\n",
      "Training Epoch: 43 [35968/50000]\tLoss: 4.6691\tLR: 4.271611\n",
      "Training Epoch: 43 [36096/50000]\tLoss: 4.7121\tLR: 4.271867\n",
      "Training Epoch: 43 [36224/50000]\tLoss: 4.6790\tLR: 4.272123\n",
      "Training Epoch: 43 [36352/50000]\tLoss: 4.5956\tLR: 4.272379\n",
      "Training Epoch: 43 [36480/50000]\tLoss: 4.6394\tLR: 4.272634\n",
      "Training Epoch: 43 [36608/50000]\tLoss: 4.6914\tLR: 4.272890\n",
      "Training Epoch: 43 [36736/50000]\tLoss: 4.7241\tLR: 4.273146\n",
      "Training Epoch: 43 [36864/50000]\tLoss: 4.7163\tLR: 4.273402\n",
      "Training Epoch: 43 [36992/50000]\tLoss: 4.6477\tLR: 4.273657\n",
      "Training Epoch: 43 [37120/50000]\tLoss: 4.6624\tLR: 4.273913\n",
      "Training Epoch: 43 [37248/50000]\tLoss: 4.7553\tLR: 4.274169\n",
      "Training Epoch: 43 [37376/50000]\tLoss: 4.6651\tLR: 4.274425\n",
      "Training Epoch: 43 [37504/50000]\tLoss: 4.6763\tLR: 4.274680\n",
      "Training Epoch: 43 [37632/50000]\tLoss: 4.6663\tLR: 4.274936\n",
      "Training Epoch: 43 [37760/50000]\tLoss: 4.6191\tLR: 4.275192\n",
      "Training Epoch: 43 [37888/50000]\tLoss: 4.6736\tLR: 4.275448\n",
      "Training Epoch: 43 [38016/50000]\tLoss: 4.7333\tLR: 4.275703\n",
      "Training Epoch: 43 [38144/50000]\tLoss: 4.6499\tLR: 4.275959\n",
      "Training Epoch: 43 [38272/50000]\tLoss: 4.7539\tLR: 4.276215\n",
      "Training Epoch: 43 [38400/50000]\tLoss: 4.6464\tLR: 4.276471\n",
      "Training Epoch: 43 [38528/50000]\tLoss: 4.7022\tLR: 4.276726\n",
      "Training Epoch: 43 [38656/50000]\tLoss: 4.5603\tLR: 4.276982\n",
      "Training Epoch: 43 [38784/50000]\tLoss: 4.6636\tLR: 4.277238\n",
      "Training Epoch: 43 [38912/50000]\tLoss: 4.6644\tLR: 4.277494\n",
      "Training Epoch: 43 [39040/50000]\tLoss: 4.6990\tLR: 4.277749\n",
      "Training Epoch: 43 [39168/50000]\tLoss: 4.6277\tLR: 4.278005\n",
      "Training Epoch: 43 [39296/50000]\tLoss: 4.6759\tLR: 4.278261\n",
      "Training Epoch: 43 [39424/50000]\tLoss: 4.6684\tLR: 4.278517\n",
      "Training Epoch: 43 [39552/50000]\tLoss: 4.6864\tLR: 4.278772\n",
      "Training Epoch: 43 [39680/50000]\tLoss: 4.6670\tLR: 4.279028\n",
      "Training Epoch: 43 [39808/50000]\tLoss: 4.6732\tLR: 4.279284\n",
      "Training Epoch: 43 [39936/50000]\tLoss: 4.6481\tLR: 4.279540\n",
      "Training Epoch: 43 [40064/50000]\tLoss: 4.6813\tLR: 4.279795\n",
      "Training Epoch: 43 [40192/50000]\tLoss: 4.6083\tLR: 4.280051\n",
      "Training Epoch: 43 [40320/50000]\tLoss: 4.6780\tLR: 4.280307\n",
      "Training Epoch: 43 [40448/50000]\tLoss: 4.6488\tLR: 4.280563\n",
      "Training Epoch: 43 [40576/50000]\tLoss: 4.6273\tLR: 4.280818\n",
      "Training Epoch: 43 [40704/50000]\tLoss: 4.7077\tLR: 4.281074\n",
      "Training Epoch: 43 [40832/50000]\tLoss: 4.7055\tLR: 4.281330\n",
      "Training Epoch: 43 [40960/50000]\tLoss: 4.6604\tLR: 4.281586\n",
      "Training Epoch: 43 [41088/50000]\tLoss: 4.6682\tLR: 4.281841\n",
      "Training Epoch: 43 [41216/50000]\tLoss: 4.6695\tLR: 4.282097\n",
      "Training Epoch: 43 [41344/50000]\tLoss: 4.6793\tLR: 4.282353\n",
      "Training Epoch: 43 [41472/50000]\tLoss: 4.6685\tLR: 4.282609\n",
      "Training Epoch: 43 [41600/50000]\tLoss: 4.6751\tLR: 4.282864\n",
      "Training Epoch: 43 [41728/50000]\tLoss: 4.6601\tLR: 4.283120\n",
      "Training Epoch: 43 [41856/50000]\tLoss: 4.7134\tLR: 4.283376\n",
      "Training Epoch: 43 [41984/50000]\tLoss: 4.6696\tLR: 4.283632\n",
      "Training Epoch: 43 [42112/50000]\tLoss: 4.6219\tLR: 4.283887\n",
      "Training Epoch: 43 [42240/50000]\tLoss: 4.7043\tLR: 4.284143\n",
      "Training Epoch: 43 [42368/50000]\tLoss: 4.6406\tLR: 4.284399\n",
      "Training Epoch: 43 [42496/50000]\tLoss: 4.6215\tLR: 4.284655\n",
      "Training Epoch: 43 [42624/50000]\tLoss: 4.6946\tLR: 4.284910\n",
      "Training Epoch: 43 [42752/50000]\tLoss: 4.7457\tLR: 4.285166\n",
      "Training Epoch: 43 [42880/50000]\tLoss: 4.7031\tLR: 4.285422\n",
      "Training Epoch: 43 [43008/50000]\tLoss: 4.6841\tLR: 4.285678\n",
      "Training Epoch: 43 [43136/50000]\tLoss: 4.6503\tLR: 4.285934\n",
      "Training Epoch: 43 [43264/50000]\tLoss: 4.6889\tLR: 4.286189\n",
      "Training Epoch: 43 [43392/50000]\tLoss: 4.7048\tLR: 4.286445\n",
      "Training Epoch: 43 [43520/50000]\tLoss: 4.7523\tLR: 4.286701\n",
      "Training Epoch: 43 [43648/50000]\tLoss: 4.7127\tLR: 4.286957\n",
      "Training Epoch: 43 [43776/50000]\tLoss: 4.6801\tLR: 4.287212\n",
      "Training Epoch: 43 [43904/50000]\tLoss: 4.6997\tLR: 4.287468\n",
      "Training Epoch: 43 [44032/50000]\tLoss: 4.6569\tLR: 4.287724\n",
      "Training Epoch: 43 [44160/50000]\tLoss: 4.6963\tLR: 4.287980\n",
      "Training Epoch: 43 [44288/50000]\tLoss: 4.6274\tLR: 4.288235\n",
      "Training Epoch: 43 [44416/50000]\tLoss: 4.7083\tLR: 4.288491\n",
      "Training Epoch: 43 [44544/50000]\tLoss: 4.7702\tLR: 4.288747\n",
      "Training Epoch: 43 [44672/50000]\tLoss: 4.7353\tLR: 4.289003\n",
      "Training Epoch: 43 [44800/50000]\tLoss: 4.6929\tLR: 4.289258\n",
      "Training Epoch: 43 [44928/50000]\tLoss: 4.6650\tLR: 4.289514\n",
      "Training Epoch: 43 [45056/50000]\tLoss: 4.7171\tLR: 4.289770\n",
      "Training Epoch: 43 [45184/50000]\tLoss: 4.6691\tLR: 4.290026\n",
      "Training Epoch: 43 [45312/50000]\tLoss: 4.7066\tLR: 4.290281\n",
      "Training Epoch: 43 [45440/50000]\tLoss: 4.6623\tLR: 4.290537\n",
      "Training Epoch: 43 [45568/50000]\tLoss: 4.6557\tLR: 4.290793\n",
      "Training Epoch: 43 [45696/50000]\tLoss: 4.6818\tLR: 4.291049\n",
      "Training Epoch: 43 [45824/50000]\tLoss: 4.6204\tLR: 4.291304\n",
      "Training Epoch: 43 [45952/50000]\tLoss: 4.6752\tLR: 4.291560\n",
      "Training Epoch: 43 [46080/50000]\tLoss: 4.6638\tLR: 4.291816\n",
      "Training Epoch: 43 [46208/50000]\tLoss: 4.7582\tLR: 4.292072\n",
      "Training Epoch: 43 [46336/50000]\tLoss: 4.7701\tLR: 4.292327\n",
      "Training Epoch: 43 [46464/50000]\tLoss: 4.7771\tLR: 4.292583\n",
      "Training Epoch: 43 [46592/50000]\tLoss: 4.7192\tLR: 4.292839\n",
      "Training Epoch: 43 [46720/50000]\tLoss: 4.6643\tLR: 4.293095\n",
      "Training Epoch: 43 [46848/50000]\tLoss: 4.6764\tLR: 4.293350\n",
      "Training Epoch: 43 [46976/50000]\tLoss: 4.7276\tLR: 4.293606\n",
      "Training Epoch: 43 [47104/50000]\tLoss: 4.6920\tLR: 4.293862\n",
      "Training Epoch: 43 [47232/50000]\tLoss: 4.6610\tLR: 4.294118\n",
      "Training Epoch: 43 [47360/50000]\tLoss: 4.6957\tLR: 4.294373\n",
      "Training Epoch: 43 [47488/50000]\tLoss: 4.7572\tLR: 4.294629\n",
      "Training Epoch: 43 [47616/50000]\tLoss: 4.7386\tLR: 4.294885\n",
      "Training Epoch: 43 [47744/50000]\tLoss: 4.6979\tLR: 4.295141\n",
      "Training Epoch: 43 [47872/50000]\tLoss: 4.6802\tLR: 4.295396\n",
      "Training Epoch: 43 [48000/50000]\tLoss: 4.6563\tLR: 4.295652\n",
      "Training Epoch: 43 [48128/50000]\tLoss: 4.6779\tLR: 4.295908\n",
      "Training Epoch: 43 [48256/50000]\tLoss: 4.7261\tLR: 4.296164\n",
      "Training Epoch: 43 [48384/50000]\tLoss: 4.7086\tLR: 4.296419\n",
      "Training Epoch: 43 [48512/50000]\tLoss: 4.6626\tLR: 4.296675\n",
      "Training Epoch: 43 [48640/50000]\tLoss: 4.6682\tLR: 4.296931\n",
      "Training Epoch: 43 [48768/50000]\tLoss: 4.6423\tLR: 4.297187\n",
      "Training Epoch: 43 [48896/50000]\tLoss: 4.6866\tLR: 4.297442\n",
      "Training Epoch: 43 [49024/50000]\tLoss: 4.7177\tLR: 4.297698\n",
      "Training Epoch: 43 [49152/50000]\tLoss: 4.6295\tLR: 4.297954\n",
      "Training Epoch: 43 [49280/50000]\tLoss: 4.6585\tLR: 4.298210\n",
      "Training Epoch: 43 [49408/50000]\tLoss: 4.6667\tLR: 4.298465\n",
      "Training Epoch: 43 [49536/50000]\tLoss: 4.7142\tLR: 4.298721\n",
      "Training Epoch: 43 [49664/50000]\tLoss: 4.6635\tLR: 4.298977\n",
      "Training Epoch: 43 [49792/50000]\tLoss: 4.6654\tLR: 4.299233\n",
      "Training Epoch: 43 [49920/50000]\tLoss: 4.6219\tLR: 4.299488\n",
      "Training Epoch: 43 [50000/50000]\tLoss: 4.6302\tLR: 4.299744\n",
      "epoch 43 training time consumed: 488.87s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   60283 GB |   60283 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   60098 GB |   60098 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     185 GB |     185 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   60283 GB |   60283 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   60098 GB |   60098 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     185 GB |     185 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   59435 GB |   59434 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   59249 GB |   59249 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     185 GB |     185 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    6392 K  |    6392 K  |\n",
      "|       from large pool |      24    |      65    |    2724 K  |    2724 K  |\n",
      "|       from small pool |     231    |     274    |    3667 K  |    3667 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    6392 K  |    6392 K  |\n",
      "|       from large pool |      24    |      65    |    2724 K  |    2724 K  |\n",
      "|       from small pool |     231    |     274    |    3667 K  |    3667 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      46    |    3705 K  |    3705 K  |\n",
      "|       from large pool |      10    |      23    |    1309 K  |    1309 K  |\n",
      "|       from small pool |      26    |      35    |    2395 K  |    2395 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 43, Average loss: 0.0370, Accuracy: 0.0100, Time consumed:31.07s\n",
      "\n",
      "Training Epoch: 44 [128/50000]\tLoss: 4.6353\tLR: 0.100000\n",
      "Training Epoch: 44 [256/50000]\tLoss: 4.6597\tLR: 4.300256\n",
      "Training Epoch: 44 [384/50000]\tLoss: 4.7170\tLR: 4.300512\n",
      "Training Epoch: 44 [512/50000]\tLoss: 4.6532\tLR: 4.300767\n",
      "Training Epoch: 44 [640/50000]\tLoss: 4.6455\tLR: 4.301023\n",
      "Training Epoch: 44 [768/50000]\tLoss: 4.6492\tLR: 4.301279\n",
      "Training Epoch: 44 [896/50000]\tLoss: 4.6552\tLR: 4.301535\n",
      "Training Epoch: 44 [1024/50000]\tLoss: 4.6894\tLR: 4.301790\n",
      "Training Epoch: 44 [1152/50000]\tLoss: 4.6798\tLR: 4.302046\n",
      "Training Epoch: 44 [1280/50000]\tLoss: 4.7075\tLR: 4.302302\n",
      "Training Epoch: 44 [1408/50000]\tLoss: 4.6569\tLR: 4.302558\n",
      "Training Epoch: 44 [1536/50000]\tLoss: 4.7035\tLR: 4.302813\n",
      "Training Epoch: 44 [1664/50000]\tLoss: 4.6965\tLR: 4.303069\n",
      "Training Epoch: 44 [1792/50000]\tLoss: 4.6163\tLR: 4.303325\n",
      "Training Epoch: 44 [1920/50000]\tLoss: 4.6491\tLR: 4.303581\n",
      "Training Epoch: 44 [2048/50000]\tLoss: 4.7268\tLR: 4.303836\n",
      "Training Epoch: 44 [2176/50000]\tLoss: 4.6325\tLR: 4.304092\n",
      "Training Epoch: 44 [2304/50000]\tLoss: 4.6470\tLR: 4.304348\n",
      "Training Epoch: 44 [2432/50000]\tLoss: 4.6997\tLR: 4.304604\n",
      "Training Epoch: 44 [2560/50000]\tLoss: 4.6366\tLR: 4.304859\n",
      "Training Epoch: 44 [2688/50000]\tLoss: 4.6715\tLR: 4.305115\n",
      "Training Epoch: 44 [2816/50000]\tLoss: 4.6788\tLR: 4.305371\n",
      "Training Epoch: 44 [2944/50000]\tLoss: 4.6392\tLR: 4.305627\n",
      "Training Epoch: 44 [3072/50000]\tLoss: 4.6805\tLR: 4.305882\n",
      "Training Epoch: 44 [3200/50000]\tLoss: 4.6741\tLR: 4.306138\n",
      "Training Epoch: 44 [3328/50000]\tLoss: 4.6685\tLR: 4.306394\n",
      "Training Epoch: 44 [3456/50000]\tLoss: 4.7428\tLR: 4.306650\n",
      "Training Epoch: 44 [3584/50000]\tLoss: 4.6887\tLR: 4.306905\n",
      "Training Epoch: 44 [3712/50000]\tLoss: 4.6485\tLR: 4.307161\n",
      "Training Epoch: 44 [3840/50000]\tLoss: 4.6969\tLR: 4.307417\n",
      "Training Epoch: 44 [3968/50000]\tLoss: 4.6892\tLR: 4.307673\n",
      "Training Epoch: 44 [4096/50000]\tLoss: 4.7001\tLR: 4.307928\n",
      "Training Epoch: 44 [4224/50000]\tLoss: 4.7509\tLR: 4.308184\n",
      "Training Epoch: 44 [4352/50000]\tLoss: 4.7325\tLR: 4.308440\n",
      "Training Epoch: 44 [4480/50000]\tLoss: 4.6357\tLR: 4.308696\n",
      "Training Epoch: 44 [4608/50000]\tLoss: 4.6816\tLR: 4.308951\n",
      "Training Epoch: 44 [4736/50000]\tLoss: 4.6660\tLR: 4.309207\n",
      "Training Epoch: 44 [4864/50000]\tLoss: 4.7098\tLR: 4.309463\n",
      "Training Epoch: 44 [4992/50000]\tLoss: 4.6906\tLR: 4.309719\n",
      "Training Epoch: 44 [5120/50000]\tLoss: 4.6521\tLR: 4.309974\n",
      "Training Epoch: 44 [5248/50000]\tLoss: 4.6821\tLR: 4.310230\n",
      "Training Epoch: 44 [5376/50000]\tLoss: 4.6507\tLR: 4.310486\n",
      "Training Epoch: 44 [5504/50000]\tLoss: 4.7135\tLR: 4.310742\n",
      "Training Epoch: 44 [5632/50000]\tLoss: 4.6814\tLR: 4.310997\n",
      "Training Epoch: 44 [5760/50000]\tLoss: 4.7852\tLR: 4.311253\n",
      "Training Epoch: 44 [5888/50000]\tLoss: 4.6774\tLR: 4.311509\n",
      "Training Epoch: 44 [6016/50000]\tLoss: 4.6483\tLR: 4.311765\n",
      "Training Epoch: 44 [6144/50000]\tLoss: 4.6975\tLR: 4.312020\n",
      "Training Epoch: 44 [6272/50000]\tLoss: 4.7187\tLR: 4.312276\n",
      "Training Epoch: 44 [6400/50000]\tLoss: 4.6451\tLR: 4.312532\n",
      "Training Epoch: 44 [6528/50000]\tLoss: 4.6372\tLR: 4.312788\n",
      "Training Epoch: 44 [6656/50000]\tLoss: 4.6879\tLR: 4.313043\n",
      "Training Epoch: 44 [6784/50000]\tLoss: 4.7030\tLR: 4.313299\n",
      "Training Epoch: 44 [6912/50000]\tLoss: 4.6935\tLR: 4.313555\n",
      "Training Epoch: 44 [7040/50000]\tLoss: 4.7093\tLR: 4.313811\n",
      "Training Epoch: 44 [7168/50000]\tLoss: 4.7161\tLR: 4.314066\n",
      "Training Epoch: 44 [7296/50000]\tLoss: 4.6591\tLR: 4.314322\n",
      "Training Epoch: 44 [7424/50000]\tLoss: 4.6175\tLR: 4.314578\n",
      "Training Epoch: 44 [7552/50000]\tLoss: 4.6936\tLR: 4.314834\n",
      "Training Epoch: 44 [7680/50000]\tLoss: 4.7153\tLR: 4.315090\n",
      "Training Epoch: 44 [7808/50000]\tLoss: 4.6662\tLR: 4.315345\n",
      "Training Epoch: 44 [7936/50000]\tLoss: 4.6144\tLR: 4.315601\n",
      "Training Epoch: 44 [8064/50000]\tLoss: 4.6611\tLR: 4.315857\n",
      "Training Epoch: 44 [8192/50000]\tLoss: 4.6030\tLR: 4.316113\n",
      "Training Epoch: 44 [8320/50000]\tLoss: 4.6559\tLR: 4.316368\n",
      "Training Epoch: 44 [8448/50000]\tLoss: 4.6549\tLR: 4.316624\n",
      "Training Epoch: 44 [8576/50000]\tLoss: 4.6375\tLR: 4.316880\n",
      "Training Epoch: 44 [8704/50000]\tLoss: 4.7011\tLR: 4.317136\n",
      "Training Epoch: 44 [8832/50000]\tLoss: 4.7178\tLR: 4.317391\n",
      "Training Epoch: 44 [8960/50000]\tLoss: 4.6630\tLR: 4.317647\n",
      "Training Epoch: 44 [9088/50000]\tLoss: 4.6732\tLR: 4.317903\n",
      "Training Epoch: 44 [9216/50000]\tLoss: 4.6995\tLR: 4.318159\n",
      "Training Epoch: 44 [9344/50000]\tLoss: 4.7291\tLR: 4.318414\n",
      "Training Epoch: 44 [9472/50000]\tLoss: 4.6889\tLR: 4.318670\n",
      "Training Epoch: 44 [9600/50000]\tLoss: 4.6705\tLR: 4.318926\n",
      "Training Epoch: 44 [9728/50000]\tLoss: 4.7106\tLR: 4.319182\n",
      "Training Epoch: 44 [9856/50000]\tLoss: 4.6363\tLR: 4.319437\n",
      "Training Epoch: 44 [9984/50000]\tLoss: 4.6675\tLR: 4.319693\n",
      "Training Epoch: 44 [10112/50000]\tLoss: 4.6910\tLR: 4.319949\n",
      "Training Epoch: 44 [10240/50000]\tLoss: 4.6595\tLR: 4.320205\n",
      "Training Epoch: 44 [10368/50000]\tLoss: 4.6789\tLR: 4.320460\n",
      "Training Epoch: 44 [10496/50000]\tLoss: 4.7646\tLR: 4.320716\n",
      "Training Epoch: 44 [10624/50000]\tLoss: 4.7253\tLR: 4.320972\n",
      "Training Epoch: 44 [10752/50000]\tLoss: 4.6893\tLR: 4.321228\n",
      "Training Epoch: 44 [10880/50000]\tLoss: 4.6251\tLR: 4.321483\n",
      "Training Epoch: 44 [11008/50000]\tLoss: 4.6740\tLR: 4.321739\n",
      "Training Epoch: 44 [11136/50000]\tLoss: 4.6980\tLR: 4.321995\n",
      "Training Epoch: 44 [11264/50000]\tLoss: 4.7487\tLR: 4.322251\n",
      "Training Epoch: 44 [11392/50000]\tLoss: 4.7279\tLR: 4.322506\n",
      "Training Epoch: 44 [11520/50000]\tLoss: 4.7109\tLR: 4.322762\n",
      "Training Epoch: 44 [11648/50000]\tLoss: 4.7165\tLR: 4.323018\n",
      "Training Epoch: 44 [11776/50000]\tLoss: 4.7034\tLR: 4.323274\n",
      "Training Epoch: 44 [11904/50000]\tLoss: 4.6823\tLR: 4.323529\n",
      "Training Epoch: 44 [12032/50000]\tLoss: 4.6970\tLR: 4.323785\n",
      "Training Epoch: 44 [12160/50000]\tLoss: 4.6188\tLR: 4.324041\n",
      "Training Epoch: 44 [12288/50000]\tLoss: 4.6922\tLR: 4.324297\n",
      "Training Epoch: 44 [12416/50000]\tLoss: 4.6105\tLR: 4.324552\n",
      "Training Epoch: 44 [12544/50000]\tLoss: 4.6707\tLR: 4.324808\n",
      "Training Epoch: 44 [12672/50000]\tLoss: 4.7483\tLR: 4.325064\n",
      "Training Epoch: 44 [12800/50000]\tLoss: 4.6266\tLR: 4.325320\n",
      "Training Epoch: 44 [12928/50000]\tLoss: 4.7020\tLR: 4.325575\n",
      "Training Epoch: 44 [13056/50000]\tLoss: 4.7032\tLR: 4.325831\n",
      "Training Epoch: 44 [13184/50000]\tLoss: 4.6582\tLR: 4.326087\n",
      "Training Epoch: 44 [13312/50000]\tLoss: 4.6524\tLR: 4.326343\n",
      "Training Epoch: 44 [13440/50000]\tLoss: 4.6972\tLR: 4.326598\n",
      "Training Epoch: 44 [13568/50000]\tLoss: 4.7239\tLR: 4.326854\n",
      "Training Epoch: 44 [13696/50000]\tLoss: 4.6788\tLR: 4.327110\n",
      "Training Epoch: 44 [13824/50000]\tLoss: 4.6692\tLR: 4.327366\n",
      "Training Epoch: 44 [13952/50000]\tLoss: 4.6971\tLR: 4.327621\n",
      "Training Epoch: 44 [14080/50000]\tLoss: 4.6804\tLR: 4.327877\n",
      "Training Epoch: 44 [14208/50000]\tLoss: 4.6165\tLR: 4.328133\n",
      "Training Epoch: 44 [14336/50000]\tLoss: 4.6721\tLR: 4.328389\n",
      "Training Epoch: 44 [14464/50000]\tLoss: 4.7388\tLR: 4.328645\n",
      "Training Epoch: 44 [14592/50000]\tLoss: 4.7748\tLR: 4.328900\n",
      "Training Epoch: 44 [14720/50000]\tLoss: 4.7518\tLR: 4.329156\n",
      "Training Epoch: 44 [14848/50000]\tLoss: 4.6965\tLR: 4.329412\n",
      "Training Epoch: 44 [14976/50000]\tLoss: 4.6602\tLR: 4.329668\n",
      "Training Epoch: 44 [15104/50000]\tLoss: 4.6871\tLR: 4.329923\n",
      "Training Epoch: 44 [15232/50000]\tLoss: 4.6979\tLR: 4.330179\n",
      "Training Epoch: 44 [15360/50000]\tLoss: 4.7180\tLR: 4.330435\n",
      "Training Epoch: 44 [15488/50000]\tLoss: 4.7074\tLR: 4.330691\n",
      "Training Epoch: 44 [15616/50000]\tLoss: 4.6584\tLR: 4.330946\n",
      "Training Epoch: 44 [15744/50000]\tLoss: 4.6396\tLR: 4.331202\n",
      "Training Epoch: 44 [15872/50000]\tLoss: 4.6257\tLR: 4.331458\n",
      "Training Epoch: 44 [16000/50000]\tLoss: 4.6357\tLR: 4.331714\n",
      "Training Epoch: 44 [16128/50000]\tLoss: 4.6904\tLR: 4.331969\n",
      "Training Epoch: 44 [16256/50000]\tLoss: 4.7009\tLR: 4.332225\n",
      "Training Epoch: 44 [16384/50000]\tLoss: 4.7093\tLR: 4.332481\n",
      "Training Epoch: 44 [16512/50000]\tLoss: 4.6768\tLR: 4.332737\n",
      "Training Epoch: 44 [16640/50000]\tLoss: 4.6862\tLR: 4.332992\n",
      "Training Epoch: 44 [16768/50000]\tLoss: 4.7182\tLR: 4.333248\n",
      "Training Epoch: 44 [16896/50000]\tLoss: 4.7488\tLR: 4.333504\n",
      "Training Epoch: 44 [17024/50000]\tLoss: 4.7306\tLR: 4.333760\n",
      "Training Epoch: 44 [17152/50000]\tLoss: 4.7278\tLR: 4.334015\n",
      "Training Epoch: 44 [17280/50000]\tLoss: 4.6594\tLR: 4.334271\n",
      "Training Epoch: 44 [17408/50000]\tLoss: 4.7131\tLR: 4.334527\n",
      "Training Epoch: 44 [17536/50000]\tLoss: 4.6836\tLR: 4.334783\n",
      "Training Epoch: 44 [17664/50000]\tLoss: 4.7173\tLR: 4.335038\n",
      "Training Epoch: 44 [17792/50000]\tLoss: 4.7135\tLR: 4.335294\n",
      "Training Epoch: 44 [17920/50000]\tLoss: 4.6781\tLR: 4.335550\n",
      "Training Epoch: 44 [18048/50000]\tLoss: 4.7557\tLR: 4.335806\n",
      "Training Epoch: 44 [18176/50000]\tLoss: 4.6808\tLR: 4.336061\n",
      "Training Epoch: 44 [18304/50000]\tLoss: 4.6092\tLR: 4.336317\n",
      "Training Epoch: 44 [18432/50000]\tLoss: 4.6698\tLR: 4.336573\n",
      "Training Epoch: 44 [18560/50000]\tLoss: 4.7018\tLR: 4.336829\n",
      "Training Epoch: 44 [18688/50000]\tLoss: 4.6472\tLR: 4.337084\n",
      "Training Epoch: 44 [18816/50000]\tLoss: 4.7520\tLR: 4.337340\n",
      "Training Epoch: 44 [18944/50000]\tLoss: 4.7128\tLR: 4.337596\n",
      "Training Epoch: 44 [19072/50000]\tLoss: 4.6587\tLR: 4.337852\n",
      "Training Epoch: 44 [19200/50000]\tLoss: 4.6673\tLR: 4.338107\n",
      "Training Epoch: 44 [19328/50000]\tLoss: 4.6664\tLR: 4.338363\n",
      "Training Epoch: 44 [19456/50000]\tLoss: 4.7398\tLR: 4.338619\n",
      "Training Epoch: 44 [19584/50000]\tLoss: 4.7383\tLR: 4.338875\n",
      "Training Epoch: 44 [19712/50000]\tLoss: 4.6516\tLR: 4.339130\n",
      "Training Epoch: 44 [19840/50000]\tLoss: 4.6464\tLR: 4.339386\n",
      "Training Epoch: 44 [19968/50000]\tLoss: 4.7203\tLR: 4.339642\n",
      "Training Epoch: 44 [20096/50000]\tLoss: 4.6301\tLR: 4.339898\n",
      "Training Epoch: 44 [20224/50000]\tLoss: 4.7329\tLR: 4.340153\n",
      "Training Epoch: 44 [20352/50000]\tLoss: 4.7319\tLR: 4.340409\n",
      "Training Epoch: 44 [20480/50000]\tLoss: 4.7256\tLR: 4.340665\n",
      "Training Epoch: 44 [20608/50000]\tLoss: 4.7311\tLR: 4.340921\n",
      "Training Epoch: 44 [20736/50000]\tLoss: 4.7162\tLR: 4.341176\n",
      "Training Epoch: 44 [20864/50000]\tLoss: 4.6430\tLR: 4.341432\n",
      "Training Epoch: 44 [20992/50000]\tLoss: 4.6632\tLR: 4.341688\n",
      "Training Epoch: 44 [21120/50000]\tLoss: 4.7853\tLR: 4.341944\n",
      "Training Epoch: 44 [21248/50000]\tLoss: 4.7275\tLR: 4.342199\n",
      "Training Epoch: 44 [21376/50000]\tLoss: 4.6945\tLR: 4.342455\n",
      "Training Epoch: 44 [21504/50000]\tLoss: 4.6518\tLR: 4.342711\n",
      "Training Epoch: 44 [21632/50000]\tLoss: 4.6504\tLR: 4.342967\n",
      "Training Epoch: 44 [21760/50000]\tLoss: 4.7153\tLR: 4.343223\n",
      "Training Epoch: 44 [21888/50000]\tLoss: 4.6696\tLR: 4.343478\n",
      "Training Epoch: 44 [22016/50000]\tLoss: 4.6847\tLR: 4.343734\n",
      "Training Epoch: 44 [22144/50000]\tLoss: 4.6273\tLR: 4.343990\n",
      "Training Epoch: 44 [22272/50000]\tLoss: 4.7320\tLR: 4.344246\n",
      "Training Epoch: 44 [22400/50000]\tLoss: 4.6386\tLR: 4.344501\n",
      "Training Epoch: 44 [22528/50000]\tLoss: 4.7371\tLR: 4.344757\n",
      "Training Epoch: 44 [22656/50000]\tLoss: 4.7148\tLR: 4.345013\n",
      "Training Epoch: 44 [22784/50000]\tLoss: 4.7252\tLR: 4.345269\n",
      "Training Epoch: 44 [22912/50000]\tLoss: 4.6707\tLR: 4.345524\n",
      "Training Epoch: 44 [23040/50000]\tLoss: 4.7231\tLR: 4.345780\n",
      "Training Epoch: 44 [23168/50000]\tLoss: 4.6731\tLR: 4.346036\n",
      "Training Epoch: 44 [23296/50000]\tLoss: 4.7421\tLR: 4.346292\n",
      "Training Epoch: 44 [23424/50000]\tLoss: 4.7251\tLR: 4.346547\n",
      "Training Epoch: 44 [23552/50000]\tLoss: 4.7557\tLR: 4.346803\n",
      "Training Epoch: 44 [23680/50000]\tLoss: 4.7301\tLR: 4.347059\n",
      "Training Epoch: 44 [23808/50000]\tLoss: 4.7488\tLR: 4.347315\n",
      "Training Epoch: 44 [23936/50000]\tLoss: 4.7074\tLR: 4.347570\n",
      "Training Epoch: 44 [24064/50000]\tLoss: 4.6582\tLR: 4.347826\n",
      "Training Epoch: 44 [24192/50000]\tLoss: 4.6588\tLR: 4.348082\n",
      "Training Epoch: 44 [24320/50000]\tLoss: 4.6548\tLR: 4.348338\n",
      "Training Epoch: 44 [24448/50000]\tLoss: 4.6805\tLR: 4.348593\n",
      "Training Epoch: 44 [24576/50000]\tLoss: 4.6029\tLR: 4.348849\n",
      "Training Epoch: 44 [24704/50000]\tLoss: 4.7046\tLR: 4.349105\n",
      "Training Epoch: 44 [24832/50000]\tLoss: 4.6624\tLR: 4.349361\n",
      "Training Epoch: 44 [24960/50000]\tLoss: 4.6980\tLR: 4.349616\n",
      "Training Epoch: 44 [25088/50000]\tLoss: 4.6945\tLR: 4.349872\n",
      "Training Epoch: 44 [25216/50000]\tLoss: 4.6941\tLR: 4.350128\n",
      "Training Epoch: 44 [25344/50000]\tLoss: 4.6714\tLR: 4.350384\n",
      "Training Epoch: 44 [25472/50000]\tLoss: 4.6470\tLR: 4.350639\n",
      "Training Epoch: 44 [25600/50000]\tLoss: 4.6866\tLR: 4.350895\n",
      "Training Epoch: 44 [25728/50000]\tLoss: 4.7034\tLR: 4.351151\n",
      "Training Epoch: 44 [25856/50000]\tLoss: 4.7374\tLR: 4.351407\n",
      "Training Epoch: 44 [25984/50000]\tLoss: 4.6903\tLR: 4.351662\n",
      "Training Epoch: 44 [26112/50000]\tLoss: 4.6847\tLR: 4.351918\n",
      "Training Epoch: 44 [26240/50000]\tLoss: 4.6777\tLR: 4.352174\n",
      "Training Epoch: 44 [26368/50000]\tLoss: 4.5920\tLR: 4.352430\n",
      "Training Epoch: 44 [26496/50000]\tLoss: 4.6278\tLR: 4.352685\n",
      "Training Epoch: 44 [26624/50000]\tLoss: 4.7021\tLR: 4.352941\n",
      "Training Epoch: 44 [26752/50000]\tLoss: 4.7104\tLR: 4.353197\n",
      "Training Epoch: 44 [26880/50000]\tLoss: 4.7232\tLR: 4.353453\n",
      "Training Epoch: 44 [27008/50000]\tLoss: 4.6760\tLR: 4.353708\n",
      "Training Epoch: 44 [27136/50000]\tLoss: 4.7047\tLR: 4.353964\n",
      "Training Epoch: 44 [27264/50000]\tLoss: 4.6575\tLR: 4.354220\n",
      "Training Epoch: 44 [27392/50000]\tLoss: 4.6685\tLR: 4.354476\n",
      "Training Epoch: 44 [27520/50000]\tLoss: 4.7596\tLR: 4.354731\n",
      "Training Epoch: 44 [27648/50000]\tLoss: 4.7278\tLR: 4.354987\n",
      "Training Epoch: 44 [27776/50000]\tLoss: 4.6691\tLR: 4.355243\n",
      "Training Epoch: 44 [27904/50000]\tLoss: 4.6905\tLR: 4.355499\n",
      "Training Epoch: 44 [28032/50000]\tLoss: 4.6837\tLR: 4.355754\n",
      "Training Epoch: 44 [28160/50000]\tLoss: 4.6393\tLR: 4.356010\n",
      "Training Epoch: 44 [28288/50000]\tLoss: 4.7214\tLR: 4.356266\n",
      "Training Epoch: 44 [28416/50000]\tLoss: 4.7088\tLR: 4.356522\n",
      "Training Epoch: 44 [28544/50000]\tLoss: 4.6091\tLR: 4.356777\n",
      "Training Epoch: 44 [28672/50000]\tLoss: 4.6242\tLR: 4.357033\n",
      "Training Epoch: 44 [28800/50000]\tLoss: 4.6973\tLR: 4.357289\n",
      "Training Epoch: 44 [28928/50000]\tLoss: 4.6946\tLR: 4.357545\n",
      "Training Epoch: 44 [29056/50000]\tLoss: 4.7632\tLR: 4.357801\n",
      "Training Epoch: 44 [29184/50000]\tLoss: 4.6712\tLR: 4.358056\n",
      "Training Epoch: 44 [29312/50000]\tLoss: 4.6709\tLR: 4.358312\n",
      "Training Epoch: 44 [29440/50000]\tLoss: 4.7731\tLR: 4.358568\n",
      "Training Epoch: 44 [29568/50000]\tLoss: 4.7939\tLR: 4.358824\n",
      "Training Epoch: 44 [29696/50000]\tLoss: 4.7036\tLR: 4.359079\n",
      "Training Epoch: 44 [29824/50000]\tLoss: 4.7085\tLR: 4.359335\n",
      "Training Epoch: 44 [29952/50000]\tLoss: 4.6818\tLR: 4.359591\n",
      "Training Epoch: 44 [30080/50000]\tLoss: 4.7188\tLR: 4.359847\n",
      "Training Epoch: 44 [30208/50000]\tLoss: 4.6786\tLR: 4.360102\n",
      "Training Epoch: 44 [30336/50000]\tLoss: 4.6807\tLR: 4.360358\n",
      "Training Epoch: 44 [30464/50000]\tLoss: 4.6505\tLR: 4.360614\n",
      "Training Epoch: 44 [30592/50000]\tLoss: 4.6462\tLR: 4.360870\n",
      "Training Epoch: 44 [30720/50000]\tLoss: 4.7296\tLR: 4.361125\n",
      "Training Epoch: 44 [30848/50000]\tLoss: 4.6658\tLR: 4.361381\n",
      "Training Epoch: 44 [30976/50000]\tLoss: 4.6994\tLR: 4.361637\n",
      "Training Epoch: 44 [31104/50000]\tLoss: 4.6991\tLR: 4.361893\n",
      "Training Epoch: 44 [31232/50000]\tLoss: 4.6888\tLR: 4.362148\n",
      "Training Epoch: 44 [31360/50000]\tLoss: 4.7151\tLR: 4.362404\n",
      "Training Epoch: 44 [31488/50000]\tLoss: 4.6864\tLR: 4.362660\n",
      "Training Epoch: 44 [31616/50000]\tLoss: 4.7313\tLR: 4.362916\n",
      "Training Epoch: 44 [31744/50000]\tLoss: 4.6242\tLR: 4.363171\n",
      "Training Epoch: 44 [31872/50000]\tLoss: 4.6501\tLR: 4.363427\n",
      "Training Epoch: 44 [32000/50000]\tLoss: 4.6592\tLR: 4.363683\n",
      "Training Epoch: 44 [32128/50000]\tLoss: 4.6778\tLR: 4.363939\n",
      "Training Epoch: 44 [32256/50000]\tLoss: 4.6978\tLR: 4.364194\n",
      "Training Epoch: 44 [32384/50000]\tLoss: 4.7308\tLR: 4.364450\n",
      "Training Epoch: 44 [32512/50000]\tLoss: 4.7173\tLR: 4.364706\n",
      "Training Epoch: 44 [32640/50000]\tLoss: 4.6887\tLR: 4.364962\n",
      "Training Epoch: 44 [32768/50000]\tLoss: 4.6651\tLR: 4.365217\n",
      "Training Epoch: 44 [32896/50000]\tLoss: 4.6506\tLR: 4.365473\n",
      "Training Epoch: 44 [33024/50000]\tLoss: 4.7016\tLR: 4.365729\n",
      "Training Epoch: 44 [33152/50000]\tLoss: 4.6615\tLR: 4.365985\n",
      "Training Epoch: 44 [33280/50000]\tLoss: 4.6922\tLR: 4.366240\n",
      "Training Epoch: 44 [33408/50000]\tLoss: 4.6485\tLR: 4.366496\n",
      "Training Epoch: 44 [33536/50000]\tLoss: 4.7105\tLR: 4.366752\n",
      "Training Epoch: 44 [33664/50000]\tLoss: 4.7044\tLR: 4.367008\n",
      "Training Epoch: 44 [33792/50000]\tLoss: 4.6390\tLR: 4.367263\n",
      "Training Epoch: 44 [33920/50000]\tLoss: 4.7016\tLR: 4.367519\n",
      "Training Epoch: 44 [34048/50000]\tLoss: 4.6556\tLR: 4.367775\n",
      "Training Epoch: 44 [34176/50000]\tLoss: 4.6882\tLR: 4.368031\n",
      "Training Epoch: 44 [34304/50000]\tLoss: 4.6330\tLR: 4.368286\n",
      "Training Epoch: 44 [34432/50000]\tLoss: 4.7301\tLR: 4.368542\n",
      "Training Epoch: 44 [34560/50000]\tLoss: 4.6431\tLR: 4.368798\n",
      "Training Epoch: 44 [34688/50000]\tLoss: 4.7059\tLR: 4.369054\n",
      "Training Epoch: 44 [34816/50000]\tLoss: 4.7045\tLR: 4.369309\n",
      "Training Epoch: 44 [34944/50000]\tLoss: 4.7426\tLR: 4.369565\n",
      "Training Epoch: 44 [35072/50000]\tLoss: 4.7156\tLR: 4.369821\n",
      "Training Epoch: 44 [35200/50000]\tLoss: 4.6790\tLR: 4.370077\n",
      "Training Epoch: 44 [35328/50000]\tLoss: 4.6667\tLR: 4.370332\n",
      "Training Epoch: 44 [35456/50000]\tLoss: 4.6613\tLR: 4.370588\n",
      "Training Epoch: 44 [35584/50000]\tLoss: 4.7808\tLR: 4.370844\n",
      "Training Epoch: 44 [35712/50000]\tLoss: 4.7196\tLR: 4.371100\n",
      "Training Epoch: 44 [35840/50000]\tLoss: 4.7283\tLR: 4.371355\n",
      "Training Epoch: 44 [35968/50000]\tLoss: 4.6978\tLR: 4.371611\n",
      "Training Epoch: 44 [36096/50000]\tLoss: 4.6957\tLR: 4.371867\n",
      "Training Epoch: 44 [36224/50000]\tLoss: 4.6431\tLR: 4.372123\n",
      "Training Epoch: 44 [36352/50000]\tLoss: 4.6707\tLR: 4.372379\n",
      "Training Epoch: 44 [36480/50000]\tLoss: 4.6960\tLR: 4.372634\n",
      "Training Epoch: 44 [36608/50000]\tLoss: 4.6683\tLR: 4.372890\n",
      "Training Epoch: 44 [36736/50000]\tLoss: 4.6425\tLR: 4.373146\n",
      "Training Epoch: 44 [36864/50000]\tLoss: 4.6429\tLR: 4.373402\n",
      "Training Epoch: 44 [36992/50000]\tLoss: 4.7122\tLR: 4.373657\n",
      "Training Epoch: 44 [37120/50000]\tLoss: 4.6910\tLR: 4.373913\n",
      "Training Epoch: 44 [37248/50000]\tLoss: 4.6600\tLR: 4.374169\n",
      "Training Epoch: 44 [37376/50000]\tLoss: 4.6942\tLR: 4.374425\n",
      "Training Epoch: 44 [37504/50000]\tLoss: 4.7120\tLR: 4.374680\n",
      "Training Epoch: 44 [37632/50000]\tLoss: 4.6688\tLR: 4.374936\n",
      "Training Epoch: 44 [37760/50000]\tLoss: 4.6317\tLR: 4.375192\n",
      "Training Epoch: 44 [37888/50000]\tLoss: 4.7369\tLR: 4.375448\n",
      "Training Epoch: 44 [38016/50000]\tLoss: 4.7954\tLR: 4.375703\n",
      "Training Epoch: 44 [38144/50000]\tLoss: 4.6948\tLR: 4.375959\n",
      "Training Epoch: 44 [38272/50000]\tLoss: 4.6748\tLR: 4.376215\n",
      "Training Epoch: 44 [38400/50000]\tLoss: 4.6907\tLR: 4.376471\n",
      "Training Epoch: 44 [38528/50000]\tLoss: 4.7325\tLR: 4.376726\n",
      "Training Epoch: 44 [38656/50000]\tLoss: 4.7126\tLR: 4.376982\n",
      "Training Epoch: 44 [38784/50000]\tLoss: 4.7030\tLR: 4.377238\n",
      "Training Epoch: 44 [38912/50000]\tLoss: 4.7196\tLR: 4.377494\n",
      "Training Epoch: 44 [39040/50000]\tLoss: 4.6827\tLR: 4.377749\n",
      "Training Epoch: 44 [39168/50000]\tLoss: 4.6215\tLR: 4.378005\n",
      "Training Epoch: 44 [39296/50000]\tLoss: 4.7014\tLR: 4.378261\n",
      "Training Epoch: 44 [39424/50000]\tLoss: 4.6360\tLR: 4.378517\n",
      "Training Epoch: 44 [39552/50000]\tLoss: 4.7050\tLR: 4.378772\n",
      "Training Epoch: 44 [39680/50000]\tLoss: 4.6747\tLR: 4.379028\n",
      "Training Epoch: 44 [39808/50000]\tLoss: 4.6826\tLR: 4.379284\n",
      "Training Epoch: 44 [39936/50000]\tLoss: 4.6863\tLR: 4.379540\n",
      "Training Epoch: 44 [40064/50000]\tLoss: 4.6609\tLR: 4.379795\n",
      "Training Epoch: 44 [40192/50000]\tLoss: 4.6842\tLR: 4.380051\n",
      "Training Epoch: 44 [40320/50000]\tLoss: 4.7185\tLR: 4.380307\n",
      "Training Epoch: 44 [40448/50000]\tLoss: 4.6950\tLR: 4.380563\n",
      "Training Epoch: 44 [40576/50000]\tLoss: 4.6517\tLR: 4.380818\n",
      "Training Epoch: 44 [40704/50000]\tLoss: 4.6763\tLR: 4.381074\n",
      "Training Epoch: 44 [40832/50000]\tLoss: 4.6639\tLR: 4.381330\n",
      "Training Epoch: 44 [40960/50000]\tLoss: 4.6919\tLR: 4.381586\n",
      "Training Epoch: 44 [41088/50000]\tLoss: 4.6104\tLR: 4.381841\n",
      "Training Epoch: 44 [41216/50000]\tLoss: 4.6942\tLR: 4.382097\n",
      "Training Epoch: 44 [41344/50000]\tLoss: 4.6936\tLR: 4.382353\n",
      "Training Epoch: 44 [41472/50000]\tLoss: 4.6404\tLR: 4.382609\n",
      "Training Epoch: 44 [41600/50000]\tLoss: 4.7031\tLR: 4.382864\n",
      "Training Epoch: 44 [41728/50000]\tLoss: 4.6771\tLR: 4.383120\n",
      "Training Epoch: 44 [41856/50000]\tLoss: 4.6501\tLR: 4.383376\n",
      "Training Epoch: 44 [41984/50000]\tLoss: 4.6977\tLR: 4.383632\n",
      "Training Epoch: 44 [42112/50000]\tLoss: 4.7138\tLR: 4.383887\n",
      "Training Epoch: 44 [42240/50000]\tLoss: 4.7461\tLR: 4.384143\n",
      "Training Epoch: 44 [42368/50000]\tLoss: 4.7492\tLR: 4.384399\n",
      "Training Epoch: 44 [42496/50000]\tLoss: 4.6611\tLR: 4.384655\n",
      "Training Epoch: 44 [42624/50000]\tLoss: 4.7317\tLR: 4.384910\n",
      "Training Epoch: 44 [42752/50000]\tLoss: 4.6717\tLR: 4.385166\n",
      "Training Epoch: 44 [42880/50000]\tLoss: 4.7545\tLR: 4.385422\n",
      "Training Epoch: 44 [43008/50000]\tLoss: 4.6687\tLR: 4.385678\n",
      "Training Epoch: 44 [43136/50000]\tLoss: 4.6903\tLR: 4.385934\n",
      "Training Epoch: 44 [43264/50000]\tLoss: 4.6530\tLR: 4.386189\n",
      "Training Epoch: 44 [43392/50000]\tLoss: 4.7109\tLR: 4.386445\n",
      "Training Epoch: 44 [43520/50000]\tLoss: 4.7096\tLR: 4.386701\n",
      "Training Epoch: 44 [43648/50000]\tLoss: 4.6819\tLR: 4.386957\n",
      "Training Epoch: 44 [43776/50000]\tLoss: 4.6813\tLR: 4.387212\n",
      "Training Epoch: 44 [43904/50000]\tLoss: 4.6621\tLR: 4.387468\n",
      "Training Epoch: 44 [44032/50000]\tLoss: 4.6775\tLR: 4.387724\n",
      "Training Epoch: 44 [44160/50000]\tLoss: 4.6996\tLR: 4.387980\n",
      "Training Epoch: 44 [44288/50000]\tLoss: 4.7214\tLR: 4.388235\n",
      "Training Epoch: 44 [44416/50000]\tLoss: 4.7331\tLR: 4.388491\n",
      "Training Epoch: 44 [44544/50000]\tLoss: 4.7046\tLR: 4.388747\n",
      "Training Epoch: 44 [44672/50000]\tLoss: 4.7682\tLR: 4.389003\n",
      "Training Epoch: 44 [44800/50000]\tLoss: 4.6668\tLR: 4.389258\n",
      "Training Epoch: 44 [44928/50000]\tLoss: 4.6884\tLR: 4.389514\n",
      "Training Epoch: 44 [45056/50000]\tLoss: 4.6352\tLR: 4.389770\n",
      "Training Epoch: 44 [45184/50000]\tLoss: 4.6581\tLR: 4.390026\n",
      "Training Epoch: 44 [45312/50000]\tLoss: 4.7021\tLR: 4.390281\n",
      "Training Epoch: 44 [45440/50000]\tLoss: 4.6333\tLR: 4.390537\n",
      "Training Epoch: 44 [45568/50000]\tLoss: 4.6481\tLR: 4.390793\n",
      "Training Epoch: 44 [45696/50000]\tLoss: 4.6426\tLR: 4.391049\n",
      "Training Epoch: 44 [45824/50000]\tLoss: 4.6355\tLR: 4.391304\n",
      "Training Epoch: 44 [45952/50000]\tLoss: 4.6854\tLR: 4.391560\n",
      "Training Epoch: 44 [46080/50000]\tLoss: 4.7100\tLR: 4.391816\n",
      "Training Epoch: 44 [46208/50000]\tLoss: 4.6737\tLR: 4.392072\n",
      "Training Epoch: 44 [46336/50000]\tLoss: 4.7108\tLR: 4.392327\n",
      "Training Epoch: 44 [46464/50000]\tLoss: 4.6947\tLR: 4.392583\n",
      "Training Epoch: 44 [46592/50000]\tLoss: 4.6598\tLR: 4.392839\n",
      "Training Epoch: 44 [46720/50000]\tLoss: 4.7005\tLR: 4.393095\n",
      "Training Epoch: 44 [46848/50000]\tLoss: 4.6724\tLR: 4.393350\n",
      "Training Epoch: 44 [46976/50000]\tLoss: 4.6751\tLR: 4.393606\n",
      "Training Epoch: 44 [47104/50000]\tLoss: 4.6770\tLR: 4.393862\n",
      "Training Epoch: 44 [47232/50000]\tLoss: 4.6196\tLR: 4.394118\n",
      "Training Epoch: 44 [47360/50000]\tLoss: 4.7407\tLR: 4.394373\n",
      "Training Epoch: 44 [47488/50000]\tLoss: 4.7510\tLR: 4.394629\n",
      "Training Epoch: 44 [47616/50000]\tLoss: 4.6342\tLR: 4.394885\n",
      "Training Epoch: 44 [47744/50000]\tLoss: 4.6542\tLR: 4.395141\n",
      "Training Epoch: 44 [47872/50000]\tLoss: 4.6860\tLR: 4.395396\n",
      "Training Epoch: 44 [48000/50000]\tLoss: 4.6762\tLR: 4.395652\n",
      "Training Epoch: 44 [48128/50000]\tLoss: 4.6483\tLR: 4.395908\n",
      "Training Epoch: 44 [48256/50000]\tLoss: 4.6988\tLR: 4.396164\n",
      "Training Epoch: 44 [48384/50000]\tLoss: 4.6588\tLR: 4.396419\n",
      "Training Epoch: 44 [48512/50000]\tLoss: 4.6753\tLR: 4.396675\n",
      "Training Epoch: 44 [48640/50000]\tLoss: 4.7890\tLR: 4.396931\n",
      "Training Epoch: 44 [48768/50000]\tLoss: 4.6561\tLR: 4.397187\n",
      "Training Epoch: 44 [48896/50000]\tLoss: 4.7091\tLR: 4.397442\n",
      "Training Epoch: 44 [49024/50000]\tLoss: 4.6962\tLR: 4.397698\n",
      "Training Epoch: 44 [49152/50000]\tLoss: 4.6993\tLR: 4.397954\n",
      "Training Epoch: 44 [49280/50000]\tLoss: 4.6728\tLR: 4.398210\n",
      "Training Epoch: 44 [49408/50000]\tLoss: 4.7028\tLR: 4.398465\n",
      "Training Epoch: 44 [49536/50000]\tLoss: 4.6002\tLR: 4.398721\n",
      "Training Epoch: 44 [49664/50000]\tLoss: 4.7073\tLR: 4.398977\n",
      "Training Epoch: 44 [49792/50000]\tLoss: 4.7112\tLR: 4.399233\n",
      "Training Epoch: 44 [49920/50000]\tLoss: 4.6573\tLR: 4.399488\n",
      "Training Epoch: 44 [50000/50000]\tLoss: 4.7539\tLR: 4.399744\n",
      "epoch 44 training time consumed: 489.00s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   61685 GB |   61685 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   61496 GB |   61496 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     189 GB |     189 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   61685 GB |   61685 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   61496 GB |   61496 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     189 GB |     189 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   60817 GB |   60817 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   60627 GB |   60627 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     189 GB |     189 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    6541 K  |    6540 K  |\n",
      "|       from large pool |      24    |      65    |    2788 K  |    2788 K  |\n",
      "|       from small pool |     231    |     274    |    3752 K  |    3752 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    6541 K  |    6540 K  |\n",
      "|       from large pool |      24    |      65    |    2788 K  |    2788 K  |\n",
      "|       from small pool |     231    |     274    |    3752 K  |    3752 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      34    |      46    |    3791 K  |    3791 K  |\n",
      "|       from large pool |      10    |      23    |    1340 K  |    1340 K  |\n",
      "|       from small pool |      24    |      35    |    2451 K  |    2451 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 44, Average loss: 0.0370, Accuracy: 0.0100, Time consumed:31.12s\n",
      "\n",
      "Training Epoch: 45 [128/50000]\tLoss: 4.7243\tLR: 0.100000\n",
      "Training Epoch: 45 [256/50000]\tLoss: 4.6649\tLR: 4.400256\n",
      "Training Epoch: 45 [384/50000]\tLoss: 4.7105\tLR: 4.400512\n",
      "Training Epoch: 45 [512/50000]\tLoss: 4.7222\tLR: 4.400767\n",
      "Training Epoch: 45 [640/50000]\tLoss: 4.6512\tLR: 4.401023\n",
      "Training Epoch: 45 [768/50000]\tLoss: 4.6230\tLR: 4.401279\n",
      "Training Epoch: 45 [896/50000]\tLoss: 4.6694\tLR: 4.401535\n",
      "Training Epoch: 45 [1024/50000]\tLoss: 4.7306\tLR: 4.401790\n",
      "Training Epoch: 45 [1152/50000]\tLoss: 4.5970\tLR: 4.402046\n",
      "Training Epoch: 45 [1280/50000]\tLoss: 4.6308\tLR: 4.402302\n",
      "Training Epoch: 45 [1408/50000]\tLoss: 4.6011\tLR: 4.402558\n",
      "Training Epoch: 45 [1536/50000]\tLoss: 4.7111\tLR: 4.402813\n",
      "Training Epoch: 45 [1664/50000]\tLoss: 4.7293\tLR: 4.403069\n",
      "Training Epoch: 45 [1792/50000]\tLoss: 4.6687\tLR: 4.403325\n",
      "Training Epoch: 45 [1920/50000]\tLoss: 4.7130\tLR: 4.403581\n",
      "Training Epoch: 45 [2048/50000]\tLoss: 4.6726\tLR: 4.403836\n",
      "Training Epoch: 45 [2176/50000]\tLoss: 4.7095\tLR: 4.404092\n",
      "Training Epoch: 45 [2304/50000]\tLoss: 4.6921\tLR: 4.404348\n",
      "Training Epoch: 45 [2432/50000]\tLoss: 4.7101\tLR: 4.404604\n",
      "Training Epoch: 45 [2560/50000]\tLoss: 4.7342\tLR: 4.404859\n",
      "Training Epoch: 45 [2688/50000]\tLoss: 4.6627\tLR: 4.405115\n",
      "Training Epoch: 45 [2816/50000]\tLoss: 4.6816\tLR: 4.405371\n",
      "Training Epoch: 45 [2944/50000]\tLoss: 4.6206\tLR: 4.405627\n",
      "Training Epoch: 45 [3072/50000]\tLoss: 4.6374\tLR: 4.405882\n",
      "Training Epoch: 45 [3200/50000]\tLoss: 4.6759\tLR: 4.406138\n",
      "Training Epoch: 45 [3328/50000]\tLoss: 4.6953\tLR: 4.406394\n",
      "Training Epoch: 45 [3456/50000]\tLoss: 4.6782\tLR: 4.406650\n",
      "Training Epoch: 45 [3584/50000]\tLoss: 4.6819\tLR: 4.406905\n",
      "Training Epoch: 45 [3712/50000]\tLoss: 4.6628\tLR: 4.407161\n",
      "Training Epoch: 45 [3840/50000]\tLoss: 4.6684\tLR: 4.407417\n",
      "Training Epoch: 45 [3968/50000]\tLoss: 4.6780\tLR: 4.407673\n",
      "Training Epoch: 45 [4096/50000]\tLoss: 4.6981\tLR: 4.407928\n",
      "Training Epoch: 45 [4224/50000]\tLoss: 4.6729\tLR: 4.408184\n",
      "Training Epoch: 45 [4352/50000]\tLoss: 4.6683\tLR: 4.408440\n",
      "Training Epoch: 45 [4480/50000]\tLoss: 4.6896\tLR: 4.408696\n",
      "Training Epoch: 45 [4608/50000]\tLoss: 4.6625\tLR: 4.408951\n",
      "Training Epoch: 45 [4736/50000]\tLoss: 4.7036\tLR: 4.409207\n",
      "Training Epoch: 45 [4864/50000]\tLoss: 4.7146\tLR: 4.409463\n",
      "Training Epoch: 45 [4992/50000]\tLoss: 4.6984\tLR: 4.409719\n",
      "Training Epoch: 45 [5120/50000]\tLoss: 4.6675\tLR: 4.409974\n",
      "Training Epoch: 45 [5248/50000]\tLoss: 4.7263\tLR: 4.410230\n",
      "Training Epoch: 45 [5376/50000]\tLoss: 4.6915\tLR: 4.410486\n",
      "Training Epoch: 45 [5504/50000]\tLoss: 4.6530\tLR: 4.410742\n",
      "Training Epoch: 45 [5632/50000]\tLoss: 4.6588\tLR: 4.410997\n",
      "Training Epoch: 45 [5760/50000]\tLoss: 4.7182\tLR: 4.411253\n",
      "Training Epoch: 45 [5888/50000]\tLoss: 4.6819\tLR: 4.411509\n",
      "Training Epoch: 45 [6016/50000]\tLoss: 4.6902\tLR: 4.411765\n",
      "Training Epoch: 45 [6144/50000]\tLoss: 4.7476\tLR: 4.412020\n",
      "Training Epoch: 45 [6272/50000]\tLoss: 4.6391\tLR: 4.412276\n",
      "Training Epoch: 45 [6400/50000]\tLoss: 4.6356\tLR: 4.412532\n",
      "Training Epoch: 45 [6528/50000]\tLoss: 4.6332\tLR: 4.412788\n",
      "Training Epoch: 45 [6656/50000]\tLoss: 4.6386\tLR: 4.413043\n",
      "Training Epoch: 45 [6784/50000]\tLoss: 4.6526\tLR: 4.413299\n",
      "Training Epoch: 45 [6912/50000]\tLoss: 4.6984\tLR: 4.413555\n",
      "Training Epoch: 45 [7040/50000]\tLoss: 4.7292\tLR: 4.413811\n",
      "Training Epoch: 45 [7168/50000]\tLoss: 4.7218\tLR: 4.414066\n",
      "Training Epoch: 45 [7296/50000]\tLoss: 4.6599\tLR: 4.414322\n",
      "Training Epoch: 45 [7424/50000]\tLoss: 4.7046\tLR: 4.414578\n",
      "Training Epoch: 45 [7552/50000]\tLoss: 4.6578\tLR: 4.414834\n",
      "Training Epoch: 45 [7680/50000]\tLoss: 4.7326\tLR: 4.415090\n",
      "Training Epoch: 45 [7808/50000]\tLoss: 4.7054\tLR: 4.415345\n",
      "Training Epoch: 45 [7936/50000]\tLoss: 4.6741\tLR: 4.415601\n",
      "Training Epoch: 45 [8064/50000]\tLoss: 4.6126\tLR: 4.415857\n",
      "Training Epoch: 45 [8192/50000]\tLoss: 4.6705\tLR: 4.416113\n",
      "Training Epoch: 45 [8320/50000]\tLoss: 4.6702\tLR: 4.416368\n",
      "Training Epoch: 45 [8448/50000]\tLoss: 4.7170\tLR: 4.416624\n",
      "Training Epoch: 45 [8576/50000]\tLoss: 4.6865\tLR: 4.416880\n",
      "Training Epoch: 45 [8704/50000]\tLoss: 4.6742\tLR: 4.417136\n",
      "Training Epoch: 45 [8832/50000]\tLoss: 4.6885\tLR: 4.417391\n",
      "Training Epoch: 45 [8960/50000]\tLoss: 4.7091\tLR: 4.417647\n",
      "Training Epoch: 45 [9088/50000]\tLoss: 4.6918\tLR: 4.417903\n",
      "Training Epoch: 45 [9216/50000]\tLoss: 4.7380\tLR: 4.418159\n",
      "Training Epoch: 45 [9344/50000]\tLoss: 4.7052\tLR: 4.418414\n",
      "Training Epoch: 45 [9472/50000]\tLoss: 4.7933\tLR: 4.418670\n",
      "Training Epoch: 45 [9600/50000]\tLoss: 4.7352\tLR: 4.418926\n",
      "Training Epoch: 45 [9728/50000]\tLoss: 4.7211\tLR: 4.419182\n",
      "Training Epoch: 45 [9856/50000]\tLoss: 4.6563\tLR: 4.419437\n",
      "Training Epoch: 45 [9984/50000]\tLoss: 4.6732\tLR: 4.419693\n",
      "Training Epoch: 45 [10112/50000]\tLoss: 4.7195\tLR: 4.419949\n",
      "Training Epoch: 45 [10240/50000]\tLoss: 4.6842\tLR: 4.420205\n",
      "Training Epoch: 45 [10368/50000]\tLoss: 4.6609\tLR: 4.420460\n",
      "Training Epoch: 45 [10496/50000]\tLoss: 4.6628\tLR: 4.420716\n",
      "Training Epoch: 45 [10624/50000]\tLoss: 4.6149\tLR: 4.420972\n",
      "Training Epoch: 45 [10752/50000]\tLoss: 4.6946\tLR: 4.421228\n",
      "Training Epoch: 45 [10880/50000]\tLoss: 4.7449\tLR: 4.421483\n",
      "Training Epoch: 45 [11008/50000]\tLoss: 4.5780\tLR: 4.421739\n",
      "Training Epoch: 45 [11136/50000]\tLoss: 4.7298\tLR: 4.421995\n",
      "Training Epoch: 45 [11264/50000]\tLoss: 4.7185\tLR: 4.422251\n",
      "Training Epoch: 45 [11392/50000]\tLoss: 4.7478\tLR: 4.422506\n",
      "Training Epoch: 45 [11520/50000]\tLoss: 4.8014\tLR: 4.422762\n",
      "Training Epoch: 45 [11648/50000]\tLoss: 4.6547\tLR: 4.423018\n",
      "Training Epoch: 45 [11776/50000]\tLoss: 4.7177\tLR: 4.423274\n",
      "Training Epoch: 45 [11904/50000]\tLoss: 4.6925\tLR: 4.423529\n",
      "Training Epoch: 45 [12032/50000]\tLoss: 4.7410\tLR: 4.423785\n",
      "Training Epoch: 45 [12160/50000]\tLoss: 4.7132\tLR: 4.424041\n",
      "Training Epoch: 45 [12288/50000]\tLoss: 4.6656\tLR: 4.424297\n",
      "Training Epoch: 45 [12416/50000]\tLoss: 4.6191\tLR: 4.424552\n",
      "Training Epoch: 45 [12544/50000]\tLoss: 4.7066\tLR: 4.424808\n",
      "Training Epoch: 45 [12672/50000]\tLoss: 4.7091\tLR: 4.425064\n",
      "Training Epoch: 45 [12800/50000]\tLoss: 4.7049\tLR: 4.425320\n",
      "Training Epoch: 45 [12928/50000]\tLoss: 4.7330\tLR: 4.425575\n",
      "Training Epoch: 45 [13056/50000]\tLoss: 4.7572\tLR: 4.425831\n",
      "Training Epoch: 45 [13184/50000]\tLoss: 4.7198\tLR: 4.426087\n",
      "Training Epoch: 45 [13312/50000]\tLoss: 4.6656\tLR: 4.426343\n",
      "Training Epoch: 45 [13440/50000]\tLoss: 4.6824\tLR: 4.426598\n",
      "Training Epoch: 45 [13568/50000]\tLoss: 4.6583\tLR: 4.426854\n",
      "Training Epoch: 45 [13696/50000]\tLoss: 4.6620\tLR: 4.427110\n",
      "Training Epoch: 45 [13824/50000]\tLoss: 4.7077\tLR: 4.427366\n",
      "Training Epoch: 45 [13952/50000]\tLoss: 4.7657\tLR: 4.427621\n",
      "Training Epoch: 45 [14080/50000]\tLoss: 4.7678\tLR: 4.427877\n",
      "Training Epoch: 45 [14208/50000]\tLoss: 4.6937\tLR: 4.428133\n",
      "Training Epoch: 45 [14336/50000]\tLoss: 4.6851\tLR: 4.428389\n",
      "Training Epoch: 45 [14464/50000]\tLoss: 4.6889\tLR: 4.428645\n",
      "Training Epoch: 45 [14592/50000]\tLoss: 4.7238\tLR: 4.428900\n",
      "Training Epoch: 45 [14720/50000]\tLoss: 4.6444\tLR: 4.429156\n",
      "Training Epoch: 45 [14848/50000]\tLoss: 4.6854\tLR: 4.429412\n",
      "Training Epoch: 45 [14976/50000]\tLoss: 4.6869\tLR: 4.429668\n",
      "Training Epoch: 45 [15104/50000]\tLoss: 4.6731\tLR: 4.429923\n",
      "Training Epoch: 45 [15232/50000]\tLoss: 4.6996\tLR: 4.430179\n",
      "Training Epoch: 45 [15360/50000]\tLoss: 4.6271\tLR: 4.430435\n",
      "Training Epoch: 45 [15488/50000]\tLoss: 4.6834\tLR: 4.430691\n",
      "Training Epoch: 45 [15616/50000]\tLoss: 4.7275\tLR: 4.430946\n",
      "Training Epoch: 45 [15744/50000]\tLoss: 4.6499\tLR: 4.431202\n",
      "Training Epoch: 45 [15872/50000]\tLoss: 4.6877\tLR: 4.431458\n",
      "Training Epoch: 45 [16000/50000]\tLoss: 4.6792\tLR: 4.431714\n",
      "Training Epoch: 45 [16128/50000]\tLoss: 4.6112\tLR: 4.431969\n",
      "Training Epoch: 45 [16256/50000]\tLoss: 4.7147\tLR: 4.432225\n",
      "Training Epoch: 45 [16384/50000]\tLoss: 4.6975\tLR: 4.432481\n",
      "Training Epoch: 45 [16512/50000]\tLoss: 4.6727\tLR: 4.432737\n",
      "Training Epoch: 45 [16640/50000]\tLoss: 4.6864\tLR: 4.432992\n",
      "Training Epoch: 45 [16768/50000]\tLoss: 4.6664\tLR: 4.433248\n",
      "Training Epoch: 45 [16896/50000]\tLoss: 4.7449\tLR: 4.433504\n",
      "Training Epoch: 45 [17024/50000]\tLoss: 4.7107\tLR: 4.433760\n",
      "Training Epoch: 45 [17152/50000]\tLoss: 4.7056\tLR: 4.434015\n",
      "Training Epoch: 45 [17280/50000]\tLoss: 4.6320\tLR: 4.434271\n",
      "Training Epoch: 45 [17408/50000]\tLoss: 4.7234\tLR: 4.434527\n",
      "Training Epoch: 45 [17536/50000]\tLoss: 4.7017\tLR: 4.434783\n",
      "Training Epoch: 45 [17664/50000]\tLoss: 4.6836\tLR: 4.435038\n",
      "Training Epoch: 45 [17792/50000]\tLoss: 4.7015\tLR: 4.435294\n",
      "Training Epoch: 45 [17920/50000]\tLoss: 4.6886\tLR: 4.435550\n",
      "Training Epoch: 45 [18048/50000]\tLoss: 4.6709\tLR: 4.435806\n",
      "Training Epoch: 45 [18176/50000]\tLoss: 4.6543\tLR: 4.436061\n",
      "Training Epoch: 45 [18304/50000]\tLoss: 4.6625\tLR: 4.436317\n",
      "Training Epoch: 45 [18432/50000]\tLoss: 4.6735\tLR: 4.436573\n",
      "Training Epoch: 45 [18560/50000]\tLoss: 4.6099\tLR: 4.436829\n",
      "Training Epoch: 45 [18688/50000]\tLoss: 4.6886\tLR: 4.437084\n",
      "Training Epoch: 45 [18816/50000]\tLoss: 4.7204\tLR: 4.437340\n",
      "Training Epoch: 45 [18944/50000]\tLoss: 4.6776\tLR: 4.437596\n",
      "Training Epoch: 45 [19072/50000]\tLoss: 4.7456\tLR: 4.437852\n",
      "Training Epoch: 45 [19200/50000]\tLoss: 4.6356\tLR: 4.438107\n",
      "Training Epoch: 45 [19328/50000]\tLoss: 4.6815\tLR: 4.438363\n",
      "Training Epoch: 45 [19456/50000]\tLoss: 4.6825\tLR: 4.438619\n",
      "Training Epoch: 45 [19584/50000]\tLoss: 4.6646\tLR: 4.438875\n",
      "Training Epoch: 45 [19712/50000]\tLoss: 4.6846\tLR: 4.439130\n",
      "Training Epoch: 45 [19840/50000]\tLoss: 4.6803\tLR: 4.439386\n",
      "Training Epoch: 45 [19968/50000]\tLoss: 4.7456\tLR: 4.439642\n",
      "Training Epoch: 45 [20096/50000]\tLoss: 4.7375\tLR: 4.439898\n",
      "Training Epoch: 45 [20224/50000]\tLoss: 4.6806\tLR: 4.440153\n",
      "Training Epoch: 45 [20352/50000]\tLoss: 4.6888\tLR: 4.440409\n",
      "Training Epoch: 45 [20480/50000]\tLoss: 4.7208\tLR: 4.440665\n",
      "Training Epoch: 45 [20608/50000]\tLoss: 4.6898\tLR: 4.440921\n",
      "Training Epoch: 45 [20736/50000]\tLoss: 4.6646\tLR: 4.441176\n",
      "Training Epoch: 45 [20864/50000]\tLoss: 4.6601\tLR: 4.441432\n",
      "Training Epoch: 45 [20992/50000]\tLoss: 4.6630\tLR: 4.441688\n",
      "Training Epoch: 45 [21120/50000]\tLoss: 4.6475\tLR: 4.441944\n",
      "Training Epoch: 45 [21248/50000]\tLoss: 4.7200\tLR: 4.442199\n",
      "Training Epoch: 45 [21376/50000]\tLoss: 4.6914\tLR: 4.442455\n",
      "Training Epoch: 45 [21504/50000]\tLoss: 4.6951\tLR: 4.442711\n",
      "Training Epoch: 45 [21632/50000]\tLoss: 4.6953\tLR: 4.442967\n",
      "Training Epoch: 45 [21760/50000]\tLoss: 4.6899\tLR: 4.443223\n",
      "Training Epoch: 45 [21888/50000]\tLoss: 4.6857\tLR: 4.443478\n",
      "Training Epoch: 45 [22016/50000]\tLoss: 4.7534\tLR: 4.443734\n",
      "Training Epoch: 45 [22144/50000]\tLoss: 4.7441\tLR: 4.443990\n",
      "Training Epoch: 45 [22272/50000]\tLoss: 4.7159\tLR: 4.444246\n",
      "Training Epoch: 45 [22400/50000]\tLoss: 4.6783\tLR: 4.444501\n",
      "Training Epoch: 45 [22528/50000]\tLoss: 4.6778\tLR: 4.444757\n",
      "Training Epoch: 45 [22656/50000]\tLoss: 4.7433\tLR: 4.445013\n",
      "Training Epoch: 45 [22784/50000]\tLoss: 4.6838\tLR: 4.445269\n",
      "Training Epoch: 45 [22912/50000]\tLoss: 4.6106\tLR: 4.445524\n",
      "Training Epoch: 45 [23040/50000]\tLoss: 4.7246\tLR: 4.445780\n",
      "Training Epoch: 45 [23168/50000]\tLoss: 4.7145\tLR: 4.446036\n",
      "Training Epoch: 45 [23296/50000]\tLoss: 4.6877\tLR: 4.446292\n",
      "Training Epoch: 45 [23424/50000]\tLoss: 4.6992\tLR: 4.446547\n",
      "Training Epoch: 45 [23552/50000]\tLoss: 4.7249\tLR: 4.446803\n",
      "Training Epoch: 45 [23680/50000]\tLoss: 4.7216\tLR: 4.447059\n",
      "Training Epoch: 45 [23808/50000]\tLoss: 4.7457\tLR: 4.447315\n",
      "Training Epoch: 45 [23936/50000]\tLoss: 4.6710\tLR: 4.447570\n",
      "Training Epoch: 45 [24064/50000]\tLoss: 4.6701\tLR: 4.447826\n",
      "Training Epoch: 45 [24192/50000]\tLoss: 4.6582\tLR: 4.448082\n",
      "Training Epoch: 45 [24320/50000]\tLoss: 4.6744\tLR: 4.448338\n",
      "Training Epoch: 45 [24448/50000]\tLoss: 4.7122\tLR: 4.448593\n",
      "Training Epoch: 45 [24576/50000]\tLoss: 4.6247\tLR: 4.448849\n",
      "Training Epoch: 45 [24704/50000]\tLoss: 4.6441\tLR: 4.449105\n",
      "Training Epoch: 45 [24832/50000]\tLoss: 4.6799\tLR: 4.449361\n",
      "Training Epoch: 45 [24960/50000]\tLoss: 4.6748\tLR: 4.449616\n",
      "Training Epoch: 45 [25088/50000]\tLoss: 4.7074\tLR: 4.449872\n",
      "Training Epoch: 45 [25216/50000]\tLoss: 4.7293\tLR: 4.450128\n",
      "Training Epoch: 45 [25344/50000]\tLoss: 4.7238\tLR: 4.450384\n",
      "Training Epoch: 45 [25472/50000]\tLoss: 4.6517\tLR: 4.450639\n",
      "Training Epoch: 45 [25600/50000]\tLoss: 4.6393\tLR: 4.450895\n",
      "Training Epoch: 45 [25728/50000]\tLoss: 4.6644\tLR: 4.451151\n",
      "Training Epoch: 45 [25856/50000]\tLoss: 4.6972\tLR: 4.451407\n",
      "Training Epoch: 45 [25984/50000]\tLoss: 4.7252\tLR: 4.451662\n",
      "Training Epoch: 45 [26112/50000]\tLoss: 4.6921\tLR: 4.451918\n",
      "Training Epoch: 45 [26240/50000]\tLoss: 4.7017\tLR: 4.452174\n",
      "Training Epoch: 45 [26368/50000]\tLoss: 4.6663\tLR: 4.452430\n",
      "Training Epoch: 45 [26496/50000]\tLoss: 4.6956\tLR: 4.452685\n",
      "Training Epoch: 45 [26624/50000]\tLoss: 4.7243\tLR: 4.452941\n",
      "Training Epoch: 45 [26752/50000]\tLoss: 4.7103\tLR: 4.453197\n",
      "Training Epoch: 45 [26880/50000]\tLoss: 4.6977\tLR: 4.453453\n",
      "Training Epoch: 45 [27008/50000]\tLoss: 4.6755\tLR: 4.453708\n",
      "Training Epoch: 45 [27136/50000]\tLoss: 4.6546\tLR: 4.453964\n",
      "Training Epoch: 45 [27264/50000]\tLoss: 4.7450\tLR: 4.454220\n",
      "Training Epoch: 45 [27392/50000]\tLoss: 4.6876\tLR: 4.454476\n",
      "Training Epoch: 45 [27520/50000]\tLoss: 4.6702\tLR: 4.454731\n",
      "Training Epoch: 45 [27648/50000]\tLoss: 4.6647\tLR: 4.454987\n",
      "Training Epoch: 45 [27776/50000]\tLoss: 4.6783\tLR: 4.455243\n",
      "Training Epoch: 45 [27904/50000]\tLoss: 4.6996\tLR: 4.455499\n",
      "Training Epoch: 45 [28032/50000]\tLoss: 4.7045\tLR: 4.455754\n",
      "Training Epoch: 45 [28160/50000]\tLoss: 4.7143\tLR: 4.456010\n",
      "Training Epoch: 45 [28288/50000]\tLoss: 4.6513\tLR: 4.456266\n",
      "Training Epoch: 45 [28416/50000]\tLoss: 4.7124\tLR: 4.456522\n",
      "Training Epoch: 45 [28544/50000]\tLoss: 4.6609\tLR: 4.456777\n",
      "Training Epoch: 45 [28672/50000]\tLoss: 4.6427\tLR: 4.457033\n",
      "Training Epoch: 45 [28800/50000]\tLoss: 4.6312\tLR: 4.457289\n",
      "Training Epoch: 45 [28928/50000]\tLoss: 4.6687\tLR: 4.457545\n",
      "Training Epoch: 45 [29056/50000]\tLoss: 4.7320\tLR: 4.457801\n",
      "Training Epoch: 45 [29184/50000]\tLoss: 4.7117\tLR: 4.458056\n",
      "Training Epoch: 45 [29312/50000]\tLoss: 4.7457\tLR: 4.458312\n",
      "Training Epoch: 45 [29440/50000]\tLoss: 4.7270\tLR: 4.458568\n",
      "Training Epoch: 45 [29568/50000]\tLoss: 4.7524\tLR: 4.458824\n",
      "Training Epoch: 45 [29696/50000]\tLoss: 4.7339\tLR: 4.459079\n",
      "Training Epoch: 45 [29824/50000]\tLoss: 4.7164\tLR: 4.459335\n",
      "Training Epoch: 45 [29952/50000]\tLoss: 4.7083\tLR: 4.459591\n",
      "Training Epoch: 45 [30080/50000]\tLoss: 4.6520\tLR: 4.459847\n",
      "Training Epoch: 45 [30208/50000]\tLoss: 4.6883\tLR: 4.460102\n",
      "Training Epoch: 45 [30336/50000]\tLoss: 4.6737\tLR: 4.460358\n",
      "Training Epoch: 45 [30464/50000]\tLoss: 4.7249\tLR: 4.460614\n",
      "Training Epoch: 45 [30592/50000]\tLoss: 4.6774\tLR: 4.460870\n",
      "Training Epoch: 45 [30720/50000]\tLoss: 4.6952\tLR: 4.461125\n",
      "Training Epoch: 45 [30848/50000]\tLoss: 4.7047\tLR: 4.461381\n",
      "Training Epoch: 45 [30976/50000]\tLoss: 4.7294\tLR: 4.461637\n",
      "Training Epoch: 45 [31104/50000]\tLoss: 4.6456\tLR: 4.461893\n",
      "Training Epoch: 45 [31232/50000]\tLoss: 4.6107\tLR: 4.462148\n",
      "Training Epoch: 45 [31360/50000]\tLoss: 4.6514\tLR: 4.462404\n",
      "Training Epoch: 45 [31488/50000]\tLoss: 4.7142\tLR: 4.462660\n",
      "Training Epoch: 45 [31616/50000]\tLoss: 4.6821\tLR: 4.462916\n",
      "Training Epoch: 45 [31744/50000]\tLoss: 4.7222\tLR: 4.463171\n",
      "Training Epoch: 45 [31872/50000]\tLoss: 4.7139\tLR: 4.463427\n",
      "Training Epoch: 45 [32000/50000]\tLoss: 4.6820\tLR: 4.463683\n",
      "Training Epoch: 45 [32128/50000]\tLoss: 4.6817\tLR: 4.463939\n",
      "Training Epoch: 45 [32256/50000]\tLoss: 4.7551\tLR: 4.464194\n",
      "Training Epoch: 45 [32384/50000]\tLoss: 4.6948\tLR: 4.464450\n",
      "Training Epoch: 45 [32512/50000]\tLoss: 4.7382\tLR: 4.464706\n",
      "Training Epoch: 45 [32640/50000]\tLoss: 4.6647\tLR: 4.464962\n",
      "Training Epoch: 45 [32768/50000]\tLoss: 4.6774\tLR: 4.465217\n",
      "Training Epoch: 45 [32896/50000]\tLoss: 4.7308\tLR: 4.465473\n",
      "Training Epoch: 45 [33024/50000]\tLoss: 4.7316\tLR: 4.465729\n",
      "Training Epoch: 45 [33152/50000]\tLoss: 4.6947\tLR: 4.465985\n",
      "Training Epoch: 45 [33280/50000]\tLoss: 4.6731\tLR: 4.466240\n",
      "Training Epoch: 45 [33408/50000]\tLoss: 4.6911\tLR: 4.466496\n",
      "Training Epoch: 45 [33536/50000]\tLoss: 4.6611\tLR: 4.466752\n",
      "Training Epoch: 45 [33664/50000]\tLoss: 4.7301\tLR: 4.467008\n",
      "Training Epoch: 45 [33792/50000]\tLoss: 4.6748\tLR: 4.467263\n",
      "Training Epoch: 45 [33920/50000]\tLoss: 4.6845\tLR: 4.467519\n",
      "Training Epoch: 45 [34048/50000]\tLoss: 4.6649\tLR: 4.467775\n",
      "Training Epoch: 45 [34176/50000]\tLoss: 4.6603\tLR: 4.468031\n",
      "Training Epoch: 45 [34304/50000]\tLoss: 4.5972\tLR: 4.468286\n",
      "Training Epoch: 45 [34432/50000]\tLoss: 4.7775\tLR: 4.468542\n",
      "Training Epoch: 45 [34560/50000]\tLoss: 4.6840\tLR: 4.468798\n",
      "Training Epoch: 45 [34688/50000]\tLoss: 4.6806\tLR: 4.469054\n",
      "Training Epoch: 45 [34816/50000]\tLoss: 4.7209\tLR: 4.469309\n",
      "Training Epoch: 45 [34944/50000]\tLoss: 4.7122\tLR: 4.469565\n",
      "Training Epoch: 45 [35072/50000]\tLoss: 4.6969\tLR: 4.469821\n",
      "Training Epoch: 45 [35200/50000]\tLoss: 4.6674\tLR: 4.470077\n",
      "Training Epoch: 45 [35328/50000]\tLoss: 4.7037\tLR: 4.470332\n",
      "Training Epoch: 45 [35456/50000]\tLoss: 4.7405\tLR: 4.470588\n",
      "Training Epoch: 45 [35584/50000]\tLoss: 4.6916\tLR: 4.470844\n",
      "Training Epoch: 45 [35712/50000]\tLoss: 4.7643\tLR: 4.471100\n",
      "Training Epoch: 45 [35840/50000]\tLoss: 4.6829\tLR: 4.471355\n",
      "Training Epoch: 45 [35968/50000]\tLoss: 4.7106\tLR: 4.471611\n",
      "Training Epoch: 45 [36096/50000]\tLoss: 4.6570\tLR: 4.471867\n",
      "Training Epoch: 45 [36224/50000]\tLoss: 4.6267\tLR: 4.472123\n",
      "Training Epoch: 45 [36352/50000]\tLoss: 4.6640\tLR: 4.472379\n",
      "Training Epoch: 45 [36480/50000]\tLoss: 4.6588\tLR: 4.472634\n",
      "Training Epoch: 45 [36608/50000]\tLoss: 4.6925\tLR: 4.472890\n",
      "Training Epoch: 45 [36736/50000]\tLoss: 4.6642\tLR: 4.473146\n",
      "Training Epoch: 45 [36864/50000]\tLoss: 4.7190\tLR: 4.473402\n",
      "Training Epoch: 45 [36992/50000]\tLoss: 4.7204\tLR: 4.473657\n",
      "Training Epoch: 45 [37120/50000]\tLoss: 4.7656\tLR: 4.473913\n",
      "Training Epoch: 45 [37248/50000]\tLoss: 4.7394\tLR: 4.474169\n",
      "Training Epoch: 45 [37376/50000]\tLoss: 4.6522\tLR: 4.474425\n",
      "Training Epoch: 45 [37504/50000]\tLoss: 4.7226\tLR: 4.474680\n",
      "Training Epoch: 45 [37632/50000]\tLoss: 4.6445\tLR: 4.474936\n",
      "Training Epoch: 45 [37760/50000]\tLoss: 4.6938\tLR: 4.475192\n",
      "Training Epoch: 45 [37888/50000]\tLoss: 4.7113\tLR: 4.475448\n",
      "Training Epoch: 45 [38016/50000]\tLoss: 4.7097\tLR: 4.475703\n",
      "Training Epoch: 45 [38144/50000]\tLoss: 4.6710\tLR: 4.475959\n",
      "Training Epoch: 45 [38272/50000]\tLoss: 4.7151\tLR: 4.476215\n",
      "Training Epoch: 45 [38400/50000]\tLoss: 4.7359\tLR: 4.476471\n",
      "Training Epoch: 45 [38528/50000]\tLoss: 4.7010\tLR: 4.476726\n",
      "Training Epoch: 45 [38656/50000]\tLoss: 4.7533\tLR: 4.476982\n",
      "Training Epoch: 45 [38784/50000]\tLoss: 4.6986\tLR: 4.477238\n",
      "Training Epoch: 45 [38912/50000]\tLoss: 4.6968\tLR: 4.477494\n",
      "Training Epoch: 45 [39040/50000]\tLoss: 4.6669\tLR: 4.477749\n",
      "Training Epoch: 45 [39168/50000]\tLoss: 4.6713\tLR: 4.478005\n",
      "Training Epoch: 45 [39296/50000]\tLoss: 4.7002\tLR: 4.478261\n",
      "Training Epoch: 45 [39424/50000]\tLoss: 4.7444\tLR: 4.478517\n",
      "Training Epoch: 45 [39552/50000]\tLoss: 4.7476\tLR: 4.478772\n",
      "Training Epoch: 45 [39680/50000]\tLoss: 4.6756\tLR: 4.479028\n",
      "Training Epoch: 45 [39808/50000]\tLoss: 4.6492\tLR: 4.479284\n",
      "Training Epoch: 45 [39936/50000]\tLoss: 4.6413\tLR: 4.479540\n",
      "Training Epoch: 45 [40064/50000]\tLoss: 4.7487\tLR: 4.479795\n",
      "Training Epoch: 45 [40192/50000]\tLoss: 4.6478\tLR: 4.480051\n",
      "Training Epoch: 45 [40320/50000]\tLoss: 4.7095\tLR: 4.480307\n",
      "Training Epoch: 45 [40448/50000]\tLoss: 4.7214\tLR: 4.480563\n",
      "Training Epoch: 45 [40576/50000]\tLoss: 4.7452\tLR: 4.480818\n",
      "Training Epoch: 45 [40704/50000]\tLoss: 4.6866\tLR: 4.481074\n",
      "Training Epoch: 45 [40832/50000]\tLoss: 4.6496\tLR: 4.481330\n",
      "Training Epoch: 45 [40960/50000]\tLoss: 4.6269\tLR: 4.481586\n",
      "Training Epoch: 45 [41088/50000]\tLoss: 4.6400\tLR: 4.481841\n",
      "Training Epoch: 45 [41216/50000]\tLoss: 4.6615\tLR: 4.482097\n",
      "Training Epoch: 45 [41344/50000]\tLoss: 4.6780\tLR: 4.482353\n",
      "Training Epoch: 45 [41472/50000]\tLoss: 4.6879\tLR: 4.482609\n",
      "Training Epoch: 45 [41600/50000]\tLoss: 4.7042\tLR: 4.482864\n",
      "Training Epoch: 45 [41728/50000]\tLoss: 4.6727\tLR: 4.483120\n",
      "Training Epoch: 45 [41856/50000]\tLoss: 4.6466\tLR: 4.483376\n",
      "Training Epoch: 45 [41984/50000]\tLoss: 4.6798\tLR: 4.483632\n",
      "Training Epoch: 45 [42112/50000]\tLoss: 4.7687\tLR: 4.483887\n",
      "Training Epoch: 45 [42240/50000]\tLoss: 4.7005\tLR: 4.484143\n",
      "Training Epoch: 45 [42368/50000]\tLoss: 4.6284\tLR: 4.484399\n",
      "Training Epoch: 45 [42496/50000]\tLoss: 4.6774\tLR: 4.484655\n",
      "Training Epoch: 45 [42624/50000]\tLoss: 4.6314\tLR: 4.484910\n",
      "Training Epoch: 45 [42752/50000]\tLoss: 4.6788\tLR: 4.485166\n",
      "Training Epoch: 45 [42880/50000]\tLoss: 4.6737\tLR: 4.485422\n",
      "Training Epoch: 45 [43008/50000]\tLoss: 4.6964\tLR: 4.485678\n",
      "Training Epoch: 45 [43136/50000]\tLoss: 4.7024\tLR: 4.485934\n",
      "Training Epoch: 45 [43264/50000]\tLoss: 4.6901\tLR: 4.486189\n",
      "Training Epoch: 45 [43392/50000]\tLoss: 4.7119\tLR: 4.486445\n",
      "Training Epoch: 45 [43520/50000]\tLoss: 4.6871\tLR: 4.486701\n",
      "Training Epoch: 45 [43648/50000]\tLoss: 4.7383\tLR: 4.486957\n",
      "Training Epoch: 45 [43776/50000]\tLoss: 4.6733\tLR: 4.487212\n",
      "Training Epoch: 45 [43904/50000]\tLoss: 4.6913\tLR: 4.487468\n",
      "Training Epoch: 45 [44032/50000]\tLoss: 4.6446\tLR: 4.487724\n",
      "Training Epoch: 45 [44160/50000]\tLoss: 4.6681\tLR: 4.487980\n",
      "Training Epoch: 45 [44288/50000]\tLoss: 4.6917\tLR: 4.488235\n",
      "Training Epoch: 45 [44416/50000]\tLoss: 4.6257\tLR: 4.488491\n",
      "Training Epoch: 45 [44544/50000]\tLoss: 4.6616\tLR: 4.488747\n",
      "Training Epoch: 45 [44672/50000]\tLoss: 4.6917\tLR: 4.489003\n",
      "Training Epoch: 45 [44800/50000]\tLoss: 4.7275\tLR: 4.489258\n",
      "Training Epoch: 45 [44928/50000]\tLoss: 4.6900\tLR: 4.489514\n",
      "Training Epoch: 45 [45056/50000]\tLoss: 4.6464\tLR: 4.489770\n",
      "Training Epoch: 45 [45184/50000]\tLoss: 4.7429\tLR: 4.490026\n",
      "Training Epoch: 45 [45312/50000]\tLoss: 4.6902\tLR: 4.490281\n",
      "Training Epoch: 45 [45440/50000]\tLoss: 4.6555\tLR: 4.490537\n",
      "Training Epoch: 45 [45568/50000]\tLoss: 4.6800\tLR: 4.490793\n",
      "Training Epoch: 45 [45696/50000]\tLoss: 4.6955\tLR: 4.491049\n",
      "Training Epoch: 45 [45824/50000]\tLoss: 4.6663\tLR: 4.491304\n",
      "Training Epoch: 45 [45952/50000]\tLoss: 4.7069\tLR: 4.491560\n",
      "Training Epoch: 45 [46080/50000]\tLoss: 4.6791\tLR: 4.491816\n",
      "Training Epoch: 45 [46208/50000]\tLoss: 4.6962\tLR: 4.492072\n",
      "Training Epoch: 45 [46336/50000]\tLoss: 4.6164\tLR: 4.492327\n",
      "Training Epoch: 45 [46464/50000]\tLoss: 4.6546\tLR: 4.492583\n",
      "Training Epoch: 45 [46592/50000]\tLoss: 4.6996\tLR: 4.492839\n",
      "Training Epoch: 45 [46720/50000]\tLoss: 4.6919\tLR: 4.493095\n",
      "Training Epoch: 45 [46848/50000]\tLoss: 4.7527\tLR: 4.493350\n",
      "Training Epoch: 45 [46976/50000]\tLoss: 4.6632\tLR: 4.493606\n",
      "Training Epoch: 45 [47104/50000]\tLoss: 4.7670\tLR: 4.493862\n",
      "Training Epoch: 45 [47232/50000]\tLoss: 4.7706\tLR: 4.494118\n",
      "Training Epoch: 45 [47360/50000]\tLoss: 4.7129\tLR: 4.494373\n",
      "Training Epoch: 45 [47488/50000]\tLoss: 4.6788\tLR: 4.494629\n",
      "Training Epoch: 45 [47616/50000]\tLoss: 4.6698\tLR: 4.494885\n",
      "Training Epoch: 45 [47744/50000]\tLoss: 4.6722\tLR: 4.495141\n",
      "Training Epoch: 45 [47872/50000]\tLoss: 4.6942\tLR: 4.495396\n",
      "Training Epoch: 45 [48000/50000]\tLoss: 4.6566\tLR: 4.495652\n",
      "Training Epoch: 45 [48128/50000]\tLoss: 4.7167\tLR: 4.495908\n",
      "Training Epoch: 45 [48256/50000]\tLoss: 4.6845\tLR: 4.496164\n",
      "Training Epoch: 45 [48384/50000]\tLoss: 4.6811\tLR: 4.496419\n",
      "Training Epoch: 45 [48512/50000]\tLoss: 4.5877\tLR: 4.496675\n",
      "Training Epoch: 45 [48640/50000]\tLoss: 4.7373\tLR: 4.496931\n",
      "Training Epoch: 45 [48768/50000]\tLoss: 4.7207\tLR: 4.497187\n",
      "Training Epoch: 45 [48896/50000]\tLoss: 4.7245\tLR: 4.497442\n",
      "Training Epoch: 45 [49024/50000]\tLoss: 4.6115\tLR: 4.497698\n",
      "Training Epoch: 45 [49152/50000]\tLoss: 4.6598\tLR: 4.497954\n",
      "Training Epoch: 45 [49280/50000]\tLoss: 4.7465\tLR: 4.498210\n",
      "Training Epoch: 45 [49408/50000]\tLoss: 4.6845\tLR: 4.498465\n",
      "Training Epoch: 45 [49536/50000]\tLoss: 4.8054\tLR: 4.498721\n",
      "Training Epoch: 45 [49664/50000]\tLoss: 4.7085\tLR: 4.498977\n",
      "Training Epoch: 45 [49792/50000]\tLoss: 4.7089\tLR: 4.499233\n",
      "Training Epoch: 45 [49920/50000]\tLoss: 4.6803\tLR: 4.499488\n",
      "Training Epoch: 45 [50000/50000]\tLoss: 4.6737\tLR: 4.499744\n",
      "epoch 45 training time consumed: 488.83s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   63087 GB |   63087 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   62894 GB |   62894 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     193 GB |     193 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   63087 GB |   63087 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   62894 GB |   62894 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     193 GB |     193 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   62199 GB |   62199 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   62005 GB |   62005 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     193 GB |     193 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    6689 K  |    6689 K  |\n",
      "|       from large pool |      24    |      65    |    2851 K  |    2851 K  |\n",
      "|       from small pool |     231    |     274    |    3838 K  |    3837 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    6689 K  |    6689 K  |\n",
      "|       from large pool |      24    |      65    |    2851 K  |    2851 K  |\n",
      "|       from small pool |     231    |     274    |    3838 K  |    3837 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      38    |      47    |    3877 K  |    3877 K  |\n",
      "|       from large pool |      10    |      23    |    1370 K  |    1370 K  |\n",
      "|       from small pool |      28    |      35    |    2506 K  |    2506 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 45, Average loss: 0.0369, Accuracy: 0.0100, Time consumed:31.06s\n",
      "\n",
      "Training Epoch: 46 [128/50000]\tLoss: 4.6852\tLR: 0.100000\n",
      "Training Epoch: 46 [256/50000]\tLoss: 4.6620\tLR: 4.500256\n",
      "Training Epoch: 46 [384/50000]\tLoss: 4.6161\tLR: 4.500512\n",
      "Training Epoch: 46 [512/50000]\tLoss: 4.6283\tLR: 4.500767\n",
      "Training Epoch: 46 [640/50000]\tLoss: 4.6436\tLR: 4.501023\n",
      "Training Epoch: 46 [768/50000]\tLoss: 4.6744\tLR: 4.501279\n",
      "Training Epoch: 46 [896/50000]\tLoss: 4.7221\tLR: 4.501535\n",
      "Training Epoch: 46 [1024/50000]\tLoss: 4.6312\tLR: 4.501790\n",
      "Training Epoch: 46 [1152/50000]\tLoss: 4.7342\tLR: 4.502046\n",
      "Training Epoch: 46 [1280/50000]\tLoss: 4.6618\tLR: 4.502302\n",
      "Training Epoch: 46 [1408/50000]\tLoss: 4.7044\tLR: 4.502558\n",
      "Training Epoch: 46 [1536/50000]\tLoss: 4.6546\tLR: 4.502813\n",
      "Training Epoch: 46 [1664/50000]\tLoss: 4.6723\tLR: 4.503069\n",
      "Training Epoch: 46 [1792/50000]\tLoss: 4.6751\tLR: 4.503325\n",
      "Training Epoch: 46 [1920/50000]\tLoss: 4.7551\tLR: 4.503581\n",
      "Training Epoch: 46 [2048/50000]\tLoss: 4.7101\tLR: 4.503836\n",
      "Training Epoch: 46 [2176/50000]\tLoss: 4.7047\tLR: 4.504092\n",
      "Training Epoch: 46 [2304/50000]\tLoss: 4.7438\tLR: 4.504348\n",
      "Training Epoch: 46 [2432/50000]\tLoss: 4.7494\tLR: 4.504604\n",
      "Training Epoch: 46 [2560/50000]\tLoss: 4.7866\tLR: 4.504859\n",
      "Training Epoch: 46 [2688/50000]\tLoss: 4.6891\tLR: 4.505115\n",
      "Training Epoch: 46 [2816/50000]\tLoss: 4.6519\tLR: 4.505371\n",
      "Training Epoch: 46 [2944/50000]\tLoss: 4.6712\tLR: 4.505627\n",
      "Training Epoch: 46 [3072/50000]\tLoss: 4.6709\tLR: 4.505882\n",
      "Training Epoch: 46 [3200/50000]\tLoss: 4.6913\tLR: 4.506138\n",
      "Training Epoch: 46 [3328/50000]\tLoss: 4.6754\tLR: 4.506394\n",
      "Training Epoch: 46 [3456/50000]\tLoss: 4.6627\tLR: 4.506650\n",
      "Training Epoch: 46 [3584/50000]\tLoss: 4.6405\tLR: 4.506905\n",
      "Training Epoch: 46 [3712/50000]\tLoss: 4.6827\tLR: 4.507161\n",
      "Training Epoch: 46 [3840/50000]\tLoss: 4.8320\tLR: 4.507417\n",
      "Training Epoch: 46 [3968/50000]\tLoss: 4.6926\tLR: 4.507673\n",
      "Training Epoch: 46 [4096/50000]\tLoss: 4.6649\tLR: 4.507928\n",
      "Training Epoch: 46 [4224/50000]\tLoss: 4.7824\tLR: 4.508184\n",
      "Training Epoch: 46 [4352/50000]\tLoss: 4.7406\tLR: 4.508440\n",
      "Training Epoch: 46 [4480/50000]\tLoss: 4.6659\tLR: 4.508696\n",
      "Training Epoch: 46 [4608/50000]\tLoss: 4.7146\tLR: 4.508951\n",
      "Training Epoch: 46 [4736/50000]\tLoss: 4.7443\tLR: 4.509207\n",
      "Training Epoch: 46 [4864/50000]\tLoss: 4.7379\tLR: 4.509463\n",
      "Training Epoch: 46 [4992/50000]\tLoss: 4.7048\tLR: 4.509719\n",
      "Training Epoch: 46 [5120/50000]\tLoss: 4.7547\tLR: 4.509974\n",
      "Training Epoch: 46 [5248/50000]\tLoss: 4.6747\tLR: 4.510230\n",
      "Training Epoch: 46 [5376/50000]\tLoss: 4.6144\tLR: 4.510486\n",
      "Training Epoch: 46 [5504/50000]\tLoss: 4.7325\tLR: 4.510742\n",
      "Training Epoch: 46 [5632/50000]\tLoss: 4.7337\tLR: 4.510997\n",
      "Training Epoch: 46 [5760/50000]\tLoss: 4.6817\tLR: 4.511253\n",
      "Training Epoch: 46 [5888/50000]\tLoss: 4.6980\tLR: 4.511509\n",
      "Training Epoch: 46 [6016/50000]\tLoss: 4.6616\tLR: 4.511765\n",
      "Training Epoch: 46 [6144/50000]\tLoss: 4.6639\tLR: 4.512020\n",
      "Training Epoch: 46 [6272/50000]\tLoss: 4.6805\tLR: 4.512276\n",
      "Training Epoch: 46 [6400/50000]\tLoss: 4.7609\tLR: 4.512532\n",
      "Training Epoch: 46 [6528/50000]\tLoss: 4.6471\tLR: 4.512788\n",
      "Training Epoch: 46 [6656/50000]\tLoss: 4.7196\tLR: 4.513043\n",
      "Training Epoch: 46 [6784/50000]\tLoss: 4.7141\tLR: 4.513299\n",
      "Training Epoch: 46 [6912/50000]\tLoss: 4.6869\tLR: 4.513555\n",
      "Training Epoch: 46 [7040/50000]\tLoss: 4.6863\tLR: 4.513811\n",
      "Training Epoch: 46 [7168/50000]\tLoss: 4.7086\tLR: 4.514066\n",
      "Training Epoch: 46 [7296/50000]\tLoss: 4.7552\tLR: 4.514322\n",
      "Training Epoch: 46 [7424/50000]\tLoss: 4.6323\tLR: 4.514578\n",
      "Training Epoch: 46 [7552/50000]\tLoss: 4.6688\tLR: 4.514834\n",
      "Training Epoch: 46 [7680/50000]\tLoss: 4.7396\tLR: 4.515090\n",
      "Training Epoch: 46 [7808/50000]\tLoss: 4.7044\tLR: 4.515345\n",
      "Training Epoch: 46 [7936/50000]\tLoss: 4.7676\tLR: 4.515601\n",
      "Training Epoch: 46 [8064/50000]\tLoss: 4.7202\tLR: 4.515857\n",
      "Training Epoch: 46 [8192/50000]\tLoss: 4.6392\tLR: 4.516113\n",
      "Training Epoch: 46 [8320/50000]\tLoss: 4.6453\tLR: 4.516368\n",
      "Training Epoch: 46 [8448/50000]\tLoss: 4.6940\tLR: 4.516624\n",
      "Training Epoch: 46 [8576/50000]\tLoss: 4.6863\tLR: 4.516880\n",
      "Training Epoch: 46 [8704/50000]\tLoss: 4.6193\tLR: 4.517136\n",
      "Training Epoch: 46 [8832/50000]\tLoss: 4.7706\tLR: 4.517391\n",
      "Training Epoch: 46 [8960/50000]\tLoss: 4.7372\tLR: 4.517647\n",
      "Training Epoch: 46 [9088/50000]\tLoss: 4.7817\tLR: 4.517903\n",
      "Training Epoch: 46 [9216/50000]\tLoss: 4.6744\tLR: 4.518159\n",
      "Training Epoch: 46 [9344/50000]\tLoss: 4.7054\tLR: 4.518414\n",
      "Training Epoch: 46 [9472/50000]\tLoss: 4.7256\tLR: 4.518670\n",
      "Training Epoch: 46 [9600/50000]\tLoss: 4.7332\tLR: 4.518926\n",
      "Training Epoch: 46 [9728/50000]\tLoss: 4.6500\tLR: 4.519182\n",
      "Training Epoch: 46 [9856/50000]\tLoss: 4.7040\tLR: 4.519437\n",
      "Training Epoch: 46 [9984/50000]\tLoss: 4.7042\tLR: 4.519693\n",
      "Training Epoch: 46 [10112/50000]\tLoss: 4.6792\tLR: 4.519949\n",
      "Training Epoch: 46 [10240/50000]\tLoss: 4.7152\tLR: 4.520205\n",
      "Training Epoch: 46 [10368/50000]\tLoss: 4.6555\tLR: 4.520460\n",
      "Training Epoch: 46 [10496/50000]\tLoss: 4.6468\tLR: 4.520716\n",
      "Training Epoch: 46 [10624/50000]\tLoss: 4.6719\tLR: 4.520972\n",
      "Training Epoch: 46 [10752/50000]\tLoss: 4.6456\tLR: 4.521228\n",
      "Training Epoch: 46 [10880/50000]\tLoss: 4.6353\tLR: 4.521483\n",
      "Training Epoch: 46 [11008/50000]\tLoss: 4.7485\tLR: 4.521739\n",
      "Training Epoch: 46 [11136/50000]\tLoss: 4.6795\tLR: 4.521995\n",
      "Training Epoch: 46 [11264/50000]\tLoss: 4.7660\tLR: 4.522251\n",
      "Training Epoch: 46 [11392/50000]\tLoss: 4.6158\tLR: 4.522506\n",
      "Training Epoch: 46 [11520/50000]\tLoss: 4.5726\tLR: 4.522762\n",
      "Training Epoch: 46 [11648/50000]\tLoss: 4.6796\tLR: 4.523018\n",
      "Training Epoch: 46 [11776/50000]\tLoss: 4.6655\tLR: 4.523274\n",
      "Training Epoch: 46 [11904/50000]\tLoss: 4.6421\tLR: 4.523529\n",
      "Training Epoch: 46 [12032/50000]\tLoss: 4.6739\tLR: 4.523785\n",
      "Training Epoch: 46 [12160/50000]\tLoss: 4.6843\tLR: 4.524041\n",
      "Training Epoch: 46 [12288/50000]\tLoss: 4.8257\tLR: 4.524297\n",
      "Training Epoch: 46 [12416/50000]\tLoss: 4.6856\tLR: 4.524552\n",
      "Training Epoch: 46 [12544/50000]\tLoss: 4.6709\tLR: 4.524808\n",
      "Training Epoch: 46 [12672/50000]\tLoss: 4.7143\tLR: 4.525064\n",
      "Training Epoch: 46 [12800/50000]\tLoss: 4.6487\tLR: 4.525320\n",
      "Training Epoch: 46 [12928/50000]\tLoss: 4.6857\tLR: 4.525575\n",
      "Training Epoch: 46 [13056/50000]\tLoss: 4.7180\tLR: 4.525831\n",
      "Training Epoch: 46 [13184/50000]\tLoss: 4.7359\tLR: 4.526087\n",
      "Training Epoch: 46 [13312/50000]\tLoss: 4.7269\tLR: 4.526343\n",
      "Training Epoch: 46 [13440/50000]\tLoss: 4.7148\tLR: 4.526598\n",
      "Training Epoch: 46 [13568/50000]\tLoss: 4.7375\tLR: 4.526854\n",
      "Training Epoch: 46 [13696/50000]\tLoss: 4.6665\tLR: 4.527110\n",
      "Training Epoch: 46 [13824/50000]\tLoss: 4.6895\tLR: 4.527366\n",
      "Training Epoch: 46 [13952/50000]\tLoss: 4.7271\tLR: 4.527621\n",
      "Training Epoch: 46 [14080/50000]\tLoss: 4.6740\tLR: 4.527877\n",
      "Training Epoch: 46 [14208/50000]\tLoss: 4.6601\tLR: 4.528133\n",
      "Training Epoch: 46 [14336/50000]\tLoss: 4.6107\tLR: 4.528389\n",
      "Training Epoch: 46 [14464/50000]\tLoss: 4.6782\tLR: 4.528645\n",
      "Training Epoch: 46 [14592/50000]\tLoss: 4.6630\tLR: 4.528900\n",
      "Training Epoch: 46 [14720/50000]\tLoss: 4.6511\tLR: 4.529156\n",
      "Training Epoch: 46 [14848/50000]\tLoss: 4.6418\tLR: 4.529412\n",
      "Training Epoch: 46 [14976/50000]\tLoss: 4.6851\tLR: 4.529668\n",
      "Training Epoch: 46 [15104/50000]\tLoss: 4.7290\tLR: 4.529923\n",
      "Training Epoch: 46 [15232/50000]\tLoss: 4.7944\tLR: 4.530179\n",
      "Training Epoch: 46 [15360/50000]\tLoss: 4.6919\tLR: 4.530435\n",
      "Training Epoch: 46 [15488/50000]\tLoss: 4.6866\tLR: 4.530691\n",
      "Training Epoch: 46 [15616/50000]\tLoss: 4.6965\tLR: 4.530946\n",
      "Training Epoch: 46 [15744/50000]\tLoss: 4.6669\tLR: 4.531202\n",
      "Training Epoch: 46 [15872/50000]\tLoss: 4.6300\tLR: 4.531458\n",
      "Training Epoch: 46 [16000/50000]\tLoss: 4.7112\tLR: 4.531714\n",
      "Training Epoch: 46 [16128/50000]\tLoss: 4.6858\tLR: 4.531969\n",
      "Training Epoch: 46 [16256/50000]\tLoss: 4.6997\tLR: 4.532225\n",
      "Training Epoch: 46 [16384/50000]\tLoss: 4.7154\tLR: 4.532481\n",
      "Training Epoch: 46 [16512/50000]\tLoss: 4.6897\tLR: 4.532737\n",
      "Training Epoch: 46 [16640/50000]\tLoss: 4.7282\tLR: 4.532992\n",
      "Training Epoch: 46 [16768/50000]\tLoss: 4.6730\tLR: 4.533248\n",
      "Training Epoch: 46 [16896/50000]\tLoss: 4.6924\tLR: 4.533504\n",
      "Training Epoch: 46 [17024/50000]\tLoss: 4.7024\tLR: 4.533760\n",
      "Training Epoch: 46 [17152/50000]\tLoss: 4.7263\tLR: 4.534015\n",
      "Training Epoch: 46 [17280/50000]\tLoss: 4.7137\tLR: 4.534271\n",
      "Training Epoch: 46 [17408/50000]\tLoss: 4.6848\tLR: 4.534527\n",
      "Training Epoch: 46 [17536/50000]\tLoss: 4.6735\tLR: 4.534783\n",
      "Training Epoch: 46 [17664/50000]\tLoss: 4.6638\tLR: 4.535038\n",
      "Training Epoch: 46 [17792/50000]\tLoss: 4.6598\tLR: 4.535294\n",
      "Training Epoch: 46 [17920/50000]\tLoss: 4.7275\tLR: 4.535550\n",
      "Training Epoch: 46 [18048/50000]\tLoss: 4.6589\tLR: 4.535806\n",
      "Training Epoch: 46 [18176/50000]\tLoss: 4.7246\tLR: 4.536061\n",
      "Training Epoch: 46 [18304/50000]\tLoss: 4.6991\tLR: 4.536317\n",
      "Training Epoch: 46 [18432/50000]\tLoss: 4.7070\tLR: 4.536573\n",
      "Training Epoch: 46 [18560/50000]\tLoss: 4.6440\tLR: 4.536829\n",
      "Training Epoch: 46 [18688/50000]\tLoss: 4.6785\tLR: 4.537084\n",
      "Training Epoch: 46 [18816/50000]\tLoss: 4.7004\tLR: 4.537340\n",
      "Training Epoch: 46 [18944/50000]\tLoss: 4.6306\tLR: 4.537596\n",
      "Training Epoch: 46 [19072/50000]\tLoss: 4.7303\tLR: 4.537852\n",
      "Training Epoch: 46 [19200/50000]\tLoss: 4.6970\tLR: 4.538107\n",
      "Training Epoch: 46 [19328/50000]\tLoss: 4.6563\tLR: 4.538363\n",
      "Training Epoch: 46 [19456/50000]\tLoss: 4.7181\tLR: 4.538619\n",
      "Training Epoch: 46 [19584/50000]\tLoss: 4.6647\tLR: 4.538875\n",
      "Training Epoch: 46 [19712/50000]\tLoss: 4.6642\tLR: 4.539130\n",
      "Training Epoch: 46 [19840/50000]\tLoss: 4.7324\tLR: 4.539386\n",
      "Training Epoch: 46 [19968/50000]\tLoss: 4.6382\tLR: 4.539642\n",
      "Training Epoch: 46 [20096/50000]\tLoss: 4.7078\tLR: 4.539898\n",
      "Training Epoch: 46 [20224/50000]\tLoss: 4.6712\tLR: 4.540153\n",
      "Training Epoch: 46 [20352/50000]\tLoss: 4.6868\tLR: 4.540409\n",
      "Training Epoch: 46 [20480/50000]\tLoss: 4.7286\tLR: 4.540665\n",
      "Training Epoch: 46 [20608/50000]\tLoss: 4.7086\tLR: 4.540921\n",
      "Training Epoch: 46 [20736/50000]\tLoss: 4.6343\tLR: 4.541176\n",
      "Training Epoch: 46 [20864/50000]\tLoss: 4.6876\tLR: 4.541432\n",
      "Training Epoch: 46 [20992/50000]\tLoss: 4.6807\tLR: 4.541688\n",
      "Training Epoch: 46 [21120/50000]\tLoss: 4.6660\tLR: 4.541944\n",
      "Training Epoch: 46 [21248/50000]\tLoss: 4.6423\tLR: 4.542199\n",
      "Training Epoch: 46 [21376/50000]\tLoss: 4.7053\tLR: 4.542455\n",
      "Training Epoch: 46 [21504/50000]\tLoss: 4.7296\tLR: 4.542711\n",
      "Training Epoch: 46 [21632/50000]\tLoss: 4.7250\tLR: 4.542967\n",
      "Training Epoch: 46 [21760/50000]\tLoss: 4.6797\tLR: 4.543223\n",
      "Training Epoch: 46 [21888/50000]\tLoss: 4.7489\tLR: 4.543478\n",
      "Training Epoch: 46 [22016/50000]\tLoss: 4.6333\tLR: 4.543734\n",
      "Training Epoch: 46 [22144/50000]\tLoss: 4.6063\tLR: 4.543990\n",
      "Training Epoch: 46 [22272/50000]\tLoss: 4.7444\tLR: 4.544246\n",
      "Training Epoch: 46 [22400/50000]\tLoss: 4.6789\tLR: 4.544501\n",
      "Training Epoch: 46 [22528/50000]\tLoss: 4.6991\tLR: 4.544757\n",
      "Training Epoch: 46 [22656/50000]\tLoss: 4.7657\tLR: 4.545013\n",
      "Training Epoch: 46 [22784/50000]\tLoss: 4.7213\tLR: 4.545269\n",
      "Training Epoch: 46 [22912/50000]\tLoss: 4.6952\tLR: 4.545524\n",
      "Training Epoch: 46 [23040/50000]\tLoss: 4.6803\tLR: 4.545780\n",
      "Training Epoch: 46 [23168/50000]\tLoss: 4.7134\tLR: 4.546036\n",
      "Training Epoch: 46 [23296/50000]\tLoss: 4.7270\tLR: 4.546292\n",
      "Training Epoch: 46 [23424/50000]\tLoss: 4.6339\tLR: 4.546547\n",
      "Training Epoch: 46 [23552/50000]\tLoss: 4.7019\tLR: 4.546803\n",
      "Training Epoch: 46 [23680/50000]\tLoss: 4.6610\tLR: 4.547059\n",
      "Training Epoch: 46 [23808/50000]\tLoss: 4.7271\tLR: 4.547315\n",
      "Training Epoch: 46 [23936/50000]\tLoss: 4.6633\tLR: 4.547570\n",
      "Training Epoch: 46 [24064/50000]\tLoss: 4.6387\tLR: 4.547826\n",
      "Training Epoch: 46 [24192/50000]\tLoss: 4.7286\tLR: 4.548082\n",
      "Training Epoch: 46 [24320/50000]\tLoss: 4.7181\tLR: 4.548338\n",
      "Training Epoch: 46 [24448/50000]\tLoss: 4.6955\tLR: 4.548593\n",
      "Training Epoch: 46 [24576/50000]\tLoss: 4.7703\tLR: 4.548849\n",
      "Training Epoch: 46 [24704/50000]\tLoss: 4.6949\tLR: 4.549105\n",
      "Training Epoch: 46 [24832/50000]\tLoss: 4.6787\tLR: 4.549361\n",
      "Training Epoch: 46 [24960/50000]\tLoss: 4.7532\tLR: 4.549616\n",
      "Training Epoch: 46 [25088/50000]\tLoss: 4.7424\tLR: 4.549872\n",
      "Training Epoch: 46 [25216/50000]\tLoss: 4.7146\tLR: 4.550128\n",
      "Training Epoch: 46 [25344/50000]\tLoss: 4.7041\tLR: 4.550384\n",
      "Training Epoch: 46 [25472/50000]\tLoss: 4.6340\tLR: 4.550639\n",
      "Training Epoch: 46 [25600/50000]\tLoss: 4.6911\tLR: 4.550895\n",
      "Training Epoch: 46 [25728/50000]\tLoss: 4.6330\tLR: 4.551151\n",
      "Training Epoch: 46 [25856/50000]\tLoss: 4.7780\tLR: 4.551407\n",
      "Training Epoch: 46 [25984/50000]\tLoss: 4.7146\tLR: 4.551662\n",
      "Training Epoch: 46 [26112/50000]\tLoss: 4.7748\tLR: 4.551918\n",
      "Training Epoch: 46 [26240/50000]\tLoss: 4.7433\tLR: 4.552174\n",
      "Training Epoch: 46 [26368/50000]\tLoss: 4.7403\tLR: 4.552430\n",
      "Training Epoch: 46 [26496/50000]\tLoss: 4.6893\tLR: 4.552685\n",
      "Training Epoch: 46 [26624/50000]\tLoss: 4.7087\tLR: 4.552941\n",
      "Training Epoch: 46 [26752/50000]\tLoss: 4.6640\tLR: 4.553197\n",
      "Training Epoch: 46 [26880/50000]\tLoss: 4.7042\tLR: 4.553453\n",
      "Training Epoch: 46 [27008/50000]\tLoss: 4.6087\tLR: 4.553708\n",
      "Training Epoch: 46 [27136/50000]\tLoss: 4.6998\tLR: 4.553964\n",
      "Training Epoch: 46 [27264/50000]\tLoss: 4.7840\tLR: 4.554220\n",
      "Training Epoch: 46 [27392/50000]\tLoss: 4.7526\tLR: 4.554476\n",
      "Training Epoch: 46 [27520/50000]\tLoss: 4.6498\tLR: 4.554731\n",
      "Training Epoch: 46 [27648/50000]\tLoss: 4.7520\tLR: 4.554987\n",
      "Training Epoch: 46 [27776/50000]\tLoss: 4.7186\tLR: 4.555243\n",
      "Training Epoch: 46 [27904/50000]\tLoss: 4.7108\tLR: 4.555499\n",
      "Training Epoch: 46 [28032/50000]\tLoss: 4.7331\tLR: 4.555754\n",
      "Training Epoch: 46 [28160/50000]\tLoss: 4.6636\tLR: 4.556010\n",
      "Training Epoch: 46 [28288/50000]\tLoss: 4.6690\tLR: 4.556266\n",
      "Training Epoch: 46 [28416/50000]\tLoss: 4.7557\tLR: 4.556522\n",
      "Training Epoch: 46 [28544/50000]\tLoss: 4.6699\tLR: 4.556777\n",
      "Training Epoch: 46 [28672/50000]\tLoss: 4.6854\tLR: 4.557033\n",
      "Training Epoch: 46 [28800/50000]\tLoss: 4.6773\tLR: 4.557289\n",
      "Training Epoch: 46 [28928/50000]\tLoss: 4.6813\tLR: 4.557545\n",
      "Training Epoch: 46 [29056/50000]\tLoss: 4.7033\tLR: 4.557801\n",
      "Training Epoch: 46 [29184/50000]\tLoss: 4.7447\tLR: 4.558056\n",
      "Training Epoch: 46 [29312/50000]\tLoss: 4.6184\tLR: 4.558312\n",
      "Training Epoch: 46 [29440/50000]\tLoss: 4.6783\tLR: 4.558568\n",
      "Training Epoch: 46 [29568/50000]\tLoss: 4.6411\tLR: 4.558824\n",
      "Training Epoch: 46 [29696/50000]\tLoss: 4.6813\tLR: 4.559079\n",
      "Training Epoch: 46 [29824/50000]\tLoss: 4.7151\tLR: 4.559335\n",
      "Training Epoch: 46 [29952/50000]\tLoss: 4.7274\tLR: 4.559591\n",
      "Training Epoch: 46 [30080/50000]\tLoss: 4.6594\tLR: 4.559847\n",
      "Training Epoch: 46 [30208/50000]\tLoss: 4.6478\tLR: 4.560102\n",
      "Training Epoch: 46 [30336/50000]\tLoss: 4.6305\tLR: 4.560358\n",
      "Training Epoch: 46 [30464/50000]\tLoss: 4.6948\tLR: 4.560614\n",
      "Training Epoch: 46 [30592/50000]\tLoss: 4.6428\tLR: 4.560870\n",
      "Training Epoch: 46 [30720/50000]\tLoss: 4.7224\tLR: 4.561125\n",
      "Training Epoch: 46 [30848/50000]\tLoss: 4.6941\tLR: 4.561381\n",
      "Training Epoch: 46 [30976/50000]\tLoss: 4.6811\tLR: 4.561637\n",
      "Training Epoch: 46 [31104/50000]\tLoss: 4.7421\tLR: 4.561893\n",
      "Training Epoch: 46 [31232/50000]\tLoss: 4.7647\tLR: 4.562148\n",
      "Training Epoch: 46 [31360/50000]\tLoss: 4.6778\tLR: 4.562404\n",
      "Training Epoch: 46 [31488/50000]\tLoss: 4.6654\tLR: 4.562660\n",
      "Training Epoch: 46 [31616/50000]\tLoss: 4.6736\tLR: 4.562916\n",
      "Training Epoch: 46 [31744/50000]\tLoss: 4.6493\tLR: 4.563171\n",
      "Training Epoch: 46 [31872/50000]\tLoss: 4.6561\tLR: 4.563427\n",
      "Training Epoch: 46 [32000/50000]\tLoss: 4.6284\tLR: 4.563683\n",
      "Training Epoch: 46 [32128/50000]\tLoss: 4.7324\tLR: 4.563939\n",
      "Training Epoch: 46 [32256/50000]\tLoss: 4.7630\tLR: 4.564194\n",
      "Training Epoch: 46 [32384/50000]\tLoss: 4.6741\tLR: 4.564450\n",
      "Training Epoch: 46 [32512/50000]\tLoss: 4.7569\tLR: 4.564706\n",
      "Training Epoch: 46 [32640/50000]\tLoss: 4.6616\tLR: 4.564962\n",
      "Training Epoch: 46 [32768/50000]\tLoss: 4.7286\tLR: 4.565217\n",
      "Training Epoch: 46 [32896/50000]\tLoss: 4.7268\tLR: 4.565473\n",
      "Training Epoch: 46 [33024/50000]\tLoss: 4.6296\tLR: 4.565729\n",
      "Training Epoch: 46 [33152/50000]\tLoss: 4.7105\tLR: 4.565985\n",
      "Training Epoch: 46 [33280/50000]\tLoss: 4.6671\tLR: 4.566240\n",
      "Training Epoch: 46 [33408/50000]\tLoss: 4.6690\tLR: 4.566496\n",
      "Training Epoch: 46 [33536/50000]\tLoss: 4.7158\tLR: 4.566752\n",
      "Training Epoch: 46 [33664/50000]\tLoss: 4.6934\tLR: 4.567008\n",
      "Training Epoch: 46 [33792/50000]\tLoss: 4.6442\tLR: 4.567263\n",
      "Training Epoch: 46 [33920/50000]\tLoss: 4.6651\tLR: 4.567519\n",
      "Training Epoch: 46 [34048/50000]\tLoss: 4.7258\tLR: 4.567775\n",
      "Training Epoch: 46 [34176/50000]\tLoss: 4.6858\tLR: 4.568031\n",
      "Training Epoch: 46 [34304/50000]\tLoss: 4.7492\tLR: 4.568286\n",
      "Training Epoch: 46 [34432/50000]\tLoss: 4.6268\tLR: 4.568542\n",
      "Training Epoch: 46 [34560/50000]\tLoss: 4.6201\tLR: 4.568798\n",
      "Training Epoch: 46 [34688/50000]\tLoss: 4.6556\tLR: 4.569054\n",
      "Training Epoch: 46 [34816/50000]\tLoss: 4.6816\tLR: 4.569309\n",
      "Training Epoch: 46 [34944/50000]\tLoss: 4.6585\tLR: 4.569565\n",
      "Training Epoch: 46 [35072/50000]\tLoss: 4.6911\tLR: 4.569821\n",
      "Training Epoch: 46 [35200/50000]\tLoss: 4.6345\tLR: 4.570077\n",
      "Training Epoch: 46 [35328/50000]\tLoss: 4.6928\tLR: 4.570332\n",
      "Training Epoch: 46 [35456/50000]\tLoss: 4.7660\tLR: 4.570588\n",
      "Training Epoch: 46 [35584/50000]\tLoss: 4.7486\tLR: 4.570844\n",
      "Training Epoch: 46 [35712/50000]\tLoss: 4.7352\tLR: 4.571100\n",
      "Training Epoch: 46 [35840/50000]\tLoss: 4.7418\tLR: 4.571355\n",
      "Training Epoch: 46 [35968/50000]\tLoss: 4.7240\tLR: 4.571611\n",
      "Training Epoch: 46 [36096/50000]\tLoss: 4.7214\tLR: 4.571867\n",
      "Training Epoch: 46 [36224/50000]\tLoss: 4.7528\tLR: 4.572123\n",
      "Training Epoch: 46 [36352/50000]\tLoss: 4.6998\tLR: 4.572379\n",
      "Training Epoch: 46 [36480/50000]\tLoss: 4.7293\tLR: 4.572634\n",
      "Training Epoch: 46 [36608/50000]\tLoss: 4.6559\tLR: 4.572890\n",
      "Training Epoch: 46 [36736/50000]\tLoss: 4.6915\tLR: 4.573146\n",
      "Training Epoch: 46 [36864/50000]\tLoss: 4.6331\tLR: 4.573402\n",
      "Training Epoch: 46 [36992/50000]\tLoss: 4.7403\tLR: 4.573657\n",
      "Training Epoch: 46 [37120/50000]\tLoss: 4.6838\tLR: 4.573913\n",
      "Training Epoch: 46 [37248/50000]\tLoss: 4.6962\tLR: 4.574169\n",
      "Training Epoch: 46 [37376/50000]\tLoss: 4.6929\tLR: 4.574425\n",
      "Training Epoch: 46 [37504/50000]\tLoss: 4.6643\tLR: 4.574680\n",
      "Training Epoch: 46 [37632/50000]\tLoss: 4.6572\tLR: 4.574936\n",
      "Training Epoch: 46 [37760/50000]\tLoss: 4.6832\tLR: 4.575192\n",
      "Training Epoch: 46 [37888/50000]\tLoss: 4.6704\tLR: 4.575448\n",
      "Training Epoch: 46 [38016/50000]\tLoss: 4.7501\tLR: 4.575703\n",
      "Training Epoch: 46 [38144/50000]\tLoss: 4.7757\tLR: 4.575959\n",
      "Training Epoch: 46 [38272/50000]\tLoss: 4.7244\tLR: 4.576215\n",
      "Training Epoch: 46 [38400/50000]\tLoss: 4.6674\tLR: 4.576471\n",
      "Training Epoch: 46 [38528/50000]\tLoss: 4.7301\tLR: 4.576726\n",
      "Training Epoch: 46 [38656/50000]\tLoss: 4.5939\tLR: 4.576982\n",
      "Training Epoch: 46 [38784/50000]\tLoss: 4.6463\tLR: 4.577238\n",
      "Training Epoch: 46 [38912/50000]\tLoss: 4.7015\tLR: 4.577494\n",
      "Training Epoch: 46 [39040/50000]\tLoss: 4.6980\tLR: 4.577749\n",
      "Training Epoch: 46 [39168/50000]\tLoss: 4.7227\tLR: 4.578005\n",
      "Training Epoch: 46 [39296/50000]\tLoss: 4.7687\tLR: 4.578261\n",
      "Training Epoch: 46 [39424/50000]\tLoss: 4.6851\tLR: 4.578517\n",
      "Training Epoch: 46 [39552/50000]\tLoss: 4.6841\tLR: 4.578772\n",
      "Training Epoch: 46 [39680/50000]\tLoss: 4.6503\tLR: 4.579028\n",
      "Training Epoch: 46 [39808/50000]\tLoss: 4.7234\tLR: 4.579284\n",
      "Training Epoch: 46 [39936/50000]\tLoss: 4.7005\tLR: 4.579540\n",
      "Training Epoch: 46 [40064/50000]\tLoss: 4.6401\tLR: 4.579795\n",
      "Training Epoch: 46 [40192/50000]\tLoss: 4.7770\tLR: 4.580051\n",
      "Training Epoch: 46 [40320/50000]\tLoss: 4.6669\tLR: 4.580307\n",
      "Training Epoch: 46 [40448/50000]\tLoss: 4.6523\tLR: 4.580563\n",
      "Training Epoch: 46 [40576/50000]\tLoss: 4.6645\tLR: 4.580818\n",
      "Training Epoch: 46 [40704/50000]\tLoss: 4.7213\tLR: 4.581074\n",
      "Training Epoch: 46 [40832/50000]\tLoss: 4.7629\tLR: 4.581330\n",
      "Training Epoch: 46 [40960/50000]\tLoss: 4.6583\tLR: 4.581586\n",
      "Training Epoch: 46 [41088/50000]\tLoss: 4.6940\tLR: 4.581841\n",
      "Training Epoch: 46 [41216/50000]\tLoss: 4.6963\tLR: 4.582097\n",
      "Training Epoch: 46 [41344/50000]\tLoss: 4.7047\tLR: 4.582353\n",
      "Training Epoch: 46 [41472/50000]\tLoss: 4.6780\tLR: 4.582609\n",
      "Training Epoch: 46 [41600/50000]\tLoss: 4.6963\tLR: 4.582864\n",
      "Training Epoch: 46 [41728/50000]\tLoss: 4.6674\tLR: 4.583120\n",
      "Training Epoch: 46 [41856/50000]\tLoss: 4.7241\tLR: 4.583376\n",
      "Training Epoch: 46 [41984/50000]\tLoss: 4.7174\tLR: 4.583632\n",
      "Training Epoch: 46 [42112/50000]\tLoss: 4.6775\tLR: 4.583887\n",
      "Training Epoch: 46 [42240/50000]\tLoss: 4.6431\tLR: 4.584143\n",
      "Training Epoch: 46 [42368/50000]\tLoss: 4.7503\tLR: 4.584399\n",
      "Training Epoch: 46 [42496/50000]\tLoss: 4.6884\tLR: 4.584655\n",
      "Training Epoch: 46 [42624/50000]\tLoss: 4.7601\tLR: 4.584910\n",
      "Training Epoch: 46 [42752/50000]\tLoss: 4.6773\tLR: 4.585166\n",
      "Training Epoch: 46 [42880/50000]\tLoss: 4.7696\tLR: 4.585422\n",
      "Training Epoch: 46 [43008/50000]\tLoss: 4.7201\tLR: 4.585678\n",
      "Training Epoch: 46 [43136/50000]\tLoss: 4.6662\tLR: 4.585934\n",
      "Training Epoch: 46 [43264/50000]\tLoss: 4.7089\tLR: 4.586189\n",
      "Training Epoch: 46 [43392/50000]\tLoss: 4.7216\tLR: 4.586445\n",
      "Training Epoch: 46 [43520/50000]\tLoss: 4.6743\tLR: 4.586701\n",
      "Training Epoch: 46 [43648/50000]\tLoss: 4.7044\tLR: 4.586957\n",
      "Training Epoch: 46 [43776/50000]\tLoss: 4.7509\tLR: 4.587212\n",
      "Training Epoch: 46 [43904/50000]\tLoss: 4.7273\tLR: 4.587468\n",
      "Training Epoch: 46 [44032/50000]\tLoss: 4.6687\tLR: 4.587724\n",
      "Training Epoch: 46 [44160/50000]\tLoss: 4.7253\tLR: 4.587980\n",
      "Training Epoch: 46 [44288/50000]\tLoss: 4.6368\tLR: 4.588235\n",
      "Training Epoch: 46 [44416/50000]\tLoss: 4.6947\tLR: 4.588491\n",
      "Training Epoch: 46 [44544/50000]\tLoss: 4.6802\tLR: 4.588747\n",
      "Training Epoch: 46 [44672/50000]\tLoss: 4.6663\tLR: 4.589003\n",
      "Training Epoch: 46 [44800/50000]\tLoss: 4.7165\tLR: 4.589258\n",
      "Training Epoch: 46 [44928/50000]\tLoss: 4.7194\tLR: 4.589514\n",
      "Training Epoch: 46 [45056/50000]\tLoss: 4.7385\tLR: 4.589770\n",
      "Training Epoch: 46 [45184/50000]\tLoss: 4.7205\tLR: 4.590026\n",
      "Training Epoch: 46 [45312/50000]\tLoss: 4.6574\tLR: 4.590281\n",
      "Training Epoch: 46 [45440/50000]\tLoss: 4.6730\tLR: 4.590537\n",
      "Training Epoch: 46 [45568/50000]\tLoss: 4.6563\tLR: 4.590793\n",
      "Training Epoch: 46 [45696/50000]\tLoss: 4.7609\tLR: 4.591049\n",
      "Training Epoch: 46 [45824/50000]\tLoss: 4.7482\tLR: 4.591304\n",
      "Training Epoch: 46 [45952/50000]\tLoss: 4.7151\tLR: 4.591560\n",
      "Training Epoch: 46 [46080/50000]\tLoss: 4.6574\tLR: 4.591816\n",
      "Training Epoch: 46 [46208/50000]\tLoss: 4.7157\tLR: 4.592072\n",
      "Training Epoch: 46 [46336/50000]\tLoss: 4.7096\tLR: 4.592327\n",
      "Training Epoch: 46 [46464/50000]\tLoss: 4.6103\tLR: 4.592583\n",
      "Training Epoch: 46 [46592/50000]\tLoss: 4.6346\tLR: 4.592839\n",
      "Training Epoch: 46 [46720/50000]\tLoss: 4.6531\tLR: 4.593095\n",
      "Training Epoch: 46 [46848/50000]\tLoss: 4.6254\tLR: 4.593350\n",
      "Training Epoch: 46 [46976/50000]\tLoss: 4.6860\tLR: 4.593606\n",
      "Training Epoch: 46 [47104/50000]\tLoss: 4.6886\tLR: 4.593862\n",
      "Training Epoch: 46 [47232/50000]\tLoss: 4.7144\tLR: 4.594118\n",
      "Training Epoch: 46 [47360/50000]\tLoss: 4.6390\tLR: 4.594373\n",
      "Training Epoch: 46 [47488/50000]\tLoss: 4.7065\tLR: 4.594629\n",
      "Training Epoch: 46 [47616/50000]\tLoss: 4.6905\tLR: 4.594885\n",
      "Training Epoch: 46 [47744/50000]\tLoss: 4.7689\tLR: 4.595141\n",
      "Training Epoch: 46 [47872/50000]\tLoss: 4.6606\tLR: 4.595396\n",
      "Training Epoch: 46 [48000/50000]\tLoss: 4.6918\tLR: 4.595652\n",
      "Training Epoch: 46 [48128/50000]\tLoss: 4.6643\tLR: 4.595908\n",
      "Training Epoch: 46 [48256/50000]\tLoss: 4.6546\tLR: 4.596164\n",
      "Training Epoch: 46 [48384/50000]\tLoss: 4.7864\tLR: 4.596419\n",
      "Training Epoch: 46 [48512/50000]\tLoss: 4.6946\tLR: 4.596675\n",
      "Training Epoch: 46 [48640/50000]\tLoss: 4.7200\tLR: 4.596931\n",
      "Training Epoch: 46 [48768/50000]\tLoss: 4.6838\tLR: 4.597187\n",
      "Training Epoch: 46 [48896/50000]\tLoss: 4.6747\tLR: 4.597442\n",
      "Training Epoch: 46 [49024/50000]\tLoss: 4.6638\tLR: 4.597698\n",
      "Training Epoch: 46 [49152/50000]\tLoss: 4.7187\tLR: 4.597954\n",
      "Training Epoch: 46 [49280/50000]\tLoss: 4.7479\tLR: 4.598210\n",
      "Training Epoch: 46 [49408/50000]\tLoss: 4.7017\tLR: 4.598465\n",
      "Training Epoch: 46 [49536/50000]\tLoss: 4.6948\tLR: 4.598721\n",
      "Training Epoch: 46 [49664/50000]\tLoss: 4.7514\tLR: 4.598977\n",
      "Training Epoch: 46 [49792/50000]\tLoss: 4.6821\tLR: 4.599233\n",
      "Training Epoch: 46 [49920/50000]\tLoss: 4.6451\tLR: 4.599488\n",
      "Training Epoch: 46 [50000/50000]\tLoss: 4.6177\tLR: 4.599744\n",
      "epoch 46 training time consumed: 488.95s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   64489 GB |   64489 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   64291 GB |   64291 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     197 GB |     197 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   64489 GB |   64489 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   64291 GB |   64291 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     197 GB |     197 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   63581 GB |   63581 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   63383 GB |   63383 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     198 GB |     198 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    6838 K  |    6838 K  |\n",
      "|       from large pool |      24    |      65    |    2915 K  |    2915 K  |\n",
      "|       from small pool |     231    |     274    |    3923 K  |    3923 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    6838 K  |    6838 K  |\n",
      "|       from large pool |      24    |      65    |    2915 K  |    2915 K  |\n",
      "|       from small pool |     231    |     274    |    3923 K  |    3923 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      47    |    3963 K  |    3963 K  |\n",
      "|       from large pool |      10    |      23    |    1401 K  |    1401 K  |\n",
      "|       from small pool |      25    |      35    |    2562 K  |    2562 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 46, Average loss: 0.0371, Accuracy: 0.0100, Time consumed:31.01s\n",
      "\n",
      "Training Epoch: 47 [128/50000]\tLoss: 4.6420\tLR: 0.100000\n",
      "Training Epoch: 47 [256/50000]\tLoss: 4.6647\tLR: 4.600256\n",
      "Training Epoch: 47 [384/50000]\tLoss: 4.7237\tLR: 4.600512\n",
      "Training Epoch: 47 [512/50000]\tLoss: 4.7396\tLR: 4.600767\n",
      "Training Epoch: 47 [640/50000]\tLoss: 4.6568\tLR: 4.601023\n",
      "Training Epoch: 47 [768/50000]\tLoss: 4.6990\tLR: 4.601279\n",
      "Training Epoch: 47 [896/50000]\tLoss: 4.6210\tLR: 4.601535\n",
      "Training Epoch: 47 [1024/50000]\tLoss: 4.7028\tLR: 4.601790\n",
      "Training Epoch: 47 [1152/50000]\tLoss: 4.7531\tLR: 4.602046\n",
      "Training Epoch: 47 [1280/50000]\tLoss: 4.7196\tLR: 4.602302\n",
      "Training Epoch: 47 [1408/50000]\tLoss: 4.7250\tLR: 4.602558\n",
      "Training Epoch: 47 [1536/50000]\tLoss: 4.6414\tLR: 4.602813\n",
      "Training Epoch: 47 [1664/50000]\tLoss: 4.6386\tLR: 4.603069\n",
      "Training Epoch: 47 [1792/50000]\tLoss: 4.6529\tLR: 4.603325\n",
      "Training Epoch: 47 [1920/50000]\tLoss: 4.6519\tLR: 4.603581\n",
      "Training Epoch: 47 [2048/50000]\tLoss: 4.7083\tLR: 4.603836\n",
      "Training Epoch: 47 [2176/50000]\tLoss: 4.6352\tLR: 4.604092\n",
      "Training Epoch: 47 [2304/50000]\tLoss: 4.7342\tLR: 4.604348\n",
      "Training Epoch: 47 [2432/50000]\tLoss: 4.7089\tLR: 4.604604\n",
      "Training Epoch: 47 [2560/50000]\tLoss: 4.6404\tLR: 4.604859\n",
      "Training Epoch: 47 [2688/50000]\tLoss: 4.6943\tLR: 4.605115\n",
      "Training Epoch: 47 [2816/50000]\tLoss: 4.7442\tLR: 4.605371\n",
      "Training Epoch: 47 [2944/50000]\tLoss: 4.7187\tLR: 4.605627\n",
      "Training Epoch: 47 [3072/50000]\tLoss: 4.7052\tLR: 4.605882\n",
      "Training Epoch: 47 [3200/50000]\tLoss: 4.7281\tLR: 4.606138\n",
      "Training Epoch: 47 [3328/50000]\tLoss: 4.7383\tLR: 4.606394\n",
      "Training Epoch: 47 [3456/50000]\tLoss: 4.7198\tLR: 4.606650\n",
      "Training Epoch: 47 [3584/50000]\tLoss: 4.6579\tLR: 4.606905\n",
      "Training Epoch: 47 [3712/50000]\tLoss: 4.7160\tLR: 4.607161\n",
      "Training Epoch: 47 [3840/50000]\tLoss: 4.7018\tLR: 4.607417\n",
      "Training Epoch: 47 [3968/50000]\tLoss: 4.7414\tLR: 4.607673\n",
      "Training Epoch: 47 [4096/50000]\tLoss: 4.6744\tLR: 4.607928\n",
      "Training Epoch: 47 [4224/50000]\tLoss: 4.6853\tLR: 4.608184\n",
      "Training Epoch: 47 [4352/50000]\tLoss: 4.7312\tLR: 4.608440\n",
      "Training Epoch: 47 [4480/50000]\tLoss: 4.6565\tLR: 4.608696\n",
      "Training Epoch: 47 [4608/50000]\tLoss: 4.6828\tLR: 4.608951\n",
      "Training Epoch: 47 [4736/50000]\tLoss: 4.6850\tLR: 4.609207\n",
      "Training Epoch: 47 [4864/50000]\tLoss: 4.7056\tLR: 4.609463\n",
      "Training Epoch: 47 [4992/50000]\tLoss: 4.6918\tLR: 4.609719\n",
      "Training Epoch: 47 [5120/50000]\tLoss: 4.7350\tLR: 4.609974\n",
      "Training Epoch: 47 [5248/50000]\tLoss: 4.6484\tLR: 4.610230\n",
      "Training Epoch: 47 [5376/50000]\tLoss: 4.6815\tLR: 4.610486\n",
      "Training Epoch: 47 [5504/50000]\tLoss: 4.6625\tLR: 4.610742\n",
      "Training Epoch: 47 [5632/50000]\tLoss: 4.7089\tLR: 4.610997\n",
      "Training Epoch: 47 [5760/50000]\tLoss: 4.7415\tLR: 4.611253\n",
      "Training Epoch: 47 [5888/50000]\tLoss: 4.6879\tLR: 4.611509\n",
      "Training Epoch: 47 [6016/50000]\tLoss: 4.7043\tLR: 4.611765\n",
      "Training Epoch: 47 [6144/50000]\tLoss: 4.7082\tLR: 4.612020\n",
      "Training Epoch: 47 [6272/50000]\tLoss: 4.6399\tLR: 4.612276\n",
      "Training Epoch: 47 [6400/50000]\tLoss: 4.7080\tLR: 4.612532\n",
      "Training Epoch: 47 [6528/50000]\tLoss: 4.6877\tLR: 4.612788\n",
      "Training Epoch: 47 [6656/50000]\tLoss: 4.6883\tLR: 4.613043\n",
      "Training Epoch: 47 [6784/50000]\tLoss: 4.6675\tLR: 4.613299\n",
      "Training Epoch: 47 [6912/50000]\tLoss: 4.6750\tLR: 4.613555\n",
      "Training Epoch: 47 [7040/50000]\tLoss: 4.7250\tLR: 4.613811\n",
      "Training Epoch: 47 [7168/50000]\tLoss: 4.6705\tLR: 4.614066\n",
      "Training Epoch: 47 [7296/50000]\tLoss: 4.6392\tLR: 4.614322\n",
      "Training Epoch: 47 [7424/50000]\tLoss: 4.6481\tLR: 4.614578\n",
      "Training Epoch: 47 [7552/50000]\tLoss: 4.6935\tLR: 4.614834\n",
      "Training Epoch: 47 [7680/50000]\tLoss: 4.6873\tLR: 4.615090\n",
      "Training Epoch: 47 [7808/50000]\tLoss: 4.6637\tLR: 4.615345\n",
      "Training Epoch: 47 [7936/50000]\tLoss: 4.7082\tLR: 4.615601\n",
      "Training Epoch: 47 [8064/50000]\tLoss: 4.6989\tLR: 4.615857\n",
      "Training Epoch: 47 [8192/50000]\tLoss: 4.7807\tLR: 4.616113\n",
      "Training Epoch: 47 [8320/50000]\tLoss: 4.6715\tLR: 4.616368\n",
      "Training Epoch: 47 [8448/50000]\tLoss: 4.7693\tLR: 4.616624\n",
      "Training Epoch: 47 [8576/50000]\tLoss: 4.7124\tLR: 4.616880\n",
      "Training Epoch: 47 [8704/50000]\tLoss: 4.6200\tLR: 4.617136\n",
      "Training Epoch: 47 [8832/50000]\tLoss: 4.6428\tLR: 4.617391\n",
      "Training Epoch: 47 [8960/50000]\tLoss: 4.6903\tLR: 4.617647\n",
      "Training Epoch: 47 [9088/50000]\tLoss: 4.6522\tLR: 4.617903\n",
      "Training Epoch: 47 [9216/50000]\tLoss: 4.7189\tLR: 4.618159\n",
      "Training Epoch: 47 [9344/50000]\tLoss: 4.7285\tLR: 4.618414\n",
      "Training Epoch: 47 [9472/50000]\tLoss: 4.6316\tLR: 4.618670\n",
      "Training Epoch: 47 [9600/50000]\tLoss: 4.7474\tLR: 4.618926\n",
      "Training Epoch: 47 [9728/50000]\tLoss: 4.6874\tLR: 4.619182\n",
      "Training Epoch: 47 [9856/50000]\tLoss: 4.7122\tLR: 4.619437\n",
      "Training Epoch: 47 [9984/50000]\tLoss: 4.7024\tLR: 4.619693\n",
      "Training Epoch: 47 [10112/50000]\tLoss: 4.5993\tLR: 4.619949\n",
      "Training Epoch: 47 [10240/50000]\tLoss: 4.7319\tLR: 4.620205\n",
      "Training Epoch: 47 [10368/50000]\tLoss: 4.5980\tLR: 4.620460\n",
      "Training Epoch: 47 [10496/50000]\tLoss: 4.7121\tLR: 4.620716\n",
      "Training Epoch: 47 [10624/50000]\tLoss: 4.7578\tLR: 4.620972\n",
      "Training Epoch: 47 [10752/50000]\tLoss: 4.6032\tLR: 4.621228\n",
      "Training Epoch: 47 [10880/50000]\tLoss: 4.6950\tLR: 4.621483\n",
      "Training Epoch: 47 [11008/50000]\tLoss: 4.7289\tLR: 4.621739\n",
      "Training Epoch: 47 [11136/50000]\tLoss: 4.7150\tLR: 4.621995\n",
      "Training Epoch: 47 [11264/50000]\tLoss: 4.6739\tLR: 4.622251\n",
      "Training Epoch: 47 [11392/50000]\tLoss: 4.7277\tLR: 4.622506\n",
      "Training Epoch: 47 [11520/50000]\tLoss: 4.6653\tLR: 4.622762\n",
      "Training Epoch: 47 [11648/50000]\tLoss: 4.7007\tLR: 4.623018\n",
      "Training Epoch: 47 [11776/50000]\tLoss: 4.7077\tLR: 4.623274\n",
      "Training Epoch: 47 [11904/50000]\tLoss: 4.6784\tLR: 4.623529\n",
      "Training Epoch: 47 [12032/50000]\tLoss: 4.7194\tLR: 4.623785\n",
      "Training Epoch: 47 [12160/50000]\tLoss: 4.7358\tLR: 4.624041\n",
      "Training Epoch: 47 [12288/50000]\tLoss: 4.6640\tLR: 4.624297\n",
      "Training Epoch: 47 [12416/50000]\tLoss: 4.7360\tLR: 4.624552\n",
      "Training Epoch: 47 [12544/50000]\tLoss: 4.6807\tLR: 4.624808\n",
      "Training Epoch: 47 [12672/50000]\tLoss: 4.7669\tLR: 4.625064\n",
      "Training Epoch: 47 [12800/50000]\tLoss: 4.7191\tLR: 4.625320\n",
      "Training Epoch: 47 [12928/50000]\tLoss: 4.6706\tLR: 4.625575\n",
      "Training Epoch: 47 [13056/50000]\tLoss: 4.6868\tLR: 4.625831\n",
      "Training Epoch: 47 [13184/50000]\tLoss: 4.7105\tLR: 4.626087\n",
      "Training Epoch: 47 [13312/50000]\tLoss: 4.6552\tLR: 4.626343\n",
      "Training Epoch: 47 [13440/50000]\tLoss: 4.6407\tLR: 4.626598\n",
      "Training Epoch: 47 [13568/50000]\tLoss: 4.7232\tLR: 4.626854\n",
      "Training Epoch: 47 [13696/50000]\tLoss: 4.7999\tLR: 4.627110\n",
      "Training Epoch: 47 [13824/50000]\tLoss: 4.6876\tLR: 4.627366\n",
      "Training Epoch: 47 [13952/50000]\tLoss: 4.7061\tLR: 4.627621\n",
      "Training Epoch: 47 [14080/50000]\tLoss: 4.7511\tLR: 4.627877\n",
      "Training Epoch: 47 [14208/50000]\tLoss: 4.6991\tLR: 4.628133\n",
      "Training Epoch: 47 [14336/50000]\tLoss: 4.7358\tLR: 4.628389\n",
      "Training Epoch: 47 [14464/50000]\tLoss: 4.6663\tLR: 4.628645\n",
      "Training Epoch: 47 [14592/50000]\tLoss: 4.7390\tLR: 4.628900\n",
      "Training Epoch: 47 [14720/50000]\tLoss: 4.6482\tLR: 4.629156\n",
      "Training Epoch: 47 [14848/50000]\tLoss: 4.6452\tLR: 4.629412\n",
      "Training Epoch: 47 [14976/50000]\tLoss: 4.6801\tLR: 4.629668\n",
      "Training Epoch: 47 [15104/50000]\tLoss: 4.6552\tLR: 4.629923\n",
      "Training Epoch: 47 [15232/50000]\tLoss: 4.6954\tLR: 4.630179\n",
      "Training Epoch: 47 [15360/50000]\tLoss: 4.7149\tLR: 4.630435\n",
      "Training Epoch: 47 [15488/50000]\tLoss: 4.6541\tLR: 4.630691\n",
      "Training Epoch: 47 [15616/50000]\tLoss: 4.6519\tLR: 4.630946\n",
      "Training Epoch: 47 [15744/50000]\tLoss: 4.7421\tLR: 4.631202\n",
      "Training Epoch: 47 [15872/50000]\tLoss: 4.6694\tLR: 4.631458\n",
      "Training Epoch: 47 [16000/50000]\tLoss: 4.7658\tLR: 4.631714\n",
      "Training Epoch: 47 [16128/50000]\tLoss: 4.7940\tLR: 4.631969\n",
      "Training Epoch: 47 [16256/50000]\tLoss: 4.6869\tLR: 4.632225\n",
      "Training Epoch: 47 [16384/50000]\tLoss: 4.6976\tLR: 4.632481\n",
      "Training Epoch: 47 [16512/50000]\tLoss: 4.6679\tLR: 4.632737\n",
      "Training Epoch: 47 [16640/50000]\tLoss: 4.6888\tLR: 4.632992\n",
      "Training Epoch: 47 [16768/50000]\tLoss: 4.6885\tLR: 4.633248\n",
      "Training Epoch: 47 [16896/50000]\tLoss: 4.6997\tLR: 4.633504\n",
      "Training Epoch: 47 [17024/50000]\tLoss: 4.7155\tLR: 4.633760\n",
      "Training Epoch: 47 [17152/50000]\tLoss: 4.7071\tLR: 4.634015\n",
      "Training Epoch: 47 [17280/50000]\tLoss: 4.6930\tLR: 4.634271\n",
      "Training Epoch: 47 [17408/50000]\tLoss: 4.6948\tLR: 4.634527\n",
      "Training Epoch: 47 [17536/50000]\tLoss: 4.6759\tLR: 4.634783\n",
      "Training Epoch: 47 [17664/50000]\tLoss: 4.7141\tLR: 4.635038\n",
      "Training Epoch: 47 [17792/50000]\tLoss: 4.6694\tLR: 4.635294\n",
      "Training Epoch: 47 [17920/50000]\tLoss: 4.6701\tLR: 4.635550\n",
      "Training Epoch: 47 [18048/50000]\tLoss: 4.6586\tLR: 4.635806\n",
      "Training Epoch: 47 [18176/50000]\tLoss: 4.7331\tLR: 4.636061\n",
      "Training Epoch: 47 [18304/50000]\tLoss: 4.6143\tLR: 4.636317\n",
      "Training Epoch: 47 [18432/50000]\tLoss: 4.7397\tLR: 4.636573\n",
      "Training Epoch: 47 [18560/50000]\tLoss: 4.7338\tLR: 4.636829\n",
      "Training Epoch: 47 [18688/50000]\tLoss: 4.6680\tLR: 4.637084\n",
      "Training Epoch: 47 [18816/50000]\tLoss: 4.5852\tLR: 4.637340\n",
      "Training Epoch: 47 [18944/50000]\tLoss: 4.6672\tLR: 4.637596\n",
      "Training Epoch: 47 [19072/50000]\tLoss: 4.7086\tLR: 4.637852\n",
      "Training Epoch: 47 [19200/50000]\tLoss: 4.6891\tLR: 4.638107\n",
      "Training Epoch: 47 [19328/50000]\tLoss: 4.7011\tLR: 4.638363\n",
      "Training Epoch: 47 [19456/50000]\tLoss: 4.7323\tLR: 4.638619\n",
      "Training Epoch: 47 [19584/50000]\tLoss: 4.6845\tLR: 4.638875\n",
      "Training Epoch: 47 [19712/50000]\tLoss: 4.6706\tLR: 4.639130\n",
      "Training Epoch: 47 [19840/50000]\tLoss: 4.6585\tLR: 4.639386\n",
      "Training Epoch: 47 [19968/50000]\tLoss: 4.6545\tLR: 4.639642\n",
      "Training Epoch: 47 [20096/50000]\tLoss: 4.6946\tLR: 4.639898\n",
      "Training Epoch: 47 [20224/50000]\tLoss: 4.6949\tLR: 4.640153\n",
      "Training Epoch: 47 [20352/50000]\tLoss: 4.6493\tLR: 4.640409\n",
      "Training Epoch: 47 [20480/50000]\tLoss: 4.6426\tLR: 4.640665\n",
      "Training Epoch: 47 [20608/50000]\tLoss: 4.6694\tLR: 4.640921\n",
      "Training Epoch: 47 [20736/50000]\tLoss: 4.6781\tLR: 4.641176\n",
      "Training Epoch: 47 [20864/50000]\tLoss: 4.6663\tLR: 4.641432\n",
      "Training Epoch: 47 [20992/50000]\tLoss: 4.7462\tLR: 4.641688\n",
      "Training Epoch: 47 [21120/50000]\tLoss: 4.6704\tLR: 4.641944\n",
      "Training Epoch: 47 [21248/50000]\tLoss: 4.6906\tLR: 4.642199\n",
      "Training Epoch: 47 [21376/50000]\tLoss: 4.7077\tLR: 4.642455\n",
      "Training Epoch: 47 [21504/50000]\tLoss: 4.7096\tLR: 4.642711\n",
      "Training Epoch: 47 [21632/50000]\tLoss: 4.7098\tLR: 4.642967\n",
      "Training Epoch: 47 [21760/50000]\tLoss: 4.7519\tLR: 4.643223\n",
      "Training Epoch: 47 [21888/50000]\tLoss: 4.7193\tLR: 4.643478\n",
      "Training Epoch: 47 [22016/50000]\tLoss: 4.7327\tLR: 4.643734\n",
      "Training Epoch: 47 [22144/50000]\tLoss: 4.6483\tLR: 4.643990\n",
      "Training Epoch: 47 [22272/50000]\tLoss: 4.6988\tLR: 4.644246\n",
      "Training Epoch: 47 [22400/50000]\tLoss: 4.7088\tLR: 4.644501\n",
      "Training Epoch: 47 [22528/50000]\tLoss: 4.6222\tLR: 4.644757\n",
      "Training Epoch: 47 [22656/50000]\tLoss: 4.6723\tLR: 4.645013\n",
      "Training Epoch: 47 [22784/50000]\tLoss: 4.6526\tLR: 4.645269\n",
      "Training Epoch: 47 [22912/50000]\tLoss: 4.7314\tLR: 4.645524\n",
      "Training Epoch: 47 [23040/50000]\tLoss: 4.6998\tLR: 4.645780\n",
      "Training Epoch: 47 [23168/50000]\tLoss: 4.7234\tLR: 4.646036\n",
      "Training Epoch: 47 [23296/50000]\tLoss: 4.7721\tLR: 4.646292\n",
      "Training Epoch: 47 [23424/50000]\tLoss: 4.7070\tLR: 4.646547\n",
      "Training Epoch: 47 [23552/50000]\tLoss: 4.6297\tLR: 4.646803\n",
      "Training Epoch: 47 [23680/50000]\tLoss: 4.7034\tLR: 4.647059\n",
      "Training Epoch: 47 [23808/50000]\tLoss: 4.6761\tLR: 4.647315\n",
      "Training Epoch: 47 [23936/50000]\tLoss: 4.6435\tLR: 4.647570\n",
      "Training Epoch: 47 [24064/50000]\tLoss: 4.6978\tLR: 4.647826\n",
      "Training Epoch: 47 [24192/50000]\tLoss: 4.7082\tLR: 4.648082\n",
      "Training Epoch: 47 [24320/50000]\tLoss: 4.6691\tLR: 4.648338\n",
      "Training Epoch: 47 [24448/50000]\tLoss: 4.7155\tLR: 4.648593\n",
      "Training Epoch: 47 [24576/50000]\tLoss: 4.6826\tLR: 4.648849\n",
      "Training Epoch: 47 [24704/50000]\tLoss: 4.6871\tLR: 4.649105\n",
      "Training Epoch: 47 [24832/50000]\tLoss: 4.7405\tLR: 4.649361\n",
      "Training Epoch: 47 [24960/50000]\tLoss: 4.7284\tLR: 4.649616\n",
      "Training Epoch: 47 [25088/50000]\tLoss: 4.6384\tLR: 4.649872\n",
      "Training Epoch: 47 [25216/50000]\tLoss: 4.6951\tLR: 4.650128\n",
      "Training Epoch: 47 [25344/50000]\tLoss: 4.6819\tLR: 4.650384\n",
      "Training Epoch: 47 [25472/50000]\tLoss: 4.7298\tLR: 4.650639\n",
      "Training Epoch: 47 [25600/50000]\tLoss: 4.6677\tLR: 4.650895\n",
      "Training Epoch: 47 [25728/50000]\tLoss: 4.7284\tLR: 4.651151\n",
      "Training Epoch: 47 [25856/50000]\tLoss: 4.7027\tLR: 4.651407\n",
      "Training Epoch: 47 [25984/50000]\tLoss: 4.6980\tLR: 4.651662\n",
      "Training Epoch: 47 [26112/50000]\tLoss: 4.7392\tLR: 4.651918\n",
      "Training Epoch: 47 [26240/50000]\tLoss: 4.7208\tLR: 4.652174\n",
      "Training Epoch: 47 [26368/50000]\tLoss: 4.7362\tLR: 4.652430\n",
      "Training Epoch: 47 [26496/50000]\tLoss: 4.7181\tLR: 4.652685\n",
      "Training Epoch: 47 [26624/50000]\tLoss: 4.7779\tLR: 4.652941\n",
      "Training Epoch: 47 [26752/50000]\tLoss: 4.6419\tLR: 4.653197\n",
      "Training Epoch: 47 [26880/50000]\tLoss: 4.6750\tLR: 4.653453\n",
      "Training Epoch: 47 [27008/50000]\tLoss: 4.6671\tLR: 4.653708\n",
      "Training Epoch: 47 [27136/50000]\tLoss: 4.6704\tLR: 4.653964\n",
      "Training Epoch: 47 [27264/50000]\tLoss: 4.6518\tLR: 4.654220\n",
      "Training Epoch: 47 [27392/50000]\tLoss: 4.6735\tLR: 4.654476\n",
      "Training Epoch: 47 [27520/50000]\tLoss: 4.6582\tLR: 4.654731\n",
      "Training Epoch: 47 [27648/50000]\tLoss: 4.6817\tLR: 4.654987\n",
      "Training Epoch: 47 [27776/50000]\tLoss: 4.6961\tLR: 4.655243\n",
      "Training Epoch: 47 [27904/50000]\tLoss: 4.6851\tLR: 4.655499\n",
      "Training Epoch: 47 [28032/50000]\tLoss: 4.6567\tLR: 4.655754\n",
      "Training Epoch: 47 [28160/50000]\tLoss: 4.7279\tLR: 4.656010\n",
      "Training Epoch: 47 [28288/50000]\tLoss: 4.7657\tLR: 4.656266\n",
      "Training Epoch: 47 [28416/50000]\tLoss: 4.6861\tLR: 4.656522\n",
      "Training Epoch: 47 [28544/50000]\tLoss: 4.7209\tLR: 4.656777\n",
      "Training Epoch: 47 [28672/50000]\tLoss: 4.7083\tLR: 4.657033\n",
      "Training Epoch: 47 [28800/50000]\tLoss: 4.6444\tLR: 4.657289\n",
      "Training Epoch: 47 [28928/50000]\tLoss: 4.6725\tLR: 4.657545\n",
      "Training Epoch: 47 [29056/50000]\tLoss: 4.7861\tLR: 4.657801\n",
      "Training Epoch: 47 [29184/50000]\tLoss: 4.6936\tLR: 4.658056\n",
      "Training Epoch: 47 [29312/50000]\tLoss: 4.7658\tLR: 4.658312\n",
      "Training Epoch: 47 [29440/50000]\tLoss: 4.7341\tLR: 4.658568\n",
      "Training Epoch: 47 [29568/50000]\tLoss: 4.7745\tLR: 4.658824\n",
      "Training Epoch: 47 [29696/50000]\tLoss: 4.7170\tLR: 4.659079\n",
      "Training Epoch: 47 [29824/50000]\tLoss: 4.7190\tLR: 4.659335\n",
      "Training Epoch: 47 [29952/50000]\tLoss: 4.7175\tLR: 4.659591\n",
      "Training Epoch: 47 [30080/50000]\tLoss: 4.5848\tLR: 4.659847\n",
      "Training Epoch: 47 [30208/50000]\tLoss: 4.6479\tLR: 4.660102\n",
      "Training Epoch: 47 [30336/50000]\tLoss: 4.6436\tLR: 4.660358\n",
      "Training Epoch: 47 [30464/50000]\tLoss: 4.7377\tLR: 4.660614\n",
      "Training Epoch: 47 [30592/50000]\tLoss: 4.6762\tLR: 4.660870\n",
      "Training Epoch: 47 [30720/50000]\tLoss: 4.6830\tLR: 4.661125\n",
      "Training Epoch: 47 [30848/50000]\tLoss: 4.7326\tLR: 4.661381\n",
      "Training Epoch: 47 [30976/50000]\tLoss: 4.6907\tLR: 4.661637\n",
      "Training Epoch: 47 [31104/50000]\tLoss: 4.6446\tLR: 4.661893\n",
      "Training Epoch: 47 [31232/50000]\tLoss: 4.6547\tLR: 4.662148\n",
      "Training Epoch: 47 [31360/50000]\tLoss: 4.6818\tLR: 4.662404\n",
      "Training Epoch: 47 [31488/50000]\tLoss: 4.6809\tLR: 4.662660\n",
      "Training Epoch: 47 [31616/50000]\tLoss: 4.6743\tLR: 4.662916\n",
      "Training Epoch: 47 [31744/50000]\tLoss: 4.6618\tLR: 4.663171\n",
      "Training Epoch: 47 [31872/50000]\tLoss: 4.6918\tLR: 4.663427\n",
      "Training Epoch: 47 [32000/50000]\tLoss: 4.7328\tLR: 4.663683\n",
      "Training Epoch: 47 [32128/50000]\tLoss: 4.7309\tLR: 4.663939\n",
      "Training Epoch: 47 [32256/50000]\tLoss: 4.6914\tLR: 4.664194\n",
      "Training Epoch: 47 [32384/50000]\tLoss: 4.8274\tLR: 4.664450\n",
      "Training Epoch: 47 [32512/50000]\tLoss: 4.6802\tLR: 4.664706\n",
      "Training Epoch: 47 [32640/50000]\tLoss: 4.7170\tLR: 4.664962\n",
      "Training Epoch: 47 [32768/50000]\tLoss: 4.6661\tLR: 4.665217\n",
      "Training Epoch: 47 [32896/50000]\tLoss: 4.6949\tLR: 4.665473\n",
      "Training Epoch: 47 [33024/50000]\tLoss: 4.6948\tLR: 4.665729\n",
      "Training Epoch: 47 [33152/50000]\tLoss: 4.6793\tLR: 4.665985\n",
      "Training Epoch: 47 [33280/50000]\tLoss: 4.6519\tLR: 4.666240\n",
      "Training Epoch: 47 [33408/50000]\tLoss: 4.6486\tLR: 4.666496\n",
      "Training Epoch: 47 [33536/50000]\tLoss: 4.7013\tLR: 4.666752\n",
      "Training Epoch: 47 [33664/50000]\tLoss: 4.6630\tLR: 4.667008\n",
      "Training Epoch: 47 [33792/50000]\tLoss: 4.6935\tLR: 4.667263\n",
      "Training Epoch: 47 [33920/50000]\tLoss: 4.6938\tLR: 4.667519\n",
      "Training Epoch: 47 [34048/50000]\tLoss: 4.6909\tLR: 4.667775\n",
      "Training Epoch: 47 [34176/50000]\tLoss: 4.6660\tLR: 4.668031\n",
      "Training Epoch: 47 [34304/50000]\tLoss: 4.7446\tLR: 4.668286\n",
      "Training Epoch: 47 [34432/50000]\tLoss: 4.7075\tLR: 4.668542\n",
      "Training Epoch: 47 [34560/50000]\tLoss: 4.7428\tLR: 4.668798\n",
      "Training Epoch: 47 [34688/50000]\tLoss: 4.7008\tLR: 4.669054\n",
      "Training Epoch: 47 [34816/50000]\tLoss: 4.7625\tLR: 4.669309\n",
      "Training Epoch: 47 [34944/50000]\tLoss: 4.7162\tLR: 4.669565\n",
      "Training Epoch: 47 [35072/50000]\tLoss: 4.7395\tLR: 4.669821\n",
      "Training Epoch: 47 [35200/50000]\tLoss: 4.6716\tLR: 4.670077\n",
      "Training Epoch: 47 [35328/50000]\tLoss: 4.6709\tLR: 4.670332\n",
      "Training Epoch: 47 [35456/50000]\tLoss: 4.6635\tLR: 4.670588\n",
      "Training Epoch: 47 [35584/50000]\tLoss: 4.6065\tLR: 4.670844\n",
      "Training Epoch: 47 [35712/50000]\tLoss: 4.7053\tLR: 4.671100\n",
      "Training Epoch: 47 [35840/50000]\tLoss: 4.6991\tLR: 4.671355\n",
      "Training Epoch: 47 [35968/50000]\tLoss: 4.7675\tLR: 4.671611\n",
      "Training Epoch: 47 [36096/50000]\tLoss: 4.7214\tLR: 4.671867\n",
      "Training Epoch: 47 [36224/50000]\tLoss: 4.7308\tLR: 4.672123\n",
      "Training Epoch: 47 [36352/50000]\tLoss: 4.7309\tLR: 4.672379\n",
      "Training Epoch: 47 [36480/50000]\tLoss: 4.7128\tLR: 4.672634\n",
      "Training Epoch: 47 [36608/50000]\tLoss: 4.6877\tLR: 4.672890\n",
      "Training Epoch: 47 [36736/50000]\tLoss: 4.6861\tLR: 4.673146\n",
      "Training Epoch: 47 [36864/50000]\tLoss: 4.7466\tLR: 4.673402\n",
      "Training Epoch: 47 [36992/50000]\tLoss: 4.7016\tLR: 4.673657\n",
      "Training Epoch: 47 [37120/50000]\tLoss: 4.6788\tLR: 4.673913\n",
      "Training Epoch: 47 [37248/50000]\tLoss: 4.6986\tLR: 4.674169\n",
      "Training Epoch: 47 [37376/50000]\tLoss: 4.6668\tLR: 4.674425\n",
      "Training Epoch: 47 [37504/50000]\tLoss: 4.6945\tLR: 4.674680\n",
      "Training Epoch: 47 [37632/50000]\tLoss: 4.6756\tLR: 4.674936\n",
      "Training Epoch: 47 [37760/50000]\tLoss: 4.6851\tLR: 4.675192\n",
      "Training Epoch: 47 [37888/50000]\tLoss: 4.7297\tLR: 4.675448\n",
      "Training Epoch: 47 [38016/50000]\tLoss: 4.5902\tLR: 4.675703\n",
      "Training Epoch: 47 [38144/50000]\tLoss: 4.7050\tLR: 4.675959\n",
      "Training Epoch: 47 [38272/50000]\tLoss: 4.7390\tLR: 4.676215\n",
      "Training Epoch: 47 [38400/50000]\tLoss: 4.6548\tLR: 4.676471\n",
      "Training Epoch: 47 [38528/50000]\tLoss: 4.7683\tLR: 4.676726\n",
      "Training Epoch: 47 [38656/50000]\tLoss: 4.7407\tLR: 4.676982\n",
      "Training Epoch: 47 [38784/50000]\tLoss: 4.7378\tLR: 4.677238\n",
      "Training Epoch: 47 [38912/50000]\tLoss: 4.6464\tLR: 4.677494\n",
      "Training Epoch: 47 [39040/50000]\tLoss: 4.6970\tLR: 4.677749\n",
      "Training Epoch: 47 [39168/50000]\tLoss: 4.7112\tLR: 4.678005\n",
      "Training Epoch: 47 [39296/50000]\tLoss: 4.6424\tLR: 4.678261\n",
      "Training Epoch: 47 [39424/50000]\tLoss: 4.6310\tLR: 4.678517\n",
      "Training Epoch: 47 [39552/50000]\tLoss: 4.6937\tLR: 4.678772\n",
      "Training Epoch: 47 [39680/50000]\tLoss: 4.6858\tLR: 4.679028\n",
      "Training Epoch: 47 [39808/50000]\tLoss: 4.7152\tLR: 4.679284\n",
      "Training Epoch: 47 [39936/50000]\tLoss: 4.6766\tLR: 4.679540\n",
      "Training Epoch: 47 [40064/50000]\tLoss: 4.6831\tLR: 4.679795\n",
      "Training Epoch: 47 [40192/50000]\tLoss: 4.6461\tLR: 4.680051\n",
      "Training Epoch: 47 [40320/50000]\tLoss: 4.6864\tLR: 4.680307\n",
      "Training Epoch: 47 [40448/50000]\tLoss: 4.6518\tLR: 4.680563\n",
      "Training Epoch: 47 [40576/50000]\tLoss: 4.7710\tLR: 4.680818\n",
      "Training Epoch: 47 [40704/50000]\tLoss: 4.7902\tLR: 4.681074\n",
      "Training Epoch: 47 [40832/50000]\tLoss: 4.7252\tLR: 4.681330\n",
      "Training Epoch: 47 [40960/50000]\tLoss: 4.6721\tLR: 4.681586\n",
      "Training Epoch: 47 [41088/50000]\tLoss: 4.6848\tLR: 4.681841\n",
      "Training Epoch: 47 [41216/50000]\tLoss: 4.6810\tLR: 4.682097\n",
      "Training Epoch: 47 [41344/50000]\tLoss: 4.7064\tLR: 4.682353\n",
      "Training Epoch: 47 [41472/50000]\tLoss: 4.6110\tLR: 4.682609\n",
      "Training Epoch: 47 [41600/50000]\tLoss: 4.6325\tLR: 4.682864\n",
      "Training Epoch: 47 [41728/50000]\tLoss: 4.6796\tLR: 4.683120\n",
      "Training Epoch: 47 [41856/50000]\tLoss: 4.6667\tLR: 4.683376\n",
      "Training Epoch: 47 [41984/50000]\tLoss: 4.7258\tLR: 4.683632\n",
      "Training Epoch: 47 [42112/50000]\tLoss: 4.7264\tLR: 4.683887\n",
      "Training Epoch: 47 [42240/50000]\tLoss: 4.6962\tLR: 4.684143\n",
      "Training Epoch: 47 [42368/50000]\tLoss: 4.7645\tLR: 4.684399\n",
      "Training Epoch: 47 [42496/50000]\tLoss: 4.6827\tLR: 4.684655\n",
      "Training Epoch: 47 [42624/50000]\tLoss: 4.7179\tLR: 4.684910\n",
      "Training Epoch: 47 [42752/50000]\tLoss: 4.6594\tLR: 4.685166\n",
      "Training Epoch: 47 [42880/50000]\tLoss: 4.6879\tLR: 4.685422\n",
      "Training Epoch: 47 [43008/50000]\tLoss: 4.5855\tLR: 4.685678\n",
      "Training Epoch: 47 [43136/50000]\tLoss: 4.7093\tLR: 4.685934\n",
      "Training Epoch: 47 [43264/50000]\tLoss: 4.6432\tLR: 4.686189\n",
      "Training Epoch: 47 [43392/50000]\tLoss: 4.7189\tLR: 4.686445\n",
      "Training Epoch: 47 [43520/50000]\tLoss: 4.6778\tLR: 4.686701\n",
      "Training Epoch: 47 [43648/50000]\tLoss: 4.6693\tLR: 4.686957\n",
      "Training Epoch: 47 [43776/50000]\tLoss: 4.6779\tLR: 4.687212\n",
      "Training Epoch: 47 [43904/50000]\tLoss: 4.6799\tLR: 4.687468\n",
      "Training Epoch: 47 [44032/50000]\tLoss: 4.6914\tLR: 4.687724\n",
      "Training Epoch: 47 [44160/50000]\tLoss: 4.6937\tLR: 4.687980\n",
      "Training Epoch: 47 [44288/50000]\tLoss: 4.7466\tLR: 4.688235\n",
      "Training Epoch: 47 [44416/50000]\tLoss: 4.6841\tLR: 4.688491\n",
      "Training Epoch: 47 [44544/50000]\tLoss: 4.6251\tLR: 4.688747\n",
      "Training Epoch: 47 [44672/50000]\tLoss: 4.6845\tLR: 4.689003\n",
      "Training Epoch: 47 [44800/50000]\tLoss: 4.6548\tLR: 4.689258\n",
      "Training Epoch: 47 [44928/50000]\tLoss: 4.7080\tLR: 4.689514\n",
      "Training Epoch: 47 [45056/50000]\tLoss: 4.7191\tLR: 4.689770\n",
      "Training Epoch: 47 [45184/50000]\tLoss: 4.7264\tLR: 4.690026\n",
      "Training Epoch: 47 [45312/50000]\tLoss: 4.6721\tLR: 4.690281\n",
      "Training Epoch: 47 [45440/50000]\tLoss: 4.6856\tLR: 4.690537\n",
      "Training Epoch: 47 [45568/50000]\tLoss: 4.7533\tLR: 4.690793\n",
      "Training Epoch: 47 [45696/50000]\tLoss: 4.7401\tLR: 4.691049\n",
      "Training Epoch: 47 [45824/50000]\tLoss: 4.7145\tLR: 4.691304\n",
      "Training Epoch: 47 [45952/50000]\tLoss: 4.6988\tLR: 4.691560\n",
      "Training Epoch: 47 [46080/50000]\tLoss: 4.6305\tLR: 4.691816\n",
      "Training Epoch: 47 [46208/50000]\tLoss: 4.6691\tLR: 4.692072\n",
      "Training Epoch: 47 [46336/50000]\tLoss: 4.6178\tLR: 4.692327\n",
      "Training Epoch: 47 [46464/50000]\tLoss: 4.6835\tLR: 4.692583\n",
      "Training Epoch: 47 [46592/50000]\tLoss: 4.5873\tLR: 4.692839\n",
      "Training Epoch: 47 [46720/50000]\tLoss: 4.6897\tLR: 4.693095\n",
      "Training Epoch: 47 [46848/50000]\tLoss: 4.7086\tLR: 4.693350\n",
      "Training Epoch: 47 [46976/50000]\tLoss: 4.8006\tLR: 4.693606\n",
      "Training Epoch: 47 [47104/50000]\tLoss: 4.6748\tLR: 4.693862\n",
      "Training Epoch: 47 [47232/50000]\tLoss: 4.6984\tLR: 4.694118\n",
      "Training Epoch: 47 [47360/50000]\tLoss: 4.6762\tLR: 4.694373\n",
      "Training Epoch: 47 [47488/50000]\tLoss: 4.7111\tLR: 4.694629\n",
      "Training Epoch: 47 [47616/50000]\tLoss: 4.6903\tLR: 4.694885\n",
      "Training Epoch: 47 [47744/50000]\tLoss: 4.7073\tLR: 4.695141\n",
      "Training Epoch: 47 [47872/50000]\tLoss: 4.6667\tLR: 4.695396\n",
      "Training Epoch: 47 [48000/50000]\tLoss: 4.7155\tLR: 4.695652\n",
      "Training Epoch: 47 [48128/50000]\tLoss: 4.6667\tLR: 4.695908\n",
      "Training Epoch: 47 [48256/50000]\tLoss: 4.6771\tLR: 4.696164\n",
      "Training Epoch: 47 [48384/50000]\tLoss: 4.7870\tLR: 4.696419\n",
      "Training Epoch: 47 [48512/50000]\tLoss: 4.7066\tLR: 4.696675\n",
      "Training Epoch: 47 [48640/50000]\tLoss: 4.7267\tLR: 4.696931\n",
      "Training Epoch: 47 [48768/50000]\tLoss: 4.6585\tLR: 4.697187\n",
      "Training Epoch: 47 [48896/50000]\tLoss: 4.6842\tLR: 4.697442\n",
      "Training Epoch: 47 [49024/50000]\tLoss: 4.7152\tLR: 4.697698\n",
      "Training Epoch: 47 [49152/50000]\tLoss: 4.7214\tLR: 4.697954\n",
      "Training Epoch: 47 [49280/50000]\tLoss: 4.7107\tLR: 4.698210\n",
      "Training Epoch: 47 [49408/50000]\tLoss: 4.6259\tLR: 4.698465\n",
      "Training Epoch: 47 [49536/50000]\tLoss: 4.6372\tLR: 4.698721\n",
      "Training Epoch: 47 [49664/50000]\tLoss: 4.6870\tLR: 4.698977\n",
      "Training Epoch: 47 [49792/50000]\tLoss: 4.6828\tLR: 4.699233\n",
      "Training Epoch: 47 [49920/50000]\tLoss: 4.6736\tLR: 4.699488\n",
      "Training Epoch: 47 [50000/50000]\tLoss: 4.7628\tLR: 4.699744\n",
      "epoch 47 training time consumed: 489.11s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   65891 GB |   65891 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   65689 GB |   65689 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     202 GB |     202 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   65891 GB |   65891 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   65689 GB |   65689 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     202 GB |     202 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   64964 GB |   64963 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   64761 GB |   64761 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     202 GB |     202 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    6986 K  |    6986 K  |\n",
      "|       from large pool |      24    |      65    |    2978 K  |    2978 K  |\n",
      "|       from small pool |     231    |     274    |    4008 K  |    4008 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    6986 K  |    6986 K  |\n",
      "|       from large pool |      24    |      65    |    2978 K  |    2978 K  |\n",
      "|       from small pool |     231    |     274    |    4008 K  |    4008 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      47    |    4049 K  |    4049 K  |\n",
      "|       from large pool |      10    |      23    |    1431 K  |    1431 K  |\n",
      "|       from small pool |      25    |      35    |    2617 K  |    2617 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 47, Average loss: 0.0370, Accuracy: 0.0100, Time consumed:31.09s\n",
      "\n",
      "Training Epoch: 48 [128/50000]\tLoss: 4.7253\tLR: 0.100000\n",
      "Training Epoch: 48 [256/50000]\tLoss: 4.6861\tLR: 4.700256\n",
      "Training Epoch: 48 [384/50000]\tLoss: 4.6530\tLR: 4.700512\n",
      "Training Epoch: 48 [512/50000]\tLoss: 4.6873\tLR: 4.700767\n",
      "Training Epoch: 48 [640/50000]\tLoss: 4.7029\tLR: 4.701023\n",
      "Training Epoch: 48 [768/50000]\tLoss: 4.6674\tLR: 4.701279\n",
      "Training Epoch: 48 [896/50000]\tLoss: 4.6344\tLR: 4.701535\n",
      "Training Epoch: 48 [1024/50000]\tLoss: 4.7275\tLR: 4.701790\n",
      "Training Epoch: 48 [1152/50000]\tLoss: 4.6860\tLR: 4.702046\n",
      "Training Epoch: 48 [1280/50000]\tLoss: 4.7112\tLR: 4.702302\n",
      "Training Epoch: 48 [1408/50000]\tLoss: 4.7397\tLR: 4.702558\n",
      "Training Epoch: 48 [1536/50000]\tLoss: 4.6722\tLR: 4.702813\n",
      "Training Epoch: 48 [1664/50000]\tLoss: 4.6717\tLR: 4.703069\n",
      "Training Epoch: 48 [1792/50000]\tLoss: 4.5855\tLR: 4.703325\n",
      "Training Epoch: 48 [1920/50000]\tLoss: 4.7285\tLR: 4.703581\n",
      "Training Epoch: 48 [2048/50000]\tLoss: 4.7166\tLR: 4.703836\n",
      "Training Epoch: 48 [2176/50000]\tLoss: 4.6181\tLR: 4.704092\n",
      "Training Epoch: 48 [2304/50000]\tLoss: 4.7280\tLR: 4.704348\n",
      "Training Epoch: 48 [2432/50000]\tLoss: 4.6625\tLR: 4.704604\n",
      "Training Epoch: 48 [2560/50000]\tLoss: 4.7275\tLR: 4.704859\n",
      "Training Epoch: 48 [2688/50000]\tLoss: 4.7176\tLR: 4.705115\n",
      "Training Epoch: 48 [2816/50000]\tLoss: 4.7314\tLR: 4.705371\n",
      "Training Epoch: 48 [2944/50000]\tLoss: 4.7101\tLR: 4.705627\n",
      "Training Epoch: 48 [3072/50000]\tLoss: 4.6662\tLR: 4.705882\n",
      "Training Epoch: 48 [3200/50000]\tLoss: 4.6797\tLR: 4.706138\n",
      "Training Epoch: 48 [3328/50000]\tLoss: 4.6542\tLR: 4.706394\n",
      "Training Epoch: 48 [3456/50000]\tLoss: 4.7506\tLR: 4.706650\n",
      "Training Epoch: 48 [3584/50000]\tLoss: 4.7419\tLR: 4.706905\n",
      "Training Epoch: 48 [3712/50000]\tLoss: 4.6007\tLR: 4.707161\n",
      "Training Epoch: 48 [3840/50000]\tLoss: 4.6915\tLR: 4.707417\n",
      "Training Epoch: 48 [3968/50000]\tLoss: 4.6285\tLR: 4.707673\n",
      "Training Epoch: 48 [4096/50000]\tLoss: 4.6819\tLR: 4.707928\n",
      "Training Epoch: 48 [4224/50000]\tLoss: 4.7551\tLR: 4.708184\n",
      "Training Epoch: 48 [4352/50000]\tLoss: 4.6935\tLR: 4.708440\n",
      "Training Epoch: 48 [4480/50000]\tLoss: 4.7304\tLR: 4.708696\n",
      "Training Epoch: 48 [4608/50000]\tLoss: 4.6739\tLR: 4.708951\n",
      "Training Epoch: 48 [4736/50000]\tLoss: 4.6930\tLR: 4.709207\n",
      "Training Epoch: 48 [4864/50000]\tLoss: 4.7158\tLR: 4.709463\n",
      "Training Epoch: 48 [4992/50000]\tLoss: 4.7302\tLR: 4.709719\n",
      "Training Epoch: 48 [5120/50000]\tLoss: 4.7099\tLR: 4.709974\n",
      "Training Epoch: 48 [5248/50000]\tLoss: 4.7606\tLR: 4.710230\n",
      "Training Epoch: 48 [5376/50000]\tLoss: 4.6738\tLR: 4.710486\n",
      "Training Epoch: 48 [5504/50000]\tLoss: 4.6371\tLR: 4.710742\n",
      "Training Epoch: 48 [5632/50000]\tLoss: 4.7300\tLR: 4.710997\n",
      "Training Epoch: 48 [5760/50000]\tLoss: 4.6712\tLR: 4.711253\n",
      "Training Epoch: 48 [5888/50000]\tLoss: 4.7717\tLR: 4.711509\n",
      "Training Epoch: 48 [6016/50000]\tLoss: 4.7405\tLR: 4.711765\n",
      "Training Epoch: 48 [6144/50000]\tLoss: 4.7025\tLR: 4.712020\n",
      "Training Epoch: 48 [6272/50000]\tLoss: 4.7640\tLR: 4.712276\n",
      "Training Epoch: 48 [6400/50000]\tLoss: 4.7155\tLR: 4.712532\n",
      "Training Epoch: 48 [6528/50000]\tLoss: 4.7386\tLR: 4.712788\n",
      "Training Epoch: 48 [6656/50000]\tLoss: 4.6937\tLR: 4.713043\n",
      "Training Epoch: 48 [6784/50000]\tLoss: 4.7042\tLR: 4.713299\n",
      "Training Epoch: 48 [6912/50000]\tLoss: 4.6298\tLR: 4.713555\n",
      "Training Epoch: 48 [7040/50000]\tLoss: 4.6583\tLR: 4.713811\n",
      "Training Epoch: 48 [7168/50000]\tLoss: 4.7400\tLR: 4.714066\n",
      "Training Epoch: 48 [7296/50000]\tLoss: 4.7175\tLR: 4.714322\n",
      "Training Epoch: 48 [7424/50000]\tLoss: 4.7330\tLR: 4.714578\n",
      "Training Epoch: 48 [7552/50000]\tLoss: 4.7710\tLR: 4.714834\n",
      "Training Epoch: 48 [7680/50000]\tLoss: 4.7656\tLR: 4.715090\n",
      "Training Epoch: 48 [7808/50000]\tLoss: 4.7311\tLR: 4.715345\n",
      "Training Epoch: 48 [7936/50000]\tLoss: 4.7074\tLR: 4.715601\n",
      "Training Epoch: 48 [8064/50000]\tLoss: 4.6838\tLR: 4.715857\n",
      "Training Epoch: 48 [8192/50000]\tLoss: 4.6871\tLR: 4.716113\n",
      "Training Epoch: 48 [8320/50000]\tLoss: 4.6386\tLR: 4.716368\n",
      "Training Epoch: 48 [8448/50000]\tLoss: 4.7301\tLR: 4.716624\n",
      "Training Epoch: 48 [8576/50000]\tLoss: 4.6882\tLR: 4.716880\n",
      "Training Epoch: 48 [8704/50000]\tLoss: 4.6996\tLR: 4.717136\n",
      "Training Epoch: 48 [8832/50000]\tLoss: 4.6633\tLR: 4.717391\n",
      "Training Epoch: 48 [8960/50000]\tLoss: 4.7511\tLR: 4.717647\n",
      "Training Epoch: 48 [9088/50000]\tLoss: 4.7293\tLR: 4.717903\n",
      "Training Epoch: 48 [9216/50000]\tLoss: 4.7122\tLR: 4.718159\n",
      "Training Epoch: 48 [9344/50000]\tLoss: 4.6362\tLR: 4.718414\n",
      "Training Epoch: 48 [9472/50000]\tLoss: 4.6835\tLR: 4.718670\n",
      "Training Epoch: 48 [9600/50000]\tLoss: 4.7143\tLR: 4.718926\n",
      "Training Epoch: 48 [9728/50000]\tLoss: 4.7155\tLR: 4.719182\n",
      "Training Epoch: 48 [9856/50000]\tLoss: 4.6557\tLR: 4.719437\n",
      "Training Epoch: 48 [9984/50000]\tLoss: 4.7682\tLR: 4.719693\n",
      "Training Epoch: 48 [10112/50000]\tLoss: 4.6892\tLR: 4.719949\n",
      "Training Epoch: 48 [10240/50000]\tLoss: 4.7522\tLR: 4.720205\n",
      "Training Epoch: 48 [10368/50000]\tLoss: 4.7162\tLR: 4.720460\n",
      "Training Epoch: 48 [10496/50000]\tLoss: 4.7163\tLR: 4.720716\n",
      "Training Epoch: 48 [10624/50000]\tLoss: 4.6668\tLR: 4.720972\n",
      "Training Epoch: 48 [10752/50000]\tLoss: 4.7555\tLR: 4.721228\n",
      "Training Epoch: 48 [10880/50000]\tLoss: 4.7661\tLR: 4.721483\n",
      "Training Epoch: 48 [11008/50000]\tLoss: 4.6420\tLR: 4.721739\n",
      "Training Epoch: 48 [11136/50000]\tLoss: 4.7410\tLR: 4.721995\n",
      "Training Epoch: 48 [11264/50000]\tLoss: 4.7512\tLR: 4.722251\n",
      "Training Epoch: 48 [11392/50000]\tLoss: 4.6331\tLR: 4.722506\n",
      "Training Epoch: 48 [11520/50000]\tLoss: 4.7097\tLR: 4.722762\n",
      "Training Epoch: 48 [11648/50000]\tLoss: 4.6688\tLR: 4.723018\n",
      "Training Epoch: 48 [11776/50000]\tLoss: 4.6643\tLR: 4.723274\n",
      "Training Epoch: 48 [11904/50000]\tLoss: 4.7131\tLR: 4.723529\n",
      "Training Epoch: 48 [12032/50000]\tLoss: 4.7307\tLR: 4.723785\n",
      "Training Epoch: 48 [12160/50000]\tLoss: 4.6525\tLR: 4.724041\n",
      "Training Epoch: 48 [12288/50000]\tLoss: 4.6522\tLR: 4.724297\n",
      "Training Epoch: 48 [12416/50000]\tLoss: 4.6225\tLR: 4.724552\n",
      "Training Epoch: 48 [12544/50000]\tLoss: 4.7714\tLR: 4.724808\n",
      "Training Epoch: 48 [12672/50000]\tLoss: 4.6712\tLR: 4.725064\n",
      "Training Epoch: 48 [12800/50000]\tLoss: 4.5965\tLR: 4.725320\n",
      "Training Epoch: 48 [12928/50000]\tLoss: 4.7082\tLR: 4.725575\n",
      "Training Epoch: 48 [13056/50000]\tLoss: 4.7449\tLR: 4.725831\n",
      "Training Epoch: 48 [13184/50000]\tLoss: 4.7137\tLR: 4.726087\n",
      "Training Epoch: 48 [13312/50000]\tLoss: 4.6841\tLR: 4.726343\n",
      "Training Epoch: 48 [13440/50000]\tLoss: 4.6733\tLR: 4.726598\n",
      "Training Epoch: 48 [13568/50000]\tLoss: 4.6758\tLR: 4.726854\n",
      "Training Epoch: 48 [13696/50000]\tLoss: 4.6604\tLR: 4.727110\n",
      "Training Epoch: 48 [13824/50000]\tLoss: 4.5822\tLR: 4.727366\n",
      "Training Epoch: 48 [13952/50000]\tLoss: 4.6697\tLR: 4.727621\n",
      "Training Epoch: 48 [14080/50000]\tLoss: 4.6748\tLR: 4.727877\n",
      "Training Epoch: 48 [14208/50000]\tLoss: 4.6976\tLR: 4.728133\n",
      "Training Epoch: 48 [14336/50000]\tLoss: 4.6896\tLR: 4.728389\n",
      "Training Epoch: 48 [14464/50000]\tLoss: 4.6455\tLR: 4.728645\n",
      "Training Epoch: 48 [14592/50000]\tLoss: 4.7692\tLR: 4.728900\n",
      "Training Epoch: 48 [14720/50000]\tLoss: 4.7711\tLR: 4.729156\n",
      "Training Epoch: 48 [14848/50000]\tLoss: 4.6161\tLR: 4.729412\n",
      "Training Epoch: 48 [14976/50000]\tLoss: 4.7514\tLR: 4.729668\n",
      "Training Epoch: 48 [15104/50000]\tLoss: 4.7340\tLR: 4.729923\n",
      "Training Epoch: 48 [15232/50000]\tLoss: 4.6938\tLR: 4.730179\n",
      "Training Epoch: 48 [15360/50000]\tLoss: 4.6848\tLR: 4.730435\n",
      "Training Epoch: 48 [15488/50000]\tLoss: 4.6491\tLR: 4.730691\n",
      "Training Epoch: 48 [15616/50000]\tLoss: 4.6633\tLR: 4.730946\n",
      "Training Epoch: 48 [15744/50000]\tLoss: 4.7087\tLR: 4.731202\n",
      "Training Epoch: 48 [15872/50000]\tLoss: 4.7132\tLR: 4.731458\n",
      "Training Epoch: 48 [16000/50000]\tLoss: 4.6589\tLR: 4.731714\n",
      "Training Epoch: 48 [16128/50000]\tLoss: 4.7453\tLR: 4.731969\n",
      "Training Epoch: 48 [16256/50000]\tLoss: 4.6952\tLR: 4.732225\n",
      "Training Epoch: 48 [16384/50000]\tLoss: 4.7287\tLR: 4.732481\n",
      "Training Epoch: 48 [16512/50000]\tLoss: 4.7486\tLR: 4.732737\n",
      "Training Epoch: 48 [16640/50000]\tLoss: 4.6486\tLR: 4.732992\n",
      "Training Epoch: 48 [16768/50000]\tLoss: 4.6849\tLR: 4.733248\n",
      "Training Epoch: 48 [16896/50000]\tLoss: 4.6423\tLR: 4.733504\n",
      "Training Epoch: 48 [17024/50000]\tLoss: 4.6425\tLR: 4.733760\n",
      "Training Epoch: 48 [17152/50000]\tLoss: 4.6753\tLR: 4.734015\n",
      "Training Epoch: 48 [17280/50000]\tLoss: 4.6511\tLR: 4.734271\n",
      "Training Epoch: 48 [17408/50000]\tLoss: 4.7304\tLR: 4.734527\n",
      "Training Epoch: 48 [17536/50000]\tLoss: 4.6353\tLR: 4.734783\n",
      "Training Epoch: 48 [17664/50000]\tLoss: 4.6699\tLR: 4.735038\n",
      "Training Epoch: 48 [17792/50000]\tLoss: 4.6627\tLR: 4.735294\n",
      "Training Epoch: 48 [17920/50000]\tLoss: 4.6769\tLR: 4.735550\n",
      "Training Epoch: 48 [18048/50000]\tLoss: 4.7059\tLR: 4.735806\n",
      "Training Epoch: 48 [18176/50000]\tLoss: 4.6736\tLR: 4.736061\n",
      "Training Epoch: 48 [18304/50000]\tLoss: 4.7068\tLR: 4.736317\n",
      "Training Epoch: 48 [18432/50000]\tLoss: 4.6392\tLR: 4.736573\n",
      "Training Epoch: 48 [18560/50000]\tLoss: 4.7329\tLR: 4.736829\n",
      "Training Epoch: 48 [18688/50000]\tLoss: 4.6347\tLR: 4.737084\n",
      "Training Epoch: 48 [18816/50000]\tLoss: 4.7162\tLR: 4.737340\n",
      "Training Epoch: 48 [18944/50000]\tLoss: 4.6552\tLR: 4.737596\n",
      "Training Epoch: 48 [19072/50000]\tLoss: 4.6648\tLR: 4.737852\n",
      "Training Epoch: 48 [19200/50000]\tLoss: 4.5644\tLR: 4.738107\n",
      "Training Epoch: 48 [19328/50000]\tLoss: 4.6878\tLR: 4.738363\n",
      "Training Epoch: 48 [19456/50000]\tLoss: 4.7130\tLR: 4.738619\n",
      "Training Epoch: 48 [19584/50000]\tLoss: 4.5776\tLR: 4.738875\n",
      "Training Epoch: 48 [19712/50000]\tLoss: 4.6744\tLR: 4.739130\n",
      "Training Epoch: 48 [19840/50000]\tLoss: 4.7310\tLR: 4.739386\n",
      "Training Epoch: 48 [19968/50000]\tLoss: 4.7257\tLR: 4.739642\n",
      "Training Epoch: 48 [20096/50000]\tLoss: 4.6600\tLR: 4.739898\n",
      "Training Epoch: 48 [20224/50000]\tLoss: 4.6621\tLR: 4.740153\n",
      "Training Epoch: 48 [20352/50000]\tLoss: 4.6945\tLR: 4.740409\n",
      "Training Epoch: 48 [20480/50000]\tLoss: 4.6830\tLR: 4.740665\n",
      "Training Epoch: 48 [20608/50000]\tLoss: 4.6892\tLR: 4.740921\n",
      "Training Epoch: 48 [20736/50000]\tLoss: 4.7506\tLR: 4.741176\n",
      "Training Epoch: 48 [20864/50000]\tLoss: 4.6962\tLR: 4.741432\n",
      "Training Epoch: 48 [20992/50000]\tLoss: 4.6898\tLR: 4.741688\n",
      "Training Epoch: 48 [21120/50000]\tLoss: 4.6768\tLR: 4.741944\n",
      "Training Epoch: 48 [21248/50000]\tLoss: 4.7154\tLR: 4.742199\n",
      "Training Epoch: 48 [21376/50000]\tLoss: 4.7099\tLR: 4.742455\n",
      "Training Epoch: 48 [21504/50000]\tLoss: 4.6937\tLR: 4.742711\n",
      "Training Epoch: 48 [21632/50000]\tLoss: 4.6171\tLR: 4.742967\n",
      "Training Epoch: 48 [21760/50000]\tLoss: 4.7098\tLR: 4.743223\n",
      "Training Epoch: 48 [21888/50000]\tLoss: 4.7631\tLR: 4.743478\n",
      "Training Epoch: 48 [22016/50000]\tLoss: 4.7611\tLR: 4.743734\n",
      "Training Epoch: 48 [22144/50000]\tLoss: 4.7370\tLR: 4.743990\n",
      "Training Epoch: 48 [22272/50000]\tLoss: 4.7096\tLR: 4.744246\n",
      "Training Epoch: 48 [22400/50000]\tLoss: 4.6968\tLR: 4.744501\n",
      "Training Epoch: 48 [22528/50000]\tLoss: 4.6909\tLR: 4.744757\n",
      "Training Epoch: 48 [22656/50000]\tLoss: 4.6920\tLR: 4.745013\n",
      "Training Epoch: 48 [22784/50000]\tLoss: 4.6095\tLR: 4.745269\n",
      "Training Epoch: 48 [22912/50000]\tLoss: 4.6695\tLR: 4.745524\n",
      "Training Epoch: 48 [23040/50000]\tLoss: 4.7244\tLR: 4.745780\n",
      "Training Epoch: 48 [23168/50000]\tLoss: 4.7233\tLR: 4.746036\n",
      "Training Epoch: 48 [23296/50000]\tLoss: 4.7202\tLR: 4.746292\n",
      "Training Epoch: 48 [23424/50000]\tLoss: 4.6458\tLR: 4.746547\n",
      "Training Epoch: 48 [23552/50000]\tLoss: 4.7236\tLR: 4.746803\n",
      "Training Epoch: 48 [23680/50000]\tLoss: 4.6697\tLR: 4.747059\n",
      "Training Epoch: 48 [23808/50000]\tLoss: 4.7807\tLR: 4.747315\n",
      "Training Epoch: 48 [23936/50000]\tLoss: 4.6668\tLR: 4.747570\n",
      "Training Epoch: 48 [24064/50000]\tLoss: 4.7071\tLR: 4.747826\n",
      "Training Epoch: 48 [24192/50000]\tLoss: 4.6957\tLR: 4.748082\n",
      "Training Epoch: 48 [24320/50000]\tLoss: 4.7082\tLR: 4.748338\n",
      "Training Epoch: 48 [24448/50000]\tLoss: 4.6783\tLR: 4.748593\n",
      "Training Epoch: 48 [24576/50000]\tLoss: 4.6614\tLR: 4.748849\n",
      "Training Epoch: 48 [24704/50000]\tLoss: 4.7241\tLR: 4.749105\n",
      "Training Epoch: 48 [24832/50000]\tLoss: 4.7272\tLR: 4.749361\n",
      "Training Epoch: 48 [24960/50000]\tLoss: 4.6919\tLR: 4.749616\n",
      "Training Epoch: 48 [25088/50000]\tLoss: 4.7272\tLR: 4.749872\n",
      "Training Epoch: 48 [25216/50000]\tLoss: 4.6233\tLR: 4.750128\n",
      "Training Epoch: 48 [25344/50000]\tLoss: 4.7139\tLR: 4.750384\n",
      "Training Epoch: 48 [25472/50000]\tLoss: 4.7430\tLR: 4.750639\n",
      "Training Epoch: 48 [25600/50000]\tLoss: 4.6779\tLR: 4.750895\n",
      "Training Epoch: 48 [25728/50000]\tLoss: 4.6950\tLR: 4.751151\n",
      "Training Epoch: 48 [25856/50000]\tLoss: 4.7025\tLR: 4.751407\n",
      "Training Epoch: 48 [25984/50000]\tLoss: 4.6560\tLR: 4.751662\n",
      "Training Epoch: 48 [26112/50000]\tLoss: 4.6564\tLR: 4.751918\n",
      "Training Epoch: 48 [26240/50000]\tLoss: 4.6773\tLR: 4.752174\n",
      "Training Epoch: 48 [26368/50000]\tLoss: 4.6989\tLR: 4.752430\n",
      "Training Epoch: 48 [26496/50000]\tLoss: 4.7422\tLR: 4.752685\n",
      "Training Epoch: 48 [26624/50000]\tLoss: 4.7300\tLR: 4.752941\n",
      "Training Epoch: 48 [26752/50000]\tLoss: 4.6854\tLR: 4.753197\n",
      "Training Epoch: 48 [26880/50000]\tLoss: 4.7024\tLR: 4.753453\n",
      "Training Epoch: 48 [27008/50000]\tLoss: 4.6466\tLR: 4.753708\n",
      "Training Epoch: 48 [27136/50000]\tLoss: 4.6800\tLR: 4.753964\n",
      "Training Epoch: 48 [27264/50000]\tLoss: 4.7050\tLR: 4.754220\n",
      "Training Epoch: 48 [27392/50000]\tLoss: 4.7010\tLR: 4.754476\n",
      "Training Epoch: 48 [27520/50000]\tLoss: 4.7386\tLR: 4.754731\n",
      "Training Epoch: 48 [27648/50000]\tLoss: 4.6952\tLR: 4.754987\n",
      "Training Epoch: 48 [27776/50000]\tLoss: 4.7514\tLR: 4.755243\n",
      "Training Epoch: 48 [27904/50000]\tLoss: 4.7491\tLR: 4.755499\n",
      "Training Epoch: 48 [28032/50000]\tLoss: 4.6487\tLR: 4.755754\n",
      "Training Epoch: 48 [28160/50000]\tLoss: 4.6959\tLR: 4.756010\n",
      "Training Epoch: 48 [28288/50000]\tLoss: 4.7394\tLR: 4.756266\n",
      "Training Epoch: 48 [28416/50000]\tLoss: 4.7082\tLR: 4.756522\n",
      "Training Epoch: 48 [28544/50000]\tLoss: 4.6907\tLR: 4.756777\n",
      "Training Epoch: 48 [28672/50000]\tLoss: 4.7374\tLR: 4.757033\n",
      "Training Epoch: 48 [28800/50000]\tLoss: 4.6690\tLR: 4.757289\n",
      "Training Epoch: 48 [28928/50000]\tLoss: 4.6361\tLR: 4.757545\n",
      "Training Epoch: 48 [29056/50000]\tLoss: 4.7362\tLR: 4.757801\n",
      "Training Epoch: 48 [29184/50000]\tLoss: 4.6453\tLR: 4.758056\n",
      "Training Epoch: 48 [29312/50000]\tLoss: 4.7163\tLR: 4.758312\n",
      "Training Epoch: 48 [29440/50000]\tLoss: 4.7591\tLR: 4.758568\n",
      "Training Epoch: 48 [29568/50000]\tLoss: 4.7652\tLR: 4.758824\n",
      "Training Epoch: 48 [29696/50000]\tLoss: 4.6809\tLR: 4.759079\n",
      "Training Epoch: 48 [29824/50000]\tLoss: 4.7494\tLR: 4.759335\n",
      "Training Epoch: 48 [29952/50000]\tLoss: 4.6326\tLR: 4.759591\n",
      "Training Epoch: 48 [30080/50000]\tLoss: 4.6330\tLR: 4.759847\n",
      "Training Epoch: 48 [30208/50000]\tLoss: 4.6970\tLR: 4.760102\n",
      "Training Epoch: 48 [30336/50000]\tLoss: 4.6237\tLR: 4.760358\n",
      "Training Epoch: 48 [30464/50000]\tLoss: 4.7619\tLR: 4.760614\n",
      "Training Epoch: 48 [30592/50000]\tLoss: 4.6500\tLR: 4.760870\n",
      "Training Epoch: 48 [30720/50000]\tLoss: 4.6878\tLR: 4.761125\n",
      "Training Epoch: 48 [30848/50000]\tLoss: 4.6494\tLR: 4.761381\n",
      "Training Epoch: 48 [30976/50000]\tLoss: 4.6953\tLR: 4.761637\n",
      "Training Epoch: 48 [31104/50000]\tLoss: 4.7178\tLR: 4.761893\n",
      "Training Epoch: 48 [31232/50000]\tLoss: 4.7181\tLR: 4.762148\n",
      "Training Epoch: 48 [31360/50000]\tLoss: 4.7473\tLR: 4.762404\n",
      "Training Epoch: 48 [31488/50000]\tLoss: 4.7040\tLR: 4.762660\n",
      "Training Epoch: 48 [31616/50000]\tLoss: 4.7220\tLR: 4.762916\n",
      "Training Epoch: 48 [31744/50000]\tLoss: 4.7630\tLR: 4.763171\n",
      "Training Epoch: 48 [31872/50000]\tLoss: 4.7403\tLR: 4.763427\n",
      "Training Epoch: 48 [32000/50000]\tLoss: 4.6744\tLR: 4.763683\n",
      "Training Epoch: 48 [32128/50000]\tLoss: 4.7090\tLR: 4.763939\n",
      "Training Epoch: 48 [32256/50000]\tLoss: 4.7506\tLR: 4.764194\n",
      "Training Epoch: 48 [32384/50000]\tLoss: 4.6971\tLR: 4.764450\n",
      "Training Epoch: 48 [32512/50000]\tLoss: 4.6872\tLR: 4.764706\n",
      "Training Epoch: 48 [32640/50000]\tLoss: 4.6122\tLR: 4.764962\n",
      "Training Epoch: 48 [32768/50000]\tLoss: 4.6969\tLR: 4.765217\n",
      "Training Epoch: 48 [32896/50000]\tLoss: 4.6962\tLR: 4.765473\n",
      "Training Epoch: 48 [33024/50000]\tLoss: 4.7518\tLR: 4.765729\n",
      "Training Epoch: 48 [33152/50000]\tLoss: 4.7643\tLR: 4.765985\n",
      "Training Epoch: 48 [33280/50000]\tLoss: 4.6799\tLR: 4.766240\n",
      "Training Epoch: 48 [33408/50000]\tLoss: 4.6350\tLR: 4.766496\n",
      "Training Epoch: 48 [33536/50000]\tLoss: 4.6926\tLR: 4.766752\n",
      "Training Epoch: 48 [33664/50000]\tLoss: 4.7567\tLR: 4.767008\n",
      "Training Epoch: 48 [33792/50000]\tLoss: 4.7095\tLR: 4.767263\n",
      "Training Epoch: 48 [33920/50000]\tLoss: 4.6878\tLR: 4.767519\n",
      "Training Epoch: 48 [34048/50000]\tLoss: 4.6529\tLR: 4.767775\n",
      "Training Epoch: 48 [34176/50000]\tLoss: 4.6947\tLR: 4.768031\n",
      "Training Epoch: 48 [34304/50000]\tLoss: 4.6858\tLR: 4.768286\n",
      "Training Epoch: 48 [34432/50000]\tLoss: 4.6542\tLR: 4.768542\n",
      "Training Epoch: 48 [34560/50000]\tLoss: 4.7759\tLR: 4.768798\n",
      "Training Epoch: 48 [34688/50000]\tLoss: 4.6944\tLR: 4.769054\n",
      "Training Epoch: 48 [34816/50000]\tLoss: 4.6826\tLR: 4.769309\n",
      "Training Epoch: 48 [34944/50000]\tLoss: 4.7155\tLR: 4.769565\n",
      "Training Epoch: 48 [35072/50000]\tLoss: 4.7163\tLR: 4.769821\n",
      "Training Epoch: 48 [35200/50000]\tLoss: 4.7240\tLR: 4.770077\n",
      "Training Epoch: 48 [35328/50000]\tLoss: 4.6621\tLR: 4.770332\n",
      "Training Epoch: 48 [35456/50000]\tLoss: 4.7097\tLR: 4.770588\n",
      "Training Epoch: 48 [35584/50000]\tLoss: 4.7643\tLR: 4.770844\n",
      "Training Epoch: 48 [35712/50000]\tLoss: 4.7644\tLR: 4.771100\n",
      "Training Epoch: 48 [35840/50000]\tLoss: 4.7009\tLR: 4.771355\n",
      "Training Epoch: 48 [35968/50000]\tLoss: 4.6690\tLR: 4.771611\n",
      "Training Epoch: 48 [36096/50000]\tLoss: 4.6393\tLR: 4.771867\n",
      "Training Epoch: 48 [36224/50000]\tLoss: 4.6394\tLR: 4.772123\n",
      "Training Epoch: 48 [36352/50000]\tLoss: 4.7394\tLR: 4.772379\n",
      "Training Epoch: 48 [36480/50000]\tLoss: 4.6748\tLR: 4.772634\n",
      "Training Epoch: 48 [36608/50000]\tLoss: 4.7941\tLR: 4.772890\n",
      "Training Epoch: 48 [36736/50000]\tLoss: 4.7096\tLR: 4.773146\n",
      "Training Epoch: 48 [36864/50000]\tLoss: 4.7411\tLR: 4.773402\n",
      "Training Epoch: 48 [36992/50000]\tLoss: 4.7130\tLR: 4.773657\n",
      "Training Epoch: 48 [37120/50000]\tLoss: 4.6221\tLR: 4.773913\n",
      "Training Epoch: 48 [37248/50000]\tLoss: 4.6741\tLR: 4.774169\n",
      "Training Epoch: 48 [37376/50000]\tLoss: 4.7795\tLR: 4.774425\n",
      "Training Epoch: 48 [37504/50000]\tLoss: 4.6737\tLR: 4.774680\n",
      "Training Epoch: 48 [37632/50000]\tLoss: 4.6562\tLR: 4.774936\n",
      "Training Epoch: 48 [37760/50000]\tLoss: 4.7243\tLR: 4.775192\n",
      "Training Epoch: 48 [37888/50000]\tLoss: 4.7246\tLR: 4.775448\n",
      "Training Epoch: 48 [38016/50000]\tLoss: 4.7641\tLR: 4.775703\n",
      "Training Epoch: 48 [38144/50000]\tLoss: 4.6928\tLR: 4.775959\n",
      "Training Epoch: 48 [38272/50000]\tLoss: 4.6893\tLR: 4.776215\n",
      "Training Epoch: 48 [38400/50000]\tLoss: 4.6670\tLR: 4.776471\n",
      "Training Epoch: 48 [38528/50000]\tLoss: 4.7038\tLR: 4.776726\n",
      "Training Epoch: 48 [38656/50000]\tLoss: 4.6933\tLR: 4.776982\n",
      "Training Epoch: 48 [38784/50000]\tLoss: 4.6629\tLR: 4.777238\n",
      "Training Epoch: 48 [38912/50000]\tLoss: 4.6950\tLR: 4.777494\n",
      "Training Epoch: 48 [39040/50000]\tLoss: 4.7679\tLR: 4.777749\n",
      "Training Epoch: 48 [39168/50000]\tLoss: 4.6405\tLR: 4.778005\n",
      "Training Epoch: 48 [39296/50000]\tLoss: 4.6985\tLR: 4.778261\n",
      "Training Epoch: 48 [39424/50000]\tLoss: 4.7011\tLR: 4.778517\n",
      "Training Epoch: 48 [39552/50000]\tLoss: 4.7469\tLR: 4.778772\n",
      "Training Epoch: 48 [39680/50000]\tLoss: 4.7503\tLR: 4.779028\n",
      "Training Epoch: 48 [39808/50000]\tLoss: 4.6501\tLR: 4.779284\n",
      "Training Epoch: 48 [39936/50000]\tLoss: 4.6770\tLR: 4.779540\n",
      "Training Epoch: 48 [40064/50000]\tLoss: 4.6765\tLR: 4.779795\n",
      "Training Epoch: 48 [40192/50000]\tLoss: 4.6145\tLR: 4.780051\n",
      "Training Epoch: 48 [40320/50000]\tLoss: 4.7154\tLR: 4.780307\n",
      "Training Epoch: 48 [40448/50000]\tLoss: 4.7318\tLR: 4.780563\n",
      "Training Epoch: 48 [40576/50000]\tLoss: 4.6031\tLR: 4.780818\n",
      "Training Epoch: 48 [40704/50000]\tLoss: 4.7175\tLR: 4.781074\n",
      "Training Epoch: 48 [40832/50000]\tLoss: 4.6155\tLR: 4.781330\n",
      "Training Epoch: 48 [40960/50000]\tLoss: 4.7173\tLR: 4.781586\n",
      "Training Epoch: 48 [41088/50000]\tLoss: 4.7527\tLR: 4.781841\n",
      "Training Epoch: 48 [41216/50000]\tLoss: 4.6868\tLR: 4.782097\n",
      "Training Epoch: 48 [41344/50000]\tLoss: 4.6840\tLR: 4.782353\n",
      "Training Epoch: 48 [41472/50000]\tLoss: 4.7179\tLR: 4.782609\n",
      "Training Epoch: 48 [41600/50000]\tLoss: 4.7396\tLR: 4.782864\n",
      "Training Epoch: 48 [41728/50000]\tLoss: 4.7125\tLR: 4.783120\n",
      "Training Epoch: 48 [41856/50000]\tLoss: 4.7863\tLR: 4.783376\n",
      "Training Epoch: 48 [41984/50000]\tLoss: 4.7823\tLR: 4.783632\n",
      "Training Epoch: 48 [42112/50000]\tLoss: 4.7640\tLR: 4.783887\n",
      "Training Epoch: 48 [42240/50000]\tLoss: 4.7061\tLR: 4.784143\n",
      "Training Epoch: 48 [42368/50000]\tLoss: 4.7439\tLR: 4.784399\n",
      "Training Epoch: 48 [42496/50000]\tLoss: 4.7088\tLR: 4.784655\n",
      "Training Epoch: 48 [42624/50000]\tLoss: 4.6495\tLR: 4.784910\n",
      "Training Epoch: 48 [42752/50000]\tLoss: 4.6788\tLR: 4.785166\n",
      "Training Epoch: 48 [42880/50000]\tLoss: 4.5994\tLR: 4.785422\n",
      "Training Epoch: 48 [43008/50000]\tLoss: 4.6909\tLR: 4.785678\n",
      "Training Epoch: 48 [43136/50000]\tLoss: 4.6802\tLR: 4.785934\n",
      "Training Epoch: 48 [43264/50000]\tLoss: 4.7206\tLR: 4.786189\n",
      "Training Epoch: 48 [43392/50000]\tLoss: 4.7386\tLR: 4.786445\n",
      "Training Epoch: 48 [43520/50000]\tLoss: 4.7761\tLR: 4.786701\n",
      "Training Epoch: 48 [43648/50000]\tLoss: 4.7107\tLR: 4.786957\n",
      "Training Epoch: 48 [43776/50000]\tLoss: 4.6164\tLR: 4.787212\n",
      "Training Epoch: 48 [43904/50000]\tLoss: 4.6770\tLR: 4.787468\n",
      "Training Epoch: 48 [44032/50000]\tLoss: 4.6539\tLR: 4.787724\n",
      "Training Epoch: 48 [44160/50000]\tLoss: 4.6965\tLR: 4.787980\n",
      "Training Epoch: 48 [44288/50000]\tLoss: 4.6868\tLR: 4.788235\n",
      "Training Epoch: 48 [44416/50000]\tLoss: 4.6949\tLR: 4.788491\n",
      "Training Epoch: 48 [44544/50000]\tLoss: 4.6129\tLR: 4.788747\n",
      "Training Epoch: 48 [44672/50000]\tLoss: 4.6795\tLR: 4.789003\n",
      "Training Epoch: 48 [44800/50000]\tLoss: 4.6956\tLR: 4.789258\n",
      "Training Epoch: 48 [44928/50000]\tLoss: 4.6757\tLR: 4.789514\n",
      "Training Epoch: 48 [45056/50000]\tLoss: 4.6305\tLR: 4.789770\n",
      "Training Epoch: 48 [45184/50000]\tLoss: 4.6747\tLR: 4.790026\n",
      "Training Epoch: 48 [45312/50000]\tLoss: 4.6637\tLR: 4.790281\n",
      "Training Epoch: 48 [45440/50000]\tLoss: 4.7242\tLR: 4.790537\n",
      "Training Epoch: 48 [45568/50000]\tLoss: 4.6743\tLR: 4.790793\n",
      "Training Epoch: 48 [45696/50000]\tLoss: 4.7386\tLR: 4.791049\n",
      "Training Epoch: 48 [45824/50000]\tLoss: 4.7816\tLR: 4.791304\n",
      "Training Epoch: 48 [45952/50000]\tLoss: 4.6084\tLR: 4.791560\n",
      "Training Epoch: 48 [46080/50000]\tLoss: 4.6712\tLR: 4.791816\n",
      "Training Epoch: 48 [46208/50000]\tLoss: 4.7216\tLR: 4.792072\n",
      "Training Epoch: 48 [46336/50000]\tLoss: 4.7413\tLR: 4.792327\n",
      "Training Epoch: 48 [46464/50000]\tLoss: 4.7521\tLR: 4.792583\n",
      "Training Epoch: 48 [46592/50000]\tLoss: 4.6563\tLR: 4.792839\n",
      "Training Epoch: 48 [46720/50000]\tLoss: 4.6604\tLR: 4.793095\n",
      "Training Epoch: 48 [46848/50000]\tLoss: 4.7359\tLR: 4.793350\n",
      "Training Epoch: 48 [46976/50000]\tLoss: 4.7596\tLR: 4.793606\n",
      "Training Epoch: 48 [47104/50000]\tLoss: 4.7060\tLR: 4.793862\n",
      "Training Epoch: 48 [47232/50000]\tLoss: 4.6979\tLR: 4.794118\n",
      "Training Epoch: 48 [47360/50000]\tLoss: 4.7208\tLR: 4.794373\n",
      "Training Epoch: 48 [47488/50000]\tLoss: 4.6989\tLR: 4.794629\n",
      "Training Epoch: 48 [47616/50000]\tLoss: 4.6473\tLR: 4.794885\n",
      "Training Epoch: 48 [47744/50000]\tLoss: 4.6516\tLR: 4.795141\n",
      "Training Epoch: 48 [47872/50000]\tLoss: 4.6666\tLR: 4.795396\n",
      "Training Epoch: 48 [48000/50000]\tLoss: 4.6913\tLR: 4.795652\n",
      "Training Epoch: 48 [48128/50000]\tLoss: 4.7167\tLR: 4.795908\n",
      "Training Epoch: 48 [48256/50000]\tLoss: 4.7772\tLR: 4.796164\n",
      "Training Epoch: 48 [48384/50000]\tLoss: 4.7291\tLR: 4.796419\n",
      "Training Epoch: 48 [48512/50000]\tLoss: 4.7117\tLR: 4.796675\n",
      "Training Epoch: 48 [48640/50000]\tLoss: 4.7065\tLR: 4.796931\n",
      "Training Epoch: 48 [48768/50000]\tLoss: 4.6874\tLR: 4.797187\n",
      "Training Epoch: 48 [48896/50000]\tLoss: 4.7839\tLR: 4.797442\n",
      "Training Epoch: 48 [49024/50000]\tLoss: 4.7118\tLR: 4.797698\n",
      "Training Epoch: 48 [49152/50000]\tLoss: 4.6949\tLR: 4.797954\n",
      "Training Epoch: 48 [49280/50000]\tLoss: 4.7077\tLR: 4.798210\n",
      "Training Epoch: 48 [49408/50000]\tLoss: 4.6269\tLR: 4.798465\n",
      "Training Epoch: 48 [49536/50000]\tLoss: 4.6809\tLR: 4.798721\n",
      "Training Epoch: 48 [49664/50000]\tLoss: 4.7440\tLR: 4.798977\n",
      "Training Epoch: 48 [49792/50000]\tLoss: 4.6796\tLR: 4.799233\n",
      "Training Epoch: 48 [49920/50000]\tLoss: 4.7611\tLR: 4.799488\n",
      "Training Epoch: 48 [50000/50000]\tLoss: 4.7506\tLR: 4.799744\n",
      "epoch 48 training time consumed: 488.85s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   67293 GB |   67293 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   67086 GB |   67086 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     206 GB |     206 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   67293 GB |   67293 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   67086 GB |   67086 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     206 GB |     206 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   66346 GB |   66346 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   66139 GB |   66139 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     206 GB |     206 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    7135 K  |    7135 K  |\n",
      "|       from large pool |      24    |      65    |    3041 K  |    3041 K  |\n",
      "|       from small pool |     231    |     274    |    4093 K  |    4093 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    7135 K  |    7135 K  |\n",
      "|       from large pool |      24    |      65    |    3041 K  |    3041 K  |\n",
      "|       from small pool |     231    |     274    |    4093 K  |    4093 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      38    |      47    |    4135 K  |    4135 K  |\n",
      "|       from large pool |      10    |      23    |    1462 K  |    1462 K  |\n",
      "|       from small pool |      28    |      35    |    2673 K  |    2673 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 48, Average loss: 0.0370, Accuracy: 0.0100, Time consumed:31.09s\n",
      "\n",
      "Training Epoch: 49 [128/50000]\tLoss: 4.7045\tLR: 0.100000\n",
      "Training Epoch: 49 [256/50000]\tLoss: 4.7113\tLR: 4.800256\n",
      "Training Epoch: 49 [384/50000]\tLoss: 4.6668\tLR: 4.800512\n",
      "Training Epoch: 49 [512/50000]\tLoss: 4.6982\tLR: 4.800767\n",
      "Training Epoch: 49 [640/50000]\tLoss: 4.6886\tLR: 4.801023\n",
      "Training Epoch: 49 [768/50000]\tLoss: 4.7013\tLR: 4.801279\n",
      "Training Epoch: 49 [896/50000]\tLoss: 4.6981\tLR: 4.801535\n",
      "Training Epoch: 49 [1024/50000]\tLoss: 4.6683\tLR: 4.801790\n",
      "Training Epoch: 49 [1152/50000]\tLoss: 4.7067\tLR: 4.802046\n",
      "Training Epoch: 49 [1280/50000]\tLoss: 4.6148\tLR: 4.802302\n",
      "Training Epoch: 49 [1408/50000]\tLoss: 4.6952\tLR: 4.802558\n",
      "Training Epoch: 49 [1536/50000]\tLoss: 4.6906\tLR: 4.802813\n",
      "Training Epoch: 49 [1664/50000]\tLoss: 4.6687\tLR: 4.803069\n",
      "Training Epoch: 49 [1792/50000]\tLoss: 4.6663\tLR: 4.803325\n",
      "Training Epoch: 49 [1920/50000]\tLoss: 4.7557\tLR: 4.803581\n",
      "Training Epoch: 49 [2048/50000]\tLoss: 4.7064\tLR: 4.803836\n",
      "Training Epoch: 49 [2176/50000]\tLoss: 4.7322\tLR: 4.804092\n",
      "Training Epoch: 49 [2304/50000]\tLoss: 4.7481\tLR: 4.804348\n",
      "Training Epoch: 49 [2432/50000]\tLoss: 4.7012\tLR: 4.804604\n",
      "Training Epoch: 49 [2560/50000]\tLoss: 4.6922\tLR: 4.804859\n",
      "Training Epoch: 49 [2688/50000]\tLoss: 4.6238\tLR: 4.805115\n",
      "Training Epoch: 49 [2816/50000]\tLoss: 4.7186\tLR: 4.805371\n",
      "Training Epoch: 49 [2944/50000]\tLoss: 4.6485\tLR: 4.805627\n",
      "Training Epoch: 49 [3072/50000]\tLoss: 4.6625\tLR: 4.805882\n",
      "Training Epoch: 49 [3200/50000]\tLoss: 4.6094\tLR: 4.806138\n",
      "Training Epoch: 49 [3328/50000]\tLoss: 4.6357\tLR: 4.806394\n",
      "Training Epoch: 49 [3456/50000]\tLoss: 4.7199\tLR: 4.806650\n",
      "Training Epoch: 49 [3584/50000]\tLoss: 4.7520\tLR: 4.806905\n",
      "Training Epoch: 49 [3712/50000]\tLoss: 4.7667\tLR: 4.807161\n",
      "Training Epoch: 49 [3840/50000]\tLoss: 4.6854\tLR: 4.807417\n",
      "Training Epoch: 49 [3968/50000]\tLoss: 4.6996\tLR: 4.807673\n",
      "Training Epoch: 49 [4096/50000]\tLoss: 4.6443\tLR: 4.807928\n",
      "Training Epoch: 49 [4224/50000]\tLoss: 4.6344\tLR: 4.808184\n",
      "Training Epoch: 49 [4352/50000]\tLoss: 4.7339\tLR: 4.808440\n",
      "Training Epoch: 49 [4480/50000]\tLoss: 4.7335\tLR: 4.808696\n",
      "Training Epoch: 49 [4608/50000]\tLoss: 4.7464\tLR: 4.808951\n",
      "Training Epoch: 49 [4736/50000]\tLoss: 4.6799\tLR: 4.809207\n",
      "Training Epoch: 49 [4864/50000]\tLoss: 4.6028\tLR: 4.809463\n",
      "Training Epoch: 49 [4992/50000]\tLoss: 4.6648\tLR: 4.809719\n",
      "Training Epoch: 49 [5120/50000]\tLoss: 4.6756\tLR: 4.809974\n",
      "Training Epoch: 49 [5248/50000]\tLoss: 4.7246\tLR: 4.810230\n",
      "Training Epoch: 49 [5376/50000]\tLoss: 4.7000\tLR: 4.810486\n",
      "Training Epoch: 49 [5504/50000]\tLoss: 4.7182\tLR: 4.810742\n",
      "Training Epoch: 49 [5632/50000]\tLoss: 4.6725\tLR: 4.810997\n",
      "Training Epoch: 49 [5760/50000]\tLoss: 4.6836\tLR: 4.811253\n",
      "Training Epoch: 49 [5888/50000]\tLoss: 4.6680\tLR: 4.811509\n",
      "Training Epoch: 49 [6016/50000]\tLoss: 4.6653\tLR: 4.811765\n",
      "Training Epoch: 49 [6144/50000]\tLoss: 4.7964\tLR: 4.812020\n",
      "Training Epoch: 49 [6272/50000]\tLoss: 4.6802\tLR: 4.812276\n",
      "Training Epoch: 49 [6400/50000]\tLoss: 4.7031\tLR: 4.812532\n",
      "Training Epoch: 49 [6528/50000]\tLoss: 4.6636\tLR: 4.812788\n",
      "Training Epoch: 49 [6656/50000]\tLoss: 4.6477\tLR: 4.813043\n",
      "Training Epoch: 49 [6784/50000]\tLoss: 4.7284\tLR: 4.813299\n",
      "Training Epoch: 49 [6912/50000]\tLoss: 4.6460\tLR: 4.813555\n",
      "Training Epoch: 49 [7040/50000]\tLoss: 4.6635\tLR: 4.813811\n",
      "Training Epoch: 49 [7168/50000]\tLoss: 4.6969\tLR: 4.814066\n",
      "Training Epoch: 49 [7296/50000]\tLoss: 4.6982\tLR: 4.814322\n",
      "Training Epoch: 49 [7424/50000]\tLoss: 4.6877\tLR: 4.814578\n",
      "Training Epoch: 49 [7552/50000]\tLoss: 4.6445\tLR: 4.814834\n",
      "Training Epoch: 49 [7680/50000]\tLoss: 4.6898\tLR: 4.815090\n",
      "Training Epoch: 49 [7808/50000]\tLoss: 4.6416\tLR: 4.815345\n",
      "Training Epoch: 49 [7936/50000]\tLoss: 4.7294\tLR: 4.815601\n",
      "Training Epoch: 49 [8064/50000]\tLoss: 4.6976\tLR: 4.815857\n",
      "Training Epoch: 49 [8192/50000]\tLoss: 4.6979\tLR: 4.816113\n",
      "Training Epoch: 49 [8320/50000]\tLoss: 4.6785\tLR: 4.816368\n",
      "Training Epoch: 49 [8448/50000]\tLoss: 4.6912\tLR: 4.816624\n",
      "Training Epoch: 49 [8576/50000]\tLoss: 4.6668\tLR: 4.816880\n",
      "Training Epoch: 49 [8704/50000]\tLoss: 4.7456\tLR: 4.817136\n",
      "Training Epoch: 49 [8832/50000]\tLoss: 4.7307\tLR: 4.817391\n",
      "Training Epoch: 49 [8960/50000]\tLoss: 4.7356\tLR: 4.817647\n",
      "Training Epoch: 49 [9088/50000]\tLoss: 4.6680\tLR: 4.817903\n",
      "Training Epoch: 49 [9216/50000]\tLoss: 4.6479\tLR: 4.818159\n",
      "Training Epoch: 49 [9344/50000]\tLoss: 4.6847\tLR: 4.818414\n",
      "Training Epoch: 49 [9472/50000]\tLoss: 4.7024\tLR: 4.818670\n",
      "Training Epoch: 49 [9600/50000]\tLoss: 4.6954\tLR: 4.818926\n",
      "Training Epoch: 49 [9728/50000]\tLoss: 4.6909\tLR: 4.819182\n",
      "Training Epoch: 49 [9856/50000]\tLoss: 4.7435\tLR: 4.819437\n",
      "Training Epoch: 49 [9984/50000]\tLoss: 4.7517\tLR: 4.819693\n",
      "Training Epoch: 49 [10112/50000]\tLoss: 4.6925\tLR: 4.819949\n",
      "Training Epoch: 49 [10240/50000]\tLoss: 4.7183\tLR: 4.820205\n",
      "Training Epoch: 49 [10368/50000]\tLoss: 4.7044\tLR: 4.820460\n",
      "Training Epoch: 49 [10496/50000]\tLoss: 4.7326\tLR: 4.820716\n",
      "Training Epoch: 49 [10624/50000]\tLoss: 4.6660\tLR: 4.820972\n",
      "Training Epoch: 49 [10752/50000]\tLoss: 4.7122\tLR: 4.821228\n",
      "Training Epoch: 49 [10880/50000]\tLoss: 4.6818\tLR: 4.821483\n",
      "Training Epoch: 49 [11008/50000]\tLoss: 4.6768\tLR: 4.821739\n",
      "Training Epoch: 49 [11136/50000]\tLoss: 4.6595\tLR: 4.821995\n",
      "Training Epoch: 49 [11264/50000]\tLoss: 4.6860\tLR: 4.822251\n",
      "Training Epoch: 49 [11392/50000]\tLoss: 4.6887\tLR: 4.822506\n",
      "Training Epoch: 49 [11520/50000]\tLoss: 4.6337\tLR: 4.822762\n",
      "Training Epoch: 49 [11648/50000]\tLoss: 4.7841\tLR: 4.823018\n",
      "Training Epoch: 49 [11776/50000]\tLoss: 4.6428\tLR: 4.823274\n",
      "Training Epoch: 49 [11904/50000]\tLoss: 4.7395\tLR: 4.823529\n",
      "Training Epoch: 49 [12032/50000]\tLoss: 4.7161\tLR: 4.823785\n",
      "Training Epoch: 49 [12160/50000]\tLoss: 4.6539\tLR: 4.824041\n",
      "Training Epoch: 49 [12288/50000]\tLoss: 4.6728\tLR: 4.824297\n",
      "Training Epoch: 49 [12416/50000]\tLoss: 4.6407\tLR: 4.824552\n",
      "Training Epoch: 49 [12544/50000]\tLoss: 4.7572\tLR: 4.824808\n",
      "Training Epoch: 49 [12672/50000]\tLoss: 4.7778\tLR: 4.825064\n",
      "Training Epoch: 49 [12800/50000]\tLoss: 4.7618\tLR: 4.825320\n",
      "Training Epoch: 49 [12928/50000]\tLoss: 4.7756\tLR: 4.825575\n",
      "Training Epoch: 49 [13056/50000]\tLoss: 4.7940\tLR: 4.825831\n",
      "Training Epoch: 49 [13184/50000]\tLoss: 4.6350\tLR: 4.826087\n",
      "Training Epoch: 49 [13312/50000]\tLoss: 4.6621\tLR: 4.826343\n",
      "Training Epoch: 49 [13440/50000]\tLoss: 4.7076\tLR: 4.826598\n",
      "Training Epoch: 49 [13568/50000]\tLoss: 4.6996\tLR: 4.826854\n",
      "Training Epoch: 49 [13696/50000]\tLoss: 4.6880\tLR: 4.827110\n",
      "Training Epoch: 49 [13824/50000]\tLoss: 4.6905\tLR: 4.827366\n",
      "Training Epoch: 49 [13952/50000]\tLoss: 4.7200\tLR: 4.827621\n",
      "Training Epoch: 49 [14080/50000]\tLoss: 4.7231\tLR: 4.827877\n",
      "Training Epoch: 49 [14208/50000]\tLoss: 4.7345\tLR: 4.828133\n",
      "Training Epoch: 49 [14336/50000]\tLoss: 4.6852\tLR: 4.828389\n",
      "Training Epoch: 49 [14464/50000]\tLoss: 4.7300\tLR: 4.828645\n",
      "Training Epoch: 49 [14592/50000]\tLoss: 4.7060\tLR: 4.828900\n",
      "Training Epoch: 49 [14720/50000]\tLoss: 4.7148\tLR: 4.829156\n",
      "Training Epoch: 49 [14848/50000]\tLoss: 4.6376\tLR: 4.829412\n",
      "Training Epoch: 49 [14976/50000]\tLoss: 4.7249\tLR: 4.829668\n",
      "Training Epoch: 49 [15104/50000]\tLoss: 4.7148\tLR: 4.829923\n",
      "Training Epoch: 49 [15232/50000]\tLoss: 4.7374\tLR: 4.830179\n",
      "Training Epoch: 49 [15360/50000]\tLoss: 4.6760\tLR: 4.830435\n",
      "Training Epoch: 49 [15488/50000]\tLoss: 4.7210\tLR: 4.830691\n",
      "Training Epoch: 49 [15616/50000]\tLoss: 4.6972\tLR: 4.830946\n",
      "Training Epoch: 49 [15744/50000]\tLoss: 4.6882\tLR: 4.831202\n",
      "Training Epoch: 49 [15872/50000]\tLoss: 4.7229\tLR: 4.831458\n",
      "Training Epoch: 49 [16000/50000]\tLoss: 4.7337\tLR: 4.831714\n",
      "Training Epoch: 49 [16128/50000]\tLoss: 4.6518\tLR: 4.831969\n",
      "Training Epoch: 49 [16256/50000]\tLoss: 4.6638\tLR: 4.832225\n",
      "Training Epoch: 49 [16384/50000]\tLoss: 4.7210\tLR: 4.832481\n",
      "Training Epoch: 49 [16512/50000]\tLoss: 4.6616\tLR: 4.832737\n",
      "Training Epoch: 49 [16640/50000]\tLoss: 4.7449\tLR: 4.832992\n",
      "Training Epoch: 49 [16768/50000]\tLoss: 4.7710\tLR: 4.833248\n",
      "Training Epoch: 49 [16896/50000]\tLoss: 4.6306\tLR: 4.833504\n",
      "Training Epoch: 49 [17024/50000]\tLoss: 4.6748\tLR: 4.833760\n",
      "Training Epoch: 49 [17152/50000]\tLoss: 4.6961\tLR: 4.834015\n",
      "Training Epoch: 49 [17280/50000]\tLoss: 4.6737\tLR: 4.834271\n",
      "Training Epoch: 49 [17408/50000]\tLoss: 4.6527\tLR: 4.834527\n",
      "Training Epoch: 49 [17536/50000]\tLoss: 4.7702\tLR: 4.834783\n",
      "Training Epoch: 49 [17664/50000]\tLoss: 4.7136\tLR: 4.835038\n",
      "Training Epoch: 49 [17792/50000]\tLoss: 4.6998\tLR: 4.835294\n",
      "Training Epoch: 49 [17920/50000]\tLoss: 4.6844\tLR: 4.835550\n",
      "Training Epoch: 49 [18048/50000]\tLoss: 4.7496\tLR: 4.835806\n",
      "Training Epoch: 49 [18176/50000]\tLoss: 4.7556\tLR: 4.836061\n",
      "Training Epoch: 49 [18304/50000]\tLoss: 4.6896\tLR: 4.836317\n",
      "Training Epoch: 49 [18432/50000]\tLoss: 4.6995\tLR: 4.836573\n",
      "Training Epoch: 49 [18560/50000]\tLoss: 4.7040\tLR: 4.836829\n",
      "Training Epoch: 49 [18688/50000]\tLoss: 4.6931\tLR: 4.837084\n",
      "Training Epoch: 49 [18816/50000]\tLoss: 4.6712\tLR: 4.837340\n",
      "Training Epoch: 49 [18944/50000]\tLoss: 4.6971\tLR: 4.837596\n",
      "Training Epoch: 49 [19072/50000]\tLoss: 4.7214\tLR: 4.837852\n",
      "Training Epoch: 49 [19200/50000]\tLoss: 4.5997\tLR: 4.838107\n",
      "Training Epoch: 49 [19328/50000]\tLoss: 4.6858\tLR: 4.838363\n",
      "Training Epoch: 49 [19456/50000]\tLoss: 4.7128\tLR: 4.838619\n",
      "Training Epoch: 49 [19584/50000]\tLoss: 4.6645\tLR: 4.838875\n",
      "Training Epoch: 49 [19712/50000]\tLoss: 4.6788\tLR: 4.839130\n",
      "Training Epoch: 49 [19840/50000]\tLoss: 4.7410\tLR: 4.839386\n",
      "Training Epoch: 49 [19968/50000]\tLoss: 4.7971\tLR: 4.839642\n",
      "Training Epoch: 49 [20096/50000]\tLoss: 4.6907\tLR: 4.839898\n",
      "Training Epoch: 49 [20224/50000]\tLoss: 4.6772\tLR: 4.840153\n",
      "Training Epoch: 49 [20352/50000]\tLoss: 4.6738\tLR: 4.840409\n",
      "Training Epoch: 49 [20480/50000]\tLoss: 4.7109\tLR: 4.840665\n",
      "Training Epoch: 49 [20608/50000]\tLoss: 4.7113\tLR: 4.840921\n",
      "Training Epoch: 49 [20736/50000]\tLoss: 4.7033\tLR: 4.841176\n",
      "Training Epoch: 49 [20864/50000]\tLoss: 4.6316\tLR: 4.841432\n",
      "Training Epoch: 49 [20992/50000]\tLoss: 4.7533\tLR: 4.841688\n",
      "Training Epoch: 49 [21120/50000]\tLoss: 4.6328\tLR: 4.841944\n",
      "Training Epoch: 49 [21248/50000]\tLoss: 4.7126\tLR: 4.842199\n",
      "Training Epoch: 49 [21376/50000]\tLoss: 4.7516\tLR: 4.842455\n",
      "Training Epoch: 49 [21504/50000]\tLoss: 4.8046\tLR: 4.842711\n",
      "Training Epoch: 49 [21632/50000]\tLoss: 4.7276\tLR: 4.842967\n",
      "Training Epoch: 49 [21760/50000]\tLoss: 4.7281\tLR: 4.843223\n",
      "Training Epoch: 49 [21888/50000]\tLoss: 4.6354\tLR: 4.843478\n",
      "Training Epoch: 49 [22016/50000]\tLoss: 4.6858\tLR: 4.843734\n",
      "Training Epoch: 49 [22144/50000]\tLoss: 4.6186\tLR: 4.843990\n",
      "Training Epoch: 49 [22272/50000]\tLoss: 4.6802\tLR: 4.844246\n",
      "Training Epoch: 49 [22400/50000]\tLoss: 4.7255\tLR: 4.844501\n",
      "Training Epoch: 49 [22528/50000]\tLoss: 4.7717\tLR: 4.844757\n",
      "Training Epoch: 49 [22656/50000]\tLoss: 4.5703\tLR: 4.845013\n",
      "Training Epoch: 49 [22784/50000]\tLoss: 4.7511\tLR: 4.845269\n",
      "Training Epoch: 49 [22912/50000]\tLoss: 4.7604\tLR: 4.845524\n",
      "Training Epoch: 49 [23040/50000]\tLoss: 4.7222\tLR: 4.845780\n",
      "Training Epoch: 49 [23168/50000]\tLoss: 4.6645\tLR: 4.846036\n",
      "Training Epoch: 49 [23296/50000]\tLoss: 4.6759\tLR: 4.846292\n",
      "Training Epoch: 49 [23424/50000]\tLoss: 4.6551\tLR: 4.846547\n",
      "Training Epoch: 49 [23552/50000]\tLoss: 4.8031\tLR: 4.846803\n",
      "Training Epoch: 49 [23680/50000]\tLoss: 4.7187\tLR: 4.847059\n",
      "Training Epoch: 49 [23808/50000]\tLoss: 4.6724\tLR: 4.847315\n",
      "Training Epoch: 49 [23936/50000]\tLoss: 4.6706\tLR: 4.847570\n",
      "Training Epoch: 49 [24064/50000]\tLoss: 4.7225\tLR: 4.847826\n",
      "Training Epoch: 49 [24192/50000]\tLoss: 4.6921\tLR: 4.848082\n",
      "Training Epoch: 49 [24320/50000]\tLoss: 4.7524\tLR: 4.848338\n",
      "Training Epoch: 49 [24448/50000]\tLoss: 4.6545\tLR: 4.848593\n",
      "Training Epoch: 49 [24576/50000]\tLoss: 4.7051\tLR: 4.848849\n",
      "Training Epoch: 49 [24704/50000]\tLoss: 4.6235\tLR: 4.849105\n",
      "Training Epoch: 49 [24832/50000]\tLoss: 4.5994\tLR: 4.849361\n",
      "Training Epoch: 49 [24960/50000]\tLoss: 4.7267\tLR: 4.849616\n",
      "Training Epoch: 49 [25088/50000]\tLoss: 4.6971\tLR: 4.849872\n",
      "Training Epoch: 49 [25216/50000]\tLoss: 4.6594\tLR: 4.850128\n",
      "Training Epoch: 49 [25344/50000]\tLoss: 4.6699\tLR: 4.850384\n",
      "Training Epoch: 49 [25472/50000]\tLoss: 4.6801\tLR: 4.850639\n",
      "Training Epoch: 49 [25600/50000]\tLoss: 4.8341\tLR: 4.850895\n",
      "Training Epoch: 49 [25728/50000]\tLoss: 4.6289\tLR: 4.851151\n",
      "Training Epoch: 49 [25856/50000]\tLoss: 4.7712\tLR: 4.851407\n",
      "Training Epoch: 49 [25984/50000]\tLoss: 4.7654\tLR: 4.851662\n",
      "Training Epoch: 49 [26112/50000]\tLoss: 4.7457\tLR: 4.851918\n",
      "Training Epoch: 49 [26240/50000]\tLoss: 4.7313\tLR: 4.852174\n",
      "Training Epoch: 49 [26368/50000]\tLoss: 4.7473\tLR: 4.852430\n",
      "Training Epoch: 49 [26496/50000]\tLoss: 4.7115\tLR: 4.852685\n",
      "Training Epoch: 49 [26624/50000]\tLoss: 4.6670\tLR: 4.852941\n",
      "Training Epoch: 49 [26752/50000]\tLoss: 4.7350\tLR: 4.853197\n",
      "Training Epoch: 49 [26880/50000]\tLoss: 4.6839\tLR: 4.853453\n",
      "Training Epoch: 49 [27008/50000]\tLoss: 4.7086\tLR: 4.853708\n",
      "Training Epoch: 49 [27136/50000]\tLoss: 4.6865\tLR: 4.853964\n",
      "Training Epoch: 49 [27264/50000]\tLoss: 4.7179\tLR: 4.854220\n",
      "Training Epoch: 49 [27392/50000]\tLoss: 4.6772\tLR: 4.854476\n",
      "Training Epoch: 49 [27520/50000]\tLoss: 4.6411\tLR: 4.854731\n",
      "Training Epoch: 49 [27648/50000]\tLoss: 4.6732\tLR: 4.854987\n",
      "Training Epoch: 49 [27776/50000]\tLoss: 4.7744\tLR: 4.855243\n",
      "Training Epoch: 49 [27904/50000]\tLoss: 4.7481\tLR: 4.855499\n",
      "Training Epoch: 49 [28032/50000]\tLoss: 4.7347\tLR: 4.855754\n",
      "Training Epoch: 49 [28160/50000]\tLoss: 4.7658\tLR: 4.856010\n",
      "Training Epoch: 49 [28288/50000]\tLoss: 4.6705\tLR: 4.856266\n",
      "Training Epoch: 49 [28416/50000]\tLoss: 4.7419\tLR: 4.856522\n",
      "Training Epoch: 49 [28544/50000]\tLoss: 4.6413\tLR: 4.856777\n",
      "Training Epoch: 49 [28672/50000]\tLoss: 4.7390\tLR: 4.857033\n",
      "Training Epoch: 49 [28800/50000]\tLoss: 4.7335\tLR: 4.857289\n",
      "Training Epoch: 49 [28928/50000]\tLoss: 4.7346\tLR: 4.857545\n",
      "Training Epoch: 49 [29056/50000]\tLoss: 4.6813\tLR: 4.857801\n",
      "Training Epoch: 49 [29184/50000]\tLoss: 4.7504\tLR: 4.858056\n",
      "Training Epoch: 49 [29312/50000]\tLoss: 4.6279\tLR: 4.858312\n",
      "Training Epoch: 49 [29440/50000]\tLoss: 4.7206\tLR: 4.858568\n",
      "Training Epoch: 49 [29568/50000]\tLoss: 4.7004\tLR: 4.858824\n",
      "Training Epoch: 49 [29696/50000]\tLoss: 4.7065\tLR: 4.859079\n",
      "Training Epoch: 49 [29824/50000]\tLoss: 4.6947\tLR: 4.859335\n",
      "Training Epoch: 49 [29952/50000]\tLoss: 4.6886\tLR: 4.859591\n",
      "Training Epoch: 49 [30080/50000]\tLoss: 4.6891\tLR: 4.859847\n",
      "Training Epoch: 49 [30208/50000]\tLoss: 4.7032\tLR: 4.860102\n",
      "Training Epoch: 49 [30336/50000]\tLoss: 4.7330\tLR: 4.860358\n",
      "Training Epoch: 49 [30464/50000]\tLoss: 4.6965\tLR: 4.860614\n",
      "Training Epoch: 49 [30592/50000]\tLoss: 4.6769\tLR: 4.860870\n",
      "Training Epoch: 49 [30720/50000]\tLoss: 4.7057\tLR: 4.861125\n",
      "Training Epoch: 49 [30848/50000]\tLoss: 4.6987\tLR: 4.861381\n",
      "Training Epoch: 49 [30976/50000]\tLoss: 4.7218\tLR: 4.861637\n",
      "Training Epoch: 49 [31104/50000]\tLoss: 4.6916\tLR: 4.861893\n",
      "Training Epoch: 49 [31232/50000]\tLoss: 4.7157\tLR: 4.862148\n",
      "Training Epoch: 49 [31360/50000]\tLoss: 4.6901\tLR: 4.862404\n",
      "Training Epoch: 49 [31488/50000]\tLoss: 4.7341\tLR: 4.862660\n",
      "Training Epoch: 49 [31616/50000]\tLoss: 4.6658\tLR: 4.862916\n",
      "Training Epoch: 49 [31744/50000]\tLoss: 4.7398\tLR: 4.863171\n",
      "Training Epoch: 49 [31872/50000]\tLoss: 4.7049\tLR: 4.863427\n",
      "Training Epoch: 49 [32000/50000]\tLoss: 4.6180\tLR: 4.863683\n",
      "Training Epoch: 49 [32128/50000]\tLoss: 4.6608\tLR: 4.863939\n",
      "Training Epoch: 49 [32256/50000]\tLoss: 4.7002\tLR: 4.864194\n",
      "Training Epoch: 49 [32384/50000]\tLoss: 4.5724\tLR: 4.864450\n",
      "Training Epoch: 49 [32512/50000]\tLoss: 4.7852\tLR: 4.864706\n",
      "Training Epoch: 49 [32640/50000]\tLoss: 4.6890\tLR: 4.864962\n",
      "Training Epoch: 49 [32768/50000]\tLoss: 4.7346\tLR: 4.865217\n",
      "Training Epoch: 49 [32896/50000]\tLoss: 4.6645\tLR: 4.865473\n",
      "Training Epoch: 49 [33024/50000]\tLoss: 4.7258\tLR: 4.865729\n",
      "Training Epoch: 49 [33152/50000]\tLoss: 4.7164\tLR: 4.865985\n",
      "Training Epoch: 49 [33280/50000]\tLoss: 4.6388\tLR: 4.866240\n",
      "Training Epoch: 49 [33408/50000]\tLoss: 4.6735\tLR: 4.866496\n",
      "Training Epoch: 49 [33536/50000]\tLoss: 4.6778\tLR: 4.866752\n",
      "Training Epoch: 49 [33664/50000]\tLoss: 4.7125\tLR: 4.867008\n",
      "Training Epoch: 49 [33792/50000]\tLoss: 4.6814\tLR: 4.867263\n",
      "Training Epoch: 49 [33920/50000]\tLoss: 4.7206\tLR: 4.867519\n",
      "Training Epoch: 49 [34048/50000]\tLoss: 4.6640\tLR: 4.867775\n",
      "Training Epoch: 49 [34176/50000]\tLoss: 4.6566\tLR: 4.868031\n",
      "Training Epoch: 49 [34304/50000]\tLoss: 4.7226\tLR: 4.868286\n",
      "Training Epoch: 49 [34432/50000]\tLoss: 4.6802\tLR: 4.868542\n",
      "Training Epoch: 49 [34560/50000]\tLoss: 4.6756\tLR: 4.868798\n",
      "Training Epoch: 49 [34688/50000]\tLoss: 4.6862\tLR: 4.869054\n",
      "Training Epoch: 49 [34816/50000]\tLoss: 4.7183\tLR: 4.869309\n",
      "Training Epoch: 49 [34944/50000]\tLoss: 4.6796\tLR: 4.869565\n",
      "Training Epoch: 49 [35072/50000]\tLoss: 4.6709\tLR: 4.869821\n",
      "Training Epoch: 49 [35200/50000]\tLoss: 4.6662\tLR: 4.870077\n",
      "Training Epoch: 49 [35328/50000]\tLoss: 4.7499\tLR: 4.870332\n",
      "Training Epoch: 49 [35456/50000]\tLoss: 4.7378\tLR: 4.870588\n",
      "Training Epoch: 49 [35584/50000]\tLoss: 4.6670\tLR: 4.870844\n",
      "Training Epoch: 49 [35712/50000]\tLoss: 4.6797\tLR: 4.871100\n",
      "Training Epoch: 49 [35840/50000]\tLoss: 4.7261\tLR: 4.871355\n",
      "Training Epoch: 49 [35968/50000]\tLoss: 4.7614\tLR: 4.871611\n",
      "Training Epoch: 49 [36096/50000]\tLoss: 4.6992\tLR: 4.871867\n",
      "Training Epoch: 49 [36224/50000]\tLoss: 4.6971\tLR: 4.872123\n",
      "Training Epoch: 49 [36352/50000]\tLoss: 4.6660\tLR: 4.872379\n",
      "Training Epoch: 49 [36480/50000]\tLoss: 4.6882\tLR: 4.872634\n",
      "Training Epoch: 49 [36608/50000]\tLoss: 4.6529\tLR: 4.872890\n",
      "Training Epoch: 49 [36736/50000]\tLoss: 4.7193\tLR: 4.873146\n",
      "Training Epoch: 49 [36864/50000]\tLoss: 4.6785\tLR: 4.873402\n",
      "Training Epoch: 49 [36992/50000]\tLoss: 4.7033\tLR: 4.873657\n",
      "Training Epoch: 49 [37120/50000]\tLoss: 4.6821\tLR: 4.873913\n",
      "Training Epoch: 49 [37248/50000]\tLoss: 4.7122\tLR: 4.874169\n",
      "Training Epoch: 49 [37376/50000]\tLoss: 4.7116\tLR: 4.874425\n",
      "Training Epoch: 49 [37504/50000]\tLoss: 4.6550\tLR: 4.874680\n",
      "Training Epoch: 49 [37632/50000]\tLoss: 4.7041\tLR: 4.874936\n",
      "Training Epoch: 49 [37760/50000]\tLoss: 4.6722\tLR: 4.875192\n",
      "Training Epoch: 49 [37888/50000]\tLoss: 4.6989\tLR: 4.875448\n",
      "Training Epoch: 49 [38016/50000]\tLoss: 4.6767\tLR: 4.875703\n",
      "Training Epoch: 49 [38144/50000]\tLoss: 4.6941\tLR: 4.875959\n",
      "Training Epoch: 49 [38272/50000]\tLoss: 4.7046\tLR: 4.876215\n",
      "Training Epoch: 49 [38400/50000]\tLoss: 4.6557\tLR: 4.876471\n",
      "Training Epoch: 49 [38528/50000]\tLoss: 4.6693\tLR: 4.876726\n",
      "Training Epoch: 49 [38656/50000]\tLoss: 4.6927\tLR: 4.876982\n",
      "Training Epoch: 49 [38784/50000]\tLoss: 4.7097\tLR: 4.877238\n",
      "Training Epoch: 49 [38912/50000]\tLoss: 4.7163\tLR: 4.877494\n",
      "Training Epoch: 49 [39040/50000]\tLoss: 4.6213\tLR: 4.877749\n",
      "Training Epoch: 49 [39168/50000]\tLoss: 4.6368\tLR: 4.878005\n",
      "Training Epoch: 49 [39296/50000]\tLoss: 4.7623\tLR: 4.878261\n",
      "Training Epoch: 49 [39424/50000]\tLoss: 4.7039\tLR: 4.878517\n",
      "Training Epoch: 49 [39552/50000]\tLoss: 4.7628\tLR: 4.878772\n",
      "Training Epoch: 49 [39680/50000]\tLoss: 4.6994\tLR: 4.879028\n",
      "Training Epoch: 49 [39808/50000]\tLoss: 4.6486\tLR: 4.879284\n",
      "Training Epoch: 49 [39936/50000]\tLoss: 4.6999\tLR: 4.879540\n",
      "Training Epoch: 49 [40064/50000]\tLoss: 4.6710\tLR: 4.879795\n",
      "Training Epoch: 49 [40192/50000]\tLoss: 4.7163\tLR: 4.880051\n",
      "Training Epoch: 49 [40320/50000]\tLoss: 4.6332\tLR: 4.880307\n",
      "Training Epoch: 49 [40448/50000]\tLoss: 4.7558\tLR: 4.880563\n",
      "Training Epoch: 49 [40576/50000]\tLoss: 4.7593\tLR: 4.880818\n",
      "Training Epoch: 49 [40704/50000]\tLoss: 4.6725\tLR: 4.881074\n",
      "Training Epoch: 49 [40832/50000]\tLoss: 4.7030\tLR: 4.881330\n",
      "Training Epoch: 49 [40960/50000]\tLoss: 4.6988\tLR: 4.881586\n",
      "Training Epoch: 49 [41088/50000]\tLoss: 4.7057\tLR: 4.881841\n",
      "Training Epoch: 49 [41216/50000]\tLoss: 4.7091\tLR: 4.882097\n",
      "Training Epoch: 49 [41344/50000]\tLoss: 4.7116\tLR: 4.882353\n",
      "Training Epoch: 49 [41472/50000]\tLoss: 4.7370\tLR: 4.882609\n",
      "Training Epoch: 49 [41600/50000]\tLoss: 4.6809\tLR: 4.882864\n",
      "Training Epoch: 49 [41728/50000]\tLoss: 4.7371\tLR: 4.883120\n",
      "Training Epoch: 49 [41856/50000]\tLoss: 4.7485\tLR: 4.883376\n",
      "Training Epoch: 49 [41984/50000]\tLoss: 4.7045\tLR: 4.883632\n",
      "Training Epoch: 49 [42112/50000]\tLoss: 4.6593\tLR: 4.883887\n",
      "Training Epoch: 49 [42240/50000]\tLoss: 4.7722\tLR: 4.884143\n",
      "Training Epoch: 49 [42368/50000]\tLoss: 4.7520\tLR: 4.884399\n",
      "Training Epoch: 49 [42496/50000]\tLoss: 4.8012\tLR: 4.884655\n",
      "Training Epoch: 49 [42624/50000]\tLoss: 4.6755\tLR: 4.884910\n",
      "Training Epoch: 49 [42752/50000]\tLoss: 4.6831\tLR: 4.885166\n",
      "Training Epoch: 49 [42880/50000]\tLoss: 4.7419\tLR: 4.885422\n",
      "Training Epoch: 49 [43008/50000]\tLoss: 4.6920\tLR: 4.885678\n",
      "Training Epoch: 49 [43136/50000]\tLoss: 4.7191\tLR: 4.885934\n",
      "Training Epoch: 49 [43264/50000]\tLoss: 4.6888\tLR: 4.886189\n",
      "Training Epoch: 49 [43392/50000]\tLoss: 4.6962\tLR: 4.886445\n",
      "Training Epoch: 49 [43520/50000]\tLoss: 4.6528\tLR: 4.886701\n",
      "Training Epoch: 49 [43648/50000]\tLoss: 4.7247\tLR: 4.886957\n",
      "Training Epoch: 49 [43776/50000]\tLoss: 4.6967\tLR: 4.887212\n",
      "Training Epoch: 49 [43904/50000]\tLoss: 4.7047\tLR: 4.887468\n",
      "Training Epoch: 49 [44032/50000]\tLoss: 4.7340\tLR: 4.887724\n",
      "Training Epoch: 49 [44160/50000]\tLoss: 4.6517\tLR: 4.887980\n",
      "Training Epoch: 49 [44288/50000]\tLoss: 4.7054\tLR: 4.888235\n",
      "Training Epoch: 49 [44416/50000]\tLoss: 4.6350\tLR: 4.888491\n",
      "Training Epoch: 49 [44544/50000]\tLoss: 4.6859\tLR: 4.888747\n",
      "Training Epoch: 49 [44672/50000]\tLoss: 4.6482\tLR: 4.889003\n",
      "Training Epoch: 49 [44800/50000]\tLoss: 4.7070\tLR: 4.889258\n",
      "Training Epoch: 49 [44928/50000]\tLoss: 4.7169\tLR: 4.889514\n",
      "Training Epoch: 49 [45056/50000]\tLoss: 4.6679\tLR: 4.889770\n",
      "Training Epoch: 49 [45184/50000]\tLoss: 4.7128\tLR: 4.890026\n",
      "Training Epoch: 49 [45312/50000]\tLoss: 4.7343\tLR: 4.890281\n",
      "Training Epoch: 49 [45440/50000]\tLoss: 4.6867\tLR: 4.890537\n",
      "Training Epoch: 49 [45568/50000]\tLoss: 4.6157\tLR: 4.890793\n",
      "Training Epoch: 49 [45696/50000]\tLoss: 4.6810\tLR: 4.891049\n",
      "Training Epoch: 49 [45824/50000]\tLoss: 4.6904\tLR: 4.891304\n",
      "Training Epoch: 49 [45952/50000]\tLoss: 4.7669\tLR: 4.891560\n",
      "Training Epoch: 49 [46080/50000]\tLoss: 4.6830\tLR: 4.891816\n",
      "Training Epoch: 49 [46208/50000]\tLoss: 4.7104\tLR: 4.892072\n",
      "Training Epoch: 49 [46336/50000]\tLoss: 4.6873\tLR: 4.892327\n",
      "Training Epoch: 49 [46464/50000]\tLoss: 4.6745\tLR: 4.892583\n",
      "Training Epoch: 49 [46592/50000]\tLoss: 4.6634\tLR: 4.892839\n",
      "Training Epoch: 49 [46720/50000]\tLoss: 4.6807\tLR: 4.893095\n",
      "Training Epoch: 49 [46848/50000]\tLoss: 4.7187\tLR: 4.893350\n",
      "Training Epoch: 49 [46976/50000]\tLoss: 4.7575\tLR: 4.893606\n",
      "Training Epoch: 49 [47104/50000]\tLoss: 4.6615\tLR: 4.893862\n",
      "Training Epoch: 49 [47232/50000]\tLoss: 4.7355\tLR: 4.894118\n",
      "Training Epoch: 49 [47360/50000]\tLoss: 4.6651\tLR: 4.894373\n",
      "Training Epoch: 49 [47488/50000]\tLoss: 4.6608\tLR: 4.894629\n",
      "Training Epoch: 49 [47616/50000]\tLoss: 4.6965\tLR: 4.894885\n",
      "Training Epoch: 49 [47744/50000]\tLoss: 4.6823\tLR: 4.895141\n",
      "Training Epoch: 49 [47872/50000]\tLoss: 4.7590\tLR: 4.895396\n",
      "Training Epoch: 49 [48000/50000]\tLoss: 4.6752\tLR: 4.895652\n",
      "Training Epoch: 49 [48128/50000]\tLoss: 4.7054\tLR: 4.895908\n",
      "Training Epoch: 49 [48256/50000]\tLoss: 4.7503\tLR: 4.896164\n",
      "Training Epoch: 49 [48384/50000]\tLoss: 4.7107\tLR: 4.896419\n",
      "Training Epoch: 49 [48512/50000]\tLoss: 4.6877\tLR: 4.896675\n",
      "Training Epoch: 49 [48640/50000]\tLoss: 4.6789\tLR: 4.896931\n",
      "Training Epoch: 49 [48768/50000]\tLoss: 4.6464\tLR: 4.897187\n",
      "Training Epoch: 49 [48896/50000]\tLoss: 4.7050\tLR: 4.897442\n",
      "Training Epoch: 49 [49024/50000]\tLoss: 4.6785\tLR: 4.897698\n",
      "Training Epoch: 49 [49152/50000]\tLoss: 4.7034\tLR: 4.897954\n",
      "Training Epoch: 49 [49280/50000]\tLoss: 4.7136\tLR: 4.898210\n",
      "Training Epoch: 49 [49408/50000]\tLoss: 4.8198\tLR: 4.898465\n",
      "Training Epoch: 49 [49536/50000]\tLoss: 4.6854\tLR: 4.898721\n",
      "Training Epoch: 49 [49664/50000]\tLoss: 4.7784\tLR: 4.898977\n",
      "Training Epoch: 49 [49792/50000]\tLoss: 4.7656\tLR: 4.899233\n",
      "Training Epoch: 49 [49920/50000]\tLoss: 4.7135\tLR: 4.899488\n",
      "Training Epoch: 49 [50000/50000]\tLoss: 4.6402\tLR: 4.899744\n",
      "epoch 49 training time consumed: 489.06s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   68695 GB |   68695 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   68484 GB |   68484 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     210 GB |     210 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   68695 GB |   68695 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   68484 GB |   68484 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     210 GB |     210 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   67728 GB |   67728 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   67517 GB |   67517 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     210 GB |     210 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    7284 K  |    7284 K  |\n",
      "|       from large pool |      24    |      65    |    3105 K  |    3105 K  |\n",
      "|       from small pool |     231    |     274    |    4179 K  |    4178 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    7284 K  |    7284 K  |\n",
      "|       from large pool |      24    |      65    |    3105 K  |    3105 K  |\n",
      "|       from small pool |     231    |     274    |    4179 K  |    4178 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      47    |    4221 K  |    4221 K  |\n",
      "|       from large pool |      10    |      23    |    1492 K  |    1492 K  |\n",
      "|       from small pool |      26    |      35    |    2729 K  |    2729 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 49, Average loss: 0.0371, Accuracy: 0.0100, Time consumed:31.06s\n",
      "\n",
      "Training Epoch: 50 [128/50000]\tLoss: 4.6669\tLR: 0.100000\n",
      "Training Epoch: 50 [256/50000]\tLoss: 4.7075\tLR: 4.900256\n",
      "Training Epoch: 50 [384/50000]\tLoss: 4.6695\tLR: 4.900512\n",
      "Training Epoch: 50 [512/50000]\tLoss: 4.6566\tLR: 4.900767\n",
      "Training Epoch: 50 [640/50000]\tLoss: 4.6867\tLR: 4.901023\n",
      "Training Epoch: 50 [768/50000]\tLoss: 4.7421\tLR: 4.901279\n",
      "Training Epoch: 50 [896/50000]\tLoss: 4.7080\tLR: 4.901535\n",
      "Training Epoch: 50 [1024/50000]\tLoss: 4.6165\tLR: 4.901790\n",
      "Training Epoch: 50 [1152/50000]\tLoss: 4.8152\tLR: 4.902046\n",
      "Training Epoch: 50 [1280/50000]\tLoss: 4.6252\tLR: 4.902302\n",
      "Training Epoch: 50 [1408/50000]\tLoss: 4.7189\tLR: 4.902558\n",
      "Training Epoch: 50 [1536/50000]\tLoss: 4.7122\tLR: 4.902813\n",
      "Training Epoch: 50 [1664/50000]\tLoss: 4.7719\tLR: 4.903069\n",
      "Training Epoch: 50 [1792/50000]\tLoss: 4.6889\tLR: 4.903325\n",
      "Training Epoch: 50 [1920/50000]\tLoss: 4.7401\tLR: 4.903581\n",
      "Training Epoch: 50 [2048/50000]\tLoss: 4.6576\tLR: 4.903836\n",
      "Training Epoch: 50 [2176/50000]\tLoss: 4.7169\tLR: 4.904092\n",
      "Training Epoch: 50 [2304/50000]\tLoss: 4.6850\tLR: 4.904348\n",
      "Training Epoch: 50 [2432/50000]\tLoss: 4.7327\tLR: 4.904604\n",
      "Training Epoch: 50 [2560/50000]\tLoss: 4.6340\tLR: 4.904859\n",
      "Training Epoch: 50 [2688/50000]\tLoss: 4.7255\tLR: 4.905115\n",
      "Training Epoch: 50 [2816/50000]\tLoss: 4.7360\tLR: 4.905371\n",
      "Training Epoch: 50 [2944/50000]\tLoss: 4.6467\tLR: 4.905627\n",
      "Training Epoch: 50 [3072/50000]\tLoss: 4.6635\tLR: 4.905882\n",
      "Training Epoch: 50 [3200/50000]\tLoss: 4.7618\tLR: 4.906138\n",
      "Training Epoch: 50 [3328/50000]\tLoss: 4.6958\tLR: 4.906394\n",
      "Training Epoch: 50 [3456/50000]\tLoss: 4.6860\tLR: 4.906650\n",
      "Training Epoch: 50 [3584/50000]\tLoss: 4.7819\tLR: 4.906905\n",
      "Training Epoch: 50 [3712/50000]\tLoss: 4.7281\tLR: 4.907161\n",
      "Training Epoch: 50 [3840/50000]\tLoss: 4.6630\tLR: 4.907417\n",
      "Training Epoch: 50 [3968/50000]\tLoss: 4.6460\tLR: 4.907673\n",
      "Training Epoch: 50 [4096/50000]\tLoss: 4.7123\tLR: 4.907928\n",
      "Training Epoch: 50 [4224/50000]\tLoss: 4.7320\tLR: 4.908184\n",
      "Training Epoch: 50 [4352/50000]\tLoss: 4.7572\tLR: 4.908440\n",
      "Training Epoch: 50 [4480/50000]\tLoss: 4.6330\tLR: 4.908696\n",
      "Training Epoch: 50 [4608/50000]\tLoss: 4.6072\tLR: 4.908951\n",
      "Training Epoch: 50 [4736/50000]\tLoss: 4.6508\tLR: 4.909207\n",
      "Training Epoch: 50 [4864/50000]\tLoss: 4.7140\tLR: 4.909463\n",
      "Training Epoch: 50 [4992/50000]\tLoss: 4.7039\tLR: 4.909719\n",
      "Training Epoch: 50 [5120/50000]\tLoss: 4.6521\tLR: 4.909974\n",
      "Training Epoch: 50 [5248/50000]\tLoss: 4.6814\tLR: 4.910230\n",
      "Training Epoch: 50 [5376/50000]\tLoss: 4.7515\tLR: 4.910486\n",
      "Training Epoch: 50 [5504/50000]\tLoss: 4.7271\tLR: 4.910742\n",
      "Training Epoch: 50 [5632/50000]\tLoss: 4.6686\tLR: 4.910997\n",
      "Training Epoch: 50 [5760/50000]\tLoss: 4.7269\tLR: 4.911253\n",
      "Training Epoch: 50 [5888/50000]\tLoss: 4.7124\tLR: 4.911509\n",
      "Training Epoch: 50 [6016/50000]\tLoss: 4.6547\tLR: 4.911765\n",
      "Training Epoch: 50 [6144/50000]\tLoss: 4.6986\tLR: 4.912020\n",
      "Training Epoch: 50 [6272/50000]\tLoss: 4.6326\tLR: 4.912276\n",
      "Training Epoch: 50 [6400/50000]\tLoss: 4.6278\tLR: 4.912532\n",
      "Training Epoch: 50 [6528/50000]\tLoss: 4.7233\tLR: 4.912788\n",
      "Training Epoch: 50 [6656/50000]\tLoss: 4.6365\tLR: 4.913043\n",
      "Training Epoch: 50 [6784/50000]\tLoss: 4.6596\tLR: 4.913299\n",
      "Training Epoch: 50 [6912/50000]\tLoss: 4.7634\tLR: 4.913555\n",
      "Training Epoch: 50 [7040/50000]\tLoss: 4.6878\tLR: 4.913811\n",
      "Training Epoch: 50 [7168/50000]\tLoss: 4.6407\tLR: 4.914066\n",
      "Training Epoch: 50 [7296/50000]\tLoss: 4.6432\tLR: 4.914322\n",
      "Training Epoch: 50 [7424/50000]\tLoss: 4.7575\tLR: 4.914578\n",
      "Training Epoch: 50 [7552/50000]\tLoss: 4.6692\tLR: 4.914834\n",
      "Training Epoch: 50 [7680/50000]\tLoss: 4.6476\tLR: 4.915090\n",
      "Training Epoch: 50 [7808/50000]\tLoss: 4.6600\tLR: 4.915345\n",
      "Training Epoch: 50 [7936/50000]\tLoss: 4.6832\tLR: 4.915601\n",
      "Training Epoch: 50 [8064/50000]\tLoss: 4.7055\tLR: 4.915857\n",
      "Training Epoch: 50 [8192/50000]\tLoss: 4.7103\tLR: 4.916113\n",
      "Training Epoch: 50 [8320/50000]\tLoss: 4.6744\tLR: 4.916368\n",
      "Training Epoch: 50 [8448/50000]\tLoss: 4.6379\tLR: 4.916624\n",
      "Training Epoch: 50 [8576/50000]\tLoss: 4.6701\tLR: 4.916880\n",
      "Training Epoch: 50 [8704/50000]\tLoss: 4.7034\tLR: 4.917136\n",
      "Training Epoch: 50 [8832/50000]\tLoss: 4.6939\tLR: 4.917391\n",
      "Training Epoch: 50 [8960/50000]\tLoss: 4.6843\tLR: 4.917647\n",
      "Training Epoch: 50 [9088/50000]\tLoss: 4.7018\tLR: 4.917903\n",
      "Training Epoch: 50 [9216/50000]\tLoss: 4.7384\tLR: 4.918159\n",
      "Training Epoch: 50 [9344/50000]\tLoss: 4.6859\tLR: 4.918414\n",
      "Training Epoch: 50 [9472/50000]\tLoss: 4.7237\tLR: 4.918670\n",
      "Training Epoch: 50 [9600/50000]\tLoss: 4.6785\tLR: 4.918926\n",
      "Training Epoch: 50 [9728/50000]\tLoss: 4.6784\tLR: 4.919182\n",
      "Training Epoch: 50 [9856/50000]\tLoss: 4.6643\tLR: 4.919437\n",
      "Training Epoch: 50 [9984/50000]\tLoss: 4.6850\tLR: 4.919693\n",
      "Training Epoch: 50 [10112/50000]\tLoss: 4.6383\tLR: 4.919949\n",
      "Training Epoch: 50 [10240/50000]\tLoss: 4.6409\tLR: 4.920205\n",
      "Training Epoch: 50 [10368/50000]\tLoss: 4.7006\tLR: 4.920460\n",
      "Training Epoch: 50 [10496/50000]\tLoss: 4.6659\tLR: 4.920716\n",
      "Training Epoch: 50 [10624/50000]\tLoss: 4.7519\tLR: 4.920972\n",
      "Training Epoch: 50 [10752/50000]\tLoss: 4.7058\tLR: 4.921228\n",
      "Training Epoch: 50 [10880/50000]\tLoss: 4.6377\tLR: 4.921483\n",
      "Training Epoch: 50 [11008/50000]\tLoss: 4.7120\tLR: 4.921739\n",
      "Training Epoch: 50 [11136/50000]\tLoss: 4.7170\tLR: 4.921995\n",
      "Training Epoch: 50 [11264/50000]\tLoss: 4.6799\tLR: 4.922251\n",
      "Training Epoch: 50 [11392/50000]\tLoss: 4.6956\tLR: 4.922506\n",
      "Training Epoch: 50 [11520/50000]\tLoss: 4.7136\tLR: 4.922762\n",
      "Training Epoch: 50 [11648/50000]\tLoss: 4.6710\tLR: 4.923018\n",
      "Training Epoch: 50 [11776/50000]\tLoss: 4.6385\tLR: 4.923274\n",
      "Training Epoch: 50 [11904/50000]\tLoss: 4.6258\tLR: 4.923529\n",
      "Training Epoch: 50 [12032/50000]\tLoss: 4.7299\tLR: 4.923785\n",
      "Training Epoch: 50 [12160/50000]\tLoss: 4.7076\tLR: 4.924041\n",
      "Training Epoch: 50 [12288/50000]\tLoss: 4.6152\tLR: 4.924297\n",
      "Training Epoch: 50 [12416/50000]\tLoss: 4.6985\tLR: 4.924552\n",
      "Training Epoch: 50 [12544/50000]\tLoss: 4.7079\tLR: 4.924808\n",
      "Training Epoch: 50 [12672/50000]\tLoss: 4.7676\tLR: 4.925064\n",
      "Training Epoch: 50 [12800/50000]\tLoss: 4.7089\tLR: 4.925320\n",
      "Training Epoch: 50 [12928/50000]\tLoss: 4.7887\tLR: 4.925575\n",
      "Training Epoch: 50 [13056/50000]\tLoss: 4.7355\tLR: 4.925831\n",
      "Training Epoch: 50 [13184/50000]\tLoss: 4.6768\tLR: 4.926087\n",
      "Training Epoch: 50 [13312/50000]\tLoss: 4.6497\tLR: 4.926343\n",
      "Training Epoch: 50 [13440/50000]\tLoss: 4.6741\tLR: 4.926598\n",
      "Training Epoch: 50 [13568/50000]\tLoss: 4.6744\tLR: 4.926854\n",
      "Training Epoch: 50 [13696/50000]\tLoss: 4.7305\tLR: 4.927110\n",
      "Training Epoch: 50 [13824/50000]\tLoss: 4.7659\tLR: 4.927366\n",
      "Training Epoch: 50 [13952/50000]\tLoss: 4.7254\tLR: 4.927621\n",
      "Training Epoch: 50 [14080/50000]\tLoss: 4.7256\tLR: 4.927877\n",
      "Training Epoch: 50 [14208/50000]\tLoss: 4.6466\tLR: 4.928133\n",
      "Training Epoch: 50 [14336/50000]\tLoss: 4.6809\tLR: 4.928389\n",
      "Training Epoch: 50 [14464/50000]\tLoss: 4.7463\tLR: 4.928645\n",
      "Training Epoch: 50 [14592/50000]\tLoss: 4.6466\tLR: 4.928900\n",
      "Training Epoch: 50 [14720/50000]\tLoss: 4.7155\tLR: 4.929156\n",
      "Training Epoch: 50 [14848/50000]\tLoss: 4.6188\tLR: 4.929412\n",
      "Training Epoch: 50 [14976/50000]\tLoss: 4.6407\tLR: 4.929668\n",
      "Training Epoch: 50 [15104/50000]\tLoss: 4.7028\tLR: 4.929923\n",
      "Training Epoch: 50 [15232/50000]\tLoss: 4.6770\tLR: 4.930179\n",
      "Training Epoch: 50 [15360/50000]\tLoss: 4.7106\tLR: 4.930435\n",
      "Training Epoch: 50 [15488/50000]\tLoss: 4.6741\tLR: 4.930691\n",
      "Training Epoch: 50 [15616/50000]\tLoss: 4.6959\tLR: 4.930946\n",
      "Training Epoch: 50 [15744/50000]\tLoss: 4.6576\tLR: 4.931202\n",
      "Training Epoch: 50 [15872/50000]\tLoss: 4.7295\tLR: 4.931458\n",
      "Training Epoch: 50 [16000/50000]\tLoss: 4.7540\tLR: 4.931714\n",
      "Training Epoch: 50 [16128/50000]\tLoss: 4.7779\tLR: 4.931969\n",
      "Training Epoch: 50 [16256/50000]\tLoss: 4.7013\tLR: 4.932225\n",
      "Training Epoch: 50 [16384/50000]\tLoss: 4.6721\tLR: 4.932481\n",
      "Training Epoch: 50 [16512/50000]\tLoss: 4.6815\tLR: 4.932737\n",
      "Training Epoch: 50 [16640/50000]\tLoss: 4.6656\tLR: 4.932992\n",
      "Training Epoch: 50 [16768/50000]\tLoss: 4.7405\tLR: 4.933248\n",
      "Training Epoch: 50 [16896/50000]\tLoss: 4.6834\tLR: 4.933504\n",
      "Training Epoch: 50 [17024/50000]\tLoss: 4.5963\tLR: 4.933760\n",
      "Training Epoch: 50 [17152/50000]\tLoss: 4.7587\tLR: 4.934015\n",
      "Training Epoch: 50 [17280/50000]\tLoss: 4.6757\tLR: 4.934271\n",
      "Training Epoch: 50 [17408/50000]\tLoss: 4.6775\tLR: 4.934527\n",
      "Training Epoch: 50 [17536/50000]\tLoss: 4.6627\tLR: 4.934783\n",
      "Training Epoch: 50 [17664/50000]\tLoss: 4.7736\tLR: 4.935038\n",
      "Training Epoch: 50 [17792/50000]\tLoss: 4.6642\tLR: 4.935294\n",
      "Training Epoch: 50 [17920/50000]\tLoss: 4.7176\tLR: 4.935550\n",
      "Training Epoch: 50 [18048/50000]\tLoss: 4.7535\tLR: 4.935806\n",
      "Training Epoch: 50 [18176/50000]\tLoss: 4.7354\tLR: 4.936061\n",
      "Training Epoch: 50 [18304/50000]\tLoss: 4.6085\tLR: 4.936317\n",
      "Training Epoch: 50 [18432/50000]\tLoss: 4.7101\tLR: 4.936573\n",
      "Training Epoch: 50 [18560/50000]\tLoss: 4.6730\tLR: 4.936829\n",
      "Training Epoch: 50 [18688/50000]\tLoss: 4.6712\tLR: 4.937084\n",
      "Training Epoch: 50 [18816/50000]\tLoss: 4.7410\tLR: 4.937340\n",
      "Training Epoch: 50 [18944/50000]\tLoss: 4.6172\tLR: 4.937596\n",
      "Training Epoch: 50 [19072/50000]\tLoss: 4.7511\tLR: 4.937852\n",
      "Training Epoch: 50 [19200/50000]\tLoss: 4.6944\tLR: 4.938107\n",
      "Training Epoch: 50 [19328/50000]\tLoss: 4.6025\tLR: 4.938363\n",
      "Training Epoch: 50 [19456/50000]\tLoss: 4.6825\tLR: 4.938619\n",
      "Training Epoch: 50 [19584/50000]\tLoss: 4.7530\tLR: 4.938875\n",
      "Training Epoch: 50 [19712/50000]\tLoss: 4.7939\tLR: 4.939130\n",
      "Training Epoch: 50 [19840/50000]\tLoss: 4.7783\tLR: 4.939386\n",
      "Training Epoch: 50 [19968/50000]\tLoss: 4.6524\tLR: 4.939642\n",
      "Training Epoch: 50 [20096/50000]\tLoss: 4.7487\tLR: 4.939898\n",
      "Training Epoch: 50 [20224/50000]\tLoss: 4.7190\tLR: 4.940153\n",
      "Training Epoch: 50 [20352/50000]\tLoss: 4.7414\tLR: 4.940409\n",
      "Training Epoch: 50 [20480/50000]\tLoss: 4.6840\tLR: 4.940665\n",
      "Training Epoch: 50 [20608/50000]\tLoss: 4.8263\tLR: 4.940921\n",
      "Training Epoch: 50 [20736/50000]\tLoss: 4.7293\tLR: 4.941176\n",
      "Training Epoch: 50 [20864/50000]\tLoss: 4.7379\tLR: 4.941432\n",
      "Training Epoch: 50 [20992/50000]\tLoss: 4.7022\tLR: 4.941688\n",
      "Training Epoch: 50 [21120/50000]\tLoss: 4.7769\tLR: 4.941944\n",
      "Training Epoch: 50 [21248/50000]\tLoss: 4.7146\tLR: 4.942199\n",
      "Training Epoch: 50 [21376/50000]\tLoss: 4.7358\tLR: 4.942455\n",
      "Training Epoch: 50 [21504/50000]\tLoss: 4.7188\tLR: 4.942711\n",
      "Training Epoch: 50 [21632/50000]\tLoss: 4.7858\tLR: 4.942967\n",
      "Training Epoch: 50 [21760/50000]\tLoss: 4.7055\tLR: 4.943223\n",
      "Training Epoch: 50 [21888/50000]\tLoss: 4.7694\tLR: 4.943478\n",
      "Training Epoch: 50 [22016/50000]\tLoss: 4.7157\tLR: 4.943734\n",
      "Training Epoch: 50 [22144/50000]\tLoss: 4.6433\tLR: 4.943990\n",
      "Training Epoch: 50 [22272/50000]\tLoss: 4.7456\tLR: 4.944246\n",
      "Training Epoch: 50 [22400/50000]\tLoss: 4.6500\tLR: 4.944501\n",
      "Training Epoch: 50 [22528/50000]\tLoss: 4.7239\tLR: 4.944757\n",
      "Training Epoch: 50 [22656/50000]\tLoss: 4.7014\tLR: 4.945013\n",
      "Training Epoch: 50 [22784/50000]\tLoss: 4.7108\tLR: 4.945269\n",
      "Training Epoch: 50 [22912/50000]\tLoss: 4.7608\tLR: 4.945524\n",
      "Training Epoch: 50 [23040/50000]\tLoss: 4.7694\tLR: 4.945780\n",
      "Training Epoch: 50 [23168/50000]\tLoss: 4.6643\tLR: 4.946036\n",
      "Training Epoch: 50 [23296/50000]\tLoss: 4.6883\tLR: 4.946292\n",
      "Training Epoch: 50 [23424/50000]\tLoss: 4.6819\tLR: 4.946547\n",
      "Training Epoch: 50 [23552/50000]\tLoss: 4.7189\tLR: 4.946803\n",
      "Training Epoch: 50 [23680/50000]\tLoss: 4.7208\tLR: 4.947059\n",
      "Training Epoch: 50 [23808/50000]\tLoss: 4.7017\tLR: 4.947315\n",
      "Training Epoch: 50 [23936/50000]\tLoss: 4.6668\tLR: 4.947570\n",
      "Training Epoch: 50 [24064/50000]\tLoss: 4.6738\tLR: 4.947826\n",
      "Training Epoch: 50 [24192/50000]\tLoss: 4.7331\tLR: 4.948082\n",
      "Training Epoch: 50 [24320/50000]\tLoss: 4.7017\tLR: 4.948338\n",
      "Training Epoch: 50 [24448/50000]\tLoss: 4.7757\tLR: 4.948593\n",
      "Training Epoch: 50 [24576/50000]\tLoss: 4.6631\tLR: 4.948849\n",
      "Training Epoch: 50 [24704/50000]\tLoss: 4.6723\tLR: 4.949105\n",
      "Training Epoch: 50 [24832/50000]\tLoss: 4.6965\tLR: 4.949361\n",
      "Training Epoch: 50 [24960/50000]\tLoss: 4.7085\tLR: 4.949616\n",
      "Training Epoch: 50 [25088/50000]\tLoss: 4.6549\tLR: 4.949872\n",
      "Training Epoch: 50 [25216/50000]\tLoss: 4.7449\tLR: 4.950128\n",
      "Training Epoch: 50 [25344/50000]\tLoss: 4.7046\tLR: 4.950384\n",
      "Training Epoch: 50 [25472/50000]\tLoss: 4.6525\tLR: 4.950639\n",
      "Training Epoch: 50 [25600/50000]\tLoss: 4.6300\tLR: 4.950895\n",
      "Training Epoch: 50 [25728/50000]\tLoss: 4.7387\tLR: 4.951151\n",
      "Training Epoch: 50 [25856/50000]\tLoss: 4.6677\tLR: 4.951407\n",
      "Training Epoch: 50 [25984/50000]\tLoss: 4.7214\tLR: 4.951662\n",
      "Training Epoch: 50 [26112/50000]\tLoss: 4.7635\tLR: 4.951918\n",
      "Training Epoch: 50 [26240/50000]\tLoss: 4.7453\tLR: 4.952174\n",
      "Training Epoch: 50 [26368/50000]\tLoss: 4.6672\tLR: 4.952430\n",
      "Training Epoch: 50 [26496/50000]\tLoss: 4.7445\tLR: 4.952685\n",
      "Training Epoch: 50 [26624/50000]\tLoss: 4.6685\tLR: 4.952941\n",
      "Training Epoch: 50 [26752/50000]\tLoss: 4.6654\tLR: 4.953197\n",
      "Training Epoch: 50 [26880/50000]\tLoss: 4.6365\tLR: 4.953453\n",
      "Training Epoch: 50 [27008/50000]\tLoss: 4.6735\tLR: 4.953708\n",
      "Training Epoch: 50 [27136/50000]\tLoss: 4.7509\tLR: 4.953964\n",
      "Training Epoch: 50 [27264/50000]\tLoss: 4.6590\tLR: 4.954220\n",
      "Training Epoch: 50 [27392/50000]\tLoss: 4.7172\tLR: 4.954476\n",
      "Training Epoch: 50 [27520/50000]\tLoss: 4.6872\tLR: 4.954731\n",
      "Training Epoch: 50 [27648/50000]\tLoss: 4.7314\tLR: 4.954987\n",
      "Training Epoch: 50 [27776/50000]\tLoss: 4.6950\tLR: 4.955243\n",
      "Training Epoch: 50 [27904/50000]\tLoss: 4.6217\tLR: 4.955499\n",
      "Training Epoch: 50 [28032/50000]\tLoss: 4.6704\tLR: 4.955754\n",
      "Training Epoch: 50 [28160/50000]\tLoss: 4.7258\tLR: 4.956010\n",
      "Training Epoch: 50 [28288/50000]\tLoss: 4.7318\tLR: 4.956266\n",
      "Training Epoch: 50 [28416/50000]\tLoss: 4.6781\tLR: 4.956522\n",
      "Training Epoch: 50 [28544/50000]\tLoss: 4.6913\tLR: 4.956777\n",
      "Training Epoch: 50 [28672/50000]\tLoss: 4.7231\tLR: 4.957033\n",
      "Training Epoch: 50 [28800/50000]\tLoss: 4.6958\tLR: 4.957289\n",
      "Training Epoch: 50 [28928/50000]\tLoss: 4.7241\tLR: 4.957545\n",
      "Training Epoch: 50 [29056/50000]\tLoss: 4.7636\tLR: 4.957801\n",
      "Training Epoch: 50 [29184/50000]\tLoss: 4.7099\tLR: 4.958056\n",
      "Training Epoch: 50 [29312/50000]\tLoss: 4.7180\tLR: 4.958312\n",
      "Training Epoch: 50 [29440/50000]\tLoss: 4.6392\tLR: 4.958568\n",
      "Training Epoch: 50 [29568/50000]\tLoss: 4.6579\tLR: 4.958824\n",
      "Training Epoch: 50 [29696/50000]\tLoss: 4.7306\tLR: 4.959079\n",
      "Training Epoch: 50 [29824/50000]\tLoss: 4.7131\tLR: 4.959335\n",
      "Training Epoch: 50 [29952/50000]\tLoss: 4.6847\tLR: 4.959591\n",
      "Training Epoch: 50 [30080/50000]\tLoss: 4.6965\tLR: 4.959847\n",
      "Training Epoch: 50 [30208/50000]\tLoss: 4.7276\tLR: 4.960102\n",
      "Training Epoch: 50 [30336/50000]\tLoss: 4.7595\tLR: 4.960358\n",
      "Training Epoch: 50 [30464/50000]\tLoss: 4.7009\tLR: 4.960614\n",
      "Training Epoch: 50 [30592/50000]\tLoss: 4.6979\tLR: 4.960870\n",
      "Training Epoch: 50 [30720/50000]\tLoss: 4.7554\tLR: 4.961125\n",
      "Training Epoch: 50 [30848/50000]\tLoss: 4.7045\tLR: 4.961381\n",
      "Training Epoch: 50 [30976/50000]\tLoss: 4.7197\tLR: 4.961637\n",
      "Training Epoch: 50 [31104/50000]\tLoss: 4.6781\tLR: 4.961893\n",
      "Training Epoch: 50 [31232/50000]\tLoss: 4.6598\tLR: 4.962148\n",
      "Training Epoch: 50 [31360/50000]\tLoss: 4.7496\tLR: 4.962404\n",
      "Training Epoch: 50 [31488/50000]\tLoss: 4.7036\tLR: 4.962660\n",
      "Training Epoch: 50 [31616/50000]\tLoss: 4.7300\tLR: 4.962916\n",
      "Training Epoch: 50 [31744/50000]\tLoss: 4.7702\tLR: 4.963171\n",
      "Training Epoch: 50 [31872/50000]\tLoss: 4.6819\tLR: 4.963427\n",
      "Training Epoch: 50 [32000/50000]\tLoss: 4.6431\tLR: 4.963683\n",
      "Training Epoch: 50 [32128/50000]\tLoss: 4.7219\tLR: 4.963939\n",
      "Training Epoch: 50 [32256/50000]\tLoss: 4.6317\tLR: 4.964194\n",
      "Training Epoch: 50 [32384/50000]\tLoss: 4.6701\tLR: 4.964450\n",
      "Training Epoch: 50 [32512/50000]\tLoss: 4.7116\tLR: 4.964706\n",
      "Training Epoch: 50 [32640/50000]\tLoss: 4.7382\tLR: 4.964962\n",
      "Training Epoch: 50 [32768/50000]\tLoss: 4.7306\tLR: 4.965217\n",
      "Training Epoch: 50 [32896/50000]\tLoss: 4.6309\tLR: 4.965473\n",
      "Training Epoch: 50 [33024/50000]\tLoss: 4.7499\tLR: 4.965729\n",
      "Training Epoch: 50 [33152/50000]\tLoss: 4.6647\tLR: 4.965985\n",
      "Training Epoch: 50 [33280/50000]\tLoss: 4.7599\tLR: 4.966240\n",
      "Training Epoch: 50 [33408/50000]\tLoss: 4.7105\tLR: 4.966496\n",
      "Training Epoch: 50 [33536/50000]\tLoss: 4.7297\tLR: 4.966752\n",
      "Training Epoch: 50 [33664/50000]\tLoss: 4.7296\tLR: 4.967008\n",
      "Training Epoch: 50 [33792/50000]\tLoss: 4.6478\tLR: 4.967263\n",
      "Training Epoch: 50 [33920/50000]\tLoss: 4.7342\tLR: 4.967519\n",
      "Training Epoch: 50 [34048/50000]\tLoss: 4.7226\tLR: 4.967775\n",
      "Training Epoch: 50 [34176/50000]\tLoss: 4.6709\tLR: 4.968031\n",
      "Training Epoch: 50 [34304/50000]\tLoss: 4.7377\tLR: 4.968286\n",
      "Training Epoch: 50 [34432/50000]\tLoss: 4.6989\tLR: 4.968542\n",
      "Training Epoch: 50 [34560/50000]\tLoss: 4.7049\tLR: 4.968798\n",
      "Training Epoch: 50 [34688/50000]\tLoss: 4.6860\tLR: 4.969054\n",
      "Training Epoch: 50 [34816/50000]\tLoss: 4.6828\tLR: 4.969309\n",
      "Training Epoch: 50 [34944/50000]\tLoss: 4.6634\tLR: 4.969565\n",
      "Training Epoch: 50 [35072/50000]\tLoss: 4.7162\tLR: 4.969821\n",
      "Training Epoch: 50 [35200/50000]\tLoss: 4.6315\tLR: 4.970077\n",
      "Training Epoch: 50 [35328/50000]\tLoss: 4.7304\tLR: 4.970332\n",
      "Training Epoch: 50 [35456/50000]\tLoss: 4.6884\tLR: 4.970588\n",
      "Training Epoch: 50 [35584/50000]\tLoss: 4.7530\tLR: 4.970844\n",
      "Training Epoch: 50 [35712/50000]\tLoss: 4.6574\tLR: 4.971100\n",
      "Training Epoch: 50 [35840/50000]\tLoss: 4.6452\tLR: 4.971355\n",
      "Training Epoch: 50 [35968/50000]\tLoss: 4.7214\tLR: 4.971611\n",
      "Training Epoch: 50 [36096/50000]\tLoss: 4.6575\tLR: 4.971867\n",
      "Training Epoch: 50 [36224/50000]\tLoss: 4.6225\tLR: 4.972123\n",
      "Training Epoch: 50 [36352/50000]\tLoss: 4.6393\tLR: 4.972379\n",
      "Training Epoch: 50 [36480/50000]\tLoss: 4.7636\tLR: 4.972634\n",
      "Training Epoch: 50 [36608/50000]\tLoss: 4.6793\tLR: 4.972890\n",
      "Training Epoch: 50 [36736/50000]\tLoss: 4.7361\tLR: 4.973146\n",
      "Training Epoch: 50 [36864/50000]\tLoss: 4.7735\tLR: 4.973402\n",
      "Training Epoch: 50 [36992/50000]\tLoss: 4.6317\tLR: 4.973657\n",
      "Training Epoch: 50 [37120/50000]\tLoss: 4.6871\tLR: 4.973913\n",
      "Training Epoch: 50 [37248/50000]\tLoss: 4.7093\tLR: 4.974169\n",
      "Training Epoch: 50 [37376/50000]\tLoss: 4.6811\tLR: 4.974425\n",
      "Training Epoch: 50 [37504/50000]\tLoss: 4.6865\tLR: 4.974680\n",
      "Training Epoch: 50 [37632/50000]\tLoss: 4.6841\tLR: 4.974936\n",
      "Training Epoch: 50 [37760/50000]\tLoss: 4.7416\tLR: 4.975192\n",
      "Training Epoch: 50 [37888/50000]\tLoss: 4.6543\tLR: 4.975448\n",
      "Training Epoch: 50 [38016/50000]\tLoss: 4.7357\tLR: 4.975703\n",
      "Training Epoch: 50 [38144/50000]\tLoss: 4.7025\tLR: 4.975959\n",
      "Training Epoch: 50 [38272/50000]\tLoss: 4.6669\tLR: 4.976215\n",
      "Training Epoch: 50 [38400/50000]\tLoss: 4.7200\tLR: 4.976471\n",
      "Training Epoch: 50 [38528/50000]\tLoss: 4.6807\tLR: 4.976726\n",
      "Training Epoch: 50 [38656/50000]\tLoss: 4.7469\tLR: 4.976982\n",
      "Training Epoch: 50 [38784/50000]\tLoss: 4.7327\tLR: 4.977238\n",
      "Training Epoch: 50 [38912/50000]\tLoss: 4.7250\tLR: 4.977494\n",
      "Training Epoch: 50 [39040/50000]\tLoss: 4.7006\tLR: 4.977749\n",
      "Training Epoch: 50 [39168/50000]\tLoss: 4.6470\tLR: 4.978005\n",
      "Training Epoch: 50 [39296/50000]\tLoss: 4.6679\tLR: 4.978261\n",
      "Training Epoch: 50 [39424/50000]\tLoss: 4.6879\tLR: 4.978517\n",
      "Training Epoch: 50 [39552/50000]\tLoss: 4.6110\tLR: 4.978772\n",
      "Training Epoch: 50 [39680/50000]\tLoss: 4.6948\tLR: 4.979028\n",
      "Training Epoch: 50 [39808/50000]\tLoss: 4.6489\tLR: 4.979284\n",
      "Training Epoch: 50 [39936/50000]\tLoss: 4.7125\tLR: 4.979540\n",
      "Training Epoch: 50 [40064/50000]\tLoss: 4.7735\tLR: 4.979795\n",
      "Training Epoch: 50 [40192/50000]\tLoss: 4.7269\tLR: 4.980051\n",
      "Training Epoch: 50 [40320/50000]\tLoss: 4.7707\tLR: 4.980307\n",
      "Training Epoch: 50 [40448/50000]\tLoss: 4.6860\tLR: 4.980563\n",
      "Training Epoch: 50 [40576/50000]\tLoss: 4.7215\tLR: 4.980818\n",
      "Training Epoch: 50 [40704/50000]\tLoss: 4.7508\tLR: 4.981074\n",
      "Training Epoch: 50 [40832/50000]\tLoss: 4.6968\tLR: 4.981330\n",
      "Training Epoch: 50 [40960/50000]\tLoss: 4.6989\tLR: 4.981586\n",
      "Training Epoch: 50 [41088/50000]\tLoss: 4.7388\tLR: 4.981841\n",
      "Training Epoch: 50 [41216/50000]\tLoss: 4.6607\tLR: 4.982097\n",
      "Training Epoch: 50 [41344/50000]\tLoss: 4.7061\tLR: 4.982353\n",
      "Training Epoch: 50 [41472/50000]\tLoss: 4.6401\tLR: 4.982609\n",
      "Training Epoch: 50 [41600/50000]\tLoss: 4.7320\tLR: 4.982864\n",
      "Training Epoch: 50 [41728/50000]\tLoss: 4.7474\tLR: 4.983120\n",
      "Training Epoch: 50 [41856/50000]\tLoss: 4.7231\tLR: 4.983376\n",
      "Training Epoch: 50 [41984/50000]\tLoss: 4.7013\tLR: 4.983632\n",
      "Training Epoch: 50 [42112/50000]\tLoss: 4.7887\tLR: 4.983887\n",
      "Training Epoch: 50 [42240/50000]\tLoss: 4.6705\tLR: 4.984143\n",
      "Training Epoch: 50 [42368/50000]\tLoss: 4.7011\tLR: 4.984399\n",
      "Training Epoch: 50 [42496/50000]\tLoss: 4.6556\tLR: 4.984655\n",
      "Training Epoch: 50 [42624/50000]\tLoss: 4.6608\tLR: 4.984910\n",
      "Training Epoch: 50 [42752/50000]\tLoss: 4.7005\tLR: 4.985166\n",
      "Training Epoch: 50 [42880/50000]\tLoss: 4.7420\tLR: 4.985422\n",
      "Training Epoch: 50 [43008/50000]\tLoss: 4.7505\tLR: 4.985678\n",
      "Training Epoch: 50 [43136/50000]\tLoss: 4.6781\tLR: 4.985934\n",
      "Training Epoch: 50 [43264/50000]\tLoss: 4.7570\tLR: 4.986189\n",
      "Training Epoch: 50 [43392/50000]\tLoss: 4.7915\tLR: 4.986445\n",
      "Training Epoch: 50 [43520/50000]\tLoss: 4.7012\tLR: 4.986701\n",
      "Training Epoch: 50 [43648/50000]\tLoss: 4.6961\tLR: 4.986957\n",
      "Training Epoch: 50 [43776/50000]\tLoss: 4.6907\tLR: 4.987212\n",
      "Training Epoch: 50 [43904/50000]\tLoss: 4.7123\tLR: 4.987468\n",
      "Training Epoch: 50 [44032/50000]\tLoss: 4.6714\tLR: 4.987724\n",
      "Training Epoch: 50 [44160/50000]\tLoss: 4.7966\tLR: 4.987980\n",
      "Training Epoch: 50 [44288/50000]\tLoss: 4.7395\tLR: 4.988235\n",
      "Training Epoch: 50 [44416/50000]\tLoss: 4.7044\tLR: 4.988491\n",
      "Training Epoch: 50 [44544/50000]\tLoss: 4.6230\tLR: 4.988747\n",
      "Training Epoch: 50 [44672/50000]\tLoss: 4.6475\tLR: 4.989003\n",
      "Training Epoch: 50 [44800/50000]\tLoss: 4.6724\tLR: 4.989258\n",
      "Training Epoch: 50 [44928/50000]\tLoss: 4.7124\tLR: 4.989514\n",
      "Training Epoch: 50 [45056/50000]\tLoss: 4.6587\tLR: 4.989770\n",
      "Training Epoch: 50 [45184/50000]\tLoss: 4.6612\tLR: 4.990026\n",
      "Training Epoch: 50 [45312/50000]\tLoss: 4.7464\tLR: 4.990281\n",
      "Training Epoch: 50 [45440/50000]\tLoss: 4.7036\tLR: 4.990537\n",
      "Training Epoch: 50 [45568/50000]\tLoss: 4.7500\tLR: 4.990793\n",
      "Training Epoch: 50 [45696/50000]\tLoss: 4.6907\tLR: 4.991049\n",
      "Training Epoch: 50 [45824/50000]\tLoss: 4.5983\tLR: 4.991304\n",
      "Training Epoch: 50 [45952/50000]\tLoss: 4.6772\tLR: 4.991560\n",
      "Training Epoch: 50 [46080/50000]\tLoss: 4.7351\tLR: 4.991816\n",
      "Training Epoch: 50 [46208/50000]\tLoss: 4.8292\tLR: 4.992072\n",
      "Training Epoch: 50 [46336/50000]\tLoss: 4.7526\tLR: 4.992327\n",
      "Training Epoch: 50 [46464/50000]\tLoss: 4.6132\tLR: 4.992583\n",
      "Training Epoch: 50 [46592/50000]\tLoss: 4.7427\tLR: 4.992839\n",
      "Training Epoch: 50 [46720/50000]\tLoss: 4.7363\tLR: 4.993095\n",
      "Training Epoch: 50 [46848/50000]\tLoss: 4.6649\tLR: 4.993350\n",
      "Training Epoch: 50 [46976/50000]\tLoss: 4.7237\tLR: 4.993606\n",
      "Training Epoch: 50 [47104/50000]\tLoss: 4.7099\tLR: 4.993862\n",
      "Training Epoch: 50 [47232/50000]\tLoss: 4.7126\tLR: 4.994118\n",
      "Training Epoch: 50 [47360/50000]\tLoss: 4.7358\tLR: 4.994373\n",
      "Training Epoch: 50 [47488/50000]\tLoss: 4.6967\tLR: 4.994629\n",
      "Training Epoch: 50 [47616/50000]\tLoss: 4.6276\tLR: 4.994885\n",
      "Training Epoch: 50 [47744/50000]\tLoss: 4.6620\tLR: 4.995141\n",
      "Training Epoch: 50 [47872/50000]\tLoss: 4.7205\tLR: 4.995396\n",
      "Training Epoch: 50 [48000/50000]\tLoss: 4.6765\tLR: 4.995652\n",
      "Training Epoch: 50 [48128/50000]\tLoss: 4.6781\tLR: 4.995908\n",
      "Training Epoch: 50 [48256/50000]\tLoss: 4.7316\tLR: 4.996164\n",
      "Training Epoch: 50 [48384/50000]\tLoss: 4.7407\tLR: 4.996419\n",
      "Training Epoch: 50 [48512/50000]\tLoss: 4.6863\tLR: 4.996675\n",
      "Training Epoch: 50 [48640/50000]\tLoss: 4.6707\tLR: 4.996931\n",
      "Training Epoch: 50 [48768/50000]\tLoss: 4.7249\tLR: 4.997187\n",
      "Training Epoch: 50 [48896/50000]\tLoss: 4.7191\tLR: 4.997442\n",
      "Training Epoch: 50 [49024/50000]\tLoss: 4.7178\tLR: 4.997698\n",
      "Training Epoch: 50 [49152/50000]\tLoss: 4.6342\tLR: 4.997954\n",
      "Training Epoch: 50 [49280/50000]\tLoss: 4.6527\tLR: 4.998210\n",
      "Training Epoch: 50 [49408/50000]\tLoss: 4.7458\tLR: 4.998465\n",
      "Training Epoch: 50 [49536/50000]\tLoss: 4.7532\tLR: 4.998721\n",
      "Training Epoch: 50 [49664/50000]\tLoss: 4.6653\tLR: 4.998977\n",
      "Training Epoch: 50 [49792/50000]\tLoss: 4.6515\tLR: 4.999233\n",
      "Training Epoch: 50 [49920/50000]\tLoss: 4.7806\tLR: 4.999488\n",
      "Training Epoch: 50 [50000/50000]\tLoss: 4.7584\tLR: 4.999744\n",
      "epoch 50 training time consumed: 488.94s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   70097 GB |   70097 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   69882 GB |   69882 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     215 GB |     215 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   70097 GB |   70097 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   69882 GB |   69882 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     215 GB |     215 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   69110 GB |   69110 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   68895 GB |   68895 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     215 GB |     215 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    7432 K  |    7432 K  |\n",
      "|       from large pool |      24    |      65    |    3168 K  |    3168 K  |\n",
      "|       from small pool |     231    |     274    |    4264 K  |    4264 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    7432 K  |    7432 K  |\n",
      "|       from large pool |      24    |      65    |    3168 K  |    3168 K  |\n",
      "|       from small pool |     231    |     274    |    4264 K  |    4264 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      47    |    4307 K  |    4307 K  |\n",
      "|       from large pool |      10    |      23    |    1523 K  |    1523 K  |\n",
      "|       from small pool |      27    |      35    |    2784 K  |    2784 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 50, Average loss: 0.0371, Accuracy: 0.0100, Time consumed:31.15s\n",
      "\n",
      "saving weights file to checkpoint\\resnet18\\Monday_25_July_2022_16h_56m_47s\\resnet18-50-regular.pth\n",
      "Training Epoch: 51 [128/50000]\tLoss: 4.6803\tLR: 0.100000\n",
      "Training Epoch: 51 [256/50000]\tLoss: 4.6889\tLR: 5.000256\n",
      "Training Epoch: 51 [384/50000]\tLoss: 4.6311\tLR: 5.000512\n",
      "Training Epoch: 51 [512/50000]\tLoss: 4.6846\tLR: 5.000767\n",
      "Training Epoch: 51 [640/50000]\tLoss: 4.7495\tLR: 5.001023\n",
      "Training Epoch: 51 [768/50000]\tLoss: 4.6815\tLR: 5.001279\n",
      "Training Epoch: 51 [896/50000]\tLoss: 4.7246\tLR: 5.001535\n",
      "Training Epoch: 51 [1024/50000]\tLoss: 4.7118\tLR: 5.001790\n",
      "Training Epoch: 51 [1152/50000]\tLoss: 4.6958\tLR: 5.002046\n",
      "Training Epoch: 51 [1280/50000]\tLoss: 4.7792\tLR: 5.002302\n",
      "Training Epoch: 51 [1408/50000]\tLoss: 4.7415\tLR: 5.002558\n",
      "Training Epoch: 51 [1536/50000]\tLoss: 4.7224\tLR: 5.002813\n",
      "Training Epoch: 51 [1664/50000]\tLoss: 4.6375\tLR: 5.003069\n",
      "Training Epoch: 51 [1792/50000]\tLoss: 4.7018\tLR: 5.003325\n",
      "Training Epoch: 51 [1920/50000]\tLoss: 4.7280\tLR: 5.003581\n",
      "Training Epoch: 51 [2048/50000]\tLoss: 4.7727\tLR: 5.003836\n",
      "Training Epoch: 51 [2176/50000]\tLoss: 4.6535\tLR: 5.004092\n",
      "Training Epoch: 51 [2304/50000]\tLoss: 4.7141\tLR: 5.004348\n",
      "Training Epoch: 51 [2432/50000]\tLoss: 4.7227\tLR: 5.004604\n",
      "Training Epoch: 51 [2560/50000]\tLoss: 4.6813\tLR: 5.004859\n",
      "Training Epoch: 51 [2688/50000]\tLoss: 4.7507\tLR: 5.005115\n",
      "Training Epoch: 51 [2816/50000]\tLoss: 4.7227\tLR: 5.005371\n",
      "Training Epoch: 51 [2944/50000]\tLoss: 4.7296\tLR: 5.005627\n",
      "Training Epoch: 51 [3072/50000]\tLoss: 4.7044\tLR: 5.005882\n",
      "Training Epoch: 51 [3200/50000]\tLoss: 4.6513\tLR: 5.006138\n",
      "Training Epoch: 51 [3328/50000]\tLoss: 4.6984\tLR: 5.006394\n",
      "Training Epoch: 51 [3456/50000]\tLoss: 4.6226\tLR: 5.006650\n",
      "Training Epoch: 51 [3584/50000]\tLoss: 4.7545\tLR: 5.006905\n",
      "Training Epoch: 51 [3712/50000]\tLoss: 4.6802\tLR: 5.007161\n",
      "Training Epoch: 51 [3840/50000]\tLoss: 4.7062\tLR: 5.007417\n",
      "Training Epoch: 51 [3968/50000]\tLoss: 4.7987\tLR: 5.007673\n",
      "Training Epoch: 51 [4096/50000]\tLoss: 4.6603\tLR: 5.007928\n",
      "Training Epoch: 51 [4224/50000]\tLoss: 4.7106\tLR: 5.008184\n",
      "Training Epoch: 51 [4352/50000]\tLoss: 4.7006\tLR: 5.008440\n",
      "Training Epoch: 51 [4480/50000]\tLoss: 4.7253\tLR: 5.008696\n",
      "Training Epoch: 51 [4608/50000]\tLoss: 4.6804\tLR: 5.008951\n",
      "Training Epoch: 51 [4736/50000]\tLoss: 4.6851\tLR: 5.009207\n",
      "Training Epoch: 51 [4864/50000]\tLoss: 4.6450\tLR: 5.009463\n",
      "Training Epoch: 51 [4992/50000]\tLoss: 4.7322\tLR: 5.009719\n",
      "Training Epoch: 51 [5120/50000]\tLoss: 4.6705\tLR: 5.009974\n",
      "Training Epoch: 51 [5248/50000]\tLoss: 4.7104\tLR: 5.010230\n",
      "Training Epoch: 51 [5376/50000]\tLoss: 4.6805\tLR: 5.010486\n",
      "Training Epoch: 51 [5504/50000]\tLoss: 4.7087\tLR: 5.010742\n",
      "Training Epoch: 51 [5632/50000]\tLoss: 4.6862\tLR: 5.010997\n",
      "Training Epoch: 51 [5760/50000]\tLoss: 4.6401\tLR: 5.011253\n",
      "Training Epoch: 51 [5888/50000]\tLoss: 4.6780\tLR: 5.011509\n",
      "Training Epoch: 51 [6016/50000]\tLoss: 4.7076\tLR: 5.011765\n",
      "Training Epoch: 51 [6144/50000]\tLoss: 4.6913\tLR: 5.012020\n",
      "Training Epoch: 51 [6272/50000]\tLoss: 4.7209\tLR: 5.012276\n",
      "Training Epoch: 51 [6400/50000]\tLoss: 4.7063\tLR: 5.012532\n",
      "Training Epoch: 51 [6528/50000]\tLoss: 4.7537\tLR: 5.012788\n",
      "Training Epoch: 51 [6656/50000]\tLoss: 4.8228\tLR: 5.013043\n",
      "Training Epoch: 51 [6784/50000]\tLoss: 4.7394\tLR: 5.013299\n",
      "Training Epoch: 51 [6912/50000]\tLoss: 4.6431\tLR: 5.013555\n",
      "Training Epoch: 51 [7040/50000]\tLoss: 4.7070\tLR: 5.013811\n",
      "Training Epoch: 51 [7168/50000]\tLoss: 4.6525\tLR: 5.014066\n",
      "Training Epoch: 51 [7296/50000]\tLoss: 4.7452\tLR: 5.014322\n",
      "Training Epoch: 51 [7424/50000]\tLoss: 4.6535\tLR: 5.014578\n",
      "Training Epoch: 51 [7552/50000]\tLoss: 4.7570\tLR: 5.014834\n",
      "Training Epoch: 51 [7680/50000]\tLoss: 4.6918\tLR: 5.015090\n",
      "Training Epoch: 51 [7808/50000]\tLoss: 4.6612\tLR: 5.015345\n",
      "Training Epoch: 51 [7936/50000]\tLoss: 4.7122\tLR: 5.015601\n",
      "Training Epoch: 51 [8064/50000]\tLoss: 4.6922\tLR: 5.015857\n",
      "Training Epoch: 51 [8192/50000]\tLoss: 4.7606\tLR: 5.016113\n",
      "Training Epoch: 51 [8320/50000]\tLoss: 4.7151\tLR: 5.016368\n",
      "Training Epoch: 51 [8448/50000]\tLoss: 4.7039\tLR: 5.016624\n",
      "Training Epoch: 51 [8576/50000]\tLoss: 4.6705\tLR: 5.016880\n",
      "Training Epoch: 51 [8704/50000]\tLoss: 4.6681\tLR: 5.017136\n",
      "Training Epoch: 51 [8832/50000]\tLoss: 4.6316\tLR: 5.017391\n",
      "Training Epoch: 51 [8960/50000]\tLoss: 4.6426\tLR: 5.017647\n",
      "Training Epoch: 51 [9088/50000]\tLoss: 4.7107\tLR: 5.017903\n",
      "Training Epoch: 51 [9216/50000]\tLoss: 4.7440\tLR: 5.018159\n",
      "Training Epoch: 51 [9344/50000]\tLoss: 4.6651\tLR: 5.018414\n",
      "Training Epoch: 51 [9472/50000]\tLoss: 4.8208\tLR: 5.018670\n",
      "Training Epoch: 51 [9600/50000]\tLoss: 4.7142\tLR: 5.018926\n",
      "Training Epoch: 51 [9728/50000]\tLoss: 4.6885\tLR: 5.019182\n",
      "Training Epoch: 51 [9856/50000]\tLoss: 4.7298\tLR: 5.019437\n",
      "Training Epoch: 51 [9984/50000]\tLoss: 4.6646\tLR: 5.019693\n",
      "Training Epoch: 51 [10112/50000]\tLoss: 4.6902\tLR: 5.019949\n",
      "Training Epoch: 51 [10240/50000]\tLoss: 4.7048\tLR: 5.020205\n",
      "Training Epoch: 51 [10368/50000]\tLoss: 4.7273\tLR: 5.020460\n",
      "Training Epoch: 51 [10496/50000]\tLoss: 4.7293\tLR: 5.020716\n",
      "Training Epoch: 51 [10624/50000]\tLoss: 4.7375\tLR: 5.020972\n",
      "Training Epoch: 51 [10752/50000]\tLoss: 4.6538\tLR: 5.021228\n",
      "Training Epoch: 51 [10880/50000]\tLoss: 4.6647\tLR: 5.021483\n",
      "Training Epoch: 51 [11008/50000]\tLoss: 4.7332\tLR: 5.021739\n",
      "Training Epoch: 51 [11136/50000]\tLoss: 4.6563\tLR: 5.021995\n",
      "Training Epoch: 51 [11264/50000]\tLoss: 4.7228\tLR: 5.022251\n",
      "Training Epoch: 51 [11392/50000]\tLoss: 4.6672\tLR: 5.022506\n",
      "Training Epoch: 51 [11520/50000]\tLoss: 4.7242\tLR: 5.022762\n",
      "Training Epoch: 51 [11648/50000]\tLoss: 4.8096\tLR: 5.023018\n",
      "Training Epoch: 51 [11776/50000]\tLoss: 4.7291\tLR: 5.023274\n",
      "Training Epoch: 51 [11904/50000]\tLoss: 4.7034\tLR: 5.023529\n",
      "Training Epoch: 51 [12032/50000]\tLoss: 4.7395\tLR: 5.023785\n",
      "Training Epoch: 51 [12160/50000]\tLoss: 4.7541\tLR: 5.024041\n",
      "Training Epoch: 51 [12288/50000]\tLoss: 4.6985\tLR: 5.024297\n",
      "Training Epoch: 51 [12416/50000]\tLoss: 4.6581\tLR: 5.024552\n",
      "Training Epoch: 51 [12544/50000]\tLoss: 4.6990\tLR: 5.024808\n",
      "Training Epoch: 51 [12672/50000]\tLoss: 4.6776\tLR: 5.025064\n",
      "Training Epoch: 51 [12800/50000]\tLoss: 4.7426\tLR: 5.025320\n",
      "Training Epoch: 51 [12928/50000]\tLoss: 4.6783\tLR: 5.025575\n",
      "Training Epoch: 51 [13056/50000]\tLoss: 4.6789\tLR: 5.025831\n",
      "Training Epoch: 51 [13184/50000]\tLoss: 4.7278\tLR: 5.026087\n",
      "Training Epoch: 51 [13312/50000]\tLoss: 4.7027\tLR: 5.026343\n",
      "Training Epoch: 51 [13440/50000]\tLoss: 4.6107\tLR: 5.026598\n",
      "Training Epoch: 51 [13568/50000]\tLoss: 4.7017\tLR: 5.026854\n",
      "Training Epoch: 51 [13696/50000]\tLoss: 4.7038\tLR: 5.027110\n",
      "Training Epoch: 51 [13824/50000]\tLoss: 4.7353\tLR: 5.027366\n",
      "Training Epoch: 51 [13952/50000]\tLoss: 4.7466\tLR: 5.027621\n",
      "Training Epoch: 51 [14080/50000]\tLoss: 4.7355\tLR: 5.027877\n",
      "Training Epoch: 51 [14208/50000]\tLoss: 4.6264\tLR: 5.028133\n",
      "Training Epoch: 51 [14336/50000]\tLoss: 4.7078\tLR: 5.028389\n",
      "Training Epoch: 51 [14464/50000]\tLoss: 4.7261\tLR: 5.028645\n",
      "Training Epoch: 51 [14592/50000]\tLoss: 4.6667\tLR: 5.028900\n",
      "Training Epoch: 51 [14720/50000]\tLoss: 4.6611\tLR: 5.029156\n",
      "Training Epoch: 51 [14848/50000]\tLoss: 4.7360\tLR: 5.029412\n",
      "Training Epoch: 51 [14976/50000]\tLoss: 4.6931\tLR: 5.029668\n",
      "Training Epoch: 51 [15104/50000]\tLoss: 4.6969\tLR: 5.029923\n",
      "Training Epoch: 51 [15232/50000]\tLoss: 4.7142\tLR: 5.030179\n",
      "Training Epoch: 51 [15360/50000]\tLoss: 4.6549\tLR: 5.030435\n",
      "Training Epoch: 51 [15488/50000]\tLoss: 4.7158\tLR: 5.030691\n",
      "Training Epoch: 51 [15616/50000]\tLoss: 4.7052\tLR: 5.030946\n",
      "Training Epoch: 51 [15744/50000]\tLoss: 4.6527\tLR: 5.031202\n",
      "Training Epoch: 51 [15872/50000]\tLoss: 4.7061\tLR: 5.031458\n",
      "Training Epoch: 51 [16000/50000]\tLoss: 4.6854\tLR: 5.031714\n",
      "Training Epoch: 51 [16128/50000]\tLoss: 4.7452\tLR: 5.031969\n",
      "Training Epoch: 51 [16256/50000]\tLoss: 4.6759\tLR: 5.032225\n",
      "Training Epoch: 51 [16384/50000]\tLoss: 4.7392\tLR: 5.032481\n",
      "Training Epoch: 51 [16512/50000]\tLoss: 4.7276\tLR: 5.032737\n",
      "Training Epoch: 51 [16640/50000]\tLoss: 4.6629\tLR: 5.032992\n",
      "Training Epoch: 51 [16768/50000]\tLoss: 4.6284\tLR: 5.033248\n",
      "Training Epoch: 51 [16896/50000]\tLoss: 4.7237\tLR: 5.033504\n",
      "Training Epoch: 51 [17024/50000]\tLoss: 4.6346\tLR: 5.033760\n",
      "Training Epoch: 51 [17152/50000]\tLoss: 4.6590\tLR: 5.034015\n",
      "Training Epoch: 51 [17280/50000]\tLoss: 4.7047\tLR: 5.034271\n",
      "Training Epoch: 51 [17408/50000]\tLoss: 4.6601\tLR: 5.034527\n",
      "Training Epoch: 51 [17536/50000]\tLoss: 4.7704\tLR: 5.034783\n",
      "Training Epoch: 51 [17664/50000]\tLoss: 4.7315\tLR: 5.035038\n",
      "Training Epoch: 51 [17792/50000]\tLoss: 4.7296\tLR: 5.035294\n",
      "Training Epoch: 51 [17920/50000]\tLoss: 4.6723\tLR: 5.035550\n",
      "Training Epoch: 51 [18048/50000]\tLoss: 4.7243\tLR: 5.035806\n",
      "Training Epoch: 51 [18176/50000]\tLoss: 4.6954\tLR: 5.036061\n",
      "Training Epoch: 51 [18304/50000]\tLoss: 4.6950\tLR: 5.036317\n",
      "Training Epoch: 51 [18432/50000]\tLoss: 4.6448\tLR: 5.036573\n",
      "Training Epoch: 51 [18560/50000]\tLoss: 4.7666\tLR: 5.036829\n",
      "Training Epoch: 51 [18688/50000]\tLoss: 4.6164\tLR: 5.037084\n",
      "Training Epoch: 51 [18816/50000]\tLoss: 4.6610\tLR: 5.037340\n",
      "Training Epoch: 51 [18944/50000]\tLoss: 4.7109\tLR: 5.037596\n",
      "Training Epoch: 51 [19072/50000]\tLoss: 4.6708\tLR: 5.037852\n",
      "Training Epoch: 51 [19200/50000]\tLoss: 4.7480\tLR: 5.038107\n",
      "Training Epoch: 51 [19328/50000]\tLoss: 4.7559\tLR: 5.038363\n",
      "Training Epoch: 51 [19456/50000]\tLoss: 4.7538\tLR: 5.038619\n",
      "Training Epoch: 51 [19584/50000]\tLoss: 4.7242\tLR: 5.038875\n",
      "Training Epoch: 51 [19712/50000]\tLoss: 4.6866\tLR: 5.039130\n",
      "Training Epoch: 51 [19840/50000]\tLoss: 4.6820\tLR: 5.039386\n",
      "Training Epoch: 51 [19968/50000]\tLoss: 4.7105\tLR: 5.039642\n",
      "Training Epoch: 51 [20096/50000]\tLoss: 4.7274\tLR: 5.039898\n",
      "Training Epoch: 51 [20224/50000]\tLoss: 4.7025\tLR: 5.040153\n",
      "Training Epoch: 51 [20352/50000]\tLoss: 4.7482\tLR: 5.040409\n",
      "Training Epoch: 51 [20480/50000]\tLoss: 4.6932\tLR: 5.040665\n",
      "Training Epoch: 51 [20608/50000]\tLoss: 4.6822\tLR: 5.040921\n",
      "Training Epoch: 51 [20736/50000]\tLoss: 4.6089\tLR: 5.041176\n",
      "Training Epoch: 51 [20864/50000]\tLoss: 4.7417\tLR: 5.041432\n",
      "Training Epoch: 51 [20992/50000]\tLoss: 4.7607\tLR: 5.041688\n",
      "Training Epoch: 51 [21120/50000]\tLoss: 4.7736\tLR: 5.041944\n",
      "Training Epoch: 51 [21248/50000]\tLoss: 4.7193\tLR: 5.042199\n",
      "Training Epoch: 51 [21376/50000]\tLoss: 4.6712\tLR: 5.042455\n",
      "Training Epoch: 51 [21504/50000]\tLoss: 4.6817\tLR: 5.042711\n",
      "Training Epoch: 51 [21632/50000]\tLoss: 4.7017\tLR: 5.042967\n",
      "Training Epoch: 51 [21760/50000]\tLoss: 4.7276\tLR: 5.043223\n",
      "Training Epoch: 51 [21888/50000]\tLoss: 4.6591\tLR: 5.043478\n",
      "Training Epoch: 51 [22016/50000]\tLoss: 4.7235\tLR: 5.043734\n",
      "Training Epoch: 51 [22144/50000]\tLoss: 4.7668\tLR: 5.043990\n",
      "Training Epoch: 51 [22272/50000]\tLoss: 4.7527\tLR: 5.044246\n",
      "Training Epoch: 51 [22400/50000]\tLoss: 4.6440\tLR: 5.044501\n",
      "Training Epoch: 51 [22528/50000]\tLoss: 4.6888\tLR: 5.044757\n",
      "Training Epoch: 51 [22656/50000]\tLoss: 4.6968\tLR: 5.045013\n",
      "Training Epoch: 51 [22784/50000]\tLoss: 4.7071\tLR: 5.045269\n",
      "Training Epoch: 51 [22912/50000]\tLoss: 4.6945\tLR: 5.045524\n",
      "Training Epoch: 51 [23040/50000]\tLoss: 4.7245\tLR: 5.045780\n",
      "Training Epoch: 51 [23168/50000]\tLoss: 4.6463\tLR: 5.046036\n",
      "Training Epoch: 51 [23296/50000]\tLoss: 4.7176\tLR: 5.046292\n",
      "Training Epoch: 51 [23424/50000]\tLoss: 4.6957\tLR: 5.046547\n",
      "Training Epoch: 51 [23552/50000]\tLoss: 4.7027\tLR: 5.046803\n",
      "Training Epoch: 51 [23680/50000]\tLoss: 4.7239\tLR: 5.047059\n",
      "Training Epoch: 51 [23808/50000]\tLoss: 4.6940\tLR: 5.047315\n",
      "Training Epoch: 51 [23936/50000]\tLoss: 4.7283\tLR: 5.047570\n",
      "Training Epoch: 51 [24064/50000]\tLoss: 4.6983\tLR: 5.047826\n",
      "Training Epoch: 51 [24192/50000]\tLoss: 4.7090\tLR: 5.048082\n",
      "Training Epoch: 51 [24320/50000]\tLoss: 4.6758\tLR: 5.048338\n",
      "Training Epoch: 51 [24448/50000]\tLoss: 4.7155\tLR: 5.048593\n",
      "Training Epoch: 51 [24576/50000]\tLoss: 4.7170\tLR: 5.048849\n",
      "Training Epoch: 51 [24704/50000]\tLoss: 4.7372\tLR: 5.049105\n",
      "Training Epoch: 51 [24832/50000]\tLoss: 4.7889\tLR: 5.049361\n",
      "Training Epoch: 51 [24960/50000]\tLoss: 4.7112\tLR: 5.049616\n",
      "Training Epoch: 51 [25088/50000]\tLoss: 4.7285\tLR: 5.049872\n",
      "Training Epoch: 51 [25216/50000]\tLoss: 4.7138\tLR: 5.050128\n",
      "Training Epoch: 51 [25344/50000]\tLoss: 4.7407\tLR: 5.050384\n",
      "Training Epoch: 51 [25472/50000]\tLoss: 4.6793\tLR: 5.050639\n",
      "Training Epoch: 51 [25600/50000]\tLoss: 4.6986\tLR: 5.050895\n",
      "Training Epoch: 51 [25728/50000]\tLoss: 4.7279\tLR: 5.051151\n",
      "Training Epoch: 51 [25856/50000]\tLoss: 4.7015\tLR: 5.051407\n",
      "Training Epoch: 51 [25984/50000]\tLoss: 4.6439\tLR: 5.051662\n",
      "Training Epoch: 51 [26112/50000]\tLoss: 4.7517\tLR: 5.051918\n",
      "Training Epoch: 51 [26240/50000]\tLoss: 4.7137\tLR: 5.052174\n",
      "Training Epoch: 51 [26368/50000]\tLoss: 4.7127\tLR: 5.052430\n",
      "Training Epoch: 51 [26496/50000]\tLoss: 4.7255\tLR: 5.052685\n",
      "Training Epoch: 51 [26624/50000]\tLoss: 4.7278\tLR: 5.052941\n",
      "Training Epoch: 51 [26752/50000]\tLoss: 4.7146\tLR: 5.053197\n",
      "Training Epoch: 51 [26880/50000]\tLoss: 4.6130\tLR: 5.053453\n",
      "Training Epoch: 51 [27008/50000]\tLoss: 4.7389\tLR: 5.053708\n",
      "Training Epoch: 51 [27136/50000]\tLoss: 4.6978\tLR: 5.053964\n",
      "Training Epoch: 51 [27264/50000]\tLoss: 4.6304\tLR: 5.054220\n",
      "Training Epoch: 51 [27392/50000]\tLoss: 4.7015\tLR: 5.054476\n",
      "Training Epoch: 51 [27520/50000]\tLoss: 4.7581\tLR: 5.054731\n",
      "Training Epoch: 51 [27648/50000]\tLoss: 4.7030\tLR: 5.054987\n",
      "Training Epoch: 51 [27776/50000]\tLoss: 4.7953\tLR: 5.055243\n",
      "Training Epoch: 51 [27904/50000]\tLoss: 4.7062\tLR: 5.055499\n",
      "Training Epoch: 51 [28032/50000]\tLoss: 4.7251\tLR: 5.055754\n",
      "Training Epoch: 51 [28160/50000]\tLoss: 4.7104\tLR: 5.056010\n",
      "Training Epoch: 51 [28288/50000]\tLoss: 4.7118\tLR: 5.056266\n",
      "Training Epoch: 51 [28416/50000]\tLoss: 4.7382\tLR: 5.056522\n",
      "Training Epoch: 51 [28544/50000]\tLoss: 4.6641\tLR: 5.056777\n",
      "Training Epoch: 51 [28672/50000]\tLoss: 4.7472\tLR: 5.057033\n",
      "Training Epoch: 51 [28800/50000]\tLoss: 4.7077\tLR: 5.057289\n",
      "Training Epoch: 51 [28928/50000]\tLoss: 4.6761\tLR: 5.057545\n",
      "Training Epoch: 51 [29056/50000]\tLoss: 4.6714\tLR: 5.057801\n",
      "Training Epoch: 51 [29184/50000]\tLoss: 4.7752\tLR: 5.058056\n",
      "Training Epoch: 51 [29312/50000]\tLoss: 4.7014\tLR: 5.058312\n",
      "Training Epoch: 51 [29440/50000]\tLoss: 4.6934\tLR: 5.058568\n",
      "Training Epoch: 51 [29568/50000]\tLoss: 4.7074\tLR: 5.058824\n",
      "Training Epoch: 51 [29696/50000]\tLoss: 4.7358\tLR: 5.059079\n",
      "Training Epoch: 51 [29824/50000]\tLoss: 4.7522\tLR: 5.059335\n",
      "Training Epoch: 51 [29952/50000]\tLoss: 4.7216\tLR: 5.059591\n",
      "Training Epoch: 51 [30080/50000]\tLoss: 4.6073\tLR: 5.059847\n",
      "Training Epoch: 51 [30208/50000]\tLoss: 4.7234\tLR: 5.060102\n",
      "Training Epoch: 51 [30336/50000]\tLoss: 4.7513\tLR: 5.060358\n",
      "Training Epoch: 51 [30464/50000]\tLoss: 4.7727\tLR: 5.060614\n",
      "Training Epoch: 51 [30592/50000]\tLoss: 4.7311\tLR: 5.060870\n",
      "Training Epoch: 51 [30720/50000]\tLoss: 4.7661\tLR: 5.061125\n",
      "Training Epoch: 51 [30848/50000]\tLoss: 4.6516\tLR: 5.061381\n",
      "Training Epoch: 51 [30976/50000]\tLoss: 4.7822\tLR: 5.061637\n",
      "Training Epoch: 51 [31104/50000]\tLoss: 4.6877\tLR: 5.061893\n",
      "Training Epoch: 51 [31232/50000]\tLoss: 4.6498\tLR: 5.062148\n",
      "Training Epoch: 51 [31360/50000]\tLoss: 4.7181\tLR: 5.062404\n",
      "Training Epoch: 51 [31488/50000]\tLoss: 4.7324\tLR: 5.062660\n",
      "Training Epoch: 51 [31616/50000]\tLoss: 4.6424\tLR: 5.062916\n",
      "Training Epoch: 51 [31744/50000]\tLoss: 4.7261\tLR: 5.063171\n",
      "Training Epoch: 51 [31872/50000]\tLoss: 4.7167\tLR: 5.063427\n",
      "Training Epoch: 51 [32000/50000]\tLoss: 4.6830\tLR: 5.063683\n",
      "Training Epoch: 51 [32128/50000]\tLoss: 4.7301\tLR: 5.063939\n",
      "Training Epoch: 51 [32256/50000]\tLoss: 4.7398\tLR: 5.064194\n",
      "Training Epoch: 51 [32384/50000]\tLoss: 4.6411\tLR: 5.064450\n",
      "Training Epoch: 51 [32512/50000]\tLoss: 4.7273\tLR: 5.064706\n",
      "Training Epoch: 51 [32640/50000]\tLoss: 4.6895\tLR: 5.064962\n",
      "Training Epoch: 51 [32768/50000]\tLoss: 4.7297\tLR: 5.065217\n",
      "Training Epoch: 51 [32896/50000]\tLoss: 4.6640\tLR: 5.065473\n",
      "Training Epoch: 51 [33024/50000]\tLoss: 4.6828\tLR: 5.065729\n",
      "Training Epoch: 51 [33152/50000]\tLoss: 4.6589\tLR: 5.065985\n",
      "Training Epoch: 51 [33280/50000]\tLoss: 4.7098\tLR: 5.066240\n",
      "Training Epoch: 51 [33408/50000]\tLoss: 4.6900\tLR: 5.066496\n",
      "Training Epoch: 51 [33536/50000]\tLoss: 4.6879\tLR: 5.066752\n",
      "Training Epoch: 51 [33664/50000]\tLoss: 4.6885\tLR: 5.067008\n",
      "Training Epoch: 51 [33792/50000]\tLoss: 4.7119\tLR: 5.067263\n",
      "Training Epoch: 51 [33920/50000]\tLoss: 4.6753\tLR: 5.067519\n",
      "Training Epoch: 51 [34048/50000]\tLoss: 4.7526\tLR: 5.067775\n",
      "Training Epoch: 51 [34176/50000]\tLoss: 4.6775\tLR: 5.068031\n",
      "Training Epoch: 51 [34304/50000]\tLoss: 4.7373\tLR: 5.068286\n",
      "Training Epoch: 51 [34432/50000]\tLoss: 4.7640\tLR: 5.068542\n",
      "Training Epoch: 51 [34560/50000]\tLoss: 4.7297\tLR: 5.068798\n",
      "Training Epoch: 51 [34688/50000]\tLoss: 4.6865\tLR: 5.069054\n",
      "Training Epoch: 51 [34816/50000]\tLoss: 4.6723\tLR: 5.069309\n",
      "Training Epoch: 51 [34944/50000]\tLoss: 4.6453\tLR: 5.069565\n",
      "Training Epoch: 51 [35072/50000]\tLoss: 4.6599\tLR: 5.069821\n",
      "Training Epoch: 51 [35200/50000]\tLoss: 4.6437\tLR: 5.070077\n",
      "Training Epoch: 51 [35328/50000]\tLoss: 4.7188\tLR: 5.070332\n",
      "Training Epoch: 51 [35456/50000]\tLoss: 4.7117\tLR: 5.070588\n",
      "Training Epoch: 51 [35584/50000]\tLoss: 4.7000\tLR: 5.070844\n",
      "Training Epoch: 51 [35712/50000]\tLoss: 4.6874\tLR: 5.071100\n",
      "Training Epoch: 51 [35840/50000]\tLoss: 4.6774\tLR: 5.071355\n",
      "Training Epoch: 51 [35968/50000]\tLoss: 4.7290\tLR: 5.071611\n",
      "Training Epoch: 51 [36096/50000]\tLoss: 4.6156\tLR: 5.071867\n",
      "Training Epoch: 51 [36224/50000]\tLoss: 4.7136\tLR: 5.072123\n",
      "Training Epoch: 51 [36352/50000]\tLoss: 4.7087\tLR: 5.072379\n",
      "Training Epoch: 51 [36480/50000]\tLoss: 4.7348\tLR: 5.072634\n",
      "Training Epoch: 51 [36608/50000]\tLoss: 4.7443\tLR: 5.072890\n",
      "Training Epoch: 51 [36736/50000]\tLoss: 4.7398\tLR: 5.073146\n",
      "Training Epoch: 51 [36864/50000]\tLoss: 4.7527\tLR: 5.073402\n",
      "Training Epoch: 51 [36992/50000]\tLoss: 4.7181\tLR: 5.073657\n",
      "Training Epoch: 51 [37120/50000]\tLoss: 4.7323\tLR: 5.073913\n",
      "Training Epoch: 51 [37248/50000]\tLoss: 4.6923\tLR: 5.074169\n",
      "Training Epoch: 51 [37376/50000]\tLoss: 4.6857\tLR: 5.074425\n",
      "Training Epoch: 51 [37504/50000]\tLoss: 4.7416\tLR: 5.074680\n",
      "Training Epoch: 51 [37632/50000]\tLoss: 4.7104\tLR: 5.074936\n",
      "Training Epoch: 51 [37760/50000]\tLoss: 4.6770\tLR: 5.075192\n",
      "Training Epoch: 51 [37888/50000]\tLoss: 4.6883\tLR: 5.075448\n",
      "Training Epoch: 51 [38016/50000]\tLoss: 4.6524\tLR: 5.075703\n",
      "Training Epoch: 51 [38144/50000]\tLoss: 4.6734\tLR: 5.075959\n",
      "Training Epoch: 51 [38272/50000]\tLoss: 4.7234\tLR: 5.076215\n",
      "Training Epoch: 51 [38400/50000]\tLoss: 4.6828\tLR: 5.076471\n",
      "Training Epoch: 51 [38528/50000]\tLoss: 4.6879\tLR: 5.076726\n",
      "Training Epoch: 51 [38656/50000]\tLoss: 4.7167\tLR: 5.076982\n",
      "Training Epoch: 51 [38784/50000]\tLoss: 4.7340\tLR: 5.077238\n",
      "Training Epoch: 51 [38912/50000]\tLoss: 4.7434\tLR: 5.077494\n",
      "Training Epoch: 51 [39040/50000]\tLoss: 4.7407\tLR: 5.077749\n",
      "Training Epoch: 51 [39168/50000]\tLoss: 4.7799\tLR: 5.078005\n",
      "Training Epoch: 51 [39296/50000]\tLoss: 4.7396\tLR: 5.078261\n",
      "Training Epoch: 51 [39424/50000]\tLoss: 4.7088\tLR: 5.078517\n",
      "Training Epoch: 51 [39552/50000]\tLoss: 4.7322\tLR: 5.078772\n",
      "Training Epoch: 51 [39680/50000]\tLoss: 4.7758\tLR: 5.079028\n",
      "Training Epoch: 51 [39808/50000]\tLoss: 4.7039\tLR: 5.079284\n",
      "Training Epoch: 51 [39936/50000]\tLoss: 4.7168\tLR: 5.079540\n",
      "Training Epoch: 51 [40064/50000]\tLoss: 4.7461\tLR: 5.079795\n",
      "Training Epoch: 51 [40192/50000]\tLoss: 4.7585\tLR: 5.080051\n",
      "Training Epoch: 51 [40320/50000]\tLoss: 4.7535\tLR: 5.080307\n",
      "Training Epoch: 51 [40448/50000]\tLoss: 4.6720\tLR: 5.080563\n",
      "Training Epoch: 51 [40576/50000]\tLoss: 4.7602\tLR: 5.080818\n",
      "Training Epoch: 51 [40704/50000]\tLoss: 4.6960\tLR: 5.081074\n",
      "Training Epoch: 51 [40832/50000]\tLoss: 4.7300\tLR: 5.081330\n",
      "Training Epoch: 51 [40960/50000]\tLoss: 4.5991\tLR: 5.081586\n",
      "Training Epoch: 51 [41088/50000]\tLoss: 4.7914\tLR: 5.081841\n",
      "Training Epoch: 51 [41216/50000]\tLoss: 4.7398\tLR: 5.082097\n",
      "Training Epoch: 51 [41344/50000]\tLoss: 4.7289\tLR: 5.082353\n",
      "Training Epoch: 51 [41472/50000]\tLoss: 4.7173\tLR: 5.082609\n",
      "Training Epoch: 51 [41600/50000]\tLoss: 4.7171\tLR: 5.082864\n",
      "Training Epoch: 51 [41728/50000]\tLoss: 4.7441\tLR: 5.083120\n",
      "Training Epoch: 51 [41856/50000]\tLoss: 4.6820\tLR: 5.083376\n",
      "Training Epoch: 51 [41984/50000]\tLoss: 4.6154\tLR: 5.083632\n",
      "Training Epoch: 51 [42112/50000]\tLoss: 4.7147\tLR: 5.083887\n",
      "Training Epoch: 51 [42240/50000]\tLoss: 4.7042\tLR: 5.084143\n",
      "Training Epoch: 51 [42368/50000]\tLoss: 4.6645\tLR: 5.084399\n",
      "Training Epoch: 51 [42496/50000]\tLoss: 4.6632\tLR: 5.084655\n",
      "Training Epoch: 51 [42624/50000]\tLoss: 4.7604\tLR: 5.084910\n",
      "Training Epoch: 51 [42752/50000]\tLoss: 4.6847\tLR: 5.085166\n",
      "Training Epoch: 51 [42880/50000]\tLoss: 4.7235\tLR: 5.085422\n",
      "Training Epoch: 51 [43008/50000]\tLoss: 4.6957\tLR: 5.085678\n",
      "Training Epoch: 51 [43136/50000]\tLoss: 4.6585\tLR: 5.085934\n",
      "Training Epoch: 51 [43264/50000]\tLoss: 4.6792\tLR: 5.086189\n",
      "Training Epoch: 51 [43392/50000]\tLoss: 4.7095\tLR: 5.086445\n",
      "Training Epoch: 51 [43520/50000]\tLoss: 4.6760\tLR: 5.086701\n",
      "Training Epoch: 51 [43648/50000]\tLoss: 4.6478\tLR: 5.086957\n",
      "Training Epoch: 51 [43776/50000]\tLoss: 4.7189\tLR: 5.087212\n",
      "Training Epoch: 51 [43904/50000]\tLoss: 4.6851\tLR: 5.087468\n",
      "Training Epoch: 51 [44032/50000]\tLoss: 4.6608\tLR: 5.087724\n",
      "Training Epoch: 51 [44160/50000]\tLoss: 4.7525\tLR: 5.087980\n",
      "Training Epoch: 51 [44288/50000]\tLoss: 4.6698\tLR: 5.088235\n",
      "Training Epoch: 51 [44416/50000]\tLoss: 4.7519\tLR: 5.088491\n",
      "Training Epoch: 51 [44544/50000]\tLoss: 4.6439\tLR: 5.088747\n",
      "Training Epoch: 51 [44672/50000]\tLoss: 4.6570\tLR: 5.089003\n",
      "Training Epoch: 51 [44800/50000]\tLoss: 4.6650\tLR: 5.089258\n",
      "Training Epoch: 51 [44928/50000]\tLoss: 4.6842\tLR: 5.089514\n",
      "Training Epoch: 51 [45056/50000]\tLoss: 4.7406\tLR: 5.089770\n",
      "Training Epoch: 51 [45184/50000]\tLoss: 4.7078\tLR: 5.090026\n",
      "Training Epoch: 51 [45312/50000]\tLoss: 4.7565\tLR: 5.090281\n",
      "Training Epoch: 51 [45440/50000]\tLoss: 4.6747\tLR: 5.090537\n",
      "Training Epoch: 51 [45568/50000]\tLoss: 4.6893\tLR: 5.090793\n",
      "Training Epoch: 51 [45696/50000]\tLoss: 4.6543\tLR: 5.091049\n",
      "Training Epoch: 51 [45824/50000]\tLoss: 4.6304\tLR: 5.091304\n",
      "Training Epoch: 51 [45952/50000]\tLoss: 4.6751\tLR: 5.091560\n",
      "Training Epoch: 51 [46080/50000]\tLoss: 4.6651\tLR: 5.091816\n",
      "Training Epoch: 51 [46208/50000]\tLoss: 4.6835\tLR: 5.092072\n",
      "Training Epoch: 51 [46336/50000]\tLoss: 4.7448\tLR: 5.092327\n",
      "Training Epoch: 51 [46464/50000]\tLoss: 4.6193\tLR: 5.092583\n",
      "Training Epoch: 51 [46592/50000]\tLoss: 4.7144\tLR: 5.092839\n",
      "Training Epoch: 51 [46720/50000]\tLoss: 4.6848\tLR: 5.093095\n",
      "Training Epoch: 51 [46848/50000]\tLoss: 4.7204\tLR: 5.093350\n",
      "Training Epoch: 51 [46976/50000]\tLoss: 4.7581\tLR: 5.093606\n",
      "Training Epoch: 51 [47104/50000]\tLoss: 4.7107\tLR: 5.093862\n",
      "Training Epoch: 51 [47232/50000]\tLoss: 4.7483\tLR: 5.094118\n",
      "Training Epoch: 51 [47360/50000]\tLoss: 4.7243\tLR: 5.094373\n",
      "Training Epoch: 51 [47488/50000]\tLoss: 4.7041\tLR: 5.094629\n",
      "Training Epoch: 51 [47616/50000]\tLoss: 4.7174\tLR: 5.094885\n",
      "Training Epoch: 51 [47744/50000]\tLoss: 4.6793\tLR: 5.095141\n",
      "Training Epoch: 51 [47872/50000]\tLoss: 4.6900\tLR: 5.095396\n",
      "Training Epoch: 51 [48000/50000]\tLoss: 4.7523\tLR: 5.095652\n",
      "Training Epoch: 51 [48128/50000]\tLoss: 4.7493\tLR: 5.095908\n",
      "Training Epoch: 51 [48256/50000]\tLoss: 4.7198\tLR: 5.096164\n",
      "Training Epoch: 51 [48384/50000]\tLoss: 4.6950\tLR: 5.096419\n",
      "Training Epoch: 51 [48512/50000]\tLoss: 4.7287\tLR: 5.096675\n",
      "Training Epoch: 51 [48640/50000]\tLoss: 4.6971\tLR: 5.096931\n",
      "Training Epoch: 51 [48768/50000]\tLoss: 4.7048\tLR: 5.097187\n",
      "Training Epoch: 51 [48896/50000]\tLoss: 4.6640\tLR: 5.097442\n",
      "Training Epoch: 51 [49024/50000]\tLoss: 4.6917\tLR: 5.097698\n",
      "Training Epoch: 51 [49152/50000]\tLoss: 4.6770\tLR: 5.097954\n",
      "Training Epoch: 51 [49280/50000]\tLoss: 4.7188\tLR: 5.098210\n",
      "Training Epoch: 51 [49408/50000]\tLoss: 4.6982\tLR: 5.098465\n",
      "Training Epoch: 51 [49536/50000]\tLoss: 4.6975\tLR: 5.098721\n",
      "Training Epoch: 51 [49664/50000]\tLoss: 4.6558\tLR: 5.098977\n",
      "Training Epoch: 51 [49792/50000]\tLoss: 4.6870\tLR: 5.099233\n",
      "Training Epoch: 51 [49920/50000]\tLoss: 4.6785\tLR: 5.099488\n",
      "Training Epoch: 51 [50000/50000]\tLoss: 4.7315\tLR: 5.099744\n",
      "epoch 51 training time consumed: 488.98s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   71499 GB |   71499 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   71279 GB |   71279 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     219 GB |     219 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   71499 GB |   71499 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   71279 GB |   71279 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     219 GB |     219 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   70493 GB |   70493 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   70273 GB |   70273 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     219 GB |     219 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    7581 K  |    7581 K  |\n",
      "|       from large pool |      24    |      65    |    3231 K  |    3231 K  |\n",
      "|       from small pool |     231    |     274    |    4349 K  |    4349 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    7581 K  |    7581 K  |\n",
      "|       from large pool |      24    |      65    |    3231 K  |    3231 K  |\n",
      "|       from small pool |     231    |     274    |    4349 K  |    4349 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      47    |    4394 K  |    4393 K  |\n",
      "|       from large pool |      10    |      23    |    1553 K  |    1553 K  |\n",
      "|       from small pool |      27    |      35    |    2840 K  |    2840 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 51, Average loss: 0.0372, Accuracy: 0.0100, Time consumed:31.09s\n",
      "\n",
      "Training Epoch: 52 [128/50000]\tLoss: 4.6587\tLR: 0.100000\n",
      "Training Epoch: 52 [256/50000]\tLoss: 4.7541\tLR: 5.100256\n",
      "Training Epoch: 52 [384/50000]\tLoss: 4.6943\tLR: 5.100512\n",
      "Training Epoch: 52 [512/50000]\tLoss: 4.6844\tLR: 5.100767\n",
      "Training Epoch: 52 [640/50000]\tLoss: 4.6695\tLR: 5.101023\n",
      "Training Epoch: 52 [768/50000]\tLoss: 4.6464\tLR: 5.101279\n",
      "Training Epoch: 52 [896/50000]\tLoss: 4.6212\tLR: 5.101535\n",
      "Training Epoch: 52 [1024/50000]\tLoss: 4.7522\tLR: 5.101790\n",
      "Training Epoch: 52 [1152/50000]\tLoss: 4.7006\tLR: 5.102046\n",
      "Training Epoch: 52 [1280/50000]\tLoss: 4.6985\tLR: 5.102302\n",
      "Training Epoch: 52 [1408/50000]\tLoss: 4.6795\tLR: 5.102558\n",
      "Training Epoch: 52 [1536/50000]\tLoss: 4.6668\tLR: 5.102813\n",
      "Training Epoch: 52 [1664/50000]\tLoss: 4.7209\tLR: 5.103069\n",
      "Training Epoch: 52 [1792/50000]\tLoss: 4.6714\tLR: 5.103325\n",
      "Training Epoch: 52 [1920/50000]\tLoss: 4.7300\tLR: 5.103581\n",
      "Training Epoch: 52 [2048/50000]\tLoss: 4.6591\tLR: 5.103836\n",
      "Training Epoch: 52 [2176/50000]\tLoss: 4.7299\tLR: 5.104092\n",
      "Training Epoch: 52 [2304/50000]\tLoss: 4.6834\tLR: 5.104348\n",
      "Training Epoch: 52 [2432/50000]\tLoss: 4.7793\tLR: 5.104604\n",
      "Training Epoch: 52 [2560/50000]\tLoss: 4.6651\tLR: 5.104859\n",
      "Training Epoch: 52 [2688/50000]\tLoss: 4.6735\tLR: 5.105115\n",
      "Training Epoch: 52 [2816/50000]\tLoss: 4.6106\tLR: 5.105371\n",
      "Training Epoch: 52 [2944/50000]\tLoss: 4.7164\tLR: 5.105627\n",
      "Training Epoch: 52 [3072/50000]\tLoss: 4.6670\tLR: 5.105882\n",
      "Training Epoch: 52 [3200/50000]\tLoss: 4.6904\tLR: 5.106138\n",
      "Training Epoch: 52 [3328/50000]\tLoss: 4.6912\tLR: 5.106394\n",
      "Training Epoch: 52 [3456/50000]\tLoss: 4.7279\tLR: 5.106650\n",
      "Training Epoch: 52 [3584/50000]\tLoss: 4.6821\tLR: 5.106905\n",
      "Training Epoch: 52 [3712/50000]\tLoss: 4.7329\tLR: 5.107161\n",
      "Training Epoch: 52 [3840/50000]\tLoss: 4.6868\tLR: 5.107417\n",
      "Training Epoch: 52 [3968/50000]\tLoss: 4.7710\tLR: 5.107673\n",
      "Training Epoch: 52 [4096/50000]\tLoss: 4.6243\tLR: 5.107928\n",
      "Training Epoch: 52 [4224/50000]\tLoss: 4.7084\tLR: 5.108184\n",
      "Training Epoch: 52 [4352/50000]\tLoss: 4.6723\tLR: 5.108440\n",
      "Training Epoch: 52 [4480/50000]\tLoss: 4.6843\tLR: 5.108696\n",
      "Training Epoch: 52 [4608/50000]\tLoss: 4.6857\tLR: 5.108951\n",
      "Training Epoch: 52 [4736/50000]\tLoss: 4.6998\tLR: 5.109207\n",
      "Training Epoch: 52 [4864/50000]\tLoss: 4.7579\tLR: 5.109463\n",
      "Training Epoch: 52 [4992/50000]\tLoss: 4.6812\tLR: 5.109719\n",
      "Training Epoch: 52 [5120/50000]\tLoss: 4.7112\tLR: 5.109974\n",
      "Training Epoch: 52 [5248/50000]\tLoss: 4.6175\tLR: 5.110230\n",
      "Training Epoch: 52 [5376/50000]\tLoss: 4.7850\tLR: 5.110486\n",
      "Training Epoch: 52 [5504/50000]\tLoss: 4.6652\tLR: 5.110742\n",
      "Training Epoch: 52 [5632/50000]\tLoss: 4.6079\tLR: 5.110997\n",
      "Training Epoch: 52 [5760/50000]\tLoss: 4.6839\tLR: 5.111253\n",
      "Training Epoch: 52 [5888/50000]\tLoss: 4.6400\tLR: 5.111509\n",
      "Training Epoch: 52 [6016/50000]\tLoss: 4.7084\tLR: 5.111765\n",
      "Training Epoch: 52 [6144/50000]\tLoss: 4.7522\tLR: 5.112020\n",
      "Training Epoch: 52 [6272/50000]\tLoss: 4.6694\tLR: 5.112276\n",
      "Training Epoch: 52 [6400/50000]\tLoss: 4.7403\tLR: 5.112532\n",
      "Training Epoch: 52 [6528/50000]\tLoss: 4.7031\tLR: 5.112788\n",
      "Training Epoch: 52 [6656/50000]\tLoss: 4.7121\tLR: 5.113043\n",
      "Training Epoch: 52 [6784/50000]\tLoss: 4.6932\tLR: 5.113299\n",
      "Training Epoch: 52 [6912/50000]\tLoss: 4.6854\tLR: 5.113555\n",
      "Training Epoch: 52 [7040/50000]\tLoss: 4.7637\tLR: 5.113811\n",
      "Training Epoch: 52 [7168/50000]\tLoss: 4.7220\tLR: 5.114066\n",
      "Training Epoch: 52 [7296/50000]\tLoss: 4.7026\tLR: 5.114322\n",
      "Training Epoch: 52 [7424/50000]\tLoss: 4.7152\tLR: 5.114578\n",
      "Training Epoch: 52 [7552/50000]\tLoss: 4.7135\tLR: 5.114834\n",
      "Training Epoch: 52 [7680/50000]\tLoss: 4.7299\tLR: 5.115090\n",
      "Training Epoch: 52 [7808/50000]\tLoss: 4.6659\tLR: 5.115345\n",
      "Training Epoch: 52 [7936/50000]\tLoss: 4.6806\tLR: 5.115601\n",
      "Training Epoch: 52 [8064/50000]\tLoss: 4.6907\tLR: 5.115857\n",
      "Training Epoch: 52 [8192/50000]\tLoss: 4.6665\tLR: 5.116113\n",
      "Training Epoch: 52 [8320/50000]\tLoss: 4.6958\tLR: 5.116368\n",
      "Training Epoch: 52 [8448/50000]\tLoss: 4.7492\tLR: 5.116624\n",
      "Training Epoch: 52 [8576/50000]\tLoss: 4.6419\tLR: 5.116880\n",
      "Training Epoch: 52 [8704/50000]\tLoss: 4.6707\tLR: 5.117136\n",
      "Training Epoch: 52 [8832/50000]\tLoss: 4.6793\tLR: 5.117391\n",
      "Training Epoch: 52 [8960/50000]\tLoss: 4.7195\tLR: 5.117647\n",
      "Training Epoch: 52 [9088/50000]\tLoss: 4.7848\tLR: 5.117903\n",
      "Training Epoch: 52 [9216/50000]\tLoss: 4.6930\tLR: 5.118159\n",
      "Training Epoch: 52 [9344/50000]\tLoss: 4.7164\tLR: 5.118414\n",
      "Training Epoch: 52 [9472/50000]\tLoss: 4.7495\tLR: 5.118670\n",
      "Training Epoch: 52 [9600/50000]\tLoss: 4.6641\tLR: 5.118926\n",
      "Training Epoch: 52 [9728/50000]\tLoss: 4.7324\tLR: 5.119182\n",
      "Training Epoch: 52 [9856/50000]\tLoss: 4.6952\tLR: 5.119437\n",
      "Training Epoch: 52 [9984/50000]\tLoss: 4.7397\tLR: 5.119693\n",
      "Training Epoch: 52 [10112/50000]\tLoss: 4.6485\tLR: 5.119949\n",
      "Training Epoch: 52 [10240/50000]\tLoss: 4.6273\tLR: 5.120205\n",
      "Training Epoch: 52 [10368/50000]\tLoss: 4.6436\tLR: 5.120460\n",
      "Training Epoch: 52 [10496/50000]\tLoss: 4.6985\tLR: 5.120716\n",
      "Training Epoch: 52 [10624/50000]\tLoss: 4.7073\tLR: 5.120972\n",
      "Training Epoch: 52 [10752/50000]\tLoss: 4.7332\tLR: 5.121228\n",
      "Training Epoch: 52 [10880/50000]\tLoss: 4.7536\tLR: 5.121483\n",
      "Training Epoch: 52 [11008/50000]\tLoss: 4.7466\tLR: 5.121739\n",
      "Training Epoch: 52 [11136/50000]\tLoss: 4.7032\tLR: 5.121995\n",
      "Training Epoch: 52 [11264/50000]\tLoss: 4.6801\tLR: 5.122251\n",
      "Training Epoch: 52 [11392/50000]\tLoss: 4.7174\tLR: 5.122506\n",
      "Training Epoch: 52 [11520/50000]\tLoss: 4.6986\tLR: 5.122762\n",
      "Training Epoch: 52 [11648/50000]\tLoss: 4.6926\tLR: 5.123018\n",
      "Training Epoch: 52 [11776/50000]\tLoss: 4.6718\tLR: 5.123274\n",
      "Training Epoch: 52 [11904/50000]\tLoss: 4.7649\tLR: 5.123529\n",
      "Training Epoch: 52 [12032/50000]\tLoss: 4.6417\tLR: 5.123785\n",
      "Training Epoch: 52 [12160/50000]\tLoss: 4.6461\tLR: 5.124041\n",
      "Training Epoch: 52 [12288/50000]\tLoss: 4.7157\tLR: 5.124297\n",
      "Training Epoch: 52 [12416/50000]\tLoss: 4.6617\tLR: 5.124552\n",
      "Training Epoch: 52 [12544/50000]\tLoss: 4.7650\tLR: 5.124808\n",
      "Training Epoch: 52 [12672/50000]\tLoss: 4.6659\tLR: 5.125064\n",
      "Training Epoch: 52 [12800/50000]\tLoss: 4.6802\tLR: 5.125320\n",
      "Training Epoch: 52 [12928/50000]\tLoss: 4.7626\tLR: 5.125575\n",
      "Training Epoch: 52 [13056/50000]\tLoss: 4.7013\tLR: 5.125831\n",
      "Training Epoch: 52 [13184/50000]\tLoss: 4.6985\tLR: 5.126087\n",
      "Training Epoch: 52 [13312/50000]\tLoss: 4.6617\tLR: 5.126343\n",
      "Training Epoch: 52 [13440/50000]\tLoss: 4.6628\tLR: 5.126598\n",
      "Training Epoch: 52 [13568/50000]\tLoss: 4.6648\tLR: 5.126854\n",
      "Training Epoch: 52 [13696/50000]\tLoss: 4.8033\tLR: 5.127110\n",
      "Training Epoch: 52 [13824/50000]\tLoss: 4.6918\tLR: 5.127366\n",
      "Training Epoch: 52 [13952/50000]\tLoss: 4.7016\tLR: 5.127621\n",
      "Training Epoch: 52 [14080/50000]\tLoss: 4.7255\tLR: 5.127877\n",
      "Training Epoch: 52 [14208/50000]\tLoss: 4.6537\tLR: 5.128133\n",
      "Training Epoch: 52 [14336/50000]\tLoss: 4.7303\tLR: 5.128389\n",
      "Training Epoch: 52 [14464/50000]\tLoss: 4.6627\tLR: 5.128645\n",
      "Training Epoch: 52 [14592/50000]\tLoss: 4.7164\tLR: 5.128900\n",
      "Training Epoch: 52 [14720/50000]\tLoss: 4.6564\tLR: 5.129156\n",
      "Training Epoch: 52 [14848/50000]\tLoss: 4.7593\tLR: 5.129412\n",
      "Training Epoch: 52 [14976/50000]\tLoss: 4.7532\tLR: 5.129668\n",
      "Training Epoch: 52 [15104/50000]\tLoss: 4.7022\tLR: 5.129923\n",
      "Training Epoch: 52 [15232/50000]\tLoss: 4.7790\tLR: 5.130179\n",
      "Training Epoch: 52 [15360/50000]\tLoss: 4.7047\tLR: 5.130435\n",
      "Training Epoch: 52 [15488/50000]\tLoss: 4.5864\tLR: 5.130691\n",
      "Training Epoch: 52 [15616/50000]\tLoss: 4.6762\tLR: 5.130946\n",
      "Training Epoch: 52 [15744/50000]\tLoss: 4.7632\tLR: 5.131202\n",
      "Training Epoch: 52 [15872/50000]\tLoss: 4.6574\tLR: 5.131458\n",
      "Training Epoch: 52 [16000/50000]\tLoss: 4.7126\tLR: 5.131714\n",
      "Training Epoch: 52 [16128/50000]\tLoss: 4.7380\tLR: 5.131969\n",
      "Training Epoch: 52 [16256/50000]\tLoss: 4.6670\tLR: 5.132225\n",
      "Training Epoch: 52 [16384/50000]\tLoss: 4.6809\tLR: 5.132481\n",
      "Training Epoch: 52 [16512/50000]\tLoss: 4.6665\tLR: 5.132737\n",
      "Training Epoch: 52 [16640/50000]\tLoss: 4.7089\tLR: 5.132992\n",
      "Training Epoch: 52 [16768/50000]\tLoss: 4.7308\tLR: 5.133248\n",
      "Training Epoch: 52 [16896/50000]\tLoss: 4.7625\tLR: 5.133504\n",
      "Training Epoch: 52 [17024/50000]\tLoss: 4.6968\tLR: 5.133760\n",
      "Training Epoch: 52 [17152/50000]\tLoss: 4.7064\tLR: 5.134015\n",
      "Training Epoch: 52 [17280/50000]\tLoss: 4.7428\tLR: 5.134271\n",
      "Training Epoch: 52 [17408/50000]\tLoss: 4.6584\tLR: 5.134527\n",
      "Training Epoch: 52 [17536/50000]\tLoss: 4.6554\tLR: 5.134783\n",
      "Training Epoch: 52 [17664/50000]\tLoss: 4.7053\tLR: 5.135038\n",
      "Training Epoch: 52 [17792/50000]\tLoss: 4.7668\tLR: 5.135294\n",
      "Training Epoch: 52 [17920/50000]\tLoss: 4.6892\tLR: 5.135550\n",
      "Training Epoch: 52 [18048/50000]\tLoss: 4.6513\tLR: 5.135806\n",
      "Training Epoch: 52 [18176/50000]\tLoss: 4.7480\tLR: 5.136061\n",
      "Training Epoch: 52 [18304/50000]\tLoss: 4.7340\tLR: 5.136317\n",
      "Training Epoch: 52 [18432/50000]\tLoss: 4.7627\tLR: 5.136573\n",
      "Training Epoch: 52 [18560/50000]\tLoss: 4.6497\tLR: 5.136829\n",
      "Training Epoch: 52 [18688/50000]\tLoss: 4.6623\tLR: 5.137084\n",
      "Training Epoch: 52 [18816/50000]\tLoss: 4.7893\tLR: 5.137340\n",
      "Training Epoch: 52 [18944/50000]\tLoss: 4.7419\tLR: 5.137596\n",
      "Training Epoch: 52 [19072/50000]\tLoss: 4.7059\tLR: 5.137852\n",
      "Training Epoch: 52 [19200/50000]\tLoss: 4.7472\tLR: 5.138107\n",
      "Training Epoch: 52 [19328/50000]\tLoss: 4.7197\tLR: 5.138363\n",
      "Training Epoch: 52 [19456/50000]\tLoss: 4.7134\tLR: 5.138619\n",
      "Training Epoch: 52 [19584/50000]\tLoss: 4.7458\tLR: 5.138875\n",
      "Training Epoch: 52 [19712/50000]\tLoss: 4.6734\tLR: 5.139130\n",
      "Training Epoch: 52 [19840/50000]\tLoss: 4.7115\tLR: 5.139386\n",
      "Training Epoch: 52 [19968/50000]\tLoss: 4.7223\tLR: 5.139642\n",
      "Training Epoch: 52 [20096/50000]\tLoss: 4.7946\tLR: 5.139898\n",
      "Training Epoch: 52 [20224/50000]\tLoss: 4.7215\tLR: 5.140153\n",
      "Training Epoch: 52 [20352/50000]\tLoss: 4.6475\tLR: 5.140409\n",
      "Training Epoch: 52 [20480/50000]\tLoss: 4.7742\tLR: 5.140665\n",
      "Training Epoch: 52 [20608/50000]\tLoss: 4.8147\tLR: 5.140921\n",
      "Training Epoch: 52 [20736/50000]\tLoss: 4.7262\tLR: 5.141176\n",
      "Training Epoch: 52 [20864/50000]\tLoss: 4.6941\tLR: 5.141432\n",
      "Training Epoch: 52 [20992/50000]\tLoss: 4.6707\tLR: 5.141688\n",
      "Training Epoch: 52 [21120/50000]\tLoss: 4.7422\tLR: 5.141944\n",
      "Training Epoch: 52 [21248/50000]\tLoss: 4.7522\tLR: 5.142199\n",
      "Training Epoch: 52 [21376/50000]\tLoss: 4.7378\tLR: 5.142455\n",
      "Training Epoch: 52 [21504/50000]\tLoss: 4.7505\tLR: 5.142711\n",
      "Training Epoch: 52 [21632/50000]\tLoss: 4.7752\tLR: 5.142967\n",
      "Training Epoch: 52 [21760/50000]\tLoss: 4.7301\tLR: 5.143223\n",
      "Training Epoch: 52 [21888/50000]\tLoss: 4.6826\tLR: 5.143478\n",
      "Training Epoch: 52 [22016/50000]\tLoss: 4.7369\tLR: 5.143734\n",
      "Training Epoch: 52 [22144/50000]\tLoss: 4.7589\tLR: 5.143990\n",
      "Training Epoch: 52 [22272/50000]\tLoss: 4.7735\tLR: 5.144246\n",
      "Training Epoch: 52 [22400/50000]\tLoss: 4.6678\tLR: 5.144501\n",
      "Training Epoch: 52 [22528/50000]\tLoss: 4.7596\tLR: 5.144757\n",
      "Training Epoch: 52 [22656/50000]\tLoss: 4.7177\tLR: 5.145013\n",
      "Training Epoch: 52 [22784/50000]\tLoss: 4.7365\tLR: 5.145269\n",
      "Training Epoch: 52 [22912/50000]\tLoss: 4.6917\tLR: 5.145524\n",
      "Training Epoch: 52 [23040/50000]\tLoss: 4.7075\tLR: 5.145780\n",
      "Training Epoch: 52 [23168/50000]\tLoss: 4.8157\tLR: 5.146036\n",
      "Training Epoch: 52 [23296/50000]\tLoss: 4.6241\tLR: 5.146292\n",
      "Training Epoch: 52 [23424/50000]\tLoss: 4.6657\tLR: 5.146547\n",
      "Training Epoch: 52 [23552/50000]\tLoss: 4.6593\tLR: 5.146803\n",
      "Training Epoch: 52 [23680/50000]\tLoss: 4.7976\tLR: 5.147059\n",
      "Training Epoch: 52 [23808/50000]\tLoss: 4.7419\tLR: 5.147315\n",
      "Training Epoch: 52 [23936/50000]\tLoss: 4.7843\tLR: 5.147570\n",
      "Training Epoch: 52 [24064/50000]\tLoss: 4.6544\tLR: 5.147826\n",
      "Training Epoch: 52 [24192/50000]\tLoss: 4.7634\tLR: 5.148082\n",
      "Training Epoch: 52 [24320/50000]\tLoss: 4.7065\tLR: 5.148338\n",
      "Training Epoch: 52 [24448/50000]\tLoss: 4.6396\tLR: 5.148593\n",
      "Training Epoch: 52 [24576/50000]\tLoss: 4.6541\tLR: 5.148849\n",
      "Training Epoch: 52 [24704/50000]\tLoss: 4.7328\tLR: 5.149105\n",
      "Training Epoch: 52 [24832/50000]\tLoss: 4.7122\tLR: 5.149361\n",
      "Training Epoch: 52 [24960/50000]\tLoss: 4.7488\tLR: 5.149616\n",
      "Training Epoch: 52 [25088/50000]\tLoss: 4.6801\tLR: 5.149872\n",
      "Training Epoch: 52 [25216/50000]\tLoss: 4.6348\tLR: 5.150128\n",
      "Training Epoch: 52 [25344/50000]\tLoss: 4.6822\tLR: 5.150384\n",
      "Training Epoch: 52 [25472/50000]\tLoss: 4.7131\tLR: 5.150639\n",
      "Training Epoch: 52 [25600/50000]\tLoss: 4.7249\tLR: 5.150895\n",
      "Training Epoch: 52 [25728/50000]\tLoss: 4.7533\tLR: 5.151151\n",
      "Training Epoch: 52 [25856/50000]\tLoss: 4.7310\tLR: 5.151407\n",
      "Training Epoch: 52 [25984/50000]\tLoss: 4.6626\tLR: 5.151662\n",
      "Training Epoch: 52 [26112/50000]\tLoss: 4.7250\tLR: 5.151918\n",
      "Training Epoch: 52 [26240/50000]\tLoss: 4.6887\tLR: 5.152174\n",
      "Training Epoch: 52 [26368/50000]\tLoss: 4.7268\tLR: 5.152430\n",
      "Training Epoch: 52 [26496/50000]\tLoss: 4.6376\tLR: 5.152685\n",
      "Training Epoch: 52 [26624/50000]\tLoss: 4.7035\tLR: 5.152941\n",
      "Training Epoch: 52 [26752/50000]\tLoss: 4.6696\tLR: 5.153197\n",
      "Training Epoch: 52 [26880/50000]\tLoss: 4.6728\tLR: 5.153453\n",
      "Training Epoch: 52 [27008/50000]\tLoss: 4.7076\tLR: 5.153708\n",
      "Training Epoch: 52 [27136/50000]\tLoss: 4.7193\tLR: 5.153964\n",
      "Training Epoch: 52 [27264/50000]\tLoss: 4.6998\tLR: 5.154220\n",
      "Training Epoch: 52 [27392/50000]\tLoss: 4.6099\tLR: 5.154476\n",
      "Training Epoch: 52 [27520/50000]\tLoss: 4.7631\tLR: 5.154731\n",
      "Training Epoch: 52 [27648/50000]\tLoss: 4.6700\tLR: 5.154987\n",
      "Training Epoch: 52 [27776/50000]\tLoss: 4.7114\tLR: 5.155243\n",
      "Training Epoch: 52 [27904/50000]\tLoss: 4.7079\tLR: 5.155499\n",
      "Training Epoch: 52 [28032/50000]\tLoss: 4.6378\tLR: 5.155754\n",
      "Training Epoch: 52 [28160/50000]\tLoss: 4.6276\tLR: 5.156010\n",
      "Training Epoch: 52 [28288/50000]\tLoss: 4.6262\tLR: 5.156266\n",
      "Training Epoch: 52 [28416/50000]\tLoss: 4.7619\tLR: 5.156522\n",
      "Training Epoch: 52 [28544/50000]\tLoss: 4.7223\tLR: 5.156777\n",
      "Training Epoch: 52 [28672/50000]\tLoss: 4.7066\tLR: 5.157033\n",
      "Training Epoch: 52 [28800/50000]\tLoss: 4.7269\tLR: 5.157289\n",
      "Training Epoch: 52 [28928/50000]\tLoss: 4.7155\tLR: 5.157545\n",
      "Training Epoch: 52 [29056/50000]\tLoss: 4.6839\tLR: 5.157801\n",
      "Training Epoch: 52 [29184/50000]\tLoss: 4.6932\tLR: 5.158056\n",
      "Training Epoch: 52 [29312/50000]\tLoss: 4.6888\tLR: 5.158312\n",
      "Training Epoch: 52 [29440/50000]\tLoss: 4.6756\tLR: 5.158568\n",
      "Training Epoch: 52 [29568/50000]\tLoss: 4.6962\tLR: 5.158824\n",
      "Training Epoch: 52 [29696/50000]\tLoss: 4.7269\tLR: 5.159079\n",
      "Training Epoch: 52 [29824/50000]\tLoss: 4.7528\tLR: 5.159335\n",
      "Training Epoch: 52 [29952/50000]\tLoss: 4.6048\tLR: 5.159591\n",
      "Training Epoch: 52 [30080/50000]\tLoss: 4.5983\tLR: 5.159847\n",
      "Training Epoch: 52 [30208/50000]\tLoss: 4.7142\tLR: 5.160102\n",
      "Training Epoch: 52 [30336/50000]\tLoss: 4.6702\tLR: 5.160358\n",
      "Training Epoch: 52 [30464/50000]\tLoss: 4.6447\tLR: 5.160614\n",
      "Training Epoch: 52 [30592/50000]\tLoss: 4.6355\tLR: 5.160870\n",
      "Training Epoch: 52 [30720/50000]\tLoss: 4.6625\tLR: 5.161125\n",
      "Training Epoch: 52 [30848/50000]\tLoss: 4.7257\tLR: 5.161381\n",
      "Training Epoch: 52 [30976/50000]\tLoss: 4.6905\tLR: 5.161637\n",
      "Training Epoch: 52 [31104/50000]\tLoss: 4.6393\tLR: 5.161893\n",
      "Training Epoch: 52 [31232/50000]\tLoss: 4.7731\tLR: 5.162148\n",
      "Training Epoch: 52 [31360/50000]\tLoss: 4.6851\tLR: 5.162404\n",
      "Training Epoch: 52 [31488/50000]\tLoss: 4.7587\tLR: 5.162660\n",
      "Training Epoch: 52 [31616/50000]\tLoss: 4.6750\tLR: 5.162916\n",
      "Training Epoch: 52 [31744/50000]\tLoss: 4.7260\tLR: 5.163171\n",
      "Training Epoch: 52 [31872/50000]\tLoss: 4.7525\tLR: 5.163427\n",
      "Training Epoch: 52 [32000/50000]\tLoss: 4.6748\tLR: 5.163683\n",
      "Training Epoch: 52 [32128/50000]\tLoss: 4.5579\tLR: 5.163939\n",
      "Training Epoch: 52 [32256/50000]\tLoss: 4.7422\tLR: 5.164194\n",
      "Training Epoch: 52 [32384/50000]\tLoss: 4.6928\tLR: 5.164450\n",
      "Training Epoch: 52 [32512/50000]\tLoss: 4.7875\tLR: 5.164706\n",
      "Training Epoch: 52 [32640/50000]\tLoss: 4.7161\tLR: 5.164962\n",
      "Training Epoch: 52 [32768/50000]\tLoss: 4.7754\tLR: 5.165217\n",
      "Training Epoch: 52 [32896/50000]\tLoss: 4.6930\tLR: 5.165473\n",
      "Training Epoch: 52 [33024/50000]\tLoss: 4.7251\tLR: 5.165729\n",
      "Training Epoch: 52 [33152/50000]\tLoss: 4.7028\tLR: 5.165985\n",
      "Training Epoch: 52 [33280/50000]\tLoss: 4.6526\tLR: 5.166240\n",
      "Training Epoch: 52 [33408/50000]\tLoss: 4.6965\tLR: 5.166496\n",
      "Training Epoch: 52 [33536/50000]\tLoss: 4.6644\tLR: 5.166752\n",
      "Training Epoch: 52 [33664/50000]\tLoss: 4.6495\tLR: 5.167008\n",
      "Training Epoch: 52 [33792/50000]\tLoss: 4.7401\tLR: 5.167263\n",
      "Training Epoch: 52 [33920/50000]\tLoss: 4.6925\tLR: 5.167519\n",
      "Training Epoch: 52 [34048/50000]\tLoss: 4.7451\tLR: 5.167775\n",
      "Training Epoch: 52 [34176/50000]\tLoss: 4.6910\tLR: 5.168031\n",
      "Training Epoch: 52 [34304/50000]\tLoss: 4.7017\tLR: 5.168286\n",
      "Training Epoch: 52 [34432/50000]\tLoss: 4.7533\tLR: 5.168542\n",
      "Training Epoch: 52 [34560/50000]\tLoss: 4.7026\tLR: 5.168798\n",
      "Training Epoch: 52 [34688/50000]\tLoss: 4.7294\tLR: 5.169054\n",
      "Training Epoch: 52 [34816/50000]\tLoss: 4.7757\tLR: 5.169309\n",
      "Training Epoch: 52 [34944/50000]\tLoss: 4.6694\tLR: 5.169565\n",
      "Training Epoch: 52 [35072/50000]\tLoss: 4.7390\tLR: 5.169821\n",
      "Training Epoch: 52 [35200/50000]\tLoss: 4.6472\tLR: 5.170077\n",
      "Training Epoch: 52 [35328/50000]\tLoss: 4.6183\tLR: 5.170332\n",
      "Training Epoch: 52 [35456/50000]\tLoss: 4.7020\tLR: 5.170588\n",
      "Training Epoch: 52 [35584/50000]\tLoss: 4.6800\tLR: 5.170844\n",
      "Training Epoch: 52 [35712/50000]\tLoss: 4.6802\tLR: 5.171100\n",
      "Training Epoch: 52 [35840/50000]\tLoss: 4.6464\tLR: 5.171355\n",
      "Training Epoch: 52 [35968/50000]\tLoss: 4.6572\tLR: 5.171611\n",
      "Training Epoch: 52 [36096/50000]\tLoss: 4.7384\tLR: 5.171867\n",
      "Training Epoch: 52 [36224/50000]\tLoss: 4.6579\tLR: 5.172123\n",
      "Training Epoch: 52 [36352/50000]\tLoss: 4.7243\tLR: 5.172379\n",
      "Training Epoch: 52 [36480/50000]\tLoss: 4.7081\tLR: 5.172634\n",
      "Training Epoch: 52 [36608/50000]\tLoss: 4.7568\tLR: 5.172890\n",
      "Training Epoch: 52 [36736/50000]\tLoss: 4.6391\tLR: 5.173146\n",
      "Training Epoch: 52 [36864/50000]\tLoss: 4.6391\tLR: 5.173402\n",
      "Training Epoch: 52 [36992/50000]\tLoss: 4.6722\tLR: 5.173657\n",
      "Training Epoch: 52 [37120/50000]\tLoss: 4.6138\tLR: 5.173913\n",
      "Training Epoch: 52 [37248/50000]\tLoss: 4.7070\tLR: 5.174169\n",
      "Training Epoch: 52 [37376/50000]\tLoss: 4.6680\tLR: 5.174425\n",
      "Training Epoch: 52 [37504/50000]\tLoss: 4.6858\tLR: 5.174680\n",
      "Training Epoch: 52 [37632/50000]\tLoss: 4.7320\tLR: 5.174936\n",
      "Training Epoch: 52 [37760/50000]\tLoss: 4.6993\tLR: 5.175192\n",
      "Training Epoch: 52 [37888/50000]\tLoss: 4.6294\tLR: 5.175448\n",
      "Training Epoch: 52 [38016/50000]\tLoss: 4.8000\tLR: 5.175703\n",
      "Training Epoch: 52 [38144/50000]\tLoss: 4.6864\tLR: 5.175959\n",
      "Training Epoch: 52 [38272/50000]\tLoss: 4.7364\tLR: 5.176215\n",
      "Training Epoch: 52 [38400/50000]\tLoss: 4.7281\tLR: 5.176471\n",
      "Training Epoch: 52 [38528/50000]\tLoss: 4.7081\tLR: 5.176726\n",
      "Training Epoch: 52 [38656/50000]\tLoss: 4.7522\tLR: 5.176982\n",
      "Training Epoch: 52 [38784/50000]\tLoss: 4.7011\tLR: 5.177238\n",
      "Training Epoch: 52 [38912/50000]\tLoss: 4.6690\tLR: 5.177494\n",
      "Training Epoch: 52 [39040/50000]\tLoss: 4.6898\tLR: 5.177749\n",
      "Training Epoch: 52 [39168/50000]\tLoss: 4.7506\tLR: 5.178005\n",
      "Training Epoch: 52 [39296/50000]\tLoss: 4.7120\tLR: 5.178261\n",
      "Training Epoch: 52 [39424/50000]\tLoss: 4.7343\tLR: 5.178517\n",
      "Training Epoch: 52 [39552/50000]\tLoss: 4.6738\tLR: 5.178772\n",
      "Training Epoch: 52 [39680/50000]\tLoss: 4.7235\tLR: 5.179028\n",
      "Training Epoch: 52 [39808/50000]\tLoss: 4.6929\tLR: 5.179284\n",
      "Training Epoch: 52 [39936/50000]\tLoss: 4.6392\tLR: 5.179540\n",
      "Training Epoch: 52 [40064/50000]\tLoss: 4.7249\tLR: 5.179795\n",
      "Training Epoch: 52 [40192/50000]\tLoss: 4.6366\tLR: 5.180051\n",
      "Training Epoch: 52 [40320/50000]\tLoss: 4.6832\tLR: 5.180307\n",
      "Training Epoch: 52 [40448/50000]\tLoss: 4.7132\tLR: 5.180563\n",
      "Training Epoch: 52 [40576/50000]\tLoss: 4.7572\tLR: 5.180818\n",
      "Training Epoch: 52 [40704/50000]\tLoss: 4.7226\tLR: 5.181074\n",
      "Training Epoch: 52 [40832/50000]\tLoss: 4.7397\tLR: 5.181330\n",
      "Training Epoch: 52 [40960/50000]\tLoss: 4.7754\tLR: 5.181586\n",
      "Training Epoch: 52 [41088/50000]\tLoss: 4.6971\tLR: 5.181841\n",
      "Training Epoch: 52 [41216/50000]\tLoss: 4.6643\tLR: 5.182097\n",
      "Training Epoch: 52 [41344/50000]\tLoss: 4.7100\tLR: 5.182353\n",
      "Training Epoch: 52 [41472/50000]\tLoss: 4.6719\tLR: 5.182609\n",
      "Training Epoch: 52 [41600/50000]\tLoss: 4.6733\tLR: 5.182864\n",
      "Training Epoch: 52 [41728/50000]\tLoss: 4.6860\tLR: 5.183120\n",
      "Training Epoch: 52 [41856/50000]\tLoss: 4.7274\tLR: 5.183376\n",
      "Training Epoch: 52 [41984/50000]\tLoss: 4.7026\tLR: 5.183632\n",
      "Training Epoch: 52 [42112/50000]\tLoss: 4.7518\tLR: 5.183887\n",
      "Training Epoch: 52 [42240/50000]\tLoss: 4.6412\tLR: 5.184143\n",
      "Training Epoch: 52 [42368/50000]\tLoss: 4.7374\tLR: 5.184399\n",
      "Training Epoch: 52 [42496/50000]\tLoss: 4.7503\tLR: 5.184655\n",
      "Training Epoch: 52 [42624/50000]\tLoss: 4.6692\tLR: 5.184910\n",
      "Training Epoch: 52 [42752/50000]\tLoss: 4.7111\tLR: 5.185166\n",
      "Training Epoch: 52 [42880/50000]\tLoss: 4.6918\tLR: 5.185422\n",
      "Training Epoch: 52 [43008/50000]\tLoss: 4.7078\tLR: 5.185678\n",
      "Training Epoch: 52 [43136/50000]\tLoss: 4.6504\tLR: 5.185934\n",
      "Training Epoch: 52 [43264/50000]\tLoss: 4.7186\tLR: 5.186189\n",
      "Training Epoch: 52 [43392/50000]\tLoss: 4.6362\tLR: 5.186445\n",
      "Training Epoch: 52 [43520/50000]\tLoss: 4.7050\tLR: 5.186701\n",
      "Training Epoch: 52 [43648/50000]\tLoss: 4.7269\tLR: 5.186957\n",
      "Training Epoch: 52 [43776/50000]\tLoss: 4.6800\tLR: 5.187212\n",
      "Training Epoch: 52 [43904/50000]\tLoss: 4.6857\tLR: 5.187468\n",
      "Training Epoch: 52 [44032/50000]\tLoss: 4.7719\tLR: 5.187724\n",
      "Training Epoch: 52 [44160/50000]\tLoss: 4.6766\tLR: 5.187980\n",
      "Training Epoch: 52 [44288/50000]\tLoss: 4.7127\tLR: 5.188235\n",
      "Training Epoch: 52 [44416/50000]\tLoss: 4.6761\tLR: 5.188491\n",
      "Training Epoch: 52 [44544/50000]\tLoss: 4.6930\tLR: 5.188747\n",
      "Training Epoch: 52 [44672/50000]\tLoss: 4.6524\tLR: 5.189003\n",
      "Training Epoch: 52 [44800/50000]\tLoss: 4.6783\tLR: 5.189258\n",
      "Training Epoch: 52 [44928/50000]\tLoss: 4.7066\tLR: 5.189514\n",
      "Training Epoch: 52 [45056/50000]\tLoss: 4.7111\tLR: 5.189770\n",
      "Training Epoch: 52 [45184/50000]\tLoss: 4.7002\tLR: 5.190026\n",
      "Training Epoch: 52 [45312/50000]\tLoss: 4.6615\tLR: 5.190281\n",
      "Training Epoch: 52 [45440/50000]\tLoss: 4.7375\tLR: 5.190537\n",
      "Training Epoch: 52 [45568/50000]\tLoss: 4.6603\tLR: 5.190793\n",
      "Training Epoch: 52 [45696/50000]\tLoss: 4.6739\tLR: 5.191049\n",
      "Training Epoch: 52 [45824/50000]\tLoss: 4.6424\tLR: 5.191304\n",
      "Training Epoch: 52 [45952/50000]\tLoss: 4.7092\tLR: 5.191560\n",
      "Training Epoch: 52 [46080/50000]\tLoss: 4.7165\tLR: 5.191816\n",
      "Training Epoch: 52 [46208/50000]\tLoss: 4.7213\tLR: 5.192072\n",
      "Training Epoch: 52 [46336/50000]\tLoss: 4.7327\tLR: 5.192327\n",
      "Training Epoch: 52 [46464/50000]\tLoss: 4.8026\tLR: 5.192583\n",
      "Training Epoch: 52 [46592/50000]\tLoss: 4.6709\tLR: 5.192839\n",
      "Training Epoch: 52 [46720/50000]\tLoss: 4.6911\tLR: 5.193095\n",
      "Training Epoch: 52 [46848/50000]\tLoss: 4.6997\tLR: 5.193350\n",
      "Training Epoch: 52 [46976/50000]\tLoss: 4.7272\tLR: 5.193606\n",
      "Training Epoch: 52 [47104/50000]\tLoss: 4.6474\tLR: 5.193862\n",
      "Training Epoch: 52 [47232/50000]\tLoss: 4.7090\tLR: 5.194118\n",
      "Training Epoch: 52 [47360/50000]\tLoss: 4.7398\tLR: 5.194373\n",
      "Training Epoch: 52 [47488/50000]\tLoss: 4.7028\tLR: 5.194629\n",
      "Training Epoch: 52 [47616/50000]\tLoss: 4.7509\tLR: 5.194885\n",
      "Training Epoch: 52 [47744/50000]\tLoss: 4.6730\tLR: 5.195141\n",
      "Training Epoch: 52 [47872/50000]\tLoss: 4.7296\tLR: 5.195396\n",
      "Training Epoch: 52 [48000/50000]\tLoss: 4.7290\tLR: 5.195652\n",
      "Training Epoch: 52 [48128/50000]\tLoss: 4.7029\tLR: 5.195908\n",
      "Training Epoch: 52 [48256/50000]\tLoss: 4.6862\tLR: 5.196164\n",
      "Training Epoch: 52 [48384/50000]\tLoss: 4.7089\tLR: 5.196419\n",
      "Training Epoch: 52 [48512/50000]\tLoss: 4.6599\tLR: 5.196675\n",
      "Training Epoch: 52 [48640/50000]\tLoss: 4.6748\tLR: 5.196931\n",
      "Training Epoch: 52 [48768/50000]\tLoss: 4.7579\tLR: 5.197187\n",
      "Training Epoch: 52 [48896/50000]\tLoss: 4.7171\tLR: 5.197442\n",
      "Training Epoch: 52 [49024/50000]\tLoss: 4.7129\tLR: 5.197698\n",
      "Training Epoch: 52 [49152/50000]\tLoss: 4.7270\tLR: 5.197954\n",
      "Training Epoch: 52 [49280/50000]\tLoss: 4.6831\tLR: 5.198210\n",
      "Training Epoch: 52 [49408/50000]\tLoss: 4.6920\tLR: 5.198465\n",
      "Training Epoch: 52 [49536/50000]\tLoss: 4.7159\tLR: 5.198721\n",
      "Training Epoch: 52 [49664/50000]\tLoss: 4.7035\tLR: 5.198977\n",
      "Training Epoch: 52 [49792/50000]\tLoss: 4.6659\tLR: 5.199233\n",
      "Training Epoch: 52 [49920/50000]\tLoss: 4.6571\tLR: 5.199488\n",
      "Training Epoch: 52 [50000/50000]\tLoss: 4.8199\tLR: 5.199744\n",
      "epoch 52 training time consumed: 488.88s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   72901 GB |   72901 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   72677 GB |   72677 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     223 GB |     223 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   72901 GB |   72901 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   72677 GB |   72677 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     223 GB |     223 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   71875 GB |   71875 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   71651 GB |   71651 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     223 GB |     223 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    7730 K  |    7729 K  |\n",
      "|       from large pool |      24    |      65    |    3295 K  |    3295 K  |\n",
      "|       from small pool |     231    |     274    |    4434 K  |    4434 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    7730 K  |    7729 K  |\n",
      "|       from large pool |      24    |      65    |    3295 K  |    3295 K  |\n",
      "|       from small pool |     231    |     274    |    4434 K  |    4434 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      47    |    4480 K  |    4480 K  |\n",
      "|       from large pool |      10    |      23    |    1583 K  |    1583 K  |\n",
      "|       from small pool |      27    |      35    |    2896 K  |    2896 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 52, Average loss: 0.0372, Accuracy: 0.0100, Time consumed:31.16s\n",
      "\n",
      "Training Epoch: 53 [128/50000]\tLoss: 4.7201\tLR: 0.100000\n",
      "Training Epoch: 53 [256/50000]\tLoss: 4.7066\tLR: 5.200256\n",
      "Training Epoch: 53 [384/50000]\tLoss: 4.6925\tLR: 5.200512\n",
      "Training Epoch: 53 [512/50000]\tLoss: 4.7996\tLR: 5.200767\n",
      "Training Epoch: 53 [640/50000]\tLoss: 4.7439\tLR: 5.201023\n",
      "Training Epoch: 53 [768/50000]\tLoss: 4.6821\tLR: 5.201279\n",
      "Training Epoch: 53 [896/50000]\tLoss: 4.7050\tLR: 5.201535\n",
      "Training Epoch: 53 [1024/50000]\tLoss: 4.7605\tLR: 5.201790\n",
      "Training Epoch: 53 [1152/50000]\tLoss: 4.7284\tLR: 5.202046\n",
      "Training Epoch: 53 [1280/50000]\tLoss: 4.7330\tLR: 5.202302\n",
      "Training Epoch: 53 [1408/50000]\tLoss: 4.7373\tLR: 5.202558\n",
      "Training Epoch: 53 [1536/50000]\tLoss: 4.7043\tLR: 5.202813\n",
      "Training Epoch: 53 [1664/50000]\tLoss: 4.7488\tLR: 5.203069\n",
      "Training Epoch: 53 [1792/50000]\tLoss: 4.7377\tLR: 5.203325\n",
      "Training Epoch: 53 [1920/50000]\tLoss: 4.7269\tLR: 5.203581\n",
      "Training Epoch: 53 [2048/50000]\tLoss: 4.6889\tLR: 5.203836\n",
      "Training Epoch: 53 [2176/50000]\tLoss: 4.6462\tLR: 5.204092\n",
      "Training Epoch: 53 [2304/50000]\tLoss: 4.6445\tLR: 5.204348\n",
      "Training Epoch: 53 [2432/50000]\tLoss: 4.7384\tLR: 5.204604\n",
      "Training Epoch: 53 [2560/50000]\tLoss: 4.6771\tLR: 5.204859\n",
      "Training Epoch: 53 [2688/50000]\tLoss: 4.6979\tLR: 5.205115\n",
      "Training Epoch: 53 [2816/50000]\tLoss: 4.6592\tLR: 5.205371\n",
      "Training Epoch: 53 [2944/50000]\tLoss: 4.7671\tLR: 5.205627\n",
      "Training Epoch: 53 [3072/50000]\tLoss: 4.7184\tLR: 5.205882\n",
      "Training Epoch: 53 [3200/50000]\tLoss: 4.6752\tLR: 5.206138\n",
      "Training Epoch: 53 [3328/50000]\tLoss: 4.7426\tLR: 5.206394\n",
      "Training Epoch: 53 [3456/50000]\tLoss: 4.7227\tLR: 5.206650\n",
      "Training Epoch: 53 [3584/50000]\tLoss: 4.6781\tLR: 5.206905\n",
      "Training Epoch: 53 [3712/50000]\tLoss: 4.6877\tLR: 5.207161\n",
      "Training Epoch: 53 [3840/50000]\tLoss: 4.6974\tLR: 5.207417\n",
      "Training Epoch: 53 [3968/50000]\tLoss: 4.7372\tLR: 5.207673\n",
      "Training Epoch: 53 [4096/50000]\tLoss: 4.7651\tLR: 5.207928\n",
      "Training Epoch: 53 [4224/50000]\tLoss: 4.6483\tLR: 5.208184\n",
      "Training Epoch: 53 [4352/50000]\tLoss: 4.7227\tLR: 5.208440\n",
      "Training Epoch: 53 [4480/50000]\tLoss: 4.7510\tLR: 5.208696\n",
      "Training Epoch: 53 [4608/50000]\tLoss: 4.7138\tLR: 5.208951\n",
      "Training Epoch: 53 [4736/50000]\tLoss: 4.7064\tLR: 5.209207\n",
      "Training Epoch: 53 [4864/50000]\tLoss: 4.7396\tLR: 5.209463\n",
      "Training Epoch: 53 [4992/50000]\tLoss: 4.7162\tLR: 5.209719\n",
      "Training Epoch: 53 [5120/50000]\tLoss: 4.6779\tLR: 5.209974\n",
      "Training Epoch: 53 [5248/50000]\tLoss: 4.7249\tLR: 5.210230\n",
      "Training Epoch: 53 [5376/50000]\tLoss: 4.7247\tLR: 5.210486\n",
      "Training Epoch: 53 [5504/50000]\tLoss: 4.6939\tLR: 5.210742\n",
      "Training Epoch: 53 [5632/50000]\tLoss: 4.7014\tLR: 5.210997\n",
      "Training Epoch: 53 [5760/50000]\tLoss: 4.6999\tLR: 5.211253\n",
      "Training Epoch: 53 [5888/50000]\tLoss: 4.6808\tLR: 5.211509\n",
      "Training Epoch: 53 [6016/50000]\tLoss: 4.6564\tLR: 5.211765\n",
      "Training Epoch: 53 [6144/50000]\tLoss: 4.7627\tLR: 5.212020\n",
      "Training Epoch: 53 [6272/50000]\tLoss: 4.7194\tLR: 5.212276\n",
      "Training Epoch: 53 [6400/50000]\tLoss: 4.6607\tLR: 5.212532\n",
      "Training Epoch: 53 [6528/50000]\tLoss: 4.7105\tLR: 5.212788\n",
      "Training Epoch: 53 [6656/50000]\tLoss: 4.7707\tLR: 5.213043\n",
      "Training Epoch: 53 [6784/50000]\tLoss: 4.7049\tLR: 5.213299\n",
      "Training Epoch: 53 [6912/50000]\tLoss: 4.6822\tLR: 5.213555\n",
      "Training Epoch: 53 [7040/50000]\tLoss: 4.6669\tLR: 5.213811\n",
      "Training Epoch: 53 [7168/50000]\tLoss: 4.6952\tLR: 5.214066\n",
      "Training Epoch: 53 [7296/50000]\tLoss: 4.6419\tLR: 5.214322\n",
      "Training Epoch: 53 [7424/50000]\tLoss: 4.7175\tLR: 5.214578\n",
      "Training Epoch: 53 [7552/50000]\tLoss: 4.6792\tLR: 5.214834\n",
      "Training Epoch: 53 [7680/50000]\tLoss: 4.6765\tLR: 5.215090\n",
      "Training Epoch: 53 [7808/50000]\tLoss: 4.7188\tLR: 5.215345\n",
      "Training Epoch: 53 [7936/50000]\tLoss: 4.7331\tLR: 5.215601\n",
      "Training Epoch: 53 [8064/50000]\tLoss: 4.7502\tLR: 5.215857\n",
      "Training Epoch: 53 [8192/50000]\tLoss: 4.7665\tLR: 5.216113\n",
      "Training Epoch: 53 [8320/50000]\tLoss: 4.6592\tLR: 5.216368\n",
      "Training Epoch: 53 [8448/50000]\tLoss: 4.7599\tLR: 5.216624\n",
      "Training Epoch: 53 [8576/50000]\tLoss: 4.7248\tLR: 5.216880\n",
      "Training Epoch: 53 [8704/50000]\tLoss: 4.6818\tLR: 5.217136\n",
      "Training Epoch: 53 [8832/50000]\tLoss: 4.7067\tLR: 5.217391\n",
      "Training Epoch: 53 [8960/50000]\tLoss: 4.7153\tLR: 5.217647\n",
      "Training Epoch: 53 [9088/50000]\tLoss: 4.7464\tLR: 5.217903\n",
      "Training Epoch: 53 [9216/50000]\tLoss: 4.8187\tLR: 5.218159\n",
      "Training Epoch: 53 [9344/50000]\tLoss: 4.7527\tLR: 5.218414\n",
      "Training Epoch: 53 [9472/50000]\tLoss: 4.6793\tLR: 5.218670\n",
      "Training Epoch: 53 [9600/50000]\tLoss: 4.7442\tLR: 5.218926\n",
      "Training Epoch: 53 [9728/50000]\tLoss: 4.6838\tLR: 5.219182\n",
      "Training Epoch: 53 [9856/50000]\tLoss: 4.7246\tLR: 5.219437\n",
      "Training Epoch: 53 [9984/50000]\tLoss: 4.7321\tLR: 5.219693\n",
      "Training Epoch: 53 [10112/50000]\tLoss: 4.8155\tLR: 5.219949\n",
      "Training Epoch: 53 [10240/50000]\tLoss: 4.7050\tLR: 5.220205\n",
      "Training Epoch: 53 [10368/50000]\tLoss: 4.6548\tLR: 5.220460\n",
      "Training Epoch: 53 [10496/50000]\tLoss: 4.7012\tLR: 5.220716\n",
      "Training Epoch: 53 [10624/50000]\tLoss: 4.6973\tLR: 5.220972\n",
      "Training Epoch: 53 [10752/50000]\tLoss: 4.7206\tLR: 5.221228\n",
      "Training Epoch: 53 [10880/50000]\tLoss: 4.6926\tLR: 5.221483\n",
      "Training Epoch: 53 [11008/50000]\tLoss: 4.7876\tLR: 5.221739\n",
      "Training Epoch: 53 [11136/50000]\tLoss: 4.7477\tLR: 5.221995\n",
      "Training Epoch: 53 [11264/50000]\tLoss: 4.7091\tLR: 5.222251\n",
      "Training Epoch: 53 [11392/50000]\tLoss: 4.7402\tLR: 5.222506\n",
      "Training Epoch: 53 [11520/50000]\tLoss: 4.6881\tLR: 5.222762\n",
      "Training Epoch: 53 [11648/50000]\tLoss: 4.7563\tLR: 5.223018\n",
      "Training Epoch: 53 [11776/50000]\tLoss: 4.7217\tLR: 5.223274\n",
      "Training Epoch: 53 [11904/50000]\tLoss: 4.7245\tLR: 5.223529\n",
      "Training Epoch: 53 [12032/50000]\tLoss: 4.6738\tLR: 5.223785\n",
      "Training Epoch: 53 [12160/50000]\tLoss: 4.6828\tLR: 5.224041\n",
      "Training Epoch: 53 [12288/50000]\tLoss: 4.6802\tLR: 5.224297\n",
      "Training Epoch: 53 [12416/50000]\tLoss: 4.6689\tLR: 5.224552\n",
      "Training Epoch: 53 [12544/50000]\tLoss: 4.6823\tLR: 5.224808\n",
      "Training Epoch: 53 [12672/50000]\tLoss: 4.6626\tLR: 5.225064\n",
      "Training Epoch: 53 [12800/50000]\tLoss: 4.7009\tLR: 5.225320\n",
      "Training Epoch: 53 [12928/50000]\tLoss: 4.7271\tLR: 5.225575\n",
      "Training Epoch: 53 [13056/50000]\tLoss: 4.7727\tLR: 5.225831\n",
      "Training Epoch: 53 [13184/50000]\tLoss: 4.7073\tLR: 5.226087\n",
      "Training Epoch: 53 [13312/50000]\tLoss: 4.7612\tLR: 5.226343\n",
      "Training Epoch: 53 [13440/50000]\tLoss: 4.7315\tLR: 5.226598\n",
      "Training Epoch: 53 [13568/50000]\tLoss: 4.7062\tLR: 5.226854\n",
      "Training Epoch: 53 [13696/50000]\tLoss: 4.6751\tLR: 5.227110\n",
      "Training Epoch: 53 [13824/50000]\tLoss: 4.6651\tLR: 5.227366\n",
      "Training Epoch: 53 [13952/50000]\tLoss: 4.7118\tLR: 5.227621\n",
      "Training Epoch: 53 [14080/50000]\tLoss: 4.7144\tLR: 5.227877\n",
      "Training Epoch: 53 [14208/50000]\tLoss: 4.7423\tLR: 5.228133\n",
      "Training Epoch: 53 [14336/50000]\tLoss: 4.6427\tLR: 5.228389\n",
      "Training Epoch: 53 [14464/50000]\tLoss: 4.6815\tLR: 5.228645\n",
      "Training Epoch: 53 [14592/50000]\tLoss: 4.6909\tLR: 5.228900\n",
      "Training Epoch: 53 [14720/50000]\tLoss: 4.6442\tLR: 5.229156\n",
      "Training Epoch: 53 [14848/50000]\tLoss: 4.6322\tLR: 5.229412\n",
      "Training Epoch: 53 [14976/50000]\tLoss: 4.6716\tLR: 5.229668\n",
      "Training Epoch: 53 [15104/50000]\tLoss: 4.6688\tLR: 5.229923\n",
      "Training Epoch: 53 [15232/50000]\tLoss: 4.6594\tLR: 5.230179\n",
      "Training Epoch: 53 [15360/50000]\tLoss: 4.7263\tLR: 5.230435\n",
      "Training Epoch: 53 [15488/50000]\tLoss: 4.8033\tLR: 5.230691\n",
      "Training Epoch: 53 [15616/50000]\tLoss: 4.7548\tLR: 5.230946\n",
      "Training Epoch: 53 [15744/50000]\tLoss: 4.7644\tLR: 5.231202\n",
      "Training Epoch: 53 [15872/50000]\tLoss: 4.6391\tLR: 5.231458\n",
      "Training Epoch: 53 [16000/50000]\tLoss: 4.7054\tLR: 5.231714\n",
      "Training Epoch: 53 [16128/50000]\tLoss: 4.7376\tLR: 5.231969\n",
      "Training Epoch: 53 [16256/50000]\tLoss: 4.7036\tLR: 5.232225\n",
      "Training Epoch: 53 [16384/50000]\tLoss: 4.6972\tLR: 5.232481\n",
      "Training Epoch: 53 [16512/50000]\tLoss: 4.7323\tLR: 5.232737\n",
      "Training Epoch: 53 [16640/50000]\tLoss: 4.6820\tLR: 5.232992\n",
      "Training Epoch: 53 [16768/50000]\tLoss: 4.6866\tLR: 5.233248\n",
      "Training Epoch: 53 [16896/50000]\tLoss: 4.6951\tLR: 5.233504\n",
      "Training Epoch: 53 [17024/50000]\tLoss: 4.7282\tLR: 5.233760\n",
      "Training Epoch: 53 [17152/50000]\tLoss: 4.7069\tLR: 5.234015\n",
      "Training Epoch: 53 [17280/50000]\tLoss: 4.6555\tLR: 5.234271\n",
      "Training Epoch: 53 [17408/50000]\tLoss: 4.7557\tLR: 5.234527\n",
      "Training Epoch: 53 [17536/50000]\tLoss: 4.7142\tLR: 5.234783\n",
      "Training Epoch: 53 [17664/50000]\tLoss: 4.7518\tLR: 5.235038\n",
      "Training Epoch: 53 [17792/50000]\tLoss: 4.7043\tLR: 5.235294\n",
      "Training Epoch: 53 [17920/50000]\tLoss: 4.7236\tLR: 5.235550\n",
      "Training Epoch: 53 [18048/50000]\tLoss: 4.6186\tLR: 5.235806\n",
      "Training Epoch: 53 [18176/50000]\tLoss: 4.7224\tLR: 5.236061\n",
      "Training Epoch: 53 [18304/50000]\tLoss: 4.6900\tLR: 5.236317\n",
      "Training Epoch: 53 [18432/50000]\tLoss: 4.7128\tLR: 5.236573\n",
      "Training Epoch: 53 [18560/50000]\tLoss: 4.6505\tLR: 5.236829\n",
      "Training Epoch: 53 [18688/50000]\tLoss: 4.7126\tLR: 5.237084\n",
      "Training Epoch: 53 [18816/50000]\tLoss: 4.7374\tLR: 5.237340\n",
      "Training Epoch: 53 [18944/50000]\tLoss: 4.7163\tLR: 5.237596\n",
      "Training Epoch: 53 [19072/50000]\tLoss: 4.6464\tLR: 5.237852\n",
      "Training Epoch: 53 [19200/50000]\tLoss: 4.7526\tLR: 5.238107\n",
      "Training Epoch: 53 [19328/50000]\tLoss: 4.7623\tLR: 5.238363\n",
      "Training Epoch: 53 [19456/50000]\tLoss: 4.6089\tLR: 5.238619\n",
      "Training Epoch: 53 [19584/50000]\tLoss: 4.6935\tLR: 5.238875\n",
      "Training Epoch: 53 [19712/50000]\tLoss: 4.6591\tLR: 5.239130\n",
      "Training Epoch: 53 [19840/50000]\tLoss: 4.7255\tLR: 5.239386\n",
      "Training Epoch: 53 [19968/50000]\tLoss: 4.7051\tLR: 5.239642\n",
      "Training Epoch: 53 [20096/50000]\tLoss: 4.6886\tLR: 5.239898\n",
      "Training Epoch: 53 [20224/50000]\tLoss: 4.7355\tLR: 5.240153\n",
      "Training Epoch: 53 [20352/50000]\tLoss: 4.6960\tLR: 5.240409\n",
      "Training Epoch: 53 [20480/50000]\tLoss: 4.7241\tLR: 5.240665\n",
      "Training Epoch: 53 [20608/50000]\tLoss: 4.6878\tLR: 5.240921\n",
      "Training Epoch: 53 [20736/50000]\tLoss: 4.6672\tLR: 5.241176\n",
      "Training Epoch: 53 [20864/50000]\tLoss: 4.7327\tLR: 5.241432\n",
      "Training Epoch: 53 [20992/50000]\tLoss: 4.8076\tLR: 5.241688\n",
      "Training Epoch: 53 [21120/50000]\tLoss: 4.7490\tLR: 5.241944\n",
      "Training Epoch: 53 [21248/50000]\tLoss: 4.6449\tLR: 5.242199\n",
      "Training Epoch: 53 [21376/50000]\tLoss: 4.6130\tLR: 5.242455\n",
      "Training Epoch: 53 [21504/50000]\tLoss: 4.7390\tLR: 5.242711\n",
      "Training Epoch: 53 [21632/50000]\tLoss: 4.7302\tLR: 5.242967\n",
      "Training Epoch: 53 [21760/50000]\tLoss: 4.7398\tLR: 5.243223\n",
      "Training Epoch: 53 [21888/50000]\tLoss: 4.7189\tLR: 5.243478\n",
      "Training Epoch: 53 [22016/50000]\tLoss: 4.7341\tLR: 5.243734\n",
      "Training Epoch: 53 [22144/50000]\tLoss: 4.7334\tLR: 5.243990\n",
      "Training Epoch: 53 [22272/50000]\tLoss: 4.6231\tLR: 5.244246\n",
      "Training Epoch: 53 [22400/50000]\tLoss: 4.7571\tLR: 5.244501\n",
      "Training Epoch: 53 [22528/50000]\tLoss: 4.7317\tLR: 5.244757\n",
      "Training Epoch: 53 [22656/50000]\tLoss: 4.7107\tLR: 5.245013\n",
      "Training Epoch: 53 [22784/50000]\tLoss: 4.7989\tLR: 5.245269\n",
      "Training Epoch: 53 [22912/50000]\tLoss: 4.7406\tLR: 5.245524\n",
      "Training Epoch: 53 [23040/50000]\tLoss: 4.6632\tLR: 5.245780\n",
      "Training Epoch: 53 [23168/50000]\tLoss: 4.6954\tLR: 5.246036\n",
      "Training Epoch: 53 [23296/50000]\tLoss: 4.7479\tLR: 5.246292\n",
      "Training Epoch: 53 [23424/50000]\tLoss: 4.7106\tLR: 5.246547\n",
      "Training Epoch: 53 [23552/50000]\tLoss: 4.6449\tLR: 5.246803\n",
      "Training Epoch: 53 [23680/50000]\tLoss: 4.5980\tLR: 5.247059\n",
      "Training Epoch: 53 [23808/50000]\tLoss: 4.6896\tLR: 5.247315\n",
      "Training Epoch: 53 [23936/50000]\tLoss: 4.7792\tLR: 5.247570\n",
      "Training Epoch: 53 [24064/50000]\tLoss: 4.7419\tLR: 5.247826\n",
      "Training Epoch: 53 [24192/50000]\tLoss: 4.7456\tLR: 5.248082\n",
      "Training Epoch: 53 [24320/50000]\tLoss: 4.7075\tLR: 5.248338\n",
      "Training Epoch: 53 [24448/50000]\tLoss: 4.7388\tLR: 5.248593\n",
      "Training Epoch: 53 [24576/50000]\tLoss: 4.7299\tLR: 5.248849\n",
      "Training Epoch: 53 [24704/50000]\tLoss: 4.7028\tLR: 5.249105\n",
      "Training Epoch: 53 [24832/50000]\tLoss: 4.6572\tLR: 5.249361\n",
      "Training Epoch: 53 [24960/50000]\tLoss: 4.7016\tLR: 5.249616\n",
      "Training Epoch: 53 [25088/50000]\tLoss: 4.6507\tLR: 5.249872\n",
      "Training Epoch: 53 [25216/50000]\tLoss: 4.7220\tLR: 5.250128\n",
      "Training Epoch: 53 [25344/50000]\tLoss: 4.6620\tLR: 5.250384\n",
      "Training Epoch: 53 [25472/50000]\tLoss: 4.6719\tLR: 5.250639\n",
      "Training Epoch: 53 [25600/50000]\tLoss: 4.7698\tLR: 5.250895\n",
      "Training Epoch: 53 [25728/50000]\tLoss: 4.7119\tLR: 5.251151\n",
      "Training Epoch: 53 [25856/50000]\tLoss: 4.7435\tLR: 5.251407\n",
      "Training Epoch: 53 [25984/50000]\tLoss: 4.7560\tLR: 5.251662\n",
      "Training Epoch: 53 [26112/50000]\tLoss: 4.6965\tLR: 5.251918\n",
      "Training Epoch: 53 [26240/50000]\tLoss: 4.6898\tLR: 5.252174\n",
      "Training Epoch: 53 [26368/50000]\tLoss: 4.6656\tLR: 5.252430\n",
      "Training Epoch: 53 [26496/50000]\tLoss: 4.7586\tLR: 5.252685\n",
      "Training Epoch: 53 [26624/50000]\tLoss: 4.6871\tLR: 5.252941\n",
      "Training Epoch: 53 [26752/50000]\tLoss: 4.7455\tLR: 5.253197\n",
      "Training Epoch: 53 [26880/50000]\tLoss: 4.7251\tLR: 5.253453\n",
      "Training Epoch: 53 [27008/50000]\tLoss: 4.7145\tLR: 5.253708\n",
      "Training Epoch: 53 [27136/50000]\tLoss: 4.6726\tLR: 5.253964\n",
      "Training Epoch: 53 [27264/50000]\tLoss: 4.7053\tLR: 5.254220\n",
      "Training Epoch: 53 [27392/50000]\tLoss: 4.7404\tLR: 5.254476\n",
      "Training Epoch: 53 [27520/50000]\tLoss: 4.7529\tLR: 5.254731\n",
      "Training Epoch: 53 [27648/50000]\tLoss: 4.6867\tLR: 5.254987\n",
      "Training Epoch: 53 [27776/50000]\tLoss: 4.8301\tLR: 5.255243\n",
      "Training Epoch: 53 [27904/50000]\tLoss: 4.7162\tLR: 5.255499\n",
      "Training Epoch: 53 [28032/50000]\tLoss: 4.6247\tLR: 5.255754\n",
      "Training Epoch: 53 [28160/50000]\tLoss: 4.7254\tLR: 5.256010\n",
      "Training Epoch: 53 [28288/50000]\tLoss: 4.7854\tLR: 5.256266\n",
      "Training Epoch: 53 [28416/50000]\tLoss: 4.7568\tLR: 5.256522\n",
      "Training Epoch: 53 [28544/50000]\tLoss: 4.7211\tLR: 5.256777\n",
      "Training Epoch: 53 [28672/50000]\tLoss: 4.7291\tLR: 5.257033\n",
      "Training Epoch: 53 [28800/50000]\tLoss: 4.7450\tLR: 5.257289\n",
      "Training Epoch: 53 [28928/50000]\tLoss: 4.7721\tLR: 5.257545\n",
      "Training Epoch: 53 [29056/50000]\tLoss: 4.7100\tLR: 5.257801\n",
      "Training Epoch: 53 [29184/50000]\tLoss: 4.7179\tLR: 5.258056\n",
      "Training Epoch: 53 [29312/50000]\tLoss: 4.6744\tLR: 5.258312\n",
      "Training Epoch: 53 [29440/50000]\tLoss: 4.7024\tLR: 5.258568\n",
      "Training Epoch: 53 [29568/50000]\tLoss: 4.7055\tLR: 5.258824\n",
      "Training Epoch: 53 [29696/50000]\tLoss: 4.7272\tLR: 5.259079\n",
      "Training Epoch: 53 [29824/50000]\tLoss: 4.6774\tLR: 5.259335\n",
      "Training Epoch: 53 [29952/50000]\tLoss: 4.7222\tLR: 5.259591\n",
      "Training Epoch: 53 [30080/50000]\tLoss: 4.7085\tLR: 5.259847\n",
      "Training Epoch: 53 [30208/50000]\tLoss: 4.6536\tLR: 5.260102\n",
      "Training Epoch: 53 [30336/50000]\tLoss: 4.7005\tLR: 5.260358\n",
      "Training Epoch: 53 [30464/50000]\tLoss: 4.6848\tLR: 5.260614\n",
      "Training Epoch: 53 [30592/50000]\tLoss: 4.6745\tLR: 5.260870\n",
      "Training Epoch: 53 [30720/50000]\tLoss: 4.6354\tLR: 5.261125\n",
      "Training Epoch: 53 [30848/50000]\tLoss: 4.7175\tLR: 5.261381\n",
      "Training Epoch: 53 [30976/50000]\tLoss: 4.7491\tLR: 5.261637\n",
      "Training Epoch: 53 [31104/50000]\tLoss: 4.7524\tLR: 5.261893\n",
      "Training Epoch: 53 [31232/50000]\tLoss: 4.7190\tLR: 5.262148\n",
      "Training Epoch: 53 [31360/50000]\tLoss: 4.7511\tLR: 5.262404\n",
      "Training Epoch: 53 [31488/50000]\tLoss: 4.7683\tLR: 5.262660\n",
      "Training Epoch: 53 [31616/50000]\tLoss: 4.7393\tLR: 5.262916\n",
      "Training Epoch: 53 [31744/50000]\tLoss: 4.6946\tLR: 5.263171\n",
      "Training Epoch: 53 [31872/50000]\tLoss: 4.6630\tLR: 5.263427\n",
      "Training Epoch: 53 [32000/50000]\tLoss: 4.6484\tLR: 5.263683\n",
      "Training Epoch: 53 [32128/50000]\tLoss: 4.6623\tLR: 5.263939\n",
      "Training Epoch: 53 [32256/50000]\tLoss: 4.7813\tLR: 5.264194\n",
      "Training Epoch: 53 [32384/50000]\tLoss: 4.6282\tLR: 5.264450\n",
      "Training Epoch: 53 [32512/50000]\tLoss: 4.6862\tLR: 5.264706\n",
      "Training Epoch: 53 [32640/50000]\tLoss: 4.8008\tLR: 5.264962\n",
      "Training Epoch: 53 [32768/50000]\tLoss: 4.6629\tLR: 5.265217\n",
      "Training Epoch: 53 [32896/50000]\tLoss: 4.6742\tLR: 5.265473\n",
      "Training Epoch: 53 [33024/50000]\tLoss: 4.7054\tLR: 5.265729\n",
      "Training Epoch: 53 [33152/50000]\tLoss: 4.7509\tLR: 5.265985\n",
      "Training Epoch: 53 [33280/50000]\tLoss: 4.6950\tLR: 5.266240\n",
      "Training Epoch: 53 [33408/50000]\tLoss: 4.6674\tLR: 5.266496\n",
      "Training Epoch: 53 [33536/50000]\tLoss: 4.6505\tLR: 5.266752\n",
      "Training Epoch: 53 [33664/50000]\tLoss: 4.7199\tLR: 5.267008\n",
      "Training Epoch: 53 [33792/50000]\tLoss: 4.7187\tLR: 5.267263\n",
      "Training Epoch: 53 [33920/50000]\tLoss: 4.7395\tLR: 5.267519\n",
      "Training Epoch: 53 [34048/50000]\tLoss: 4.6709\tLR: 5.267775\n",
      "Training Epoch: 53 [34176/50000]\tLoss: 4.7187\tLR: 5.268031\n",
      "Training Epoch: 53 [34304/50000]\tLoss: 4.7221\tLR: 5.268286\n",
      "Training Epoch: 53 [34432/50000]\tLoss: 4.6919\tLR: 5.268542\n",
      "Training Epoch: 53 [34560/50000]\tLoss: 4.6841\tLR: 5.268798\n",
      "Training Epoch: 53 [34688/50000]\tLoss: 4.7344\tLR: 5.269054\n",
      "Training Epoch: 53 [34816/50000]\tLoss: 4.7369\tLR: 5.269309\n",
      "Training Epoch: 53 [34944/50000]\tLoss: 4.7309\tLR: 5.269565\n",
      "Training Epoch: 53 [35072/50000]\tLoss: 4.6893\tLR: 5.269821\n",
      "Training Epoch: 53 [35200/50000]\tLoss: 4.7431\tLR: 5.270077\n",
      "Training Epoch: 53 [35328/50000]\tLoss: 4.6782\tLR: 5.270332\n",
      "Training Epoch: 53 [35456/50000]\tLoss: 4.7731\tLR: 5.270588\n",
      "Training Epoch: 53 [35584/50000]\tLoss: 4.6651\tLR: 5.270844\n",
      "Training Epoch: 53 [35712/50000]\tLoss: 4.6821\tLR: 5.271100\n",
      "Training Epoch: 53 [35840/50000]\tLoss: 4.7476\tLR: 5.271355\n",
      "Training Epoch: 53 [35968/50000]\tLoss: 4.6389\tLR: 5.271611\n",
      "Training Epoch: 53 [36096/50000]\tLoss: 4.7358\tLR: 5.271867\n",
      "Training Epoch: 53 [36224/50000]\tLoss: 4.7405\tLR: 5.272123\n",
      "Training Epoch: 53 [36352/50000]\tLoss: 4.8164\tLR: 5.272379\n",
      "Training Epoch: 53 [36480/50000]\tLoss: 4.6590\tLR: 5.272634\n",
      "Training Epoch: 53 [36608/50000]\tLoss: 4.7167\tLR: 5.272890\n",
      "Training Epoch: 53 [36736/50000]\tLoss: 4.7081\tLR: 5.273146\n",
      "Training Epoch: 53 [36864/50000]\tLoss: 4.7175\tLR: 5.273402\n",
      "Training Epoch: 53 [36992/50000]\tLoss: 4.7753\tLR: 5.273657\n",
      "Training Epoch: 53 [37120/50000]\tLoss: 4.6655\tLR: 5.273913\n",
      "Training Epoch: 53 [37248/50000]\tLoss: 4.7217\tLR: 5.274169\n",
      "Training Epoch: 53 [37376/50000]\tLoss: 4.8282\tLR: 5.274425\n",
      "Training Epoch: 53 [37504/50000]\tLoss: 4.7208\tLR: 5.274680\n",
      "Training Epoch: 53 [37632/50000]\tLoss: 4.6438\tLR: 5.274936\n",
      "Training Epoch: 53 [37760/50000]\tLoss: 4.6861\tLR: 5.275192\n",
      "Training Epoch: 53 [37888/50000]\tLoss: 4.7018\tLR: 5.275448\n",
      "Training Epoch: 53 [38016/50000]\tLoss: 4.7319\tLR: 5.275703\n",
      "Training Epoch: 53 [38144/50000]\tLoss: 4.7483\tLR: 5.275959\n",
      "Training Epoch: 53 [38272/50000]\tLoss: 4.7922\tLR: 5.276215\n",
      "Training Epoch: 53 [38400/50000]\tLoss: 4.7850\tLR: 5.276471\n",
      "Training Epoch: 53 [38528/50000]\tLoss: 4.7373\tLR: 5.276726\n",
      "Training Epoch: 53 [38656/50000]\tLoss: 4.6372\tLR: 5.276982\n",
      "Training Epoch: 53 [38784/50000]\tLoss: 4.7006\tLR: 5.277238\n",
      "Training Epoch: 53 [38912/50000]\tLoss: 4.6945\tLR: 5.277494\n",
      "Training Epoch: 53 [39040/50000]\tLoss: 4.7481\tLR: 5.277749\n",
      "Training Epoch: 53 [39168/50000]\tLoss: 4.6568\tLR: 5.278005\n",
      "Training Epoch: 53 [39296/50000]\tLoss: 4.7230\tLR: 5.278261\n",
      "Training Epoch: 53 [39424/50000]\tLoss: 4.7197\tLR: 5.278517\n",
      "Training Epoch: 53 [39552/50000]\tLoss: 4.6991\tLR: 5.278772\n",
      "Training Epoch: 53 [39680/50000]\tLoss: 4.7506\tLR: 5.279028\n",
      "Training Epoch: 53 [39808/50000]\tLoss: 4.6617\tLR: 5.279284\n",
      "Training Epoch: 53 [39936/50000]\tLoss: 4.6741\tLR: 5.279540\n",
      "Training Epoch: 53 [40064/50000]\tLoss: 4.7139\tLR: 5.279795\n",
      "Training Epoch: 53 [40192/50000]\tLoss: 4.6897\tLR: 5.280051\n",
      "Training Epoch: 53 [40320/50000]\tLoss: 4.6837\tLR: 5.280307\n",
      "Training Epoch: 53 [40448/50000]\tLoss: 4.6800\tLR: 5.280563\n",
      "Training Epoch: 53 [40576/50000]\tLoss: 4.6923\tLR: 5.280818\n",
      "Training Epoch: 53 [40704/50000]\tLoss: 4.6508\tLR: 5.281074\n",
      "Training Epoch: 53 [40832/50000]\tLoss: 4.6945\tLR: 5.281330\n",
      "Training Epoch: 53 [40960/50000]\tLoss: 4.6840\tLR: 5.281586\n",
      "Training Epoch: 53 [41088/50000]\tLoss: 4.7186\tLR: 5.281841\n",
      "Training Epoch: 53 [41216/50000]\tLoss: 4.7149\tLR: 5.282097\n",
      "Training Epoch: 53 [41344/50000]\tLoss: 4.7322\tLR: 5.282353\n",
      "Training Epoch: 53 [41472/50000]\tLoss: 4.7186\tLR: 5.282609\n",
      "Training Epoch: 53 [41600/50000]\tLoss: 4.6852\tLR: 5.282864\n",
      "Training Epoch: 53 [41728/50000]\tLoss: 4.6559\tLR: 5.283120\n",
      "Training Epoch: 53 [41856/50000]\tLoss: 4.6775\tLR: 5.283376\n",
      "Training Epoch: 53 [41984/50000]\tLoss: 4.7856\tLR: 5.283632\n",
      "Training Epoch: 53 [42112/50000]\tLoss: 4.7001\tLR: 5.283887\n",
      "Training Epoch: 53 [42240/50000]\tLoss: 4.7762\tLR: 5.284143\n",
      "Training Epoch: 53 [42368/50000]\tLoss: 4.7086\tLR: 5.284399\n",
      "Training Epoch: 53 [42496/50000]\tLoss: 4.6684\tLR: 5.284655\n",
      "Training Epoch: 53 [42624/50000]\tLoss: 4.7782\tLR: 5.284910\n",
      "Training Epoch: 53 [42752/50000]\tLoss: 4.6733\tLR: 5.285166\n",
      "Training Epoch: 53 [42880/50000]\tLoss: 4.6871\tLR: 5.285422\n",
      "Training Epoch: 53 [43008/50000]\tLoss: 4.6916\tLR: 5.285678\n",
      "Training Epoch: 53 [43136/50000]\tLoss: 4.7149\tLR: 5.285934\n",
      "Training Epoch: 53 [43264/50000]\tLoss: 4.7174\tLR: 5.286189\n",
      "Training Epoch: 53 [43392/50000]\tLoss: 4.7335\tLR: 5.286445\n",
      "Training Epoch: 53 [43520/50000]\tLoss: 4.6631\tLR: 5.286701\n",
      "Training Epoch: 53 [43648/50000]\tLoss: 4.7228\tLR: 5.286957\n",
      "Training Epoch: 53 [43776/50000]\tLoss: 4.7396\tLR: 5.287212\n",
      "Training Epoch: 53 [43904/50000]\tLoss: 4.7071\tLR: 5.287468\n",
      "Training Epoch: 53 [44032/50000]\tLoss: 4.6398\tLR: 5.287724\n",
      "Training Epoch: 53 [44160/50000]\tLoss: 4.7374\tLR: 5.287980\n",
      "Training Epoch: 53 [44288/50000]\tLoss: 4.7541\tLR: 5.288235\n",
      "Training Epoch: 53 [44416/50000]\tLoss: 4.7498\tLR: 5.288491\n",
      "Training Epoch: 53 [44544/50000]\tLoss: 4.7864\tLR: 5.288747\n",
      "Training Epoch: 53 [44672/50000]\tLoss: 4.6408\tLR: 5.289003\n",
      "Training Epoch: 53 [44800/50000]\tLoss: 4.6724\tLR: 5.289258\n",
      "Training Epoch: 53 [44928/50000]\tLoss: 4.7031\tLR: 5.289514\n",
      "Training Epoch: 53 [45056/50000]\tLoss: 4.6740\tLR: 5.289770\n",
      "Training Epoch: 53 [45184/50000]\tLoss: 4.6958\tLR: 5.290026\n",
      "Training Epoch: 53 [45312/50000]\tLoss: 4.6827\tLR: 5.290281\n",
      "Training Epoch: 53 [45440/50000]\tLoss: 4.6590\tLR: 5.290537\n",
      "Training Epoch: 53 [45568/50000]\tLoss: 4.7242\tLR: 5.290793\n",
      "Training Epoch: 53 [45696/50000]\tLoss: 4.6671\tLR: 5.291049\n",
      "Training Epoch: 53 [45824/50000]\tLoss: 4.6889\tLR: 5.291304\n",
      "Training Epoch: 53 [45952/50000]\tLoss: 4.7214\tLR: 5.291560\n",
      "Training Epoch: 53 [46080/50000]\tLoss: 4.7213\tLR: 5.291816\n",
      "Training Epoch: 53 [46208/50000]\tLoss: 4.7236\tLR: 5.292072\n",
      "Training Epoch: 53 [46336/50000]\tLoss: 4.6602\tLR: 5.292327\n",
      "Training Epoch: 53 [46464/50000]\tLoss: 4.6574\tLR: 5.292583\n",
      "Training Epoch: 53 [46592/50000]\tLoss: 4.7238\tLR: 5.292839\n",
      "Training Epoch: 53 [46720/50000]\tLoss: 4.6219\tLR: 5.293095\n",
      "Training Epoch: 53 [46848/50000]\tLoss: 4.6934\tLR: 5.293350\n",
      "Training Epoch: 53 [46976/50000]\tLoss: 4.7124\tLR: 5.293606\n",
      "Training Epoch: 53 [47104/50000]\tLoss: 4.6686\tLR: 5.293862\n",
      "Training Epoch: 53 [47232/50000]\tLoss: 4.6968\tLR: 5.294118\n",
      "Training Epoch: 53 [47360/50000]\tLoss: 4.7106\tLR: 5.294373\n",
      "Training Epoch: 53 [47488/50000]\tLoss: 4.7329\tLR: 5.294629\n",
      "Training Epoch: 53 [47616/50000]\tLoss: 4.7383\tLR: 5.294885\n",
      "Training Epoch: 53 [47744/50000]\tLoss: 4.6767\tLR: 5.295141\n",
      "Training Epoch: 53 [47872/50000]\tLoss: 4.7616\tLR: 5.295396\n",
      "Training Epoch: 53 [48000/50000]\tLoss: 4.6720\tLR: 5.295652\n",
      "Training Epoch: 53 [48128/50000]\tLoss: 4.6730\tLR: 5.295908\n",
      "Training Epoch: 53 [48256/50000]\tLoss: 4.7807\tLR: 5.296164\n",
      "Training Epoch: 53 [48384/50000]\tLoss: 4.7632\tLR: 5.296419\n",
      "Training Epoch: 53 [48512/50000]\tLoss: 4.7332\tLR: 5.296675\n",
      "Training Epoch: 53 [48640/50000]\tLoss: 4.6723\tLR: 5.296931\n",
      "Training Epoch: 53 [48768/50000]\tLoss: 4.6251\tLR: 5.297187\n",
      "Training Epoch: 53 [48896/50000]\tLoss: 4.7062\tLR: 5.297442\n",
      "Training Epoch: 53 [49024/50000]\tLoss: 4.6832\tLR: 5.297698\n",
      "Training Epoch: 53 [49152/50000]\tLoss: 4.6235\tLR: 5.297954\n",
      "Training Epoch: 53 [49280/50000]\tLoss: 4.7548\tLR: 5.298210\n",
      "Training Epoch: 53 [49408/50000]\tLoss: 4.6730\tLR: 5.298465\n",
      "Training Epoch: 53 [49536/50000]\tLoss: 4.6594\tLR: 5.298721\n",
      "Training Epoch: 53 [49664/50000]\tLoss: 4.6876\tLR: 5.298977\n",
      "Training Epoch: 53 [49792/50000]\tLoss: 4.7160\tLR: 5.299233\n",
      "Training Epoch: 53 [49920/50000]\tLoss: 4.7815\tLR: 5.299488\n",
      "Training Epoch: 53 [50000/50000]\tLoss: 4.7081\tLR: 5.299744\n",
      "epoch 53 training time consumed: 488.78s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   74303 GB |   74302 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   74075 GB |   74074 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     228 GB |     228 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   74303 GB |   74302 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   74075 GB |   74074 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     228 GB |     228 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   73257 GB |   73257 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   73029 GB |   73029 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     228 GB |     228 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    7878 K  |    7878 K  |\n",
      "|       from large pool |      24    |      65    |    3358 K  |    3358 K  |\n",
      "|       from small pool |     231    |     274    |    4520 K  |    4520 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    7878 K  |    7878 K  |\n",
      "|       from large pool |      24    |      65    |    3358 K  |    3358 K  |\n",
      "|       from small pool |     231    |     274    |    4520 K  |    4520 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      47    |    4566 K  |    4566 K  |\n",
      "|       from large pool |      10    |      23    |    1614 K  |    1614 K  |\n",
      "|       from small pool |      25    |      35    |    2951 K  |    2951 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 53, Average loss: 0.0372, Accuracy: 0.0100, Time consumed:31.24s\n",
      "\n",
      "Training Epoch: 54 [128/50000]\tLoss: 4.7216\tLR: 0.100000\n",
      "Training Epoch: 54 [256/50000]\tLoss: 4.7392\tLR: 5.300256\n",
      "Training Epoch: 54 [384/50000]\tLoss: 4.6741\tLR: 5.300512\n",
      "Training Epoch: 54 [512/50000]\tLoss: 4.7588\tLR: 5.300767\n",
      "Training Epoch: 54 [640/50000]\tLoss: 4.7028\tLR: 5.301023\n",
      "Training Epoch: 54 [768/50000]\tLoss: 4.7248\tLR: 5.301279\n",
      "Training Epoch: 54 [896/50000]\tLoss: 4.6803\tLR: 5.301535\n",
      "Training Epoch: 54 [1024/50000]\tLoss: 4.6960\tLR: 5.301790\n",
      "Training Epoch: 54 [1152/50000]\tLoss: 4.6944\tLR: 5.302046\n",
      "Training Epoch: 54 [1280/50000]\tLoss: 4.7115\tLR: 5.302302\n",
      "Training Epoch: 54 [1408/50000]\tLoss: 4.6952\tLR: 5.302558\n",
      "Training Epoch: 54 [1536/50000]\tLoss: 4.6944\tLR: 5.302813\n",
      "Training Epoch: 54 [1664/50000]\tLoss: 4.6469\tLR: 5.303069\n",
      "Training Epoch: 54 [1792/50000]\tLoss: 4.7262\tLR: 5.303325\n",
      "Training Epoch: 54 [1920/50000]\tLoss: 4.7452\tLR: 5.303581\n",
      "Training Epoch: 54 [2048/50000]\tLoss: 4.7064\tLR: 5.303836\n",
      "Training Epoch: 54 [2176/50000]\tLoss: 4.7643\tLR: 5.304092\n",
      "Training Epoch: 54 [2304/50000]\tLoss: 4.6613\tLR: 5.304348\n",
      "Training Epoch: 54 [2432/50000]\tLoss: 4.7707\tLR: 5.304604\n",
      "Training Epoch: 54 [2560/50000]\tLoss: 4.7091\tLR: 5.304859\n",
      "Training Epoch: 54 [2688/50000]\tLoss: 4.7168\tLR: 5.305115\n",
      "Training Epoch: 54 [2816/50000]\tLoss: 4.7019\tLR: 5.305371\n",
      "Training Epoch: 54 [2944/50000]\tLoss: 4.7097\tLR: 5.305627\n",
      "Training Epoch: 54 [3072/50000]\tLoss: 4.6326\tLR: 5.305882\n",
      "Training Epoch: 54 [3200/50000]\tLoss: 4.7347\tLR: 5.306138\n",
      "Training Epoch: 54 [3328/50000]\tLoss: 4.7268\tLR: 5.306394\n",
      "Training Epoch: 54 [3456/50000]\tLoss: 4.6531\tLR: 5.306650\n",
      "Training Epoch: 54 [3584/50000]\tLoss: 4.7095\tLR: 5.306905\n",
      "Training Epoch: 54 [3712/50000]\tLoss: 4.6893\tLR: 5.307161\n",
      "Training Epoch: 54 [3840/50000]\tLoss: 4.6877\tLR: 5.307417\n",
      "Training Epoch: 54 [3968/50000]\tLoss: 4.6833\tLR: 5.307673\n",
      "Training Epoch: 54 [4096/50000]\tLoss: 4.7129\tLR: 5.307928\n",
      "Training Epoch: 54 [4224/50000]\tLoss: 4.6540\tLR: 5.308184\n",
      "Training Epoch: 54 [4352/50000]\tLoss: 4.7442\tLR: 5.308440\n",
      "Training Epoch: 54 [4480/50000]\tLoss: 4.6526\tLR: 5.308696\n",
      "Training Epoch: 54 [4608/50000]\tLoss: 4.7020\tLR: 5.308951\n",
      "Training Epoch: 54 [4736/50000]\tLoss: 4.7166\tLR: 5.309207\n",
      "Training Epoch: 54 [4864/50000]\tLoss: 4.6692\tLR: 5.309463\n",
      "Training Epoch: 54 [4992/50000]\tLoss: 4.6885\tLR: 5.309719\n",
      "Training Epoch: 54 [5120/50000]\tLoss: 4.6802\tLR: 5.309974\n",
      "Training Epoch: 54 [5248/50000]\tLoss: 4.6027\tLR: 5.310230\n",
      "Training Epoch: 54 [5376/50000]\tLoss: 4.7177\tLR: 5.310486\n",
      "Training Epoch: 54 [5504/50000]\tLoss: 4.7154\tLR: 5.310742\n",
      "Training Epoch: 54 [5632/50000]\tLoss: 4.6834\tLR: 5.310997\n",
      "Training Epoch: 54 [5760/50000]\tLoss: 4.6617\tLR: 5.311253\n",
      "Training Epoch: 54 [5888/50000]\tLoss: 4.7721\tLR: 5.311509\n",
      "Training Epoch: 54 [6016/50000]\tLoss: 4.7138\tLR: 5.311765\n",
      "Training Epoch: 54 [6144/50000]\tLoss: 4.7356\tLR: 5.312020\n",
      "Training Epoch: 54 [6272/50000]\tLoss: 4.6873\tLR: 5.312276\n",
      "Training Epoch: 54 [6400/50000]\tLoss: 4.6348\tLR: 5.312532\n",
      "Training Epoch: 54 [6528/50000]\tLoss: 4.7726\tLR: 5.312788\n",
      "Training Epoch: 54 [6656/50000]\tLoss: 4.6770\tLR: 5.313043\n",
      "Training Epoch: 54 [6784/50000]\tLoss: 4.7653\tLR: 5.313299\n",
      "Training Epoch: 54 [6912/50000]\tLoss: 4.6946\tLR: 5.313555\n",
      "Training Epoch: 54 [7040/50000]\tLoss: 4.7096\tLR: 5.313811\n",
      "Training Epoch: 54 [7168/50000]\tLoss: 4.6581\tLR: 5.314066\n",
      "Training Epoch: 54 [7296/50000]\tLoss: 4.6798\tLR: 5.314322\n",
      "Training Epoch: 54 [7424/50000]\tLoss: 4.6542\tLR: 5.314578\n",
      "Training Epoch: 54 [7552/50000]\tLoss: 4.6680\tLR: 5.314834\n",
      "Training Epoch: 54 [7680/50000]\tLoss: 4.7130\tLR: 5.315090\n",
      "Training Epoch: 54 [7808/50000]\tLoss: 4.6814\tLR: 5.315345\n",
      "Training Epoch: 54 [7936/50000]\tLoss: 4.6532\tLR: 5.315601\n",
      "Training Epoch: 54 [8064/50000]\tLoss: 4.7228\tLR: 5.315857\n",
      "Training Epoch: 54 [8192/50000]\tLoss: 4.6435\tLR: 5.316113\n",
      "Training Epoch: 54 [8320/50000]\tLoss: 4.7788\tLR: 5.316368\n",
      "Training Epoch: 54 [8448/50000]\tLoss: 4.7438\tLR: 5.316624\n",
      "Training Epoch: 54 [8576/50000]\tLoss: 4.6812\tLR: 5.316880\n",
      "Training Epoch: 54 [8704/50000]\tLoss: 4.6837\tLR: 5.317136\n",
      "Training Epoch: 54 [8832/50000]\tLoss: 4.7056\tLR: 5.317391\n",
      "Training Epoch: 54 [8960/50000]\tLoss: 4.7106\tLR: 5.317647\n",
      "Training Epoch: 54 [9088/50000]\tLoss: 4.6714\tLR: 5.317903\n",
      "Training Epoch: 54 [9216/50000]\tLoss: 4.7194\tLR: 5.318159\n",
      "Training Epoch: 54 [9344/50000]\tLoss: 4.7422\tLR: 5.318414\n",
      "Training Epoch: 54 [9472/50000]\tLoss: 4.6194\tLR: 5.318670\n",
      "Training Epoch: 54 [9600/50000]\tLoss: 4.7546\tLR: 5.318926\n",
      "Training Epoch: 54 [9728/50000]\tLoss: 4.6861\tLR: 5.319182\n",
      "Training Epoch: 54 [9856/50000]\tLoss: 4.7137\tLR: 5.319437\n",
      "Training Epoch: 54 [9984/50000]\tLoss: 4.6939\tLR: 5.319693\n",
      "Training Epoch: 54 [10112/50000]\tLoss: 4.7171\tLR: 5.319949\n",
      "Training Epoch: 54 [10240/50000]\tLoss: 4.7369\tLR: 5.320205\n",
      "Training Epoch: 54 [10368/50000]\tLoss: 4.6424\tLR: 5.320460\n",
      "Training Epoch: 54 [10496/50000]\tLoss: 4.6924\tLR: 5.320716\n",
      "Training Epoch: 54 [10624/50000]\tLoss: 4.7498\tLR: 5.320972\n",
      "Training Epoch: 54 [10752/50000]\tLoss: 4.6722\tLR: 5.321228\n",
      "Training Epoch: 54 [10880/50000]\tLoss: 4.7036\tLR: 5.321483\n",
      "Training Epoch: 54 [11008/50000]\tLoss: 4.7071\tLR: 5.321739\n",
      "Training Epoch: 54 [11136/50000]\tLoss: 4.7209\tLR: 5.321995\n",
      "Training Epoch: 54 [11264/50000]\tLoss: 4.6947\tLR: 5.322251\n",
      "Training Epoch: 54 [11392/50000]\tLoss: 4.7137\tLR: 5.322506\n",
      "Training Epoch: 54 [11520/50000]\tLoss: 4.7245\tLR: 5.322762\n",
      "Training Epoch: 54 [11648/50000]\tLoss: 4.6359\tLR: 5.323018\n",
      "Training Epoch: 54 [11776/50000]\tLoss: 4.6224\tLR: 5.323274\n",
      "Training Epoch: 54 [11904/50000]\tLoss: 4.6834\tLR: 5.323529\n",
      "Training Epoch: 54 [12032/50000]\tLoss: 4.6660\tLR: 5.323785\n",
      "Training Epoch: 54 [12160/50000]\tLoss: 4.6997\tLR: 5.324041\n",
      "Training Epoch: 54 [12288/50000]\tLoss: 4.6799\tLR: 5.324297\n",
      "Training Epoch: 54 [12416/50000]\tLoss: 4.6631\tLR: 5.324552\n",
      "Training Epoch: 54 [12544/50000]\tLoss: 4.7047\tLR: 5.324808\n",
      "Training Epoch: 54 [12672/50000]\tLoss: 4.7360\tLR: 5.325064\n",
      "Training Epoch: 54 [12800/50000]\tLoss: 4.7963\tLR: 5.325320\n",
      "Training Epoch: 54 [12928/50000]\tLoss: 4.6811\tLR: 5.325575\n",
      "Training Epoch: 54 [13056/50000]\tLoss: 4.7926\tLR: 5.325831\n",
      "Training Epoch: 54 [13184/50000]\tLoss: 4.7406\tLR: 5.326087\n",
      "Training Epoch: 54 [13312/50000]\tLoss: 4.7225\tLR: 5.326343\n",
      "Training Epoch: 54 [13440/50000]\tLoss: 4.6425\tLR: 5.326598\n",
      "Training Epoch: 54 [13568/50000]\tLoss: 4.7167\tLR: 5.326854\n",
      "Training Epoch: 54 [13696/50000]\tLoss: 4.7257\tLR: 5.327110\n",
      "Training Epoch: 54 [13824/50000]\tLoss: 4.7542\tLR: 5.327366\n",
      "Training Epoch: 54 [13952/50000]\tLoss: 4.7130\tLR: 5.327621\n",
      "Training Epoch: 54 [14080/50000]\tLoss: 4.7680\tLR: 5.327877\n",
      "Training Epoch: 54 [14208/50000]\tLoss: 4.7181\tLR: 5.328133\n",
      "Training Epoch: 54 [14336/50000]\tLoss: 4.6434\tLR: 5.328389\n",
      "Training Epoch: 54 [14464/50000]\tLoss: 4.7098\tLR: 5.328645\n",
      "Training Epoch: 54 [14592/50000]\tLoss: 4.7035\tLR: 5.328900\n",
      "Training Epoch: 54 [14720/50000]\tLoss: 4.6655\tLR: 5.329156\n",
      "Training Epoch: 54 [14848/50000]\tLoss: 4.7366\tLR: 5.329412\n",
      "Training Epoch: 54 [14976/50000]\tLoss: 4.6995\tLR: 5.329668\n",
      "Training Epoch: 54 [15104/50000]\tLoss: 4.7418\tLR: 5.329923\n",
      "Training Epoch: 54 [15232/50000]\tLoss: 4.6910\tLR: 5.330179\n",
      "Training Epoch: 54 [15360/50000]\tLoss: 4.6636\tLR: 5.330435\n",
      "Training Epoch: 54 [15488/50000]\tLoss: 4.7369\tLR: 5.330691\n",
      "Training Epoch: 54 [15616/50000]\tLoss: 4.8065\tLR: 5.330946\n",
      "Training Epoch: 54 [15744/50000]\tLoss: 4.7348\tLR: 5.331202\n",
      "Training Epoch: 54 [15872/50000]\tLoss: 4.7353\tLR: 5.331458\n",
      "Training Epoch: 54 [16000/50000]\tLoss: 4.6902\tLR: 5.331714\n",
      "Training Epoch: 54 [16128/50000]\tLoss: 4.7037\tLR: 5.331969\n",
      "Training Epoch: 54 [16256/50000]\tLoss: 4.6854\tLR: 5.332225\n",
      "Training Epoch: 54 [16384/50000]\tLoss: 4.7083\tLR: 5.332481\n",
      "Training Epoch: 54 [16512/50000]\tLoss: 4.7077\tLR: 5.332737\n",
      "Training Epoch: 54 [16640/50000]\tLoss: 4.6896\tLR: 5.332992\n",
      "Training Epoch: 54 [16768/50000]\tLoss: 4.6398\tLR: 5.333248\n",
      "Training Epoch: 54 [16896/50000]\tLoss: 4.7015\tLR: 5.333504\n",
      "Training Epoch: 54 [17024/50000]\tLoss: 4.6887\tLR: 5.333760\n",
      "Training Epoch: 54 [17152/50000]\tLoss: 4.8071\tLR: 5.334015\n",
      "Training Epoch: 54 [17280/50000]\tLoss: 4.7319\tLR: 5.334271\n",
      "Training Epoch: 54 [17408/50000]\tLoss: 4.7198\tLR: 5.334527\n",
      "Training Epoch: 54 [17536/50000]\tLoss: 4.7112\tLR: 5.334783\n",
      "Training Epoch: 54 [17664/50000]\tLoss: 4.6679\tLR: 5.335038\n",
      "Training Epoch: 54 [17792/50000]\tLoss: 4.6639\tLR: 5.335294\n",
      "Training Epoch: 54 [17920/50000]\tLoss: 4.6805\tLR: 5.335550\n",
      "Training Epoch: 54 [18048/50000]\tLoss: 4.7274\tLR: 5.335806\n",
      "Training Epoch: 54 [18176/50000]\tLoss: 4.7704\tLR: 5.336061\n",
      "Training Epoch: 54 [18304/50000]\tLoss: 4.6977\tLR: 5.336317\n",
      "Training Epoch: 54 [18432/50000]\tLoss: 4.6661\tLR: 5.336573\n",
      "Training Epoch: 54 [18560/50000]\tLoss: 4.8177\tLR: 5.336829\n",
      "Training Epoch: 54 [18688/50000]\tLoss: 4.6757\tLR: 5.337084\n",
      "Training Epoch: 54 [18816/50000]\tLoss: 4.6916\tLR: 5.337340\n",
      "Training Epoch: 54 [18944/50000]\tLoss: 4.7184\tLR: 5.337596\n",
      "Training Epoch: 54 [19072/50000]\tLoss: 4.6112\tLR: 5.337852\n",
      "Training Epoch: 54 [19200/50000]\tLoss: 4.6679\tLR: 5.338107\n",
      "Training Epoch: 54 [19328/50000]\tLoss: 4.6635\tLR: 5.338363\n",
      "Training Epoch: 54 [19456/50000]\tLoss: 4.6865\tLR: 5.338619\n",
      "Training Epoch: 54 [19584/50000]\tLoss: 4.7737\tLR: 5.338875\n",
      "Training Epoch: 54 [19712/50000]\tLoss: 4.7678\tLR: 5.339130\n",
      "Training Epoch: 54 [19840/50000]\tLoss: 4.7448\tLR: 5.339386\n",
      "Training Epoch: 54 [19968/50000]\tLoss: 4.7151\tLR: 5.339642\n",
      "Training Epoch: 54 [20096/50000]\tLoss: 4.6860\tLR: 5.339898\n",
      "Training Epoch: 54 [20224/50000]\tLoss: 4.6539\tLR: 5.340153\n",
      "Training Epoch: 54 [20352/50000]\tLoss: 4.6487\tLR: 5.340409\n",
      "Training Epoch: 54 [20480/50000]\tLoss: 4.6566\tLR: 5.340665\n",
      "Training Epoch: 54 [20608/50000]\tLoss: 4.7317\tLR: 5.340921\n",
      "Training Epoch: 54 [20736/50000]\tLoss: 4.6706\tLR: 5.341176\n",
      "Training Epoch: 54 [20864/50000]\tLoss: 4.7650\tLR: 5.341432\n",
      "Training Epoch: 54 [20992/50000]\tLoss: 4.7863\tLR: 5.341688\n",
      "Training Epoch: 54 [21120/50000]\tLoss: 4.6853\tLR: 5.341944\n",
      "Training Epoch: 54 [21248/50000]\tLoss: 4.6575\tLR: 5.342199\n",
      "Training Epoch: 54 [21376/50000]\tLoss: 4.7053\tLR: 5.342455\n",
      "Training Epoch: 54 [21504/50000]\tLoss: 4.7205\tLR: 5.342711\n",
      "Training Epoch: 54 [21632/50000]\tLoss: 4.6302\tLR: 5.342967\n",
      "Training Epoch: 54 [21760/50000]\tLoss: 4.6987\tLR: 5.343223\n",
      "Training Epoch: 54 [21888/50000]\tLoss: 4.7668\tLR: 5.343478\n",
      "Training Epoch: 54 [22016/50000]\tLoss: 4.6310\tLR: 5.343734\n",
      "Training Epoch: 54 [22144/50000]\tLoss: 4.7512\tLR: 5.343990\n",
      "Training Epoch: 54 [22272/50000]\tLoss: 4.8032\tLR: 5.344246\n",
      "Training Epoch: 54 [22400/50000]\tLoss: 4.6608\tLR: 5.344501\n",
      "Training Epoch: 54 [22528/50000]\tLoss: 4.6360\tLR: 5.344757\n",
      "Training Epoch: 54 [22656/50000]\tLoss: 4.7314\tLR: 5.345013\n",
      "Training Epoch: 54 [22784/50000]\tLoss: 4.8075\tLR: 5.345269\n",
      "Training Epoch: 54 [22912/50000]\tLoss: 4.7521\tLR: 5.345524\n",
      "Training Epoch: 54 [23040/50000]\tLoss: 4.6647\tLR: 5.345780\n",
      "Training Epoch: 54 [23168/50000]\tLoss: 4.6174\tLR: 5.346036\n",
      "Training Epoch: 54 [23296/50000]\tLoss: 4.6359\tLR: 5.346292\n",
      "Training Epoch: 54 [23424/50000]\tLoss: 4.7061\tLR: 5.346547\n",
      "Training Epoch: 54 [23552/50000]\tLoss: 4.6590\tLR: 5.346803\n",
      "Training Epoch: 54 [23680/50000]\tLoss: 4.6991\tLR: 5.347059\n",
      "Training Epoch: 54 [23808/50000]\tLoss: 4.6553\tLR: 5.347315\n",
      "Training Epoch: 54 [23936/50000]\tLoss: 4.6581\tLR: 5.347570\n",
      "Training Epoch: 54 [24064/50000]\tLoss: 4.6898\tLR: 5.347826\n",
      "Training Epoch: 54 [24192/50000]\tLoss: 4.7758\tLR: 5.348082\n",
      "Training Epoch: 54 [24320/50000]\tLoss: 4.7005\tLR: 5.348338\n",
      "Training Epoch: 54 [24448/50000]\tLoss: 4.7538\tLR: 5.348593\n",
      "Training Epoch: 54 [24576/50000]\tLoss: 4.7418\tLR: 5.348849\n",
      "Training Epoch: 54 [24704/50000]\tLoss: 4.6254\tLR: 5.349105\n",
      "Training Epoch: 54 [24832/50000]\tLoss: 4.6550\tLR: 5.349361\n",
      "Training Epoch: 54 [24960/50000]\tLoss: 4.6370\tLR: 5.349616\n",
      "Training Epoch: 54 [25088/50000]\tLoss: 4.7067\tLR: 5.349872\n",
      "Training Epoch: 54 [25216/50000]\tLoss: 4.7624\tLR: 5.350128\n",
      "Training Epoch: 54 [25344/50000]\tLoss: 4.7576\tLR: 5.350384\n",
      "Training Epoch: 54 [25472/50000]\tLoss: 4.7182\tLR: 5.350639\n",
      "Training Epoch: 54 [25600/50000]\tLoss: 4.6579\tLR: 5.350895\n",
      "Training Epoch: 54 [25728/50000]\tLoss: 4.7791\tLR: 5.351151\n",
      "Training Epoch: 54 [25856/50000]\tLoss: 4.6663\tLR: 5.351407\n",
      "Training Epoch: 54 [25984/50000]\tLoss: 4.7139\tLR: 5.351662\n",
      "Training Epoch: 54 [26112/50000]\tLoss: 4.6384\tLR: 5.351918\n",
      "Training Epoch: 54 [26240/50000]\tLoss: 4.7330\tLR: 5.352174\n",
      "Training Epoch: 54 [26368/50000]\tLoss: 4.7322\tLR: 5.352430\n",
      "Training Epoch: 54 [26496/50000]\tLoss: 4.7032\tLR: 5.352685\n",
      "Training Epoch: 54 [26624/50000]\tLoss: 4.6616\tLR: 5.352941\n",
      "Training Epoch: 54 [26752/50000]\tLoss: 4.6886\tLR: 5.353197\n",
      "Training Epoch: 54 [26880/50000]\tLoss: 4.7185\tLR: 5.353453\n",
      "Training Epoch: 54 [27008/50000]\tLoss: 4.7454\tLR: 5.353708\n",
      "Training Epoch: 54 [27136/50000]\tLoss: 4.6753\tLR: 5.353964\n",
      "Training Epoch: 54 [27264/50000]\tLoss: 4.7208\tLR: 5.354220\n",
      "Training Epoch: 54 [27392/50000]\tLoss: 4.6936\tLR: 5.354476\n",
      "Training Epoch: 54 [27520/50000]\tLoss: 4.6972\tLR: 5.354731\n",
      "Training Epoch: 54 [27648/50000]\tLoss: 4.6911\tLR: 5.354987\n",
      "Training Epoch: 54 [27776/50000]\tLoss: 4.7112\tLR: 5.355243\n",
      "Training Epoch: 54 [27904/50000]\tLoss: 4.6848\tLR: 5.355499\n",
      "Training Epoch: 54 [28032/50000]\tLoss: 4.6990\tLR: 5.355754\n",
      "Training Epoch: 54 [28160/50000]\tLoss: 4.6835\tLR: 5.356010\n",
      "Training Epoch: 54 [28288/50000]\tLoss: 4.7536\tLR: 5.356266\n",
      "Training Epoch: 54 [28416/50000]\tLoss: 4.6162\tLR: 5.356522\n",
      "Training Epoch: 54 [28544/50000]\tLoss: 4.7570\tLR: 5.356777\n",
      "Training Epoch: 54 [28672/50000]\tLoss: 4.7103\tLR: 5.357033\n",
      "Training Epoch: 54 [28800/50000]\tLoss: 4.6918\tLR: 5.357289\n",
      "Training Epoch: 54 [28928/50000]\tLoss: 4.7375\tLR: 5.357545\n",
      "Training Epoch: 54 [29056/50000]\tLoss: 4.7222\tLR: 5.357801\n",
      "Training Epoch: 54 [29184/50000]\tLoss: 4.6949\tLR: 5.358056\n",
      "Training Epoch: 54 [29312/50000]\tLoss: 4.7228\tLR: 5.358312\n",
      "Training Epoch: 54 [29440/50000]\tLoss: 4.6645\tLR: 5.358568\n",
      "Training Epoch: 54 [29568/50000]\tLoss: 4.6561\tLR: 5.358824\n",
      "Training Epoch: 54 [29696/50000]\tLoss: 4.6694\tLR: 5.359079\n",
      "Training Epoch: 54 [29824/50000]\tLoss: 4.7356\tLR: 5.359335\n",
      "Training Epoch: 54 [29952/50000]\tLoss: 4.6827\tLR: 5.359591\n",
      "Training Epoch: 54 [30080/50000]\tLoss: 4.7470\tLR: 5.359847\n",
      "Training Epoch: 54 [30208/50000]\tLoss: 4.7461\tLR: 5.360102\n",
      "Training Epoch: 54 [30336/50000]\tLoss: 4.6686\tLR: 5.360358\n",
      "Training Epoch: 54 [30464/50000]\tLoss: 4.7528\tLR: 5.360614\n",
      "Training Epoch: 54 [30592/50000]\tLoss: 4.6957\tLR: 5.360870\n",
      "Training Epoch: 54 [30720/50000]\tLoss: 4.7636\tLR: 5.361125\n",
      "Training Epoch: 54 [30848/50000]\tLoss: 4.6787\tLR: 5.361381\n",
      "Training Epoch: 54 [30976/50000]\tLoss: 4.7331\tLR: 5.361637\n",
      "Training Epoch: 54 [31104/50000]\tLoss: 4.7989\tLR: 5.361893\n",
      "Training Epoch: 54 [31232/50000]\tLoss: 4.6754\tLR: 5.362148\n",
      "Training Epoch: 54 [31360/50000]\tLoss: 4.7376\tLR: 5.362404\n",
      "Training Epoch: 54 [31488/50000]\tLoss: 4.6574\tLR: 5.362660\n",
      "Training Epoch: 54 [31616/50000]\tLoss: 4.6027\tLR: 5.362916\n",
      "Training Epoch: 54 [31744/50000]\tLoss: 4.6896\tLR: 5.363171\n",
      "Training Epoch: 54 [31872/50000]\tLoss: 4.7359\tLR: 5.363427\n",
      "Training Epoch: 54 [32000/50000]\tLoss: 4.6990\tLR: 5.363683\n",
      "Training Epoch: 54 [32128/50000]\tLoss: 4.7034\tLR: 5.363939\n",
      "Training Epoch: 54 [32256/50000]\tLoss: 4.7613\tLR: 5.364194\n",
      "Training Epoch: 54 [32384/50000]\tLoss: 4.7217\tLR: 5.364450\n",
      "Training Epoch: 54 [32512/50000]\tLoss: 4.7283\tLR: 5.364706\n",
      "Training Epoch: 54 [32640/50000]\tLoss: 4.7418\tLR: 5.364962\n",
      "Training Epoch: 54 [32768/50000]\tLoss: 4.6666\tLR: 5.365217\n",
      "Training Epoch: 54 [32896/50000]\tLoss: 4.7157\tLR: 5.365473\n",
      "Training Epoch: 54 [33024/50000]\tLoss: 4.6763\tLR: 5.365729\n",
      "Training Epoch: 54 [33152/50000]\tLoss: 4.6852\tLR: 5.365985\n",
      "Training Epoch: 54 [33280/50000]\tLoss: 4.7407\tLR: 5.366240\n",
      "Training Epoch: 54 [33408/50000]\tLoss: 4.7411\tLR: 5.366496\n",
      "Training Epoch: 54 [33536/50000]\tLoss: 4.6887\tLR: 5.366752\n",
      "Training Epoch: 54 [33664/50000]\tLoss: 4.7056\tLR: 5.367008\n",
      "Training Epoch: 54 [33792/50000]\tLoss: 4.7220\tLR: 5.367263\n",
      "Training Epoch: 54 [33920/50000]\tLoss: 4.7247\tLR: 5.367519\n",
      "Training Epoch: 54 [34048/50000]\tLoss: 4.7688\tLR: 5.367775\n",
      "Training Epoch: 54 [34176/50000]\tLoss: 4.6635\tLR: 5.368031\n",
      "Training Epoch: 54 [34304/50000]\tLoss: 4.7025\tLR: 5.368286\n",
      "Training Epoch: 54 [34432/50000]\tLoss: 4.6654\tLR: 5.368542\n",
      "Training Epoch: 54 [34560/50000]\tLoss: 4.6447\tLR: 5.368798\n",
      "Training Epoch: 54 [34688/50000]\tLoss: 4.6999\tLR: 5.369054\n",
      "Training Epoch: 54 [34816/50000]\tLoss: 4.7457\tLR: 5.369309\n",
      "Training Epoch: 54 [34944/50000]\tLoss: 4.7577\tLR: 5.369565\n",
      "Training Epoch: 54 [35072/50000]\tLoss: 4.7408\tLR: 5.369821\n",
      "Training Epoch: 54 [35200/50000]\tLoss: 4.6988\tLR: 5.370077\n",
      "Training Epoch: 54 [35328/50000]\tLoss: 4.7381\tLR: 5.370332\n",
      "Training Epoch: 54 [35456/50000]\tLoss: 4.7109\tLR: 5.370588\n",
      "Training Epoch: 54 [35584/50000]\tLoss: 4.7101\tLR: 5.370844\n",
      "Training Epoch: 54 [35712/50000]\tLoss: 4.7162\tLR: 5.371100\n",
      "Training Epoch: 54 [35840/50000]\tLoss: 4.7721\tLR: 5.371355\n",
      "Training Epoch: 54 [35968/50000]\tLoss: 4.7189\tLR: 5.371611\n",
      "Training Epoch: 54 [36096/50000]\tLoss: 4.6656\tLR: 5.371867\n",
      "Training Epoch: 54 [36224/50000]\tLoss: 4.6707\tLR: 5.372123\n",
      "Training Epoch: 54 [36352/50000]\tLoss: 4.7037\tLR: 5.372379\n",
      "Training Epoch: 54 [36480/50000]\tLoss: 4.6850\tLR: 5.372634\n",
      "Training Epoch: 54 [36608/50000]\tLoss: 4.7490\tLR: 5.372890\n",
      "Training Epoch: 54 [36736/50000]\tLoss: 4.6947\tLR: 5.373146\n",
      "Training Epoch: 54 [36864/50000]\tLoss: 4.6846\tLR: 5.373402\n",
      "Training Epoch: 54 [36992/50000]\tLoss: 4.7839\tLR: 5.373657\n",
      "Training Epoch: 54 [37120/50000]\tLoss: 4.6807\tLR: 5.373913\n",
      "Training Epoch: 54 [37248/50000]\tLoss: 4.6420\tLR: 5.374169\n",
      "Training Epoch: 54 [37376/50000]\tLoss: 4.6845\tLR: 5.374425\n",
      "Training Epoch: 54 [37504/50000]\tLoss: 4.6537\tLR: 5.374680\n",
      "Training Epoch: 54 [37632/50000]\tLoss: 4.6828\tLR: 5.374936\n",
      "Training Epoch: 54 [37760/50000]\tLoss: 4.7334\tLR: 5.375192\n",
      "Training Epoch: 54 [37888/50000]\tLoss: 4.7047\tLR: 5.375448\n",
      "Training Epoch: 54 [38016/50000]\tLoss: 4.8104\tLR: 5.375703\n",
      "Training Epoch: 54 [38144/50000]\tLoss: 4.6254\tLR: 5.375959\n",
      "Training Epoch: 54 [38272/50000]\tLoss: 4.7694\tLR: 5.376215\n",
      "Training Epoch: 54 [38400/50000]\tLoss: 4.6941\tLR: 5.376471\n",
      "Training Epoch: 54 [38528/50000]\tLoss: 4.7209\tLR: 5.376726\n",
      "Training Epoch: 54 [38656/50000]\tLoss: 4.6613\tLR: 5.376982\n",
      "Training Epoch: 54 [38784/50000]\tLoss: 4.6588\tLR: 5.377238\n",
      "Training Epoch: 54 [38912/50000]\tLoss: 4.7665\tLR: 5.377494\n",
      "Training Epoch: 54 [39040/50000]\tLoss: 4.6872\tLR: 5.377749\n",
      "Training Epoch: 54 [39168/50000]\tLoss: 4.7486\tLR: 5.378005\n",
      "Training Epoch: 54 [39296/50000]\tLoss: 4.6878\tLR: 5.378261\n",
      "Training Epoch: 54 [39424/50000]\tLoss: 4.7426\tLR: 5.378517\n",
      "Training Epoch: 54 [39552/50000]\tLoss: 4.7283\tLR: 5.378772\n",
      "Training Epoch: 54 [39680/50000]\tLoss: 4.7433\tLR: 5.379028\n",
      "Training Epoch: 54 [39808/50000]\tLoss: 4.7103\tLR: 5.379284\n",
      "Training Epoch: 54 [39936/50000]\tLoss: 4.6994\tLR: 5.379540\n",
      "Training Epoch: 54 [40064/50000]\tLoss: 4.6980\tLR: 5.379795\n",
      "Training Epoch: 54 [40192/50000]\tLoss: 4.7202\tLR: 5.380051\n",
      "Training Epoch: 54 [40320/50000]\tLoss: 4.7280\tLR: 5.380307\n",
      "Training Epoch: 54 [40448/50000]\tLoss: 4.5816\tLR: 5.380563\n",
      "Training Epoch: 54 [40576/50000]\tLoss: 4.6238\tLR: 5.380818\n",
      "Training Epoch: 54 [40704/50000]\tLoss: 4.7206\tLR: 5.381074\n",
      "Training Epoch: 54 [40832/50000]\tLoss: 4.7435\tLR: 5.381330\n",
      "Training Epoch: 54 [40960/50000]\tLoss: 4.6282\tLR: 5.381586\n",
      "Training Epoch: 54 [41088/50000]\tLoss: 4.7266\tLR: 5.381841\n",
      "Training Epoch: 54 [41216/50000]\tLoss: 4.6815\tLR: 5.382097\n",
      "Training Epoch: 54 [41344/50000]\tLoss: 4.6491\tLR: 5.382353\n",
      "Training Epoch: 54 [41472/50000]\tLoss: 4.7142\tLR: 5.382609\n",
      "Training Epoch: 54 [41600/50000]\tLoss: 4.7038\tLR: 5.382864\n",
      "Training Epoch: 54 [41728/50000]\tLoss: 4.7102\tLR: 5.383120\n",
      "Training Epoch: 54 [41856/50000]\tLoss: 4.6850\tLR: 5.383376\n",
      "Training Epoch: 54 [41984/50000]\tLoss: 4.6503\tLR: 5.383632\n",
      "Training Epoch: 54 [42112/50000]\tLoss: 4.7209\tLR: 5.383887\n",
      "Training Epoch: 54 [42240/50000]\tLoss: 4.7563\tLR: 5.384143\n",
      "Training Epoch: 54 [42368/50000]\tLoss: 4.7538\tLR: 5.384399\n",
      "Training Epoch: 54 [42496/50000]\tLoss: 4.6735\tLR: 5.384655\n",
      "Training Epoch: 54 [42624/50000]\tLoss: 4.7210\tLR: 5.384910\n",
      "Training Epoch: 54 [42752/50000]\tLoss: 4.7041\tLR: 5.385166\n",
      "Training Epoch: 54 [42880/50000]\tLoss: 4.6834\tLR: 5.385422\n",
      "Training Epoch: 54 [43008/50000]\tLoss: 4.7069\tLR: 5.385678\n",
      "Training Epoch: 54 [43136/50000]\tLoss: 4.6801\tLR: 5.385934\n",
      "Training Epoch: 54 [43264/50000]\tLoss: 4.6308\tLR: 5.386189\n",
      "Training Epoch: 54 [43392/50000]\tLoss: 4.7100\tLR: 5.386445\n",
      "Training Epoch: 54 [43520/50000]\tLoss: 4.7548\tLR: 5.386701\n",
      "Training Epoch: 54 [43648/50000]\tLoss: 4.7856\tLR: 5.386957\n",
      "Training Epoch: 54 [43776/50000]\tLoss: 4.7275\tLR: 5.387212\n",
      "Training Epoch: 54 [43904/50000]\tLoss: 4.7174\tLR: 5.387468\n",
      "Training Epoch: 54 [44032/50000]\tLoss: 4.6906\tLR: 5.387724\n",
      "Training Epoch: 54 [44160/50000]\tLoss: 4.7286\tLR: 5.387980\n",
      "Training Epoch: 54 [44288/50000]\tLoss: 4.6866\tLR: 5.388235\n",
      "Training Epoch: 54 [44416/50000]\tLoss: 4.8214\tLR: 5.388491\n",
      "Training Epoch: 54 [44544/50000]\tLoss: 4.6839\tLR: 5.388747\n",
      "Training Epoch: 54 [44672/50000]\tLoss: 4.7346\tLR: 5.389003\n",
      "Training Epoch: 54 [44800/50000]\tLoss: 4.6722\tLR: 5.389258\n",
      "Training Epoch: 54 [44928/50000]\tLoss: 4.7096\tLR: 5.389514\n",
      "Training Epoch: 54 [45056/50000]\tLoss: 4.6999\tLR: 5.389770\n",
      "Training Epoch: 54 [45184/50000]\tLoss: 4.7148\tLR: 5.390026\n",
      "Training Epoch: 54 [45312/50000]\tLoss: 4.6958\tLR: 5.390281\n",
      "Training Epoch: 54 [45440/50000]\tLoss: 4.7274\tLR: 5.390537\n",
      "Training Epoch: 54 [45568/50000]\tLoss: 4.6382\tLR: 5.390793\n",
      "Training Epoch: 54 [45696/50000]\tLoss: 4.7512\tLR: 5.391049\n",
      "Training Epoch: 54 [45824/50000]\tLoss: 4.6612\tLR: 5.391304\n",
      "Training Epoch: 54 [45952/50000]\tLoss: 4.7431\tLR: 5.391560\n",
      "Training Epoch: 54 [46080/50000]\tLoss: 4.8133\tLR: 5.391816\n",
      "Training Epoch: 54 [46208/50000]\tLoss: 4.7265\tLR: 5.392072\n",
      "Training Epoch: 54 [46336/50000]\tLoss: 4.6741\tLR: 5.392327\n",
      "Training Epoch: 54 [46464/50000]\tLoss: 4.6676\tLR: 5.392583\n",
      "Training Epoch: 54 [46592/50000]\tLoss: 4.7128\tLR: 5.392839\n",
      "Training Epoch: 54 [46720/50000]\tLoss: 4.7465\tLR: 5.393095\n",
      "Training Epoch: 54 [46848/50000]\tLoss: 4.6572\tLR: 5.393350\n",
      "Training Epoch: 54 [46976/50000]\tLoss: 4.7169\tLR: 5.393606\n",
      "Training Epoch: 54 [47104/50000]\tLoss: 4.6365\tLR: 5.393862\n",
      "Training Epoch: 54 [47232/50000]\tLoss: 4.7277\tLR: 5.394118\n",
      "Training Epoch: 54 [47360/50000]\tLoss: 4.6956\tLR: 5.394373\n",
      "Training Epoch: 54 [47488/50000]\tLoss: 4.6874\tLR: 5.394629\n",
      "Training Epoch: 54 [47616/50000]\tLoss: 4.7256\tLR: 5.394885\n",
      "Training Epoch: 54 [47744/50000]\tLoss: 4.6780\tLR: 5.395141\n",
      "Training Epoch: 54 [47872/50000]\tLoss: 4.7280\tLR: 5.395396\n",
      "Training Epoch: 54 [48000/50000]\tLoss: 4.7323\tLR: 5.395652\n",
      "Training Epoch: 54 [48128/50000]\tLoss: 4.7735\tLR: 5.395908\n",
      "Training Epoch: 54 [48256/50000]\tLoss: 4.7607\tLR: 5.396164\n",
      "Training Epoch: 54 [48384/50000]\tLoss: 4.7436\tLR: 5.396419\n",
      "Training Epoch: 54 [48512/50000]\tLoss: 4.6624\tLR: 5.396675\n",
      "Training Epoch: 54 [48640/50000]\tLoss: 4.7618\tLR: 5.396931\n",
      "Training Epoch: 54 [48768/50000]\tLoss: 4.7633\tLR: 5.397187\n",
      "Training Epoch: 54 [48896/50000]\tLoss: 4.7216\tLR: 5.397442\n",
      "Training Epoch: 54 [49024/50000]\tLoss: 4.6780\tLR: 5.397698\n",
      "Training Epoch: 54 [49152/50000]\tLoss: 4.7125\tLR: 5.397954\n",
      "Training Epoch: 54 [49280/50000]\tLoss: 4.6926\tLR: 5.398210\n",
      "Training Epoch: 54 [49408/50000]\tLoss: 4.7055\tLR: 5.398465\n",
      "Training Epoch: 54 [49536/50000]\tLoss: 4.7167\tLR: 5.398721\n",
      "Training Epoch: 54 [49664/50000]\tLoss: 4.6521\tLR: 5.398977\n",
      "Training Epoch: 54 [49792/50000]\tLoss: 4.7519\tLR: 5.399233\n",
      "Training Epoch: 54 [49920/50000]\tLoss: 4.6725\tLR: 5.399488\n",
      "Training Epoch: 54 [50000/50000]\tLoss: 4.6609\tLR: 5.399744\n",
      "epoch 54 training time consumed: 489.08s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   75705 GB |   75704 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   75472 GB |   75472 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     232 GB |     232 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   75705 GB |   75704 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   75472 GB |   75472 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     232 GB |     232 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   74639 GB |   74639 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   74407 GB |   74407 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     232 GB |     232 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    8027 K  |    8027 K  |\n",
      "|       from large pool |      24    |      65    |    3421 K  |    3421 K  |\n",
      "|       from small pool |     231    |     274    |    4605 K  |    4605 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    8027 K  |    8027 K  |\n",
      "|       from large pool |      24    |      65    |    3421 K  |    3421 K  |\n",
      "|       from small pool |     231    |     274    |    4605 K  |    4605 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      47    |    4652 K  |    4652 K  |\n",
      "|       from large pool |      10    |      23    |    1644 K  |    1644 K  |\n",
      "|       from small pool |      26    |      35    |    3007 K  |    3007 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 54, Average loss: 0.0372, Accuracy: 0.0100, Time consumed:31.11s\n",
      "\n",
      "Training Epoch: 55 [128/50000]\tLoss: 4.7639\tLR: 0.100000\n",
      "Training Epoch: 55 [256/50000]\tLoss: 4.7651\tLR: 5.400256\n",
      "Training Epoch: 55 [384/50000]\tLoss: 4.6945\tLR: 5.400512\n",
      "Training Epoch: 55 [512/50000]\tLoss: 4.7555\tLR: 5.400767\n",
      "Training Epoch: 55 [640/50000]\tLoss: 4.6491\tLR: 5.401023\n",
      "Training Epoch: 55 [768/50000]\tLoss: 4.6515\tLR: 5.401279\n",
      "Training Epoch: 55 [896/50000]\tLoss: 4.7310\tLR: 5.401535\n",
      "Training Epoch: 55 [1024/50000]\tLoss: 4.6600\tLR: 5.401790\n",
      "Training Epoch: 55 [1152/50000]\tLoss: 4.7350\tLR: 5.402046\n",
      "Training Epoch: 55 [1280/50000]\tLoss: 4.7698\tLR: 5.402302\n",
      "Training Epoch: 55 [1408/50000]\tLoss: 4.7476\tLR: 5.402558\n",
      "Training Epoch: 55 [1536/50000]\tLoss: 4.6714\tLR: 5.402813\n",
      "Training Epoch: 55 [1664/50000]\tLoss: 4.7326\tLR: 5.403069\n",
      "Training Epoch: 55 [1792/50000]\tLoss: 4.8094\tLR: 5.403325\n",
      "Training Epoch: 55 [1920/50000]\tLoss: 4.7904\tLR: 5.403581\n",
      "Training Epoch: 55 [2048/50000]\tLoss: 4.7475\tLR: 5.403836\n",
      "Training Epoch: 55 [2176/50000]\tLoss: 4.7296\tLR: 5.404092\n",
      "Training Epoch: 55 [2304/50000]\tLoss: 4.7201\tLR: 5.404348\n",
      "Training Epoch: 55 [2432/50000]\tLoss: 4.6884\tLR: 5.404604\n",
      "Training Epoch: 55 [2560/50000]\tLoss: 4.7177\tLR: 5.404859\n",
      "Training Epoch: 55 [2688/50000]\tLoss: 4.6978\tLR: 5.405115\n",
      "Training Epoch: 55 [2816/50000]\tLoss: 4.6012\tLR: 5.405371\n",
      "Training Epoch: 55 [2944/50000]\tLoss: 4.7193\tLR: 5.405627\n",
      "Training Epoch: 55 [3072/50000]\tLoss: 4.6672\tLR: 5.405882\n",
      "Training Epoch: 55 [3200/50000]\tLoss: 4.6823\tLR: 5.406138\n",
      "Training Epoch: 55 [3328/50000]\tLoss: 4.5994\tLR: 5.406394\n",
      "Training Epoch: 55 [3456/50000]\tLoss: 4.7010\tLR: 5.406650\n",
      "Training Epoch: 55 [3584/50000]\tLoss: 4.7637\tLR: 5.406905\n",
      "Training Epoch: 55 [3712/50000]\tLoss: 4.7188\tLR: 5.407161\n",
      "Training Epoch: 55 [3840/50000]\tLoss: 4.6998\tLR: 5.407417\n",
      "Training Epoch: 55 [3968/50000]\tLoss: 4.7681\tLR: 5.407673\n",
      "Training Epoch: 55 [4096/50000]\tLoss: 4.6920\tLR: 5.407928\n",
      "Training Epoch: 55 [4224/50000]\tLoss: 4.5827\tLR: 5.408184\n",
      "Training Epoch: 55 [4352/50000]\tLoss: 4.6749\tLR: 5.408440\n",
      "Training Epoch: 55 [4480/50000]\tLoss: 4.7140\tLR: 5.408696\n",
      "Training Epoch: 55 [4608/50000]\tLoss: 4.7081\tLR: 5.408951\n",
      "Training Epoch: 55 [4736/50000]\tLoss: 4.7329\tLR: 5.409207\n",
      "Training Epoch: 55 [4864/50000]\tLoss: 4.7605\tLR: 5.409463\n",
      "Training Epoch: 55 [4992/50000]\tLoss: 4.6453\tLR: 5.409719\n",
      "Training Epoch: 55 [5120/50000]\tLoss: 4.7616\tLR: 5.409974\n",
      "Training Epoch: 55 [5248/50000]\tLoss: 4.7658\tLR: 5.410230\n",
      "Training Epoch: 55 [5376/50000]\tLoss: 4.6930\tLR: 5.410486\n",
      "Training Epoch: 55 [5504/50000]\tLoss: 4.6537\tLR: 5.410742\n",
      "Training Epoch: 55 [5632/50000]\tLoss: 4.7713\tLR: 5.410997\n",
      "Training Epoch: 55 [5760/50000]\tLoss: 4.7109\tLR: 5.411253\n",
      "Training Epoch: 55 [5888/50000]\tLoss: 4.6562\tLR: 5.411509\n",
      "Training Epoch: 55 [6016/50000]\tLoss: 4.6803\tLR: 5.411765\n",
      "Training Epoch: 55 [6144/50000]\tLoss: 4.6731\tLR: 5.412020\n",
      "Training Epoch: 55 [6272/50000]\tLoss: 4.7633\tLR: 5.412276\n",
      "Training Epoch: 55 [6400/50000]\tLoss: 4.7230\tLR: 5.412532\n",
      "Training Epoch: 55 [6528/50000]\tLoss: 4.6792\tLR: 5.412788\n",
      "Training Epoch: 55 [6656/50000]\tLoss: 4.7042\tLR: 5.413043\n",
      "Training Epoch: 55 [6784/50000]\tLoss: 4.7412\tLR: 5.413299\n",
      "Training Epoch: 55 [6912/50000]\tLoss: 4.6903\tLR: 5.413555\n",
      "Training Epoch: 55 [7040/50000]\tLoss: 4.7052\tLR: 5.413811\n",
      "Training Epoch: 55 [7168/50000]\tLoss: 4.7452\tLR: 5.414066\n",
      "Training Epoch: 55 [7296/50000]\tLoss: 4.6929\tLR: 5.414322\n",
      "Training Epoch: 55 [7424/50000]\tLoss: 4.7249\tLR: 5.414578\n",
      "Training Epoch: 55 [7552/50000]\tLoss: 4.6595\tLR: 5.414834\n",
      "Training Epoch: 55 [7680/50000]\tLoss: 4.6797\tLR: 5.415090\n",
      "Training Epoch: 55 [7808/50000]\tLoss: 4.7264\tLR: 5.415345\n",
      "Training Epoch: 55 [7936/50000]\tLoss: 4.7128\tLR: 5.415601\n",
      "Training Epoch: 55 [8064/50000]\tLoss: 4.6377\tLR: 5.415857\n",
      "Training Epoch: 55 [8192/50000]\tLoss: 4.6878\tLR: 5.416113\n",
      "Training Epoch: 55 [8320/50000]\tLoss: 4.7273\tLR: 5.416368\n",
      "Training Epoch: 55 [8448/50000]\tLoss: 4.7034\tLR: 5.416624\n",
      "Training Epoch: 55 [8576/50000]\tLoss: 4.7293\tLR: 5.416880\n",
      "Training Epoch: 55 [8704/50000]\tLoss: 4.6732\tLR: 5.417136\n",
      "Training Epoch: 55 [8832/50000]\tLoss: 4.7476\tLR: 5.417391\n",
      "Training Epoch: 55 [8960/50000]\tLoss: 4.7527\tLR: 5.417647\n",
      "Training Epoch: 55 [9088/50000]\tLoss: 4.6936\tLR: 5.417903\n",
      "Training Epoch: 55 [9216/50000]\tLoss: 4.7763\tLR: 5.418159\n",
      "Training Epoch: 55 [9344/50000]\tLoss: 4.6776\tLR: 5.418414\n",
      "Training Epoch: 55 [9472/50000]\tLoss: 4.7054\tLR: 5.418670\n",
      "Training Epoch: 55 [9600/50000]\tLoss: 4.7469\tLR: 5.418926\n",
      "Training Epoch: 55 [9728/50000]\tLoss: 4.6216\tLR: 5.419182\n",
      "Training Epoch: 55 [9856/50000]\tLoss: 4.6359\tLR: 5.419437\n",
      "Training Epoch: 55 [9984/50000]\tLoss: 4.6774\tLR: 5.419693\n",
      "Training Epoch: 55 [10112/50000]\tLoss: 4.6879\tLR: 5.419949\n",
      "Training Epoch: 55 [10240/50000]\tLoss: 4.6886\tLR: 5.420205\n",
      "Training Epoch: 55 [10368/50000]\tLoss: 4.6960\tLR: 5.420460\n",
      "Training Epoch: 55 [10496/50000]\tLoss: 4.6269\tLR: 5.420716\n",
      "Training Epoch: 55 [10624/50000]\tLoss: 4.7320\tLR: 5.420972\n",
      "Training Epoch: 55 [10752/50000]\tLoss: 4.7357\tLR: 5.421228\n",
      "Training Epoch: 55 [10880/50000]\tLoss: 4.7654\tLR: 5.421483\n",
      "Training Epoch: 55 [11008/50000]\tLoss: 4.7392\tLR: 5.421739\n",
      "Training Epoch: 55 [11136/50000]\tLoss: 4.6845\tLR: 5.421995\n",
      "Training Epoch: 55 [11264/50000]\tLoss: 4.6265\tLR: 5.422251\n",
      "Training Epoch: 55 [11392/50000]\tLoss: 4.7471\tLR: 5.422506\n",
      "Training Epoch: 55 [11520/50000]\tLoss: 4.7040\tLR: 5.422762\n",
      "Training Epoch: 55 [11648/50000]\tLoss: 4.7079\tLR: 5.423018\n",
      "Training Epoch: 55 [11776/50000]\tLoss: 4.7038\tLR: 5.423274\n",
      "Training Epoch: 55 [11904/50000]\tLoss: 4.6941\tLR: 5.423529\n",
      "Training Epoch: 55 [12032/50000]\tLoss: 4.6734\tLR: 5.423785\n",
      "Training Epoch: 55 [12160/50000]\tLoss: 4.6326\tLR: 5.424041\n",
      "Training Epoch: 55 [12288/50000]\tLoss: 4.6963\tLR: 5.424297\n",
      "Training Epoch: 55 [12416/50000]\tLoss: 4.7226\tLR: 5.424552\n",
      "Training Epoch: 55 [12544/50000]\tLoss: 4.7175\tLR: 5.424808\n",
      "Training Epoch: 55 [12672/50000]\tLoss: 4.6622\tLR: 5.425064\n",
      "Training Epoch: 55 [12800/50000]\tLoss: 4.7136\tLR: 5.425320\n",
      "Training Epoch: 55 [12928/50000]\tLoss: 4.7660\tLR: 5.425575\n",
      "Training Epoch: 55 [13056/50000]\tLoss: 4.7024\tLR: 5.425831\n",
      "Training Epoch: 55 [13184/50000]\tLoss: 4.6892\tLR: 5.426087\n",
      "Training Epoch: 55 [13312/50000]\tLoss: 4.6554\tLR: 5.426343\n",
      "Training Epoch: 55 [13440/50000]\tLoss: 4.6411\tLR: 5.426598\n",
      "Training Epoch: 55 [13568/50000]\tLoss: 4.7042\tLR: 5.426854\n",
      "Training Epoch: 55 [13696/50000]\tLoss: 4.7683\tLR: 5.427110\n",
      "Training Epoch: 55 [13824/50000]\tLoss: 4.6757\tLR: 5.427366\n",
      "Training Epoch: 55 [13952/50000]\tLoss: 4.6068\tLR: 5.427621\n",
      "Training Epoch: 55 [14080/50000]\tLoss: 4.6886\tLR: 5.427877\n",
      "Training Epoch: 55 [14208/50000]\tLoss: 4.8184\tLR: 5.428133\n",
      "Training Epoch: 55 [14336/50000]\tLoss: 4.7070\tLR: 5.428389\n",
      "Training Epoch: 55 [14464/50000]\tLoss: 4.7895\tLR: 5.428645\n",
      "Training Epoch: 55 [14592/50000]\tLoss: 4.7808\tLR: 5.428900\n",
      "Training Epoch: 55 [14720/50000]\tLoss: 4.7276\tLR: 5.429156\n",
      "Training Epoch: 55 [14848/50000]\tLoss: 4.6989\tLR: 5.429412\n",
      "Training Epoch: 55 [14976/50000]\tLoss: 4.6975\tLR: 5.429668\n",
      "Training Epoch: 55 [15104/50000]\tLoss: 4.6271\tLR: 5.429923\n",
      "Training Epoch: 55 [15232/50000]\tLoss: 4.6600\tLR: 5.430179\n",
      "Training Epoch: 55 [15360/50000]\tLoss: 4.6938\tLR: 5.430435\n",
      "Training Epoch: 55 [15488/50000]\tLoss: 4.7047\tLR: 5.430691\n",
      "Training Epoch: 55 [15616/50000]\tLoss: 4.6785\tLR: 5.430946\n",
      "Training Epoch: 55 [15744/50000]\tLoss: 4.7197\tLR: 5.431202\n",
      "Training Epoch: 55 [15872/50000]\tLoss: 4.7670\tLR: 5.431458\n",
      "Training Epoch: 55 [16000/50000]\tLoss: 4.6664\tLR: 5.431714\n",
      "Training Epoch: 55 [16128/50000]\tLoss: 4.6712\tLR: 5.431969\n",
      "Training Epoch: 55 [16256/50000]\tLoss: 4.6873\tLR: 5.432225\n",
      "Training Epoch: 55 [16384/50000]\tLoss: 4.7569\tLR: 5.432481\n",
      "Training Epoch: 55 [16512/50000]\tLoss: 4.8066\tLR: 5.432737\n",
      "Training Epoch: 55 [16640/50000]\tLoss: 4.6812\tLR: 5.432992\n",
      "Training Epoch: 55 [16768/50000]\tLoss: 4.6666\tLR: 5.433248\n",
      "Training Epoch: 55 [16896/50000]\tLoss: 4.7087\tLR: 5.433504\n",
      "Training Epoch: 55 [17024/50000]\tLoss: 4.6942\tLR: 5.433760\n",
      "Training Epoch: 55 [17152/50000]\tLoss: 4.7008\tLR: 5.434015\n",
      "Training Epoch: 55 [17280/50000]\tLoss: 4.6669\tLR: 5.434271\n",
      "Training Epoch: 55 [17408/50000]\tLoss: 4.6850\tLR: 5.434527\n",
      "Training Epoch: 55 [17536/50000]\tLoss: 4.7050\tLR: 5.434783\n",
      "Training Epoch: 55 [17664/50000]\tLoss: 4.7788\tLR: 5.435038\n",
      "Training Epoch: 55 [17792/50000]\tLoss: 4.7092\tLR: 5.435294\n",
      "Training Epoch: 55 [17920/50000]\tLoss: 4.6817\tLR: 5.435550\n",
      "Training Epoch: 55 [18048/50000]\tLoss: 4.7194\tLR: 5.435806\n",
      "Training Epoch: 55 [18176/50000]\tLoss: 4.6499\tLR: 5.436061\n",
      "Training Epoch: 55 [18304/50000]\tLoss: 4.7110\tLR: 5.436317\n",
      "Training Epoch: 55 [18432/50000]\tLoss: 4.6976\tLR: 5.436573\n",
      "Training Epoch: 55 [18560/50000]\tLoss: 4.6272\tLR: 5.436829\n",
      "Training Epoch: 55 [18688/50000]\tLoss: 4.7060\tLR: 5.437084\n",
      "Training Epoch: 55 [18816/50000]\tLoss: 4.6074\tLR: 5.437340\n",
      "Training Epoch: 55 [18944/50000]\tLoss: 4.7542\tLR: 5.437596\n",
      "Training Epoch: 55 [19072/50000]\tLoss: 4.6925\tLR: 5.437852\n",
      "Training Epoch: 55 [19200/50000]\tLoss: 4.6547\tLR: 5.438107\n",
      "Training Epoch: 55 [19328/50000]\tLoss: 4.7196\tLR: 5.438363\n",
      "Training Epoch: 55 [19456/50000]\tLoss: 4.6860\tLR: 5.438619\n",
      "Training Epoch: 55 [19584/50000]\tLoss: 4.7040\tLR: 5.438875\n",
      "Training Epoch: 55 [19712/50000]\tLoss: 4.7661\tLR: 5.439130\n",
      "Training Epoch: 55 [19840/50000]\tLoss: 4.7187\tLR: 5.439386\n",
      "Training Epoch: 55 [19968/50000]\tLoss: 4.7499\tLR: 5.439642\n",
      "Training Epoch: 55 [20096/50000]\tLoss: 4.7328\tLR: 5.439898\n",
      "Training Epoch: 55 [20224/50000]\tLoss: 4.7756\tLR: 5.440153\n",
      "Training Epoch: 55 [20352/50000]\tLoss: 4.7654\tLR: 5.440409\n",
      "Training Epoch: 55 [20480/50000]\tLoss: 4.6924\tLR: 5.440665\n",
      "Training Epoch: 55 [20608/50000]\tLoss: 4.6929\tLR: 5.440921\n",
      "Training Epoch: 55 [20736/50000]\tLoss: 4.7177\tLR: 5.441176\n",
      "Training Epoch: 55 [20864/50000]\tLoss: 4.7398\tLR: 5.441432\n",
      "Training Epoch: 55 [20992/50000]\tLoss: 4.6974\tLR: 5.441688\n",
      "Training Epoch: 55 [21120/50000]\tLoss: 4.7352\tLR: 5.441944\n",
      "Training Epoch: 55 [21248/50000]\tLoss: 4.7296\tLR: 5.442199\n",
      "Training Epoch: 55 [21376/50000]\tLoss: 4.7635\tLR: 5.442455\n",
      "Training Epoch: 55 [21504/50000]\tLoss: 4.7683\tLR: 5.442711\n",
      "Training Epoch: 55 [21632/50000]\tLoss: 4.7132\tLR: 5.442967\n",
      "Training Epoch: 55 [21760/50000]\tLoss: 4.6994\tLR: 5.443223\n",
      "Training Epoch: 55 [21888/50000]\tLoss: 4.5884\tLR: 5.443478\n",
      "Training Epoch: 55 [22016/50000]\tLoss: 4.7335\tLR: 5.443734\n",
      "Training Epoch: 55 [22144/50000]\tLoss: 4.6589\tLR: 5.443990\n",
      "Training Epoch: 55 [22272/50000]\tLoss: 4.7098\tLR: 5.444246\n",
      "Training Epoch: 55 [22400/50000]\tLoss: 4.7470\tLR: 5.444501\n",
      "Training Epoch: 55 [22528/50000]\tLoss: 4.8343\tLR: 5.444757\n",
      "Training Epoch: 55 [22656/50000]\tLoss: 4.6482\tLR: 5.445013\n",
      "Training Epoch: 55 [22784/50000]\tLoss: 4.6915\tLR: 5.445269\n",
      "Training Epoch: 55 [22912/50000]\tLoss: 4.6904\tLR: 5.445524\n",
      "Training Epoch: 55 [23040/50000]\tLoss: 4.7443\tLR: 5.445780\n",
      "Training Epoch: 55 [23168/50000]\tLoss: 4.7064\tLR: 5.446036\n",
      "Training Epoch: 55 [23296/50000]\tLoss: 4.8219\tLR: 5.446292\n",
      "Training Epoch: 55 [23424/50000]\tLoss: 4.7571\tLR: 5.446547\n",
      "Training Epoch: 55 [23552/50000]\tLoss: 4.7699\tLR: 5.446803\n",
      "Training Epoch: 55 [23680/50000]\tLoss: 4.7031\tLR: 5.447059\n",
      "Training Epoch: 55 [23808/50000]\tLoss: 4.7287\tLR: 5.447315\n",
      "Training Epoch: 55 [23936/50000]\tLoss: 4.6833\tLR: 5.447570\n",
      "Training Epoch: 55 [24064/50000]\tLoss: 4.6961\tLR: 5.447826\n",
      "Training Epoch: 55 [24192/50000]\tLoss: 4.7195\tLR: 5.448082\n",
      "Training Epoch: 55 [24320/50000]\tLoss: 4.7220\tLR: 5.448338\n",
      "Training Epoch: 55 [24448/50000]\tLoss: 4.7248\tLR: 5.448593\n",
      "Training Epoch: 55 [24576/50000]\tLoss: 4.7063\tLR: 5.448849\n",
      "Training Epoch: 55 [24704/50000]\tLoss: 4.6214\tLR: 5.449105\n",
      "Training Epoch: 55 [24832/50000]\tLoss: 4.6974\tLR: 5.449361\n",
      "Training Epoch: 55 [24960/50000]\tLoss: 4.7036\tLR: 5.449616\n",
      "Training Epoch: 55 [25088/50000]\tLoss: 4.7089\tLR: 5.449872\n",
      "Training Epoch: 55 [25216/50000]\tLoss: 4.6891\tLR: 5.450128\n",
      "Training Epoch: 55 [25344/50000]\tLoss: 4.6894\tLR: 5.450384\n",
      "Training Epoch: 55 [25472/50000]\tLoss: 4.7685\tLR: 5.450639\n",
      "Training Epoch: 55 [25600/50000]\tLoss: 4.7231\tLR: 5.450895\n",
      "Training Epoch: 55 [25728/50000]\tLoss: 4.7054\tLR: 5.451151\n",
      "Training Epoch: 55 [25856/50000]\tLoss: 4.6677\tLR: 5.451407\n",
      "Training Epoch: 55 [25984/50000]\tLoss: 4.5896\tLR: 5.451662\n",
      "Training Epoch: 55 [26112/50000]\tLoss: 4.6884\tLR: 5.451918\n",
      "Training Epoch: 55 [26240/50000]\tLoss: 4.6801\tLR: 5.452174\n",
      "Training Epoch: 55 [26368/50000]\tLoss: 4.7256\tLR: 5.452430\n",
      "Training Epoch: 55 [26496/50000]\tLoss: 4.6990\tLR: 5.452685\n",
      "Training Epoch: 55 [26624/50000]\tLoss: 4.7876\tLR: 5.452941\n",
      "Training Epoch: 55 [26752/50000]\tLoss: 4.7583\tLR: 5.453197\n",
      "Training Epoch: 55 [26880/50000]\tLoss: 4.7250\tLR: 5.453453\n",
      "Training Epoch: 55 [27008/50000]\tLoss: 4.7023\tLR: 5.453708\n",
      "Training Epoch: 55 [27136/50000]\tLoss: 4.7311\tLR: 5.453964\n",
      "Training Epoch: 55 [27264/50000]\tLoss: 4.6565\tLR: 5.454220\n",
      "Training Epoch: 55 [27392/50000]\tLoss: 4.6943\tLR: 5.454476\n",
      "Training Epoch: 55 [27520/50000]\tLoss: 4.7160\tLR: 5.454731\n",
      "Training Epoch: 55 [27648/50000]\tLoss: 4.6672\tLR: 5.454987\n",
      "Training Epoch: 55 [27776/50000]\tLoss: 4.7397\tLR: 5.455243\n",
      "Training Epoch: 55 [27904/50000]\tLoss: 4.6914\tLR: 5.455499\n",
      "Training Epoch: 55 [28032/50000]\tLoss: 4.6375\tLR: 5.455754\n",
      "Training Epoch: 55 [28160/50000]\tLoss: 4.7803\tLR: 5.456010\n",
      "Training Epoch: 55 [28288/50000]\tLoss: 4.6508\tLR: 5.456266\n",
      "Training Epoch: 55 [28416/50000]\tLoss: 4.7500\tLR: 5.456522\n",
      "Training Epoch: 55 [28544/50000]\tLoss: 4.6976\tLR: 5.456777\n",
      "Training Epoch: 55 [28672/50000]\tLoss: 4.6676\tLR: 5.457033\n",
      "Training Epoch: 55 [28800/50000]\tLoss: 4.6531\tLR: 5.457289\n",
      "Training Epoch: 55 [28928/50000]\tLoss: 4.7086\tLR: 5.457545\n",
      "Training Epoch: 55 [29056/50000]\tLoss: 4.6839\tLR: 5.457801\n",
      "Training Epoch: 55 [29184/50000]\tLoss: 4.6538\tLR: 5.458056\n",
      "Training Epoch: 55 [29312/50000]\tLoss: 4.6535\tLR: 5.458312\n",
      "Training Epoch: 55 [29440/50000]\tLoss: 4.6384\tLR: 5.458568\n",
      "Training Epoch: 55 [29568/50000]\tLoss: 4.6678\tLR: 5.458824\n",
      "Training Epoch: 55 [29696/50000]\tLoss: 4.7286\tLR: 5.459079\n",
      "Training Epoch: 55 [29824/50000]\tLoss: 4.7857\tLR: 5.459335\n",
      "Training Epoch: 55 [29952/50000]\tLoss: 4.6905\tLR: 5.459591\n",
      "Training Epoch: 55 [30080/50000]\tLoss: 4.7384\tLR: 5.459847\n",
      "Training Epoch: 55 [30208/50000]\tLoss: 4.6845\tLR: 5.460102\n",
      "Training Epoch: 55 [30336/50000]\tLoss: 4.7059\tLR: 5.460358\n",
      "Training Epoch: 55 [30464/50000]\tLoss: 4.6301\tLR: 5.460614\n",
      "Training Epoch: 55 [30592/50000]\tLoss: 4.6877\tLR: 5.460870\n",
      "Training Epoch: 55 [30720/50000]\tLoss: 4.6904\tLR: 5.461125\n",
      "Training Epoch: 55 [30848/50000]\tLoss: 4.6739\tLR: 5.461381\n",
      "Training Epoch: 55 [30976/50000]\tLoss: 4.7368\tLR: 5.461637\n",
      "Training Epoch: 55 [31104/50000]\tLoss: 4.7203\tLR: 5.461893\n",
      "Training Epoch: 55 [31232/50000]\tLoss: 4.6838\tLR: 5.462148\n",
      "Training Epoch: 55 [31360/50000]\tLoss: 4.6706\tLR: 5.462404\n",
      "Training Epoch: 55 [31488/50000]\tLoss: 4.6896\tLR: 5.462660\n",
      "Training Epoch: 55 [31616/50000]\tLoss: 4.7010\tLR: 5.462916\n",
      "Training Epoch: 55 [31744/50000]\tLoss: 4.6985\tLR: 5.463171\n",
      "Training Epoch: 55 [31872/50000]\tLoss: 4.7676\tLR: 5.463427\n",
      "Training Epoch: 55 [32000/50000]\tLoss: 4.7390\tLR: 5.463683\n",
      "Training Epoch: 55 [32128/50000]\tLoss: 4.6532\tLR: 5.463939\n",
      "Training Epoch: 55 [32256/50000]\tLoss: 4.7211\tLR: 5.464194\n",
      "Training Epoch: 55 [32384/50000]\tLoss: 4.7158\tLR: 5.464450\n",
      "Training Epoch: 55 [32512/50000]\tLoss: 4.6949\tLR: 5.464706\n",
      "Training Epoch: 55 [32640/50000]\tLoss: 4.7125\tLR: 5.464962\n",
      "Training Epoch: 55 [32768/50000]\tLoss: 4.6864\tLR: 5.465217\n",
      "Training Epoch: 55 [32896/50000]\tLoss: 4.7896\tLR: 5.465473\n",
      "Training Epoch: 55 [33024/50000]\tLoss: 4.7398\tLR: 5.465729\n",
      "Training Epoch: 55 [33152/50000]\tLoss: 4.7122\tLR: 5.465985\n",
      "Training Epoch: 55 [33280/50000]\tLoss: 4.7065\tLR: 5.466240\n",
      "Training Epoch: 55 [33408/50000]\tLoss: 4.6824\tLR: 5.466496\n",
      "Training Epoch: 55 [33536/50000]\tLoss: 4.6268\tLR: 5.466752\n",
      "Training Epoch: 55 [33664/50000]\tLoss: 4.6757\tLR: 5.467008\n",
      "Training Epoch: 55 [33792/50000]\tLoss: 4.7740\tLR: 5.467263\n",
      "Training Epoch: 55 [33920/50000]\tLoss: 4.7342\tLR: 5.467519\n",
      "Training Epoch: 55 [34048/50000]\tLoss: 4.7584\tLR: 5.467775\n",
      "Training Epoch: 55 [34176/50000]\tLoss: 4.7859\tLR: 5.468031\n",
      "Training Epoch: 55 [34304/50000]\tLoss: 4.6927\tLR: 5.468286\n",
      "Training Epoch: 55 [34432/50000]\tLoss: 4.7073\tLR: 5.468542\n",
      "Training Epoch: 55 [34560/50000]\tLoss: 4.7309\tLR: 5.468798\n",
      "Training Epoch: 55 [34688/50000]\tLoss: 4.7651\tLR: 5.469054\n",
      "Training Epoch: 55 [34816/50000]\tLoss: 4.7517\tLR: 5.469309\n",
      "Training Epoch: 55 [34944/50000]\tLoss: 4.6638\tLR: 5.469565\n",
      "Training Epoch: 55 [35072/50000]\tLoss: 4.6779\tLR: 5.469821\n",
      "Training Epoch: 55 [35200/50000]\tLoss: 4.7441\tLR: 5.470077\n",
      "Training Epoch: 55 [35328/50000]\tLoss: 4.7112\tLR: 5.470332\n",
      "Training Epoch: 55 [35456/50000]\tLoss: 4.7055\tLR: 5.470588\n",
      "Training Epoch: 55 [35584/50000]\tLoss: 4.7812\tLR: 5.470844\n",
      "Training Epoch: 55 [35712/50000]\tLoss: 4.7045\tLR: 5.471100\n",
      "Training Epoch: 55 [35840/50000]\tLoss: 4.6889\tLR: 5.471355\n",
      "Training Epoch: 55 [35968/50000]\tLoss: 4.7303\tLR: 5.471611\n",
      "Training Epoch: 55 [36096/50000]\tLoss: 4.6733\tLR: 5.471867\n",
      "Training Epoch: 55 [36224/50000]\tLoss: 4.6813\tLR: 5.472123\n",
      "Training Epoch: 55 [36352/50000]\tLoss: 4.7432\tLR: 5.472379\n",
      "Training Epoch: 55 [36480/50000]\tLoss: 4.7365\tLR: 5.472634\n",
      "Training Epoch: 55 [36608/50000]\tLoss: 4.7330\tLR: 5.472890\n",
      "Training Epoch: 55 [36736/50000]\tLoss: 4.7105\tLR: 5.473146\n",
      "Training Epoch: 55 [36864/50000]\tLoss: 4.7306\tLR: 5.473402\n",
      "Training Epoch: 55 [36992/50000]\tLoss: 4.6909\tLR: 5.473657\n",
      "Training Epoch: 55 [37120/50000]\tLoss: 4.7354\tLR: 5.473913\n",
      "Training Epoch: 55 [37248/50000]\tLoss: 4.6594\tLR: 5.474169\n",
      "Training Epoch: 55 [37376/50000]\tLoss: 4.7231\tLR: 5.474425\n",
      "Training Epoch: 55 [37504/50000]\tLoss: 4.7083\tLR: 5.474680\n",
      "Training Epoch: 55 [37632/50000]\tLoss: 4.7028\tLR: 5.474936\n",
      "Training Epoch: 55 [37760/50000]\tLoss: 4.6519\tLR: 5.475192\n",
      "Training Epoch: 55 [37888/50000]\tLoss: 4.6983\tLR: 5.475448\n",
      "Training Epoch: 55 [38016/50000]\tLoss: 4.7511\tLR: 5.475703\n",
      "Training Epoch: 55 [38144/50000]\tLoss: 4.7366\tLR: 5.475959\n",
      "Training Epoch: 55 [38272/50000]\tLoss: 4.7336\tLR: 5.476215\n",
      "Training Epoch: 55 [38400/50000]\tLoss: 4.6549\tLR: 5.476471\n",
      "Training Epoch: 55 [38528/50000]\tLoss: 4.6929\tLR: 5.476726\n",
      "Training Epoch: 55 [38656/50000]\tLoss: 4.6496\tLR: 5.476982\n",
      "Training Epoch: 55 [38784/50000]\tLoss: 4.7118\tLR: 5.477238\n",
      "Training Epoch: 55 [38912/50000]\tLoss: 4.7139\tLR: 5.477494\n",
      "Training Epoch: 55 [39040/50000]\tLoss: 4.7003\tLR: 5.477749\n",
      "Training Epoch: 55 [39168/50000]\tLoss: 4.7681\tLR: 5.478005\n",
      "Training Epoch: 55 [39296/50000]\tLoss: 4.7277\tLR: 5.478261\n",
      "Training Epoch: 55 [39424/50000]\tLoss: 4.7034\tLR: 5.478517\n",
      "Training Epoch: 55 [39552/50000]\tLoss: 4.7117\tLR: 5.478772\n",
      "Training Epoch: 55 [39680/50000]\tLoss: 4.6879\tLR: 5.479028\n",
      "Training Epoch: 55 [39808/50000]\tLoss: 4.7405\tLR: 5.479284\n",
      "Training Epoch: 55 [39936/50000]\tLoss: 4.7301\tLR: 5.479540\n",
      "Training Epoch: 55 [40064/50000]\tLoss: 4.6617\tLR: 5.479795\n",
      "Training Epoch: 55 [40192/50000]\tLoss: 4.7011\tLR: 5.480051\n",
      "Training Epoch: 55 [40320/50000]\tLoss: 4.7038\tLR: 5.480307\n",
      "Training Epoch: 55 [40448/50000]\tLoss: 4.7122\tLR: 5.480563\n",
      "Training Epoch: 55 [40576/50000]\tLoss: 4.7696\tLR: 5.480818\n",
      "Training Epoch: 55 [40704/50000]\tLoss: 4.7606\tLR: 5.481074\n",
      "Training Epoch: 55 [40832/50000]\tLoss: 4.6698\tLR: 5.481330\n",
      "Training Epoch: 55 [40960/50000]\tLoss: 4.7629\tLR: 5.481586\n",
      "Training Epoch: 55 [41088/50000]\tLoss: 4.7080\tLR: 5.481841\n",
      "Training Epoch: 55 [41216/50000]\tLoss: 4.6882\tLR: 5.482097\n",
      "Training Epoch: 55 [41344/50000]\tLoss: 4.6643\tLR: 5.482353\n",
      "Training Epoch: 55 [41472/50000]\tLoss: 4.7491\tLR: 5.482609\n",
      "Training Epoch: 55 [41600/50000]\tLoss: 4.7523\tLR: 5.482864\n",
      "Training Epoch: 55 [41728/50000]\tLoss: 4.7044\tLR: 5.483120\n",
      "Training Epoch: 55 [41856/50000]\tLoss: 4.7072\tLR: 5.483376\n",
      "Training Epoch: 55 [41984/50000]\tLoss: 4.6959\tLR: 5.483632\n",
      "Training Epoch: 55 [42112/50000]\tLoss: 4.7338\tLR: 5.483887\n",
      "Training Epoch: 55 [42240/50000]\tLoss: 4.7072\tLR: 5.484143\n",
      "Training Epoch: 55 [42368/50000]\tLoss: 4.6802\tLR: 5.484399\n",
      "Training Epoch: 55 [42496/50000]\tLoss: 4.6880\tLR: 5.484655\n",
      "Training Epoch: 55 [42624/50000]\tLoss: 4.7177\tLR: 5.484910\n",
      "Training Epoch: 55 [42752/50000]\tLoss: 4.7432\tLR: 5.485166\n",
      "Training Epoch: 55 [42880/50000]\tLoss: 4.7454\tLR: 5.485422\n",
      "Training Epoch: 55 [43008/50000]\tLoss: 4.7910\tLR: 5.485678\n",
      "Training Epoch: 55 [43136/50000]\tLoss: 4.6896\tLR: 5.485934\n",
      "Training Epoch: 55 [43264/50000]\tLoss: 4.6564\tLR: 5.486189\n",
      "Training Epoch: 55 [43392/50000]\tLoss: 4.6819\tLR: 5.486445\n",
      "Training Epoch: 55 [43520/50000]\tLoss: 4.6308\tLR: 5.486701\n",
      "Training Epoch: 55 [43648/50000]\tLoss: 4.7191\tLR: 5.486957\n",
      "Training Epoch: 55 [43776/50000]\tLoss: 4.6945\tLR: 5.487212\n",
      "Training Epoch: 55 [43904/50000]\tLoss: 4.7160\tLR: 5.487468\n",
      "Training Epoch: 55 [44032/50000]\tLoss: 4.7373\tLR: 5.487724\n",
      "Training Epoch: 55 [44160/50000]\tLoss: 4.6837\tLR: 5.487980\n",
      "Training Epoch: 55 [44288/50000]\tLoss: 4.7589\tLR: 5.488235\n",
      "Training Epoch: 55 [44416/50000]\tLoss: 4.7238\tLR: 5.488491\n",
      "Training Epoch: 55 [44544/50000]\tLoss: 4.6914\tLR: 5.488747\n",
      "Training Epoch: 55 [44672/50000]\tLoss: 4.7064\tLR: 5.489003\n",
      "Training Epoch: 55 [44800/50000]\tLoss: 4.7816\tLR: 5.489258\n",
      "Training Epoch: 55 [44928/50000]\tLoss: 4.6971\tLR: 5.489514\n",
      "Training Epoch: 55 [45056/50000]\tLoss: 4.7270\tLR: 5.489770\n",
      "Training Epoch: 55 [45184/50000]\tLoss: 4.7814\tLR: 5.490026\n",
      "Training Epoch: 55 [45312/50000]\tLoss: 4.6764\tLR: 5.490281\n",
      "Training Epoch: 55 [45440/50000]\tLoss: 4.6750\tLR: 5.490537\n",
      "Training Epoch: 55 [45568/50000]\tLoss: 4.6598\tLR: 5.490793\n",
      "Training Epoch: 55 [45696/50000]\tLoss: 4.7775\tLR: 5.491049\n",
      "Training Epoch: 55 [45824/50000]\tLoss: 4.6815\tLR: 5.491304\n",
      "Training Epoch: 55 [45952/50000]\tLoss: 4.7609\tLR: 5.491560\n",
      "Training Epoch: 55 [46080/50000]\tLoss: 4.7426\tLR: 5.491816\n",
      "Training Epoch: 55 [46208/50000]\tLoss: 4.7828\tLR: 5.492072\n",
      "Training Epoch: 55 [46336/50000]\tLoss: 4.6791\tLR: 5.492327\n",
      "Training Epoch: 55 [46464/50000]\tLoss: 4.7389\tLR: 5.492583\n",
      "Training Epoch: 55 [46592/50000]\tLoss: 4.7724\tLR: 5.492839\n",
      "Training Epoch: 55 [46720/50000]\tLoss: 4.7133\tLR: 5.493095\n",
      "Training Epoch: 55 [46848/50000]\tLoss: 4.7099\tLR: 5.493350\n",
      "Training Epoch: 55 [46976/50000]\tLoss: 4.7426\tLR: 5.493606\n",
      "Training Epoch: 55 [47104/50000]\tLoss: 4.7385\tLR: 5.493862\n",
      "Training Epoch: 55 [47232/50000]\tLoss: 4.7078\tLR: 5.494118\n",
      "Training Epoch: 55 [47360/50000]\tLoss: 4.6341\tLR: 5.494373\n",
      "Training Epoch: 55 [47488/50000]\tLoss: 4.6464\tLR: 5.494629\n",
      "Training Epoch: 55 [47616/50000]\tLoss: 4.7556\tLR: 5.494885\n",
      "Training Epoch: 55 [47744/50000]\tLoss: 4.7659\tLR: 5.495141\n",
      "Training Epoch: 55 [47872/50000]\tLoss: 4.6943\tLR: 5.495396\n",
      "Training Epoch: 55 [48000/50000]\tLoss: 4.7102\tLR: 5.495652\n",
      "Training Epoch: 55 [48128/50000]\tLoss: 4.6379\tLR: 5.495908\n",
      "Training Epoch: 55 [48256/50000]\tLoss: 4.7586\tLR: 5.496164\n",
      "Training Epoch: 55 [48384/50000]\tLoss: 4.6966\tLR: 5.496419\n",
      "Training Epoch: 55 [48512/50000]\tLoss: 4.6597\tLR: 5.496675\n",
      "Training Epoch: 55 [48640/50000]\tLoss: 4.7618\tLR: 5.496931\n",
      "Training Epoch: 55 [48768/50000]\tLoss: 4.7626\tLR: 5.497187\n",
      "Training Epoch: 55 [48896/50000]\tLoss: 4.7395\tLR: 5.497442\n",
      "Training Epoch: 55 [49024/50000]\tLoss: 4.8387\tLR: 5.497698\n",
      "Training Epoch: 55 [49152/50000]\tLoss: 4.7278\tLR: 5.497954\n",
      "Training Epoch: 55 [49280/50000]\tLoss: 4.7157\tLR: 5.498210\n",
      "Training Epoch: 55 [49408/50000]\tLoss: 4.6927\tLR: 5.498465\n",
      "Training Epoch: 55 [49536/50000]\tLoss: 4.7104\tLR: 5.498721\n",
      "Training Epoch: 55 [49664/50000]\tLoss: 4.7506\tLR: 5.498977\n",
      "Training Epoch: 55 [49792/50000]\tLoss: 4.7808\tLR: 5.499233\n",
      "Training Epoch: 55 [49920/50000]\tLoss: 4.7619\tLR: 5.499488\n",
      "Training Epoch: 55 [50000/50000]\tLoss: 4.6894\tLR: 5.499744\n",
      "epoch 55 training time consumed: 488.87s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   77106 GB |   77106 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   76870 GB |   76870 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     236 GB |     236 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   77106 GB |   77106 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   76870 GB |   76870 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     236 GB |     236 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   76022 GB |   76022 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   75785 GB |   75785 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     236 GB |     236 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    8176 K  |    8175 K  |\n",
      "|       from large pool |      24    |      65    |    3485 K  |    3485 K  |\n",
      "|       from small pool |     231    |     274    |    4690 K  |    4690 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    8176 K  |    8175 K  |\n",
      "|       from large pool |      24    |      65    |    3485 K  |    3485 K  |\n",
      "|       from small pool |     231    |     274    |    4690 K  |    4690 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      47    |    4738 K  |    4738 K  |\n",
      "|       from large pool |      10    |      23    |    1675 K  |    1675 K  |\n",
      "|       from small pool |      25    |      35    |    3062 K  |    3062 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 55, Average loss: 0.0373, Accuracy: 0.0100, Time consumed:31.16s\n",
      "\n",
      "Training Epoch: 56 [128/50000]\tLoss: 4.6994\tLR: 0.100000\n",
      "Training Epoch: 56 [256/50000]\tLoss: 4.7130\tLR: 5.500256\n",
      "Training Epoch: 56 [384/50000]\tLoss: 4.7098\tLR: 5.500512\n",
      "Training Epoch: 56 [512/50000]\tLoss: 4.6745\tLR: 5.500767\n",
      "Training Epoch: 56 [640/50000]\tLoss: 4.7099\tLR: 5.501023\n",
      "Training Epoch: 56 [768/50000]\tLoss: 4.7366\tLR: 5.501279\n",
      "Training Epoch: 56 [896/50000]\tLoss: 4.7367\tLR: 5.501535\n",
      "Training Epoch: 56 [1024/50000]\tLoss: 4.6994\tLR: 5.501790\n",
      "Training Epoch: 56 [1152/50000]\tLoss: 4.7674\tLR: 5.502046\n",
      "Training Epoch: 56 [1280/50000]\tLoss: 4.7842\tLR: 5.502302\n",
      "Training Epoch: 56 [1408/50000]\tLoss: 4.7703\tLR: 5.502558\n",
      "Training Epoch: 56 [1536/50000]\tLoss: 4.7598\tLR: 5.502813\n",
      "Training Epoch: 56 [1664/50000]\tLoss: 4.7089\tLR: 5.503069\n",
      "Training Epoch: 56 [1792/50000]\tLoss: 4.7458\tLR: 5.503325\n",
      "Training Epoch: 56 [1920/50000]\tLoss: 4.7481\tLR: 5.503581\n",
      "Training Epoch: 56 [2048/50000]\tLoss: 4.7420\tLR: 5.503836\n",
      "Training Epoch: 56 [2176/50000]\tLoss: 4.7391\tLR: 5.504092\n",
      "Training Epoch: 56 [2304/50000]\tLoss: 4.6815\tLR: 5.504348\n",
      "Training Epoch: 56 [2432/50000]\tLoss: 4.7184\tLR: 5.504604\n",
      "Training Epoch: 56 [2560/50000]\tLoss: 4.6869\tLR: 5.504859\n",
      "Training Epoch: 56 [2688/50000]\tLoss: 4.7353\tLR: 5.505115\n",
      "Training Epoch: 56 [2816/50000]\tLoss: 4.6731\tLR: 5.505371\n",
      "Training Epoch: 56 [2944/50000]\tLoss: 4.6482\tLR: 5.505627\n",
      "Training Epoch: 56 [3072/50000]\tLoss: 4.7262\tLR: 5.505882\n",
      "Training Epoch: 56 [3200/50000]\tLoss: 4.7156\tLR: 5.506138\n",
      "Training Epoch: 56 [3328/50000]\tLoss: 4.6835\tLR: 5.506394\n",
      "Training Epoch: 56 [3456/50000]\tLoss: 4.6784\tLR: 5.506650\n",
      "Training Epoch: 56 [3584/50000]\tLoss: 4.6568\tLR: 5.506905\n",
      "Training Epoch: 56 [3712/50000]\tLoss: 4.7169\tLR: 5.507161\n",
      "Training Epoch: 56 [3840/50000]\tLoss: 4.6129\tLR: 5.507417\n",
      "Training Epoch: 56 [3968/50000]\tLoss: 4.7229\tLR: 5.507673\n",
      "Training Epoch: 56 [4096/50000]\tLoss: 4.7376\tLR: 5.507928\n",
      "Training Epoch: 56 [4224/50000]\tLoss: 4.6870\tLR: 5.508184\n",
      "Training Epoch: 56 [4352/50000]\tLoss: 4.7844\tLR: 5.508440\n",
      "Training Epoch: 56 [4480/50000]\tLoss: 4.7253\tLR: 5.508696\n",
      "Training Epoch: 56 [4608/50000]\tLoss: 4.7189\tLR: 5.508951\n",
      "Training Epoch: 56 [4736/50000]\tLoss: 4.7371\tLR: 5.509207\n",
      "Training Epoch: 56 [4864/50000]\tLoss: 4.7216\tLR: 5.509463\n",
      "Training Epoch: 56 [4992/50000]\tLoss: 4.6608\tLR: 5.509719\n",
      "Training Epoch: 56 [5120/50000]\tLoss: 4.6675\tLR: 5.509974\n",
      "Training Epoch: 56 [5248/50000]\tLoss: 4.7145\tLR: 5.510230\n",
      "Training Epoch: 56 [5376/50000]\tLoss: 4.7018\tLR: 5.510486\n",
      "Training Epoch: 56 [5504/50000]\tLoss: 4.7695\tLR: 5.510742\n",
      "Training Epoch: 56 [5632/50000]\tLoss: 4.6933\tLR: 5.510997\n",
      "Training Epoch: 56 [5760/50000]\tLoss: 4.7082\tLR: 5.511253\n",
      "Training Epoch: 56 [5888/50000]\tLoss: 4.7572\tLR: 5.511509\n",
      "Training Epoch: 56 [6016/50000]\tLoss: 4.6687\tLR: 5.511765\n",
      "Training Epoch: 56 [6144/50000]\tLoss: 4.6575\tLR: 5.512020\n",
      "Training Epoch: 56 [6272/50000]\tLoss: 4.6224\tLR: 5.512276\n",
      "Training Epoch: 56 [6400/50000]\tLoss: 4.6716\tLR: 5.512532\n",
      "Training Epoch: 56 [6528/50000]\tLoss: 4.7105\tLR: 5.512788\n",
      "Training Epoch: 56 [6656/50000]\tLoss: 4.7618\tLR: 5.513043\n",
      "Training Epoch: 56 [6784/50000]\tLoss: 4.7340\tLR: 5.513299\n",
      "Training Epoch: 56 [6912/50000]\tLoss: 4.7825\tLR: 5.513555\n",
      "Training Epoch: 56 [7040/50000]\tLoss: 4.7099\tLR: 5.513811\n",
      "Training Epoch: 56 [7168/50000]\tLoss: 4.6957\tLR: 5.514066\n",
      "Training Epoch: 56 [7296/50000]\tLoss: 4.7072\tLR: 5.514322\n",
      "Training Epoch: 56 [7424/50000]\tLoss: 4.6303\tLR: 5.514578\n",
      "Training Epoch: 56 [7552/50000]\tLoss: 4.7250\tLR: 5.514834\n",
      "Training Epoch: 56 [7680/50000]\tLoss: 4.8008\tLR: 5.515090\n",
      "Training Epoch: 56 [7808/50000]\tLoss: 4.8717\tLR: 5.515345\n",
      "Training Epoch: 56 [7936/50000]\tLoss: 4.7728\tLR: 5.515601\n",
      "Training Epoch: 56 [8064/50000]\tLoss: 4.6940\tLR: 5.515857\n",
      "Training Epoch: 56 [8192/50000]\tLoss: 4.6981\tLR: 5.516113\n",
      "Training Epoch: 56 [8320/50000]\tLoss: 4.6983\tLR: 5.516368\n",
      "Training Epoch: 56 [8448/50000]\tLoss: 4.6833\tLR: 5.516624\n",
      "Training Epoch: 56 [8576/50000]\tLoss: 4.6658\tLR: 5.516880\n",
      "Training Epoch: 56 [8704/50000]\tLoss: 4.6421\tLR: 5.517136\n",
      "Training Epoch: 56 [8832/50000]\tLoss: 4.7136\tLR: 5.517391\n",
      "Training Epoch: 56 [8960/50000]\tLoss: 4.6900\tLR: 5.517647\n",
      "Training Epoch: 56 [9088/50000]\tLoss: 4.7489\tLR: 5.517903\n",
      "Training Epoch: 56 [9216/50000]\tLoss: 4.7369\tLR: 5.518159\n",
      "Training Epoch: 56 [9344/50000]\tLoss: 4.6665\tLR: 5.518414\n",
      "Training Epoch: 56 [9472/50000]\tLoss: 4.7391\tLR: 5.518670\n",
      "Training Epoch: 56 [9600/50000]\tLoss: 4.6921\tLR: 5.518926\n",
      "Training Epoch: 56 [9728/50000]\tLoss: 4.7428\tLR: 5.519182\n",
      "Training Epoch: 56 [9856/50000]\tLoss: 4.7790\tLR: 5.519437\n",
      "Training Epoch: 56 [9984/50000]\tLoss: 4.7577\tLR: 5.519693\n",
      "Training Epoch: 56 [10112/50000]\tLoss: 4.6493\tLR: 5.519949\n",
      "Training Epoch: 56 [10240/50000]\tLoss: 4.6653\tLR: 5.520205\n",
      "Training Epoch: 56 [10368/50000]\tLoss: 4.7073\tLR: 5.520460\n",
      "Training Epoch: 56 [10496/50000]\tLoss: 4.6891\tLR: 5.520716\n",
      "Training Epoch: 56 [10624/50000]\tLoss: 4.7298\tLR: 5.520972\n",
      "Training Epoch: 56 [10752/50000]\tLoss: 4.7140\tLR: 5.521228\n",
      "Training Epoch: 56 [10880/50000]\tLoss: 4.7372\tLR: 5.521483\n",
      "Training Epoch: 56 [11008/50000]\tLoss: 4.6731\tLR: 5.521739\n",
      "Training Epoch: 56 [11136/50000]\tLoss: 4.6782\tLR: 5.521995\n",
      "Training Epoch: 56 [11264/50000]\tLoss: 4.6837\tLR: 5.522251\n",
      "Training Epoch: 56 [11392/50000]\tLoss: 4.7062\tLR: 5.522506\n",
      "Training Epoch: 56 [11520/50000]\tLoss: 4.6903\tLR: 5.522762\n",
      "Training Epoch: 56 [11648/50000]\tLoss: 4.7235\tLR: 5.523018\n",
      "Training Epoch: 56 [11776/50000]\tLoss: 4.7320\tLR: 5.523274\n",
      "Training Epoch: 56 [11904/50000]\tLoss: 4.7637\tLR: 5.523529\n",
      "Training Epoch: 56 [12032/50000]\tLoss: 4.7378\tLR: 5.523785\n",
      "Training Epoch: 56 [12160/50000]\tLoss: 4.6746\tLR: 5.524041\n",
      "Training Epoch: 56 [12288/50000]\tLoss: 4.7687\tLR: 5.524297\n",
      "Training Epoch: 56 [12416/50000]\tLoss: 4.6948\tLR: 5.524552\n",
      "Training Epoch: 56 [12544/50000]\tLoss: 4.6777\tLR: 5.524808\n",
      "Training Epoch: 56 [12672/50000]\tLoss: 4.7285\tLR: 5.525064\n",
      "Training Epoch: 56 [12800/50000]\tLoss: 4.7256\tLR: 5.525320\n",
      "Training Epoch: 56 [12928/50000]\tLoss: 4.7713\tLR: 5.525575\n",
      "Training Epoch: 56 [13056/50000]\tLoss: 4.6612\tLR: 5.525831\n",
      "Training Epoch: 56 [13184/50000]\tLoss: 4.7565\tLR: 5.526087\n",
      "Training Epoch: 56 [13312/50000]\tLoss: 4.6263\tLR: 5.526343\n",
      "Training Epoch: 56 [13440/50000]\tLoss: 4.6592\tLR: 5.526598\n",
      "Training Epoch: 56 [13568/50000]\tLoss: 4.7547\tLR: 5.526854\n",
      "Training Epoch: 56 [13696/50000]\tLoss: 4.6983\tLR: 5.527110\n",
      "Training Epoch: 56 [13824/50000]\tLoss: 4.7161\tLR: 5.527366\n",
      "Training Epoch: 56 [13952/50000]\tLoss: 4.6858\tLR: 5.527621\n",
      "Training Epoch: 56 [14080/50000]\tLoss: 4.7489\tLR: 5.527877\n",
      "Training Epoch: 56 [14208/50000]\tLoss: 4.6892\tLR: 5.528133\n",
      "Training Epoch: 56 [14336/50000]\tLoss: 4.6969\tLR: 5.528389\n",
      "Training Epoch: 56 [14464/50000]\tLoss: 4.6389\tLR: 5.528645\n",
      "Training Epoch: 56 [14592/50000]\tLoss: 4.7366\tLR: 5.528900\n",
      "Training Epoch: 56 [14720/50000]\tLoss: 4.6454\tLR: 5.529156\n",
      "Training Epoch: 56 [14848/50000]\tLoss: 4.7313\tLR: 5.529412\n",
      "Training Epoch: 56 [14976/50000]\tLoss: 4.7174\tLR: 5.529668\n",
      "Training Epoch: 56 [15104/50000]\tLoss: 4.7095\tLR: 5.529923\n",
      "Training Epoch: 56 [15232/50000]\tLoss: 4.7905\tLR: 5.530179\n",
      "Training Epoch: 56 [15360/50000]\tLoss: 4.7298\tLR: 5.530435\n",
      "Training Epoch: 56 [15488/50000]\tLoss: 4.7511\tLR: 5.530691\n",
      "Training Epoch: 56 [15616/50000]\tLoss: 4.7403\tLR: 5.530946\n",
      "Training Epoch: 56 [15744/50000]\tLoss: 4.6258\tLR: 5.531202\n",
      "Training Epoch: 56 [15872/50000]\tLoss: 4.7657\tLR: 5.531458\n",
      "Training Epoch: 56 [16000/50000]\tLoss: 4.7558\tLR: 5.531714\n",
      "Training Epoch: 56 [16128/50000]\tLoss: 4.7872\tLR: 5.531969\n",
      "Training Epoch: 56 [16256/50000]\tLoss: 4.8035\tLR: 5.532225\n",
      "Training Epoch: 56 [16384/50000]\tLoss: 4.7192\tLR: 5.532481\n",
      "Training Epoch: 56 [16512/50000]\tLoss: 4.7167\tLR: 5.532737\n",
      "Training Epoch: 56 [16640/50000]\tLoss: 4.6560\tLR: 5.532992\n",
      "Training Epoch: 56 [16768/50000]\tLoss: 4.7187\tLR: 5.533248\n",
      "Training Epoch: 56 [16896/50000]\tLoss: 4.7813\tLR: 5.533504\n",
      "Training Epoch: 56 [17024/50000]\tLoss: 4.7520\tLR: 5.533760\n",
      "Training Epoch: 56 [17152/50000]\tLoss: 4.6784\tLR: 5.534015\n",
      "Training Epoch: 56 [17280/50000]\tLoss: 4.8517\tLR: 5.534271\n",
      "Training Epoch: 56 [17408/50000]\tLoss: 4.6951\tLR: 5.534527\n",
      "Training Epoch: 56 [17536/50000]\tLoss: 4.7312\tLR: 5.534783\n",
      "Training Epoch: 56 [17664/50000]\tLoss: 4.6487\tLR: 5.535038\n",
      "Training Epoch: 56 [17792/50000]\tLoss: 4.7180\tLR: 5.535294\n",
      "Training Epoch: 56 [17920/50000]\tLoss: 4.6872\tLR: 5.535550\n",
      "Training Epoch: 56 [18048/50000]\tLoss: 4.7241\tLR: 5.535806\n",
      "Training Epoch: 56 [18176/50000]\tLoss: 4.7073\tLR: 5.536061\n",
      "Training Epoch: 56 [18304/50000]\tLoss: 4.6717\tLR: 5.536317\n",
      "Training Epoch: 56 [18432/50000]\tLoss: 4.7528\tLR: 5.536573\n",
      "Training Epoch: 56 [18560/50000]\tLoss: 4.7354\tLR: 5.536829\n",
      "Training Epoch: 56 [18688/50000]\tLoss: 4.8239\tLR: 5.537084\n",
      "Training Epoch: 56 [18816/50000]\tLoss: 4.7453\tLR: 5.537340\n",
      "Training Epoch: 56 [18944/50000]\tLoss: 4.6955\tLR: 5.537596\n",
      "Training Epoch: 56 [19072/50000]\tLoss: 4.7285\tLR: 5.537852\n",
      "Training Epoch: 56 [19200/50000]\tLoss: 4.7620\tLR: 5.538107\n",
      "Training Epoch: 56 [19328/50000]\tLoss: 4.7647\tLR: 5.538363\n",
      "Training Epoch: 56 [19456/50000]\tLoss: 4.7527\tLR: 5.538619\n",
      "Training Epoch: 56 [19584/50000]\tLoss: 4.7461\tLR: 5.538875\n",
      "Training Epoch: 56 [19712/50000]\tLoss: 4.6076\tLR: 5.539130\n",
      "Training Epoch: 56 [19840/50000]\tLoss: 4.6793\tLR: 5.539386\n",
      "Training Epoch: 56 [19968/50000]\tLoss: 4.7715\tLR: 5.539642\n",
      "Training Epoch: 56 [20096/50000]\tLoss: 4.7732\tLR: 5.539898\n",
      "Training Epoch: 56 [20224/50000]\tLoss: 4.7275\tLR: 5.540153\n",
      "Training Epoch: 56 [20352/50000]\tLoss: 4.8305\tLR: 5.540409\n",
      "Training Epoch: 56 [20480/50000]\tLoss: 4.8495\tLR: 5.540665\n",
      "Training Epoch: 56 [20608/50000]\tLoss: 4.7108\tLR: 5.540921\n",
      "Training Epoch: 56 [20736/50000]\tLoss: 4.7584\tLR: 5.541176\n",
      "Training Epoch: 56 [20864/50000]\tLoss: 4.7242\tLR: 5.541432\n",
      "Training Epoch: 56 [20992/50000]\tLoss: 4.6891\tLR: 5.541688\n",
      "Training Epoch: 56 [21120/50000]\tLoss: 4.6815\tLR: 5.541944\n",
      "Training Epoch: 56 [21248/50000]\tLoss: 4.6898\tLR: 5.542199\n",
      "Training Epoch: 56 [21376/50000]\tLoss: 4.6602\tLR: 5.542455\n",
      "Training Epoch: 56 [21504/50000]\tLoss: 4.7269\tLR: 5.542711\n",
      "Training Epoch: 56 [21632/50000]\tLoss: 4.6579\tLR: 5.542967\n",
      "Training Epoch: 56 [21760/50000]\tLoss: 4.8012\tLR: 5.543223\n",
      "Training Epoch: 56 [21888/50000]\tLoss: 4.7746\tLR: 5.543478\n",
      "Training Epoch: 56 [22016/50000]\tLoss: 4.7855\tLR: 5.543734\n",
      "Training Epoch: 56 [22144/50000]\tLoss: 4.6891\tLR: 5.543990\n",
      "Training Epoch: 56 [22272/50000]\tLoss: 4.7126\tLR: 5.544246\n",
      "Training Epoch: 56 [22400/50000]\tLoss: 4.6679\tLR: 5.544501\n",
      "Training Epoch: 56 [22528/50000]\tLoss: 4.7306\tLR: 5.544757\n",
      "Training Epoch: 56 [22656/50000]\tLoss: 4.7599\tLR: 5.545013\n",
      "Training Epoch: 56 [22784/50000]\tLoss: 4.7001\tLR: 5.545269\n",
      "Training Epoch: 56 [22912/50000]\tLoss: 4.7553\tLR: 5.545524\n",
      "Training Epoch: 56 [23040/50000]\tLoss: 4.6515\tLR: 5.545780\n",
      "Training Epoch: 56 [23168/50000]\tLoss: 4.6305\tLR: 5.546036\n",
      "Training Epoch: 56 [23296/50000]\tLoss: 4.7726\tLR: 5.546292\n",
      "Training Epoch: 56 [23424/50000]\tLoss: 4.7247\tLR: 5.546547\n",
      "Training Epoch: 56 [23552/50000]\tLoss: 4.6405\tLR: 5.546803\n",
      "Training Epoch: 56 [23680/50000]\tLoss: 4.6887\tLR: 5.547059\n",
      "Training Epoch: 56 [23808/50000]\tLoss: 4.7188\tLR: 5.547315\n",
      "Training Epoch: 56 [23936/50000]\tLoss: 4.7906\tLR: 5.547570\n",
      "Training Epoch: 56 [24064/50000]\tLoss: 4.6721\tLR: 5.547826\n",
      "Training Epoch: 56 [24192/50000]\tLoss: 4.5996\tLR: 5.548082\n",
      "Training Epoch: 56 [24320/50000]\tLoss: 4.6686\tLR: 5.548338\n",
      "Training Epoch: 56 [24448/50000]\tLoss: 4.6925\tLR: 5.548593\n",
      "Training Epoch: 56 [24576/50000]\tLoss: 4.7109\tLR: 5.548849\n",
      "Training Epoch: 56 [24704/50000]\tLoss: 4.7796\tLR: 5.549105\n",
      "Training Epoch: 56 [24832/50000]\tLoss: 4.6508\tLR: 5.549361\n",
      "Training Epoch: 56 [24960/50000]\tLoss: 4.6714\tLR: 5.549616\n",
      "Training Epoch: 56 [25088/50000]\tLoss: 4.6779\tLR: 5.549872\n",
      "Training Epoch: 56 [25216/50000]\tLoss: 4.6995\tLR: 5.550128\n",
      "Training Epoch: 56 [25344/50000]\tLoss: 4.6896\tLR: 5.550384\n",
      "Training Epoch: 56 [25472/50000]\tLoss: 4.7735\tLR: 5.550639\n",
      "Training Epoch: 56 [25600/50000]\tLoss: 4.7361\tLR: 5.550895\n",
      "Training Epoch: 56 [25728/50000]\tLoss: 4.7513\tLR: 5.551151\n",
      "Training Epoch: 56 [25856/50000]\tLoss: 4.6997\tLR: 5.551407\n",
      "Training Epoch: 56 [25984/50000]\tLoss: 4.6973\tLR: 5.551662\n",
      "Training Epoch: 56 [26112/50000]\tLoss: 4.6758\tLR: 5.551918\n",
      "Training Epoch: 56 [26240/50000]\tLoss: 4.7031\tLR: 5.552174\n",
      "Training Epoch: 56 [26368/50000]\tLoss: 4.7118\tLR: 5.552430\n",
      "Training Epoch: 56 [26496/50000]\tLoss: 4.6508\tLR: 5.552685\n",
      "Training Epoch: 56 [26624/50000]\tLoss: 4.6335\tLR: 5.552941\n",
      "Training Epoch: 56 [26752/50000]\tLoss: 4.6818\tLR: 5.553197\n",
      "Training Epoch: 56 [26880/50000]\tLoss: 4.7218\tLR: 5.553453\n",
      "Training Epoch: 56 [27008/50000]\tLoss: 4.7552\tLR: 5.553708\n",
      "Training Epoch: 56 [27136/50000]\tLoss: 4.6865\tLR: 5.553964\n",
      "Training Epoch: 56 [27264/50000]\tLoss: 4.7479\tLR: 5.554220\n",
      "Training Epoch: 56 [27392/50000]\tLoss: 4.7456\tLR: 5.554476\n",
      "Training Epoch: 56 [27520/50000]\tLoss: 4.7264\tLR: 5.554731\n",
      "Training Epoch: 56 [27648/50000]\tLoss: 4.7416\tLR: 5.554987\n",
      "Training Epoch: 56 [27776/50000]\tLoss: 4.7728\tLR: 5.555243\n",
      "Training Epoch: 56 [27904/50000]\tLoss: 4.6486\tLR: 5.555499\n",
      "Training Epoch: 56 [28032/50000]\tLoss: 4.7284\tLR: 5.555754\n",
      "Training Epoch: 56 [28160/50000]\tLoss: 4.6565\tLR: 5.556010\n",
      "Training Epoch: 56 [28288/50000]\tLoss: 4.7084\tLR: 5.556266\n",
      "Training Epoch: 56 [28416/50000]\tLoss: 4.6505\tLR: 5.556522\n",
      "Training Epoch: 56 [28544/50000]\tLoss: 4.6929\tLR: 5.556777\n",
      "Training Epoch: 56 [28672/50000]\tLoss: 4.7302\tLR: 5.557033\n",
      "Training Epoch: 56 [28800/50000]\tLoss: 4.8084\tLR: 5.557289\n",
      "Training Epoch: 56 [28928/50000]\tLoss: 4.6993\tLR: 5.557545\n",
      "Training Epoch: 56 [29056/50000]\tLoss: 4.6918\tLR: 5.557801\n",
      "Training Epoch: 56 [29184/50000]\tLoss: 4.6484\tLR: 5.558056\n",
      "Training Epoch: 56 [29312/50000]\tLoss: 4.7403\tLR: 5.558312\n",
      "Training Epoch: 56 [29440/50000]\tLoss: 4.6968\tLR: 5.558568\n",
      "Training Epoch: 56 [29568/50000]\tLoss: 4.6772\tLR: 5.558824\n",
      "Training Epoch: 56 [29696/50000]\tLoss: 4.7035\tLR: 5.559079\n",
      "Training Epoch: 56 [29824/50000]\tLoss: 4.8233\tLR: 5.559335\n",
      "Training Epoch: 56 [29952/50000]\tLoss: 4.6743\tLR: 5.559591\n",
      "Training Epoch: 56 [30080/50000]\tLoss: 4.6817\tLR: 5.559847\n",
      "Training Epoch: 56 [30208/50000]\tLoss: 4.6549\tLR: 5.560102\n",
      "Training Epoch: 56 [30336/50000]\tLoss: 4.7376\tLR: 5.560358\n",
      "Training Epoch: 56 [30464/50000]\tLoss: 4.7160\tLR: 5.560614\n",
      "Training Epoch: 56 [30592/50000]\tLoss: 4.6949\tLR: 5.560870\n",
      "Training Epoch: 56 [30720/50000]\tLoss: 4.7498\tLR: 5.561125\n",
      "Training Epoch: 56 [30848/50000]\tLoss: 4.7753\tLR: 5.561381\n",
      "Training Epoch: 56 [30976/50000]\tLoss: 4.7647\tLR: 5.561637\n",
      "Training Epoch: 56 [31104/50000]\tLoss: 4.7349\tLR: 5.561893\n",
      "Training Epoch: 56 [31232/50000]\tLoss: 4.6606\tLR: 5.562148\n",
      "Training Epoch: 56 [31360/50000]\tLoss: 4.7099\tLR: 5.562404\n",
      "Training Epoch: 56 [31488/50000]\tLoss: 4.7266\tLR: 5.562660\n",
      "Training Epoch: 56 [31616/50000]\tLoss: 4.7028\tLR: 5.562916\n",
      "Training Epoch: 56 [31744/50000]\tLoss: 4.7000\tLR: 5.563171\n",
      "Training Epoch: 56 [31872/50000]\tLoss: 4.6781\tLR: 5.563427\n",
      "Training Epoch: 56 [32000/50000]\tLoss: 4.7243\tLR: 5.563683\n",
      "Training Epoch: 56 [32128/50000]\tLoss: 4.7480\tLR: 5.563939\n",
      "Training Epoch: 56 [32256/50000]\tLoss: 4.6871\tLR: 5.564194\n",
      "Training Epoch: 56 [32384/50000]\tLoss: 4.8191\tLR: 5.564450\n",
      "Training Epoch: 56 [32512/50000]\tLoss: 4.7523\tLR: 5.564706\n",
      "Training Epoch: 56 [32640/50000]\tLoss: 4.7153\tLR: 5.564962\n",
      "Training Epoch: 56 [32768/50000]\tLoss: 4.7266\tLR: 5.565217\n",
      "Training Epoch: 56 [32896/50000]\tLoss: 4.6532\tLR: 5.565473\n",
      "Training Epoch: 56 [33024/50000]\tLoss: 4.6753\tLR: 5.565729\n",
      "Training Epoch: 56 [33152/50000]\tLoss: 4.6468\tLR: 5.565985\n",
      "Training Epoch: 56 [33280/50000]\tLoss: 4.7738\tLR: 5.566240\n",
      "Training Epoch: 56 [33408/50000]\tLoss: 4.7031\tLR: 5.566496\n",
      "Training Epoch: 56 [33536/50000]\tLoss: 4.7138\tLR: 5.566752\n",
      "Training Epoch: 56 [33664/50000]\tLoss: 4.7756\tLR: 5.567008\n",
      "Training Epoch: 56 [33792/50000]\tLoss: 4.7000\tLR: 5.567263\n",
      "Training Epoch: 56 [33920/50000]\tLoss: 4.7045\tLR: 5.567519\n",
      "Training Epoch: 56 [34048/50000]\tLoss: 4.6921\tLR: 5.567775\n",
      "Training Epoch: 56 [34176/50000]\tLoss: 4.6598\tLR: 5.568031\n",
      "Training Epoch: 56 [34304/50000]\tLoss: 4.6687\tLR: 5.568286\n",
      "Training Epoch: 56 [34432/50000]\tLoss: 4.7384\tLR: 5.568542\n",
      "Training Epoch: 56 [34560/50000]\tLoss: 4.6611\tLR: 5.568798\n",
      "Training Epoch: 56 [34688/50000]\tLoss: 4.7053\tLR: 5.569054\n",
      "Training Epoch: 56 [34816/50000]\tLoss: 4.7167\tLR: 5.569309\n",
      "Training Epoch: 56 [34944/50000]\tLoss: 4.7645\tLR: 5.569565\n",
      "Training Epoch: 56 [35072/50000]\tLoss: 4.7425\tLR: 5.569821\n",
      "Training Epoch: 56 [35200/50000]\tLoss: 4.7800\tLR: 5.570077\n",
      "Training Epoch: 56 [35328/50000]\tLoss: 4.7776\tLR: 5.570332\n",
      "Training Epoch: 56 [35456/50000]\tLoss: 4.7301\tLR: 5.570588\n",
      "Training Epoch: 56 [35584/50000]\tLoss: 4.6901\tLR: 5.570844\n",
      "Training Epoch: 56 [35712/50000]\tLoss: 4.7011\tLR: 5.571100\n",
      "Training Epoch: 56 [35840/50000]\tLoss: 4.7420\tLR: 5.571355\n",
      "Training Epoch: 56 [35968/50000]\tLoss: 4.7146\tLR: 5.571611\n",
      "Training Epoch: 56 [36096/50000]\tLoss: 4.8059\tLR: 5.571867\n",
      "Training Epoch: 56 [36224/50000]\tLoss: 4.6947\tLR: 5.572123\n",
      "Training Epoch: 56 [36352/50000]\tLoss: 4.6816\tLR: 5.572379\n",
      "Training Epoch: 56 [36480/50000]\tLoss: 4.7151\tLR: 5.572634\n",
      "Training Epoch: 56 [36608/50000]\tLoss: 4.7039\tLR: 5.572890\n",
      "Training Epoch: 56 [36736/50000]\tLoss: 4.7571\tLR: 5.573146\n",
      "Training Epoch: 56 [36864/50000]\tLoss: 4.6839\tLR: 5.573402\n",
      "Training Epoch: 56 [36992/50000]\tLoss: 4.8334\tLR: 5.573657\n",
      "Training Epoch: 56 [37120/50000]\tLoss: 4.7801\tLR: 5.573913\n",
      "Training Epoch: 56 [37248/50000]\tLoss: 4.6815\tLR: 5.574169\n",
      "Training Epoch: 56 [37376/50000]\tLoss: 4.6372\tLR: 5.574425\n",
      "Training Epoch: 56 [37504/50000]\tLoss: 4.8128\tLR: 5.574680\n",
      "Training Epoch: 56 [37632/50000]\tLoss: 4.7143\tLR: 5.574936\n",
      "Training Epoch: 56 [37760/50000]\tLoss: 4.6521\tLR: 5.575192\n",
      "Training Epoch: 56 [37888/50000]\tLoss: 4.6813\tLR: 5.575448\n",
      "Training Epoch: 56 [38016/50000]\tLoss: 4.7267\tLR: 5.575703\n",
      "Training Epoch: 56 [38144/50000]\tLoss: 4.7545\tLR: 5.575959\n",
      "Training Epoch: 56 [38272/50000]\tLoss: 4.6956\tLR: 5.576215\n",
      "Training Epoch: 56 [38400/50000]\tLoss: 4.7077\tLR: 5.576471\n",
      "Training Epoch: 56 [38528/50000]\tLoss: 4.6539\tLR: 5.576726\n",
      "Training Epoch: 56 [38656/50000]\tLoss: 4.7413\tLR: 5.576982\n",
      "Training Epoch: 56 [38784/50000]\tLoss: 4.7025\tLR: 5.577238\n",
      "Training Epoch: 56 [38912/50000]\tLoss: 4.6933\tLR: 5.577494\n",
      "Training Epoch: 56 [39040/50000]\tLoss: 4.7162\tLR: 5.577749\n",
      "Training Epoch: 56 [39168/50000]\tLoss: 4.6450\tLR: 5.578005\n",
      "Training Epoch: 56 [39296/50000]\tLoss: 4.8257\tLR: 5.578261\n",
      "Training Epoch: 56 [39424/50000]\tLoss: 4.6659\tLR: 5.578517\n",
      "Training Epoch: 56 [39552/50000]\tLoss: 4.6895\tLR: 5.578772\n",
      "Training Epoch: 56 [39680/50000]\tLoss: 4.6727\tLR: 5.579028\n",
      "Training Epoch: 56 [39808/50000]\tLoss: 4.6712\tLR: 5.579284\n",
      "Training Epoch: 56 [39936/50000]\tLoss: 4.7135\tLR: 5.579540\n",
      "Training Epoch: 56 [40064/50000]\tLoss: 4.6847\tLR: 5.579795\n",
      "Training Epoch: 56 [40192/50000]\tLoss: 4.6687\tLR: 5.580051\n",
      "Training Epoch: 56 [40320/50000]\tLoss: 4.7579\tLR: 5.580307\n",
      "Training Epoch: 56 [40448/50000]\tLoss: 4.7607\tLR: 5.580563\n",
      "Training Epoch: 56 [40576/50000]\tLoss: 4.6690\tLR: 5.580818\n",
      "Training Epoch: 56 [40704/50000]\tLoss: 4.7676\tLR: 5.581074\n",
      "Training Epoch: 56 [40832/50000]\tLoss: 4.6964\tLR: 5.581330\n",
      "Training Epoch: 56 [40960/50000]\tLoss: 4.6996\tLR: 5.581586\n",
      "Training Epoch: 56 [41088/50000]\tLoss: 4.7484\tLR: 5.581841\n",
      "Training Epoch: 56 [41216/50000]\tLoss: 4.7059\tLR: 5.582097\n",
      "Training Epoch: 56 [41344/50000]\tLoss: 4.7437\tLR: 5.582353\n",
      "Training Epoch: 56 [41472/50000]\tLoss: 4.7263\tLR: 5.582609\n",
      "Training Epoch: 56 [41600/50000]\tLoss: 4.7440\tLR: 5.582864\n",
      "Training Epoch: 56 [41728/50000]\tLoss: 4.6862\tLR: 5.583120\n",
      "Training Epoch: 56 [41856/50000]\tLoss: 4.7201\tLR: 5.583376\n",
      "Training Epoch: 56 [41984/50000]\tLoss: 4.7158\tLR: 5.583632\n",
      "Training Epoch: 56 [42112/50000]\tLoss: 4.7498\tLR: 5.583887\n",
      "Training Epoch: 56 [42240/50000]\tLoss: 4.7471\tLR: 5.584143\n",
      "Training Epoch: 56 [42368/50000]\tLoss: 4.7284\tLR: 5.584399\n",
      "Training Epoch: 56 [42496/50000]\tLoss: 4.6481\tLR: 5.584655\n",
      "Training Epoch: 56 [42624/50000]\tLoss: 4.7646\tLR: 5.584910\n",
      "Training Epoch: 56 [42752/50000]\tLoss: 4.6977\tLR: 5.585166\n",
      "Training Epoch: 56 [42880/50000]\tLoss: 4.7140\tLR: 5.585422\n",
      "Training Epoch: 56 [43008/50000]\tLoss: 4.7616\tLR: 5.585678\n",
      "Training Epoch: 56 [43136/50000]\tLoss: 4.7244\tLR: 5.585934\n",
      "Training Epoch: 56 [43264/50000]\tLoss: 4.6923\tLR: 5.586189\n",
      "Training Epoch: 56 [43392/50000]\tLoss: 4.7198\tLR: 5.586445\n",
      "Training Epoch: 56 [43520/50000]\tLoss: 4.7090\tLR: 5.586701\n",
      "Training Epoch: 56 [43648/50000]\tLoss: 4.6817\tLR: 5.586957\n",
      "Training Epoch: 56 [43776/50000]\tLoss: 4.7986\tLR: 5.587212\n",
      "Training Epoch: 56 [43904/50000]\tLoss: 4.7007\tLR: 5.587468\n",
      "Training Epoch: 56 [44032/50000]\tLoss: 4.7441\tLR: 5.587724\n",
      "Training Epoch: 56 [44160/50000]\tLoss: 4.7568\tLR: 5.587980\n",
      "Training Epoch: 56 [44288/50000]\tLoss: 4.7432\tLR: 5.588235\n",
      "Training Epoch: 56 [44416/50000]\tLoss: 4.7426\tLR: 5.588491\n",
      "Training Epoch: 56 [44544/50000]\tLoss: 4.7318\tLR: 5.588747\n",
      "Training Epoch: 56 [44672/50000]\tLoss: 4.7044\tLR: 5.589003\n",
      "Training Epoch: 56 [44800/50000]\tLoss: 4.7871\tLR: 5.589258\n",
      "Training Epoch: 56 [44928/50000]\tLoss: 4.7226\tLR: 5.589514\n",
      "Training Epoch: 56 [45056/50000]\tLoss: 4.6937\tLR: 5.589770\n",
      "Training Epoch: 56 [45184/50000]\tLoss: 4.7226\tLR: 5.590026\n",
      "Training Epoch: 56 [45312/50000]\tLoss: 4.7778\tLR: 5.590281\n",
      "Training Epoch: 56 [45440/50000]\tLoss: 4.7556\tLR: 5.590537\n",
      "Training Epoch: 56 [45568/50000]\tLoss: 4.7476\tLR: 5.590793\n",
      "Training Epoch: 56 [45696/50000]\tLoss: 4.6977\tLR: 5.591049\n",
      "Training Epoch: 56 [45824/50000]\tLoss: 4.6951\tLR: 5.591304\n",
      "Training Epoch: 56 [45952/50000]\tLoss: 4.6967\tLR: 5.591560\n",
      "Training Epoch: 56 [46080/50000]\tLoss: 4.7318\tLR: 5.591816\n",
      "Training Epoch: 56 [46208/50000]\tLoss: 4.6576\tLR: 5.592072\n",
      "Training Epoch: 56 [46336/50000]\tLoss: 4.7253\tLR: 5.592327\n",
      "Training Epoch: 56 [46464/50000]\tLoss: 4.7018\tLR: 5.592583\n",
      "Training Epoch: 56 [46592/50000]\tLoss: 4.7228\tLR: 5.592839\n",
      "Training Epoch: 56 [46720/50000]\tLoss: 4.6751\tLR: 5.593095\n",
      "Training Epoch: 56 [46848/50000]\tLoss: 4.7527\tLR: 5.593350\n",
      "Training Epoch: 56 [46976/50000]\tLoss: 4.7005\tLR: 5.593606\n",
      "Training Epoch: 56 [47104/50000]\tLoss: 4.6949\tLR: 5.593862\n",
      "Training Epoch: 56 [47232/50000]\tLoss: 4.7504\tLR: 5.594118\n",
      "Training Epoch: 56 [47360/50000]\tLoss: 4.7210\tLR: 5.594373\n",
      "Training Epoch: 56 [47488/50000]\tLoss: 4.7567\tLR: 5.594629\n",
      "Training Epoch: 56 [47616/50000]\tLoss: 4.6853\tLR: 5.594885\n",
      "Training Epoch: 56 [47744/50000]\tLoss: 4.6504\tLR: 5.595141\n",
      "Training Epoch: 56 [47872/50000]\tLoss: 4.7475\tLR: 5.595396\n",
      "Training Epoch: 56 [48000/50000]\tLoss: 4.7729\tLR: 5.595652\n",
      "Training Epoch: 56 [48128/50000]\tLoss: 4.7506\tLR: 5.595908\n",
      "Training Epoch: 56 [48256/50000]\tLoss: 4.7814\tLR: 5.596164\n",
      "Training Epoch: 56 [48384/50000]\tLoss: 4.7129\tLR: 5.596419\n",
      "Training Epoch: 56 [48512/50000]\tLoss: 4.7187\tLR: 5.596675\n",
      "Training Epoch: 56 [48640/50000]\tLoss: 4.6958\tLR: 5.596931\n",
      "Training Epoch: 56 [48768/50000]\tLoss: 4.6885\tLR: 5.597187\n",
      "Training Epoch: 56 [48896/50000]\tLoss: 4.7226\tLR: 5.597442\n",
      "Training Epoch: 56 [49024/50000]\tLoss: 4.6181\tLR: 5.597698\n",
      "Training Epoch: 56 [49152/50000]\tLoss: 4.7158\tLR: 5.597954\n",
      "Training Epoch: 56 [49280/50000]\tLoss: 4.7164\tLR: 5.598210\n",
      "Training Epoch: 56 [49408/50000]\tLoss: 4.7561\tLR: 5.598465\n",
      "Training Epoch: 56 [49536/50000]\tLoss: 4.7785\tLR: 5.598721\n",
      "Training Epoch: 56 [49664/50000]\tLoss: 4.6818\tLR: 5.598977\n",
      "Training Epoch: 56 [49792/50000]\tLoss: 4.7114\tLR: 5.599233\n",
      "Training Epoch: 56 [49920/50000]\tLoss: 4.7327\tLR: 5.599488\n",
      "Training Epoch: 56 [50000/50000]\tLoss: 4.7041\tLR: 5.599744\n",
      "epoch 56 training time consumed: 488.95s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   78508 GB |   78508 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   78267 GB |   78267 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     241 GB |     241 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   78508 GB |   78508 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   78267 GB |   78267 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     241 GB |     241 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   77404 GB |   77404 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   77163 GB |   77163 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     241 GB |     241 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    8324 K  |    8324 K  |\n",
      "|       from large pool |      24    |      65    |    3548 K  |    3548 K  |\n",
      "|       from small pool |     231    |     274    |    4776 K  |    4775 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    8324 K  |    8324 K  |\n",
      "|       from large pool |      24    |      65    |    3548 K  |    3548 K  |\n",
      "|       from small pool |     231    |     274    |    4776 K  |    4775 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      47    |    4824 K  |    4824 K  |\n",
      "|       from large pool |      10    |      23    |    1705 K  |    1705 K  |\n",
      "|       from small pool |      26    |      35    |    3118 K  |    3118 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 56, Average loss: 0.0371, Accuracy: 0.0100, Time consumed:31.13s\n",
      "\n",
      "Training Epoch: 57 [128/50000]\tLoss: 4.7088\tLR: 0.100000\n",
      "Training Epoch: 57 [256/50000]\tLoss: 4.7411\tLR: 5.600256\n",
      "Training Epoch: 57 [384/50000]\tLoss: 4.6745\tLR: 5.600512\n",
      "Training Epoch: 57 [512/50000]\tLoss: 4.7034\tLR: 5.600767\n",
      "Training Epoch: 57 [640/50000]\tLoss: 4.6686\tLR: 5.601023\n",
      "Training Epoch: 57 [768/50000]\tLoss: 4.7471\tLR: 5.601279\n",
      "Training Epoch: 57 [896/50000]\tLoss: 4.6559\tLR: 5.601535\n",
      "Training Epoch: 57 [1024/50000]\tLoss: 4.7320\tLR: 5.601790\n",
      "Training Epoch: 57 [1152/50000]\tLoss: 4.7396\tLR: 5.602046\n",
      "Training Epoch: 57 [1280/50000]\tLoss: 4.7413\tLR: 5.602302\n",
      "Training Epoch: 57 [1408/50000]\tLoss: 4.7175\tLR: 5.602558\n",
      "Training Epoch: 57 [1536/50000]\tLoss: 4.7246\tLR: 5.602813\n",
      "Training Epoch: 57 [1664/50000]\tLoss: 4.6716\tLR: 5.603069\n",
      "Training Epoch: 57 [1792/50000]\tLoss: 4.6810\tLR: 5.603325\n",
      "Training Epoch: 57 [1920/50000]\tLoss: 4.6841\tLR: 5.603581\n",
      "Training Epoch: 57 [2048/50000]\tLoss: 4.6918\tLR: 5.603836\n",
      "Training Epoch: 57 [2176/50000]\tLoss: 4.7265\tLR: 5.604092\n",
      "Training Epoch: 57 [2304/50000]\tLoss: 4.7667\tLR: 5.604348\n",
      "Training Epoch: 57 [2432/50000]\tLoss: 4.7968\tLR: 5.604604\n",
      "Training Epoch: 57 [2560/50000]\tLoss: 4.7400\tLR: 5.604859\n",
      "Training Epoch: 57 [2688/50000]\tLoss: 4.8016\tLR: 5.605115\n",
      "Training Epoch: 57 [2816/50000]\tLoss: 4.7134\tLR: 5.605371\n",
      "Training Epoch: 57 [2944/50000]\tLoss: 4.6512\tLR: 5.605627\n",
      "Training Epoch: 57 [3072/50000]\tLoss: 4.6380\tLR: 5.605882\n",
      "Training Epoch: 57 [3200/50000]\tLoss: 4.6958\tLR: 5.606138\n",
      "Training Epoch: 57 [3328/50000]\tLoss: 4.7977\tLR: 5.606394\n",
      "Training Epoch: 57 [3456/50000]\tLoss: 4.7735\tLR: 5.606650\n",
      "Training Epoch: 57 [3584/50000]\tLoss: 4.7289\tLR: 5.606905\n",
      "Training Epoch: 57 [3712/50000]\tLoss: 4.7126\tLR: 5.607161\n",
      "Training Epoch: 57 [3840/50000]\tLoss: 4.7197\tLR: 5.607417\n",
      "Training Epoch: 57 [3968/50000]\tLoss: 4.7380\tLR: 5.607673\n",
      "Training Epoch: 57 [4096/50000]\tLoss: 4.6884\tLR: 5.607928\n",
      "Training Epoch: 57 [4224/50000]\tLoss: 4.8070\tLR: 5.608184\n",
      "Training Epoch: 57 [4352/50000]\tLoss: 4.7137\tLR: 5.608440\n",
      "Training Epoch: 57 [4480/50000]\tLoss: 4.7323\tLR: 5.608696\n",
      "Training Epoch: 57 [4608/50000]\tLoss: 4.6818\tLR: 5.608951\n",
      "Training Epoch: 57 [4736/50000]\tLoss: 4.6605\tLR: 5.609207\n",
      "Training Epoch: 57 [4864/50000]\tLoss: 4.7067\tLR: 5.609463\n",
      "Training Epoch: 57 [4992/50000]\tLoss: 4.7505\tLR: 5.609719\n",
      "Training Epoch: 57 [5120/50000]\tLoss: 4.7411\tLR: 5.609974\n",
      "Training Epoch: 57 [5248/50000]\tLoss: 4.7314\tLR: 5.610230\n",
      "Training Epoch: 57 [5376/50000]\tLoss: 4.7900\tLR: 5.610486\n",
      "Training Epoch: 57 [5504/50000]\tLoss: 4.7463\tLR: 5.610742\n",
      "Training Epoch: 57 [5632/50000]\tLoss: 4.7527\tLR: 5.610997\n",
      "Training Epoch: 57 [5760/50000]\tLoss: 4.7543\tLR: 5.611253\n",
      "Training Epoch: 57 [5888/50000]\tLoss: 4.6886\tLR: 5.611509\n",
      "Training Epoch: 57 [6016/50000]\tLoss: 4.7065\tLR: 5.611765\n",
      "Training Epoch: 57 [6144/50000]\tLoss: 4.6646\tLR: 5.612020\n",
      "Training Epoch: 57 [6272/50000]\tLoss: 4.7583\tLR: 5.612276\n",
      "Training Epoch: 57 [6400/50000]\tLoss: 4.6447\tLR: 5.612532\n",
      "Training Epoch: 57 [6528/50000]\tLoss: 4.7381\tLR: 5.612788\n",
      "Training Epoch: 57 [6656/50000]\tLoss: 4.7271\tLR: 5.613043\n",
      "Training Epoch: 57 [6784/50000]\tLoss: 4.7019\tLR: 5.613299\n",
      "Training Epoch: 57 [6912/50000]\tLoss: 4.6921\tLR: 5.613555\n",
      "Training Epoch: 57 [7040/50000]\tLoss: 4.6777\tLR: 5.613811\n",
      "Training Epoch: 57 [7168/50000]\tLoss: 4.8076\tLR: 5.614066\n",
      "Training Epoch: 57 [7296/50000]\tLoss: 4.7666\tLR: 5.614322\n",
      "Training Epoch: 57 [7424/50000]\tLoss: 4.6631\tLR: 5.614578\n",
      "Training Epoch: 57 [7552/50000]\tLoss: 4.7567\tLR: 5.614834\n",
      "Training Epoch: 57 [7680/50000]\tLoss: 4.7100\tLR: 5.615090\n",
      "Training Epoch: 57 [7808/50000]\tLoss: 4.7425\tLR: 5.615345\n",
      "Training Epoch: 57 [7936/50000]\tLoss: 4.7599\tLR: 5.615601\n",
      "Training Epoch: 57 [8064/50000]\tLoss: 4.7085\tLR: 5.615857\n",
      "Training Epoch: 57 [8192/50000]\tLoss: 4.6725\tLR: 5.616113\n",
      "Training Epoch: 57 [8320/50000]\tLoss: 4.7515\tLR: 5.616368\n",
      "Training Epoch: 57 [8448/50000]\tLoss: 4.7239\tLR: 5.616624\n",
      "Training Epoch: 57 [8576/50000]\tLoss: 4.6602\tLR: 5.616880\n",
      "Training Epoch: 57 [8704/50000]\tLoss: 4.6332\tLR: 5.617136\n",
      "Training Epoch: 57 [8832/50000]\tLoss: 4.7654\tLR: 5.617391\n",
      "Training Epoch: 57 [8960/50000]\tLoss: 4.6437\tLR: 5.617647\n",
      "Training Epoch: 57 [9088/50000]\tLoss: 4.6962\tLR: 5.617903\n",
      "Training Epoch: 57 [9216/50000]\tLoss: 4.7179\tLR: 5.618159\n",
      "Training Epoch: 57 [9344/50000]\tLoss: 4.7481\tLR: 5.618414\n",
      "Training Epoch: 57 [9472/50000]\tLoss: 4.7196\tLR: 5.618670\n",
      "Training Epoch: 57 [9600/50000]\tLoss: 4.7203\tLR: 5.618926\n",
      "Training Epoch: 57 [9728/50000]\tLoss: 4.7584\tLR: 5.619182\n",
      "Training Epoch: 57 [9856/50000]\tLoss: 4.7320\tLR: 5.619437\n",
      "Training Epoch: 57 [9984/50000]\tLoss: 4.6560\tLR: 5.619693\n",
      "Training Epoch: 57 [10112/50000]\tLoss: 4.7690\tLR: 5.619949\n",
      "Training Epoch: 57 [10240/50000]\tLoss: 4.7307\tLR: 5.620205\n",
      "Training Epoch: 57 [10368/50000]\tLoss: 4.6492\tLR: 5.620460\n",
      "Training Epoch: 57 [10496/50000]\tLoss: 4.7076\tLR: 5.620716\n",
      "Training Epoch: 57 [10624/50000]\tLoss: 4.6563\tLR: 5.620972\n",
      "Training Epoch: 57 [10752/50000]\tLoss: 4.6309\tLR: 5.621228\n",
      "Training Epoch: 57 [10880/50000]\tLoss: 4.6866\tLR: 5.621483\n",
      "Training Epoch: 57 [11008/50000]\tLoss: 4.7215\tLR: 5.621739\n",
      "Training Epoch: 57 [11136/50000]\tLoss: 4.7719\tLR: 5.621995\n",
      "Training Epoch: 57 [11264/50000]\tLoss: 4.7188\tLR: 5.622251\n",
      "Training Epoch: 57 [11392/50000]\tLoss: 4.6376\tLR: 5.622506\n",
      "Training Epoch: 57 [11520/50000]\tLoss: 4.6768\tLR: 5.622762\n",
      "Training Epoch: 57 [11648/50000]\tLoss: 4.7608\tLR: 5.623018\n",
      "Training Epoch: 57 [11776/50000]\tLoss: 4.6544\tLR: 5.623274\n",
      "Training Epoch: 57 [11904/50000]\tLoss: 4.7427\tLR: 5.623529\n",
      "Training Epoch: 57 [12032/50000]\tLoss: 4.6754\tLR: 5.623785\n",
      "Training Epoch: 57 [12160/50000]\tLoss: 4.7700\tLR: 5.624041\n",
      "Training Epoch: 57 [12288/50000]\tLoss: 4.7099\tLR: 5.624297\n",
      "Training Epoch: 57 [12416/50000]\tLoss: 4.6287\tLR: 5.624552\n",
      "Training Epoch: 57 [12544/50000]\tLoss: 4.7537\tLR: 5.624808\n",
      "Training Epoch: 57 [12672/50000]\tLoss: 4.7003\tLR: 5.625064\n",
      "Training Epoch: 57 [12800/50000]\tLoss: 4.7017\tLR: 5.625320\n",
      "Training Epoch: 57 [12928/50000]\tLoss: 4.6298\tLR: 5.625575\n",
      "Training Epoch: 57 [13056/50000]\tLoss: 4.7176\tLR: 5.625831\n",
      "Training Epoch: 57 [13184/50000]\tLoss: 4.6823\tLR: 5.626087\n",
      "Training Epoch: 57 [13312/50000]\tLoss: 4.8019\tLR: 5.626343\n",
      "Training Epoch: 57 [13440/50000]\tLoss: 4.7266\tLR: 5.626598\n",
      "Training Epoch: 57 [13568/50000]\tLoss: 4.6737\tLR: 5.626854\n",
      "Training Epoch: 57 [13696/50000]\tLoss: 4.6794\tLR: 5.627110\n",
      "Training Epoch: 57 [13824/50000]\tLoss: 4.6673\tLR: 5.627366\n",
      "Training Epoch: 57 [13952/50000]\tLoss: 4.7130\tLR: 5.627621\n",
      "Training Epoch: 57 [14080/50000]\tLoss: 4.6802\tLR: 5.627877\n",
      "Training Epoch: 57 [14208/50000]\tLoss: 4.7213\tLR: 5.628133\n",
      "Training Epoch: 57 [14336/50000]\tLoss: 4.7499\tLR: 5.628389\n",
      "Training Epoch: 57 [14464/50000]\tLoss: 4.7784\tLR: 5.628645\n",
      "Training Epoch: 57 [14592/50000]\tLoss: 4.6826\tLR: 5.628900\n",
      "Training Epoch: 57 [14720/50000]\tLoss: 4.6511\tLR: 5.629156\n",
      "Training Epoch: 57 [14848/50000]\tLoss: 4.7223\tLR: 5.629412\n",
      "Training Epoch: 57 [14976/50000]\tLoss: 4.6054\tLR: 5.629668\n",
      "Training Epoch: 57 [15104/50000]\tLoss: 4.7956\tLR: 5.629923\n",
      "Training Epoch: 57 [15232/50000]\tLoss: 4.7185\tLR: 5.630179\n",
      "Training Epoch: 57 [15360/50000]\tLoss: 4.7308\tLR: 5.630435\n",
      "Training Epoch: 57 [15488/50000]\tLoss: 4.7328\tLR: 5.630691\n",
      "Training Epoch: 57 [15616/50000]\tLoss: 4.7978\tLR: 5.630946\n",
      "Training Epoch: 57 [15744/50000]\tLoss: 4.7641\tLR: 5.631202\n",
      "Training Epoch: 57 [15872/50000]\tLoss: 4.6785\tLR: 5.631458\n",
      "Training Epoch: 57 [16000/50000]\tLoss: 4.6368\tLR: 5.631714\n",
      "Training Epoch: 57 [16128/50000]\tLoss: 4.7171\tLR: 5.631969\n",
      "Training Epoch: 57 [16256/50000]\tLoss: 4.7460\tLR: 5.632225\n",
      "Training Epoch: 57 [16384/50000]\tLoss: 4.7063\tLR: 5.632481\n",
      "Training Epoch: 57 [16512/50000]\tLoss: 4.7475\tLR: 5.632737\n",
      "Training Epoch: 57 [16640/50000]\tLoss: 4.7072\tLR: 5.632992\n",
      "Training Epoch: 57 [16768/50000]\tLoss: 4.6754\tLR: 5.633248\n",
      "Training Epoch: 57 [16896/50000]\tLoss: 4.7268\tLR: 5.633504\n",
      "Training Epoch: 57 [17024/50000]\tLoss: 4.7200\tLR: 5.633760\n",
      "Training Epoch: 57 [17152/50000]\tLoss: 4.7476\tLR: 5.634015\n",
      "Training Epoch: 57 [17280/50000]\tLoss: 4.7093\tLR: 5.634271\n",
      "Training Epoch: 57 [17408/50000]\tLoss: 4.7721\tLR: 5.634527\n",
      "Training Epoch: 57 [17536/50000]\tLoss: 4.6881\tLR: 5.634783\n",
      "Training Epoch: 57 [17664/50000]\tLoss: 4.6789\tLR: 5.635038\n",
      "Training Epoch: 57 [17792/50000]\tLoss: 4.6863\tLR: 5.635294\n",
      "Training Epoch: 57 [17920/50000]\tLoss: 4.7096\tLR: 5.635550\n",
      "Training Epoch: 57 [18048/50000]\tLoss: 4.6651\tLR: 5.635806\n",
      "Training Epoch: 57 [18176/50000]\tLoss: 4.7032\tLR: 5.636061\n",
      "Training Epoch: 57 [18304/50000]\tLoss: 4.6654\tLR: 5.636317\n",
      "Training Epoch: 57 [18432/50000]\tLoss: 4.7236\tLR: 5.636573\n",
      "Training Epoch: 57 [18560/50000]\tLoss: 4.6649\tLR: 5.636829\n",
      "Training Epoch: 57 [18688/50000]\tLoss: 4.6524\tLR: 5.637084\n",
      "Training Epoch: 57 [18816/50000]\tLoss: 4.7309\tLR: 5.637340\n",
      "Training Epoch: 57 [18944/50000]\tLoss: 4.7759\tLR: 5.637596\n",
      "Training Epoch: 57 [19072/50000]\tLoss: 4.7755\tLR: 5.637852\n",
      "Training Epoch: 57 [19200/50000]\tLoss: 4.8081\tLR: 5.638107\n",
      "Training Epoch: 57 [19328/50000]\tLoss: 4.6751\tLR: 5.638363\n",
      "Training Epoch: 57 [19456/50000]\tLoss: 4.7549\tLR: 5.638619\n",
      "Training Epoch: 57 [19584/50000]\tLoss: 4.6688\tLR: 5.638875\n",
      "Training Epoch: 57 [19712/50000]\tLoss: 4.6455\tLR: 5.639130\n",
      "Training Epoch: 57 [19840/50000]\tLoss: 4.7023\tLR: 5.639386\n",
      "Training Epoch: 57 [19968/50000]\tLoss: 4.7237\tLR: 5.639642\n",
      "Training Epoch: 57 [20096/50000]\tLoss: 4.6866\tLR: 5.639898\n",
      "Training Epoch: 57 [20224/50000]\tLoss: 4.7389\tLR: 5.640153\n",
      "Training Epoch: 57 [20352/50000]\tLoss: 4.6912\tLR: 5.640409\n",
      "Training Epoch: 57 [20480/50000]\tLoss: 4.7245\tLR: 5.640665\n",
      "Training Epoch: 57 [20608/50000]\tLoss: 4.7174\tLR: 5.640921\n",
      "Training Epoch: 57 [20736/50000]\tLoss: 4.6894\tLR: 5.641176\n",
      "Training Epoch: 57 [20864/50000]\tLoss: 4.6508\tLR: 5.641432\n",
      "Training Epoch: 57 [20992/50000]\tLoss: 4.7877\tLR: 5.641688\n",
      "Training Epoch: 57 [21120/50000]\tLoss: 4.7864\tLR: 5.641944\n",
      "Training Epoch: 57 [21248/50000]\tLoss: 4.7528\tLR: 5.642199\n",
      "Training Epoch: 57 [21376/50000]\tLoss: 4.6968\tLR: 5.642455\n",
      "Training Epoch: 57 [21504/50000]\tLoss: 4.7281\tLR: 5.642711\n",
      "Training Epoch: 57 [21632/50000]\tLoss: 4.7912\tLR: 5.642967\n",
      "Training Epoch: 57 [21760/50000]\tLoss: 4.7360\tLR: 5.643223\n",
      "Training Epoch: 57 [21888/50000]\tLoss: 4.7153\tLR: 5.643478\n",
      "Training Epoch: 57 [22016/50000]\tLoss: 4.6498\tLR: 5.643734\n",
      "Training Epoch: 57 [22144/50000]\tLoss: 4.7392\tLR: 5.643990\n",
      "Training Epoch: 57 [22272/50000]\tLoss: 4.7507\tLR: 5.644246\n",
      "Training Epoch: 57 [22400/50000]\tLoss: 4.6995\tLR: 5.644501\n",
      "Training Epoch: 57 [22528/50000]\tLoss: 4.7255\tLR: 5.644757\n",
      "Training Epoch: 57 [22656/50000]\tLoss: 4.7703\tLR: 5.645013\n",
      "Training Epoch: 57 [22784/50000]\tLoss: 4.6558\tLR: 5.645269\n",
      "Training Epoch: 57 [22912/50000]\tLoss: 4.6829\tLR: 5.645524\n",
      "Training Epoch: 57 [23040/50000]\tLoss: 4.7402\tLR: 5.645780\n",
      "Training Epoch: 57 [23168/50000]\tLoss: 4.7035\tLR: 5.646036\n",
      "Training Epoch: 57 [23296/50000]\tLoss: 4.6461\tLR: 5.646292\n",
      "Training Epoch: 57 [23424/50000]\tLoss: 4.6927\tLR: 5.646547\n",
      "Training Epoch: 57 [23552/50000]\tLoss: 4.6792\tLR: 5.646803\n",
      "Training Epoch: 57 [23680/50000]\tLoss: 4.6846\tLR: 5.647059\n",
      "Training Epoch: 57 [23808/50000]\tLoss: 4.7180\tLR: 5.647315\n",
      "Training Epoch: 57 [23936/50000]\tLoss: 4.7846\tLR: 5.647570\n",
      "Training Epoch: 57 [24064/50000]\tLoss: 4.7356\tLR: 5.647826\n",
      "Training Epoch: 57 [24192/50000]\tLoss: 4.6542\tLR: 5.648082\n",
      "Training Epoch: 57 [24320/50000]\tLoss: 4.7069\tLR: 5.648338\n",
      "Training Epoch: 57 [24448/50000]\tLoss: 4.6695\tLR: 5.648593\n",
      "Training Epoch: 57 [24576/50000]\tLoss: 4.6761\tLR: 5.648849\n",
      "Training Epoch: 57 [24704/50000]\tLoss: 4.6884\tLR: 5.649105\n",
      "Training Epoch: 57 [24832/50000]\tLoss: 4.7380\tLR: 5.649361\n",
      "Training Epoch: 57 [24960/50000]\tLoss: 4.7265\tLR: 5.649616\n",
      "Training Epoch: 57 [25088/50000]\tLoss: 4.7252\tLR: 5.649872\n",
      "Training Epoch: 57 [25216/50000]\tLoss: 4.6824\tLR: 5.650128\n",
      "Training Epoch: 57 [25344/50000]\tLoss: 4.6720\tLR: 5.650384\n",
      "Training Epoch: 57 [25472/50000]\tLoss: 4.6371\tLR: 5.650639\n",
      "Training Epoch: 57 [25600/50000]\tLoss: 4.7263\tLR: 5.650895\n",
      "Training Epoch: 57 [25728/50000]\tLoss: 4.6858\tLR: 5.651151\n",
      "Training Epoch: 57 [25856/50000]\tLoss: 4.6744\tLR: 5.651407\n",
      "Training Epoch: 57 [25984/50000]\tLoss: 4.7204\tLR: 5.651662\n",
      "Training Epoch: 57 [26112/50000]\tLoss: 4.6787\tLR: 5.651918\n",
      "Training Epoch: 57 [26240/50000]\tLoss: 4.7102\tLR: 5.652174\n",
      "Training Epoch: 57 [26368/50000]\tLoss: 4.7618\tLR: 5.652430\n",
      "Training Epoch: 57 [26496/50000]\tLoss: 4.6789\tLR: 5.652685\n",
      "Training Epoch: 57 [26624/50000]\tLoss: 4.7361\tLR: 5.652941\n",
      "Training Epoch: 57 [26752/50000]\tLoss: 4.8022\tLR: 5.653197\n",
      "Training Epoch: 57 [26880/50000]\tLoss: 4.7159\tLR: 5.653453\n",
      "Training Epoch: 57 [27008/50000]\tLoss: 4.7174\tLR: 5.653708\n",
      "Training Epoch: 57 [27136/50000]\tLoss: 4.7182\tLR: 5.653964\n",
      "Training Epoch: 57 [27264/50000]\tLoss: 4.6766\tLR: 5.654220\n",
      "Training Epoch: 57 [27392/50000]\tLoss: 4.6918\tLR: 5.654476\n",
      "Training Epoch: 57 [27520/50000]\tLoss: 4.7199\tLR: 5.654731\n",
      "Training Epoch: 57 [27648/50000]\tLoss: 4.6912\tLR: 5.654987\n",
      "Training Epoch: 57 [27776/50000]\tLoss: 4.8241\tLR: 5.655243\n",
      "Training Epoch: 57 [27904/50000]\tLoss: 4.6630\tLR: 5.655499\n",
      "Training Epoch: 57 [28032/50000]\tLoss: 4.8171\tLR: 5.655754\n",
      "Training Epoch: 57 [28160/50000]\tLoss: 4.7207\tLR: 5.656010\n",
      "Training Epoch: 57 [28288/50000]\tLoss: 4.7084\tLR: 5.656266\n",
      "Training Epoch: 57 [28416/50000]\tLoss: 4.7670\tLR: 5.656522\n",
      "Training Epoch: 57 [28544/50000]\tLoss: 4.7068\tLR: 5.656777\n",
      "Training Epoch: 57 [28672/50000]\tLoss: 4.7393\tLR: 5.657033\n",
      "Training Epoch: 57 [28800/50000]\tLoss: 4.7019\tLR: 5.657289\n",
      "Training Epoch: 57 [28928/50000]\tLoss: 4.7245\tLR: 5.657545\n",
      "Training Epoch: 57 [29056/50000]\tLoss: 4.7132\tLR: 5.657801\n",
      "Training Epoch: 57 [29184/50000]\tLoss: 4.7763\tLR: 5.658056\n",
      "Training Epoch: 57 [29312/50000]\tLoss: 4.7420\tLR: 5.658312\n",
      "Training Epoch: 57 [29440/50000]\tLoss: 4.7381\tLR: 5.658568\n",
      "Training Epoch: 57 [29568/50000]\tLoss: 4.6888\tLR: 5.658824\n",
      "Training Epoch: 57 [29696/50000]\tLoss: 4.7203\tLR: 5.659079\n",
      "Training Epoch: 57 [29824/50000]\tLoss: 4.7010\tLR: 5.659335\n",
      "Training Epoch: 57 [29952/50000]\tLoss: 4.6604\tLR: 5.659591\n",
      "Training Epoch: 57 [30080/50000]\tLoss: 4.7059\tLR: 5.659847\n",
      "Training Epoch: 57 [30208/50000]\tLoss: 4.7640\tLR: 5.660102\n",
      "Training Epoch: 57 [30336/50000]\tLoss: 4.6878\tLR: 5.660358\n",
      "Training Epoch: 57 [30464/50000]\tLoss: 4.7353\tLR: 5.660614\n",
      "Training Epoch: 57 [30592/50000]\tLoss: 4.7034\tLR: 5.660870\n",
      "Training Epoch: 57 [30720/50000]\tLoss: 4.7696\tLR: 5.661125\n",
      "Training Epoch: 57 [30848/50000]\tLoss: 4.7224\tLR: 5.661381\n",
      "Training Epoch: 57 [30976/50000]\tLoss: 4.6742\tLR: 5.661637\n",
      "Training Epoch: 57 [31104/50000]\tLoss: 4.6946\tLR: 5.661893\n",
      "Training Epoch: 57 [31232/50000]\tLoss: 4.6801\tLR: 5.662148\n",
      "Training Epoch: 57 [31360/50000]\tLoss: 4.7948\tLR: 5.662404\n",
      "Training Epoch: 57 [31488/50000]\tLoss: 4.6908\tLR: 5.662660\n",
      "Training Epoch: 57 [31616/50000]\tLoss: 4.6904\tLR: 5.662916\n",
      "Training Epoch: 57 [31744/50000]\tLoss: 4.7089\tLR: 5.663171\n",
      "Training Epoch: 57 [31872/50000]\tLoss: 4.7417\tLR: 5.663427\n",
      "Training Epoch: 57 [32000/50000]\tLoss: 4.6524\tLR: 5.663683\n",
      "Training Epoch: 57 [32128/50000]\tLoss: 4.6997\tLR: 5.663939\n",
      "Training Epoch: 57 [32256/50000]\tLoss: 4.6185\tLR: 5.664194\n",
      "Training Epoch: 57 [32384/50000]\tLoss: 4.6740\tLR: 5.664450\n",
      "Training Epoch: 57 [32512/50000]\tLoss: 4.6808\tLR: 5.664706\n",
      "Training Epoch: 57 [32640/50000]\tLoss: 4.7615\tLR: 5.664962\n",
      "Training Epoch: 57 [32768/50000]\tLoss: 4.7114\tLR: 5.665217\n",
      "Training Epoch: 57 [32896/50000]\tLoss: 4.6963\tLR: 5.665473\n",
      "Training Epoch: 57 [33024/50000]\tLoss: 4.6585\tLR: 5.665729\n",
      "Training Epoch: 57 [33152/50000]\tLoss: 4.7744\tLR: 5.665985\n",
      "Training Epoch: 57 [33280/50000]\tLoss: 4.7029\tLR: 5.666240\n",
      "Training Epoch: 57 [33408/50000]\tLoss: 4.6735\tLR: 5.666496\n",
      "Training Epoch: 57 [33536/50000]\tLoss: 4.7372\tLR: 5.666752\n",
      "Training Epoch: 57 [33664/50000]\tLoss: 4.6959\tLR: 5.667008\n",
      "Training Epoch: 57 [33792/50000]\tLoss: 4.7322\tLR: 5.667263\n",
      "Training Epoch: 57 [33920/50000]\tLoss: 4.6474\tLR: 5.667519\n",
      "Training Epoch: 57 [34048/50000]\tLoss: 4.7544\tLR: 5.667775\n",
      "Training Epoch: 57 [34176/50000]\tLoss: 4.6455\tLR: 5.668031\n",
      "Training Epoch: 57 [34304/50000]\tLoss: 4.6856\tLR: 5.668286\n",
      "Training Epoch: 57 [34432/50000]\tLoss: 4.6929\tLR: 5.668542\n",
      "Training Epoch: 57 [34560/50000]\tLoss: 4.6584\tLR: 5.668798\n",
      "Training Epoch: 57 [34688/50000]\tLoss: 4.6754\tLR: 5.669054\n",
      "Training Epoch: 57 [34816/50000]\tLoss: 4.7481\tLR: 5.669309\n",
      "Training Epoch: 57 [34944/50000]\tLoss: 4.7393\tLR: 5.669565\n",
      "Training Epoch: 57 [35072/50000]\tLoss: 4.6858\tLR: 5.669821\n",
      "Training Epoch: 57 [35200/50000]\tLoss: 4.6397\tLR: 5.670077\n",
      "Training Epoch: 57 [35328/50000]\tLoss: 4.7639\tLR: 5.670332\n",
      "Training Epoch: 57 [35456/50000]\tLoss: 4.7485\tLR: 5.670588\n",
      "Training Epoch: 57 [35584/50000]\tLoss: 4.7057\tLR: 5.670844\n",
      "Training Epoch: 57 [35712/50000]\tLoss: 4.7190\tLR: 5.671100\n",
      "Training Epoch: 57 [35840/50000]\tLoss: 4.7230\tLR: 5.671355\n",
      "Training Epoch: 57 [35968/50000]\tLoss: 4.8311\tLR: 5.671611\n",
      "Training Epoch: 57 [36096/50000]\tLoss: 4.6620\tLR: 5.671867\n",
      "Training Epoch: 57 [36224/50000]\tLoss: 4.6896\tLR: 5.672123\n",
      "Training Epoch: 57 [36352/50000]\tLoss: 4.7011\tLR: 5.672379\n",
      "Training Epoch: 57 [36480/50000]\tLoss: 4.7086\tLR: 5.672634\n",
      "Training Epoch: 57 [36608/50000]\tLoss: 4.7017\tLR: 5.672890\n",
      "Training Epoch: 57 [36736/50000]\tLoss: 4.6860\tLR: 5.673146\n",
      "Training Epoch: 57 [36864/50000]\tLoss: 4.7864\tLR: 5.673402\n",
      "Training Epoch: 57 [36992/50000]\tLoss: 4.7409\tLR: 5.673657\n",
      "Training Epoch: 57 [37120/50000]\tLoss: 4.6512\tLR: 5.673913\n",
      "Training Epoch: 57 [37248/50000]\tLoss: 4.7319\tLR: 5.674169\n",
      "Training Epoch: 57 [37376/50000]\tLoss: 4.7225\tLR: 5.674425\n",
      "Training Epoch: 57 [37504/50000]\tLoss: 4.7829\tLR: 5.674680\n",
      "Training Epoch: 57 [37632/50000]\tLoss: 4.7654\tLR: 5.674936\n",
      "Training Epoch: 57 [37760/50000]\tLoss: 4.7043\tLR: 5.675192\n",
      "Training Epoch: 57 [37888/50000]\tLoss: 4.7110\tLR: 5.675448\n",
      "Training Epoch: 57 [38016/50000]\tLoss: 4.7403\tLR: 5.675703\n",
      "Training Epoch: 57 [38144/50000]\tLoss: 4.7009\tLR: 5.675959\n",
      "Training Epoch: 57 [38272/50000]\tLoss: 4.7091\tLR: 5.676215\n",
      "Training Epoch: 57 [38400/50000]\tLoss: 4.7036\tLR: 5.676471\n",
      "Training Epoch: 57 [38528/50000]\tLoss: 4.6699\tLR: 5.676726\n",
      "Training Epoch: 57 [38656/50000]\tLoss: 4.7643\tLR: 5.676982\n",
      "Training Epoch: 57 [38784/50000]\tLoss: 4.7035\tLR: 5.677238\n",
      "Training Epoch: 57 [38912/50000]\tLoss: 4.8027\tLR: 5.677494\n",
      "Training Epoch: 57 [39040/50000]\tLoss: 4.7720\tLR: 5.677749\n",
      "Training Epoch: 57 [39168/50000]\tLoss: 4.7112\tLR: 5.678005\n",
      "Training Epoch: 57 [39296/50000]\tLoss: 4.6499\tLR: 5.678261\n",
      "Training Epoch: 57 [39424/50000]\tLoss: 4.7446\tLR: 5.678517\n",
      "Training Epoch: 57 [39552/50000]\tLoss: 4.6587\tLR: 5.678772\n",
      "Training Epoch: 57 [39680/50000]\tLoss: 4.7133\tLR: 5.679028\n",
      "Training Epoch: 57 [39808/50000]\tLoss: 4.7072\tLR: 5.679284\n",
      "Training Epoch: 57 [39936/50000]\tLoss: 4.7046\tLR: 5.679540\n",
      "Training Epoch: 57 [40064/50000]\tLoss: 4.7197\tLR: 5.679795\n",
      "Training Epoch: 57 [40192/50000]\tLoss: 4.6841\tLR: 5.680051\n",
      "Training Epoch: 57 [40320/50000]\tLoss: 4.7215\tLR: 5.680307\n",
      "Training Epoch: 57 [40448/50000]\tLoss: 4.7750\tLR: 5.680563\n",
      "Training Epoch: 57 [40576/50000]\tLoss: 4.7963\tLR: 5.680818\n",
      "Training Epoch: 57 [40704/50000]\tLoss: 4.7264\tLR: 5.681074\n",
      "Training Epoch: 57 [40832/50000]\tLoss: 4.7034\tLR: 5.681330\n",
      "Training Epoch: 57 [40960/50000]\tLoss: 4.7098\tLR: 5.681586\n",
      "Training Epoch: 57 [41088/50000]\tLoss: 4.6526\tLR: 5.681841\n",
      "Training Epoch: 57 [41216/50000]\tLoss: 4.7815\tLR: 5.682097\n",
      "Training Epoch: 57 [41344/50000]\tLoss: 4.7462\tLR: 5.682353\n",
      "Training Epoch: 57 [41472/50000]\tLoss: 4.6997\tLR: 5.682609\n",
      "Training Epoch: 57 [41600/50000]\tLoss: 4.7048\tLR: 5.682864\n",
      "Training Epoch: 57 [41728/50000]\tLoss: 4.6915\tLR: 5.683120\n",
      "Training Epoch: 57 [41856/50000]\tLoss: 4.7048\tLR: 5.683376\n",
      "Training Epoch: 57 [41984/50000]\tLoss: 4.6594\tLR: 5.683632\n",
      "Training Epoch: 57 [42112/50000]\tLoss: 4.7062\tLR: 5.683887\n",
      "Training Epoch: 57 [42240/50000]\tLoss: 4.7265\tLR: 5.684143\n",
      "Training Epoch: 57 [42368/50000]\tLoss: 4.6745\tLR: 5.684399\n",
      "Training Epoch: 57 [42496/50000]\tLoss: 4.6865\tLR: 5.684655\n",
      "Training Epoch: 57 [42624/50000]\tLoss: 4.7575\tLR: 5.684910\n",
      "Training Epoch: 57 [42752/50000]\tLoss: 4.7151\tLR: 5.685166\n",
      "Training Epoch: 57 [42880/50000]\tLoss: 4.7228\tLR: 5.685422\n",
      "Training Epoch: 57 [43008/50000]\tLoss: 4.6619\tLR: 5.685678\n",
      "Training Epoch: 57 [43136/50000]\tLoss: 4.7393\tLR: 5.685934\n",
      "Training Epoch: 57 [43264/50000]\tLoss: 4.7126\tLR: 5.686189\n",
      "Training Epoch: 57 [43392/50000]\tLoss: 4.7197\tLR: 5.686445\n",
      "Training Epoch: 57 [43520/50000]\tLoss: 4.7566\tLR: 5.686701\n",
      "Training Epoch: 57 [43648/50000]\tLoss: 4.6672\tLR: 5.686957\n",
      "Training Epoch: 57 [43776/50000]\tLoss: 4.6994\tLR: 5.687212\n",
      "Training Epoch: 57 [43904/50000]\tLoss: 4.6923\tLR: 5.687468\n",
      "Training Epoch: 57 [44032/50000]\tLoss: 4.6944\tLR: 5.687724\n",
      "Training Epoch: 57 [44160/50000]\tLoss: 4.6833\tLR: 5.687980\n",
      "Training Epoch: 57 [44288/50000]\tLoss: 4.7133\tLR: 5.688235\n",
      "Training Epoch: 57 [44416/50000]\tLoss: 4.6855\tLR: 5.688491\n",
      "Training Epoch: 57 [44544/50000]\tLoss: 4.7597\tLR: 5.688747\n",
      "Training Epoch: 57 [44672/50000]\tLoss: 4.6375\tLR: 5.689003\n",
      "Training Epoch: 57 [44800/50000]\tLoss: 4.7157\tLR: 5.689258\n",
      "Training Epoch: 57 [44928/50000]\tLoss: 4.6980\tLR: 5.689514\n",
      "Training Epoch: 57 [45056/50000]\tLoss: 4.7268\tLR: 5.689770\n",
      "Training Epoch: 57 [45184/50000]\tLoss: 4.7006\tLR: 5.690026\n",
      "Training Epoch: 57 [45312/50000]\tLoss: 4.6444\tLR: 5.690281\n",
      "Training Epoch: 57 [45440/50000]\tLoss: 4.6651\tLR: 5.690537\n",
      "Training Epoch: 57 [45568/50000]\tLoss: 4.7150\tLR: 5.690793\n",
      "Training Epoch: 57 [45696/50000]\tLoss: 4.6419\tLR: 5.691049\n",
      "Training Epoch: 57 [45824/50000]\tLoss: 4.6489\tLR: 5.691304\n",
      "Training Epoch: 57 [45952/50000]\tLoss: 4.6240\tLR: 5.691560\n",
      "Training Epoch: 57 [46080/50000]\tLoss: 4.7542\tLR: 5.691816\n",
      "Training Epoch: 57 [46208/50000]\tLoss: 4.7064\tLR: 5.692072\n",
      "Training Epoch: 57 [46336/50000]\tLoss: 4.7004\tLR: 5.692327\n",
      "Training Epoch: 57 [46464/50000]\tLoss: 4.6588\tLR: 5.692583\n",
      "Training Epoch: 57 [46592/50000]\tLoss: 4.6343\tLR: 5.692839\n",
      "Training Epoch: 57 [46720/50000]\tLoss: 4.6916\tLR: 5.693095\n",
      "Training Epoch: 57 [46848/50000]\tLoss: 4.6904\tLR: 5.693350\n",
      "Training Epoch: 57 [46976/50000]\tLoss: 4.7495\tLR: 5.693606\n",
      "Training Epoch: 57 [47104/50000]\tLoss: 4.6602\tLR: 5.693862\n",
      "Training Epoch: 57 [47232/50000]\tLoss: 4.7360\tLR: 5.694118\n",
      "Training Epoch: 57 [47360/50000]\tLoss: 4.7193\tLR: 5.694373\n",
      "Training Epoch: 57 [47488/50000]\tLoss: 4.7647\tLR: 5.694629\n",
      "Training Epoch: 57 [47616/50000]\tLoss: 4.6977\tLR: 5.694885\n",
      "Training Epoch: 57 [47744/50000]\tLoss: 4.7295\tLR: 5.695141\n",
      "Training Epoch: 57 [47872/50000]\tLoss: 4.6927\tLR: 5.695396\n",
      "Training Epoch: 57 [48000/50000]\tLoss: 4.7351\tLR: 5.695652\n",
      "Training Epoch: 57 [48128/50000]\tLoss: 4.7079\tLR: 5.695908\n",
      "Training Epoch: 57 [48256/50000]\tLoss: 4.7044\tLR: 5.696164\n",
      "Training Epoch: 57 [48384/50000]\tLoss: 4.6626\tLR: 5.696419\n",
      "Training Epoch: 57 [48512/50000]\tLoss: 4.7617\tLR: 5.696675\n",
      "Training Epoch: 57 [48640/50000]\tLoss: 4.6667\tLR: 5.696931\n",
      "Training Epoch: 57 [48768/50000]\tLoss: 4.7050\tLR: 5.697187\n",
      "Training Epoch: 57 [48896/50000]\tLoss: 4.6479\tLR: 5.697442\n",
      "Training Epoch: 57 [49024/50000]\tLoss: 4.7225\tLR: 5.697698\n",
      "Training Epoch: 57 [49152/50000]\tLoss: 4.6949\tLR: 5.697954\n",
      "Training Epoch: 57 [49280/50000]\tLoss: 4.7787\tLR: 5.698210\n",
      "Training Epoch: 57 [49408/50000]\tLoss: 4.7068\tLR: 5.698465\n",
      "Training Epoch: 57 [49536/50000]\tLoss: 4.7171\tLR: 5.698721\n",
      "Training Epoch: 57 [49664/50000]\tLoss: 4.7936\tLR: 5.698977\n",
      "Training Epoch: 57 [49792/50000]\tLoss: 4.6457\tLR: 5.699233\n",
      "Training Epoch: 57 [49920/50000]\tLoss: 4.6884\tLR: 5.699488\n",
      "Training Epoch: 57 [50000/50000]\tLoss: 4.6774\tLR: 5.699744\n",
      "epoch 57 training time consumed: 489.04s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   79910 GB |   79910 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   79665 GB |   79665 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     245 GB |     245 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   79910 GB |   79910 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   79665 GB |   79665 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     245 GB |     245 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   78786 GB |   78786 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   78541 GB |   78541 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     245 GB |     245 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    8473 K  |    8473 K  |\n",
      "|       from large pool |      24    |      65    |    3612 K  |    3612 K  |\n",
      "|       from small pool |     231    |     274    |    4861 K  |    4861 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    8473 K  |    8473 K  |\n",
      "|       from large pool |      24    |      65    |    3612 K  |    3612 K  |\n",
      "|       from small pool |     231    |     274    |    4861 K  |    4861 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      47    |    4910 K  |    4910 K  |\n",
      "|       from large pool |      10    |      23    |    1736 K  |    1736 K  |\n",
      "|       from small pool |      27    |      35    |    3174 K  |    3174 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 57, Average loss: 0.0372, Accuracy: 0.0100, Time consumed:31.06s\n",
      "\n",
      "Training Epoch: 58 [128/50000]\tLoss: 4.7531\tLR: 0.100000\n",
      "Training Epoch: 58 [256/50000]\tLoss: 4.7740\tLR: 5.700256\n",
      "Training Epoch: 58 [384/50000]\tLoss: 4.6648\tLR: 5.700512\n",
      "Training Epoch: 58 [512/50000]\tLoss: 4.6608\tLR: 5.700767\n",
      "Training Epoch: 58 [640/50000]\tLoss: 4.6542\tLR: 5.701023\n",
      "Training Epoch: 58 [768/50000]\tLoss: 4.7279\tLR: 5.701279\n",
      "Training Epoch: 58 [896/50000]\tLoss: 4.7248\tLR: 5.701535\n",
      "Training Epoch: 58 [1024/50000]\tLoss: 4.7617\tLR: 5.701790\n",
      "Training Epoch: 58 [1152/50000]\tLoss: 4.7139\tLR: 5.702046\n",
      "Training Epoch: 58 [1280/50000]\tLoss: 4.8152\tLR: 5.702302\n",
      "Training Epoch: 58 [1408/50000]\tLoss: 4.7018\tLR: 5.702558\n",
      "Training Epoch: 58 [1536/50000]\tLoss: 4.7457\tLR: 5.702813\n",
      "Training Epoch: 58 [1664/50000]\tLoss: 4.7814\tLR: 5.703069\n",
      "Training Epoch: 58 [1792/50000]\tLoss: 4.6958\tLR: 5.703325\n",
      "Training Epoch: 58 [1920/50000]\tLoss: 4.6727\tLR: 5.703581\n",
      "Training Epoch: 58 [2048/50000]\tLoss: 4.7159\tLR: 5.703836\n",
      "Training Epoch: 58 [2176/50000]\tLoss: 4.6908\tLR: 5.704092\n",
      "Training Epoch: 58 [2304/50000]\tLoss: 4.7684\tLR: 5.704348\n",
      "Training Epoch: 58 [2432/50000]\tLoss: 4.7387\tLR: 5.704604\n",
      "Training Epoch: 58 [2560/50000]\tLoss: 4.7277\tLR: 5.704859\n",
      "Training Epoch: 58 [2688/50000]\tLoss: 4.6388\tLR: 5.705115\n",
      "Training Epoch: 58 [2816/50000]\tLoss: 4.6994\tLR: 5.705371\n",
      "Training Epoch: 58 [2944/50000]\tLoss: 4.6706\tLR: 5.705627\n",
      "Training Epoch: 58 [3072/50000]\tLoss: 4.7058\tLR: 5.705882\n",
      "Training Epoch: 58 [3200/50000]\tLoss: 4.7014\tLR: 5.706138\n",
      "Training Epoch: 58 [3328/50000]\tLoss: 4.7869\tLR: 5.706394\n",
      "Training Epoch: 58 [3456/50000]\tLoss: 4.6771\tLR: 5.706650\n",
      "Training Epoch: 58 [3584/50000]\tLoss: 4.7544\tLR: 5.706905\n",
      "Training Epoch: 58 [3712/50000]\tLoss: 4.7020\tLR: 5.707161\n",
      "Training Epoch: 58 [3840/50000]\tLoss: 4.7320\tLR: 5.707417\n",
      "Training Epoch: 58 [3968/50000]\tLoss: 4.7845\tLR: 5.707673\n",
      "Training Epoch: 58 [4096/50000]\tLoss: 4.6930\tLR: 5.707928\n",
      "Training Epoch: 58 [4224/50000]\tLoss: 4.6420\tLR: 5.708184\n",
      "Training Epoch: 58 [4352/50000]\tLoss: 4.8171\tLR: 5.708440\n",
      "Training Epoch: 58 [4480/50000]\tLoss: 4.7121\tLR: 5.708696\n",
      "Training Epoch: 58 [4608/50000]\tLoss: 4.7780\tLR: 5.708951\n",
      "Training Epoch: 58 [4736/50000]\tLoss: 4.7051\tLR: 5.709207\n",
      "Training Epoch: 58 [4864/50000]\tLoss: 4.7267\tLR: 5.709463\n",
      "Training Epoch: 58 [4992/50000]\tLoss: 4.6762\tLR: 5.709719\n",
      "Training Epoch: 58 [5120/50000]\tLoss: 4.6941\tLR: 5.709974\n",
      "Training Epoch: 58 [5248/50000]\tLoss: 4.7214\tLR: 5.710230\n",
      "Training Epoch: 58 [5376/50000]\tLoss: 4.7332\tLR: 5.710486\n",
      "Training Epoch: 58 [5504/50000]\tLoss: 4.7403\tLR: 5.710742\n",
      "Training Epoch: 58 [5632/50000]\tLoss: 4.7136\tLR: 5.710997\n",
      "Training Epoch: 58 [5760/50000]\tLoss: 4.7213\tLR: 5.711253\n",
      "Training Epoch: 58 [5888/50000]\tLoss: 4.7705\tLR: 5.711509\n",
      "Training Epoch: 58 [6016/50000]\tLoss: 4.6842\tLR: 5.711765\n",
      "Training Epoch: 58 [6144/50000]\tLoss: 4.7146\tLR: 5.712020\n",
      "Training Epoch: 58 [6272/50000]\tLoss: 4.7258\tLR: 5.712276\n",
      "Training Epoch: 58 [6400/50000]\tLoss: 4.7959\tLR: 5.712532\n",
      "Training Epoch: 58 [6528/50000]\tLoss: 4.7591\tLR: 5.712788\n",
      "Training Epoch: 58 [6656/50000]\tLoss: 4.7227\tLR: 5.713043\n",
      "Training Epoch: 58 [6784/50000]\tLoss: 4.7664\tLR: 5.713299\n",
      "Training Epoch: 58 [6912/50000]\tLoss: 4.7596\tLR: 5.713555\n",
      "Training Epoch: 58 [7040/50000]\tLoss: 4.7396\tLR: 5.713811\n",
      "Training Epoch: 58 [7168/50000]\tLoss: 4.6609\tLR: 5.714066\n",
      "Training Epoch: 58 [7296/50000]\tLoss: 4.6770\tLR: 5.714322\n",
      "Training Epoch: 58 [7424/50000]\tLoss: 4.7099\tLR: 5.714578\n",
      "Training Epoch: 58 [7552/50000]\tLoss: 4.6808\tLR: 5.714834\n",
      "Training Epoch: 58 [7680/50000]\tLoss: 4.8011\tLR: 5.715090\n",
      "Training Epoch: 58 [7808/50000]\tLoss: 4.7151\tLR: 5.715345\n",
      "Training Epoch: 58 [7936/50000]\tLoss: 4.7563\tLR: 5.715601\n",
      "Training Epoch: 58 [8064/50000]\tLoss: 4.7070\tLR: 5.715857\n",
      "Training Epoch: 58 [8192/50000]\tLoss: 4.6574\tLR: 5.716113\n",
      "Training Epoch: 58 [8320/50000]\tLoss: 4.6869\tLR: 5.716368\n",
      "Training Epoch: 58 [8448/50000]\tLoss: 4.6985\tLR: 5.716624\n",
      "Training Epoch: 58 [8576/50000]\tLoss: 4.6531\tLR: 5.716880\n",
      "Training Epoch: 58 [8704/50000]\tLoss: 4.6545\tLR: 5.717136\n",
      "Training Epoch: 58 [8832/50000]\tLoss: 4.7619\tLR: 5.717391\n",
      "Training Epoch: 58 [8960/50000]\tLoss: 4.6853\tLR: 5.717647\n",
      "Training Epoch: 58 [9088/50000]\tLoss: 4.6431\tLR: 5.717903\n",
      "Training Epoch: 58 [9216/50000]\tLoss: 4.7229\tLR: 5.718159\n",
      "Training Epoch: 58 [9344/50000]\tLoss: 4.7085\tLR: 5.718414\n",
      "Training Epoch: 58 [9472/50000]\tLoss: 4.8221\tLR: 5.718670\n",
      "Training Epoch: 58 [9600/50000]\tLoss: 4.7568\tLR: 5.718926\n",
      "Training Epoch: 58 [9728/50000]\tLoss: 4.6655\tLR: 5.719182\n",
      "Training Epoch: 58 [9856/50000]\tLoss: 4.7267\tLR: 5.719437\n",
      "Training Epoch: 58 [9984/50000]\tLoss: 4.6915\tLR: 5.719693\n",
      "Training Epoch: 58 [10112/50000]\tLoss: 4.7323\tLR: 5.719949\n",
      "Training Epoch: 58 [10240/50000]\tLoss: 4.6810\tLR: 5.720205\n",
      "Training Epoch: 58 [10368/50000]\tLoss: 4.7497\tLR: 5.720460\n",
      "Training Epoch: 58 [10496/50000]\tLoss: 4.7027\tLR: 5.720716\n",
      "Training Epoch: 58 [10624/50000]\tLoss: 4.6739\tLR: 5.720972\n",
      "Training Epoch: 58 [10752/50000]\tLoss: 4.7970\tLR: 5.721228\n",
      "Training Epoch: 58 [10880/50000]\tLoss: 4.7764\tLR: 5.721483\n",
      "Training Epoch: 58 [11008/50000]\tLoss: 4.7479\tLR: 5.721739\n",
      "Training Epoch: 58 [11136/50000]\tLoss: 4.6917\tLR: 5.721995\n",
      "Training Epoch: 58 [11264/50000]\tLoss: 4.7283\tLR: 5.722251\n",
      "Training Epoch: 58 [11392/50000]\tLoss: 4.7676\tLR: 5.722506\n",
      "Training Epoch: 58 [11520/50000]\tLoss: 4.6599\tLR: 5.722762\n",
      "Training Epoch: 58 [11648/50000]\tLoss: 4.7341\tLR: 5.723018\n",
      "Training Epoch: 58 [11776/50000]\tLoss: 4.7727\tLR: 5.723274\n",
      "Training Epoch: 58 [11904/50000]\tLoss: 4.7408\tLR: 5.723529\n",
      "Training Epoch: 58 [12032/50000]\tLoss: 4.6867\tLR: 5.723785\n",
      "Training Epoch: 58 [12160/50000]\tLoss: 4.6997\tLR: 5.724041\n",
      "Training Epoch: 58 [12288/50000]\tLoss: 4.6444\tLR: 5.724297\n",
      "Training Epoch: 58 [12416/50000]\tLoss: 4.7225\tLR: 5.724552\n",
      "Training Epoch: 58 [12544/50000]\tLoss: 4.6861\tLR: 5.724808\n",
      "Training Epoch: 58 [12672/50000]\tLoss: 4.7287\tLR: 5.725064\n",
      "Training Epoch: 58 [12800/50000]\tLoss: 4.7726\tLR: 5.725320\n",
      "Training Epoch: 58 [12928/50000]\tLoss: 4.7023\tLR: 5.725575\n",
      "Training Epoch: 58 [13056/50000]\tLoss: 4.7290\tLR: 5.725831\n",
      "Training Epoch: 58 [13184/50000]\tLoss: 4.6874\tLR: 5.726087\n",
      "Training Epoch: 58 [13312/50000]\tLoss: 4.6231\tLR: 5.726343\n",
      "Training Epoch: 58 [13440/50000]\tLoss: 4.7687\tLR: 5.726598\n",
      "Training Epoch: 58 [13568/50000]\tLoss: 4.7111\tLR: 5.726854\n",
      "Training Epoch: 58 [13696/50000]\tLoss: 4.7119\tLR: 5.727110\n",
      "Training Epoch: 58 [13824/50000]\tLoss: 4.7718\tLR: 5.727366\n",
      "Training Epoch: 58 [13952/50000]\tLoss: 4.7488\tLR: 5.727621\n",
      "Training Epoch: 58 [14080/50000]\tLoss: 4.7107\tLR: 5.727877\n",
      "Training Epoch: 58 [14208/50000]\tLoss: 4.6465\tLR: 5.728133\n",
      "Training Epoch: 58 [14336/50000]\tLoss: 4.7097\tLR: 5.728389\n",
      "Training Epoch: 58 [14464/50000]\tLoss: 4.7080\tLR: 5.728645\n",
      "Training Epoch: 58 [14592/50000]\tLoss: 4.7906\tLR: 5.728900\n",
      "Training Epoch: 58 [14720/50000]\tLoss: 4.7039\tLR: 5.729156\n",
      "Training Epoch: 58 [14848/50000]\tLoss: 4.7017\tLR: 5.729412\n",
      "Training Epoch: 58 [14976/50000]\tLoss: 4.8092\tLR: 5.729668\n",
      "Training Epoch: 58 [15104/50000]\tLoss: 4.7281\tLR: 5.729923\n",
      "Training Epoch: 58 [15232/50000]\tLoss: 4.6396\tLR: 5.730179\n",
      "Training Epoch: 58 [15360/50000]\tLoss: 4.7274\tLR: 5.730435\n",
      "Training Epoch: 58 [15488/50000]\tLoss: 4.7589\tLR: 5.730691\n",
      "Training Epoch: 58 [15616/50000]\tLoss: 4.6221\tLR: 5.730946\n",
      "Training Epoch: 58 [15744/50000]\tLoss: 4.7345\tLR: 5.731202\n",
      "Training Epoch: 58 [15872/50000]\tLoss: 4.7741\tLR: 5.731458\n",
      "Training Epoch: 58 [16000/50000]\tLoss: 4.7660\tLR: 5.731714\n",
      "Training Epoch: 58 [16128/50000]\tLoss: 4.6539\tLR: 5.731969\n",
      "Training Epoch: 58 [16256/50000]\tLoss: 4.7684\tLR: 5.732225\n",
      "Training Epoch: 58 [16384/50000]\tLoss: 4.7305\tLR: 5.732481\n",
      "Training Epoch: 58 [16512/50000]\tLoss: 4.6566\tLR: 5.732737\n",
      "Training Epoch: 58 [16640/50000]\tLoss: 4.7463\tLR: 5.732992\n",
      "Training Epoch: 58 [16768/50000]\tLoss: 4.7099\tLR: 5.733248\n",
      "Training Epoch: 58 [16896/50000]\tLoss: 4.7065\tLR: 5.733504\n",
      "Training Epoch: 58 [17024/50000]\tLoss: 4.7089\tLR: 5.733760\n",
      "Training Epoch: 58 [17152/50000]\tLoss: 4.7093\tLR: 5.734015\n",
      "Training Epoch: 58 [17280/50000]\tLoss: 4.7265\tLR: 5.734271\n",
      "Training Epoch: 58 [17408/50000]\tLoss: 4.7650\tLR: 5.734527\n",
      "Training Epoch: 58 [17536/50000]\tLoss: 4.7488\tLR: 5.734783\n",
      "Training Epoch: 58 [17664/50000]\tLoss: 4.7400\tLR: 5.735038\n",
      "Training Epoch: 58 [17792/50000]\tLoss: 4.6966\tLR: 5.735294\n",
      "Training Epoch: 58 [17920/50000]\tLoss: 4.6904\tLR: 5.735550\n",
      "Training Epoch: 58 [18048/50000]\tLoss: 4.7119\tLR: 5.735806\n",
      "Training Epoch: 58 [18176/50000]\tLoss: 4.6951\tLR: 5.736061\n",
      "Training Epoch: 58 [18304/50000]\tLoss: 4.6924\tLR: 5.736317\n",
      "Training Epoch: 58 [18432/50000]\tLoss: 4.6714\tLR: 5.736573\n",
      "Training Epoch: 58 [18560/50000]\tLoss: 4.7388\tLR: 5.736829\n",
      "Training Epoch: 58 [18688/50000]\tLoss: 4.7125\tLR: 5.737084\n",
      "Training Epoch: 58 [18816/50000]\tLoss: 4.7244\tLR: 5.737340\n",
      "Training Epoch: 58 [18944/50000]\tLoss: 4.6659\tLR: 5.737596\n",
      "Training Epoch: 58 [19072/50000]\tLoss: 4.7615\tLR: 5.737852\n",
      "Training Epoch: 58 [19200/50000]\tLoss: 4.7238\tLR: 5.738107\n",
      "Training Epoch: 58 [19328/50000]\tLoss: 4.7787\tLR: 5.738363\n",
      "Training Epoch: 58 [19456/50000]\tLoss: 4.6577\tLR: 5.738619\n",
      "Training Epoch: 58 [19584/50000]\tLoss: 4.7006\tLR: 5.738875\n",
      "Training Epoch: 58 [19712/50000]\tLoss: 4.7018\tLR: 5.739130\n",
      "Training Epoch: 58 [19840/50000]\tLoss: 4.6704\tLR: 5.739386\n",
      "Training Epoch: 58 [19968/50000]\tLoss: 4.7554\tLR: 5.739642\n",
      "Training Epoch: 58 [20096/50000]\tLoss: 4.7164\tLR: 5.739898\n",
      "Training Epoch: 58 [20224/50000]\tLoss: 4.7318\tLR: 5.740153\n",
      "Training Epoch: 58 [20352/50000]\tLoss: 4.7414\tLR: 5.740409\n",
      "Training Epoch: 58 [20480/50000]\tLoss: 4.6869\tLR: 5.740665\n",
      "Training Epoch: 58 [20608/50000]\tLoss: 4.6845\tLR: 5.740921\n",
      "Training Epoch: 58 [20736/50000]\tLoss: 4.7545\tLR: 5.741176\n",
      "Training Epoch: 58 [20864/50000]\tLoss: 4.6849\tLR: 5.741432\n",
      "Training Epoch: 58 [20992/50000]\tLoss: 4.7420\tLR: 5.741688\n",
      "Training Epoch: 58 [21120/50000]\tLoss: 4.6884\tLR: 5.741944\n",
      "Training Epoch: 58 [21248/50000]\tLoss: 4.6126\tLR: 5.742199\n",
      "Training Epoch: 58 [21376/50000]\tLoss: 4.7658\tLR: 5.742455\n",
      "Training Epoch: 58 [21504/50000]\tLoss: 4.6835\tLR: 5.742711\n",
      "Training Epoch: 58 [21632/50000]\tLoss: 4.7338\tLR: 5.742967\n",
      "Training Epoch: 58 [21760/50000]\tLoss: 4.7095\tLR: 5.743223\n",
      "Training Epoch: 58 [21888/50000]\tLoss: 4.6963\tLR: 5.743478\n",
      "Training Epoch: 58 [22016/50000]\tLoss: 4.7359\tLR: 5.743734\n",
      "Training Epoch: 58 [22144/50000]\tLoss: 4.6547\tLR: 5.743990\n",
      "Training Epoch: 58 [22272/50000]\tLoss: 4.6668\tLR: 5.744246\n",
      "Training Epoch: 58 [22400/50000]\tLoss: 4.6380\tLR: 5.744501\n",
      "Training Epoch: 58 [22528/50000]\tLoss: 4.7371\tLR: 5.744757\n",
      "Training Epoch: 58 [22656/50000]\tLoss: 4.6605\tLR: 5.745013\n",
      "Training Epoch: 58 [22784/50000]\tLoss: 4.7129\tLR: 5.745269\n",
      "Training Epoch: 58 [22912/50000]\tLoss: 4.6499\tLR: 5.745524\n",
      "Training Epoch: 58 [23040/50000]\tLoss: 4.7076\tLR: 5.745780\n",
      "Training Epoch: 58 [23168/50000]\tLoss: 4.7074\tLR: 5.746036\n",
      "Training Epoch: 58 [23296/50000]\tLoss: 4.6982\tLR: 5.746292\n",
      "Training Epoch: 58 [23424/50000]\tLoss: 4.7121\tLR: 5.746547\n",
      "Training Epoch: 58 [23552/50000]\tLoss: 4.8176\tLR: 5.746803\n",
      "Training Epoch: 58 [23680/50000]\tLoss: 4.6743\tLR: 5.747059\n",
      "Training Epoch: 58 [23808/50000]\tLoss: 4.6785\tLR: 5.747315\n",
      "Training Epoch: 58 [23936/50000]\tLoss: 4.6921\tLR: 5.747570\n",
      "Training Epoch: 58 [24064/50000]\tLoss: 4.6541\tLR: 5.747826\n",
      "Training Epoch: 58 [24192/50000]\tLoss: 4.7088\tLR: 5.748082\n",
      "Training Epoch: 58 [24320/50000]\tLoss: 4.7216\tLR: 5.748338\n",
      "Training Epoch: 58 [24448/50000]\tLoss: 4.7164\tLR: 5.748593\n",
      "Training Epoch: 58 [24576/50000]\tLoss: 4.6175\tLR: 5.748849\n",
      "Training Epoch: 58 [24704/50000]\tLoss: 4.7498\tLR: 5.749105\n",
      "Training Epoch: 58 [24832/50000]\tLoss: 4.8190\tLR: 5.749361\n",
      "Training Epoch: 58 [24960/50000]\tLoss: 4.8230\tLR: 5.749616\n",
      "Training Epoch: 58 [25088/50000]\tLoss: 4.7917\tLR: 5.749872\n",
      "Training Epoch: 58 [25216/50000]\tLoss: 4.6913\tLR: 5.750128\n",
      "Training Epoch: 58 [25344/50000]\tLoss: 4.5723\tLR: 5.750384\n",
      "Training Epoch: 58 [25472/50000]\tLoss: 4.6951\tLR: 5.750639\n",
      "Training Epoch: 58 [25600/50000]\tLoss: 4.7490\tLR: 5.750895\n",
      "Training Epoch: 58 [25728/50000]\tLoss: 4.7948\tLR: 5.751151\n",
      "Training Epoch: 58 [25856/50000]\tLoss: 4.7387\tLR: 5.751407\n",
      "Training Epoch: 58 [25984/50000]\tLoss: 4.7337\tLR: 5.751662\n",
      "Training Epoch: 58 [26112/50000]\tLoss: 4.7659\tLR: 5.751918\n",
      "Training Epoch: 58 [26240/50000]\tLoss: 4.6959\tLR: 5.752174\n",
      "Training Epoch: 58 [26368/50000]\tLoss: 4.7174\tLR: 5.752430\n",
      "Training Epoch: 58 [26496/50000]\tLoss: 4.7582\tLR: 5.752685\n",
      "Training Epoch: 58 [26624/50000]\tLoss: 4.7239\tLR: 5.752941\n",
      "Training Epoch: 58 [26752/50000]\tLoss: 4.7328\tLR: 5.753197\n",
      "Training Epoch: 58 [26880/50000]\tLoss: 4.7466\tLR: 5.753453\n",
      "Training Epoch: 58 [27008/50000]\tLoss: 4.7380\tLR: 5.753708\n",
      "Training Epoch: 58 [27136/50000]\tLoss: 4.7310\tLR: 5.753964\n",
      "Training Epoch: 58 [27264/50000]\tLoss: 4.7652\tLR: 5.754220\n",
      "Training Epoch: 58 [27392/50000]\tLoss: 4.7941\tLR: 5.754476\n",
      "Training Epoch: 58 [27520/50000]\tLoss: 4.6932\tLR: 5.754731\n",
      "Training Epoch: 58 [27648/50000]\tLoss: 4.7476\tLR: 5.754987\n",
      "Training Epoch: 58 [27776/50000]\tLoss: 4.7174\tLR: 5.755243\n",
      "Training Epoch: 58 [27904/50000]\tLoss: 4.6750\tLR: 5.755499\n",
      "Training Epoch: 58 [28032/50000]\tLoss: 4.6989\tLR: 5.755754\n",
      "Training Epoch: 58 [28160/50000]\tLoss: 4.6939\tLR: 5.756010\n",
      "Training Epoch: 58 [28288/50000]\tLoss: 4.7487\tLR: 5.756266\n",
      "Training Epoch: 58 [28416/50000]\tLoss: 4.7007\tLR: 5.756522\n",
      "Training Epoch: 58 [28544/50000]\tLoss: 4.7359\tLR: 5.756777\n",
      "Training Epoch: 58 [28672/50000]\tLoss: 4.6741\tLR: 5.757033\n",
      "Training Epoch: 58 [28800/50000]\tLoss: 4.6535\tLR: 5.757289\n",
      "Training Epoch: 58 [28928/50000]\tLoss: 4.7129\tLR: 5.757545\n",
      "Training Epoch: 58 [29056/50000]\tLoss: 4.7338\tLR: 5.757801\n",
      "Training Epoch: 58 [29184/50000]\tLoss: 4.6698\tLR: 5.758056\n",
      "Training Epoch: 58 [29312/50000]\tLoss: 4.7096\tLR: 5.758312\n",
      "Training Epoch: 58 [29440/50000]\tLoss: 4.6134\tLR: 5.758568\n",
      "Training Epoch: 58 [29568/50000]\tLoss: 4.6779\tLR: 5.758824\n",
      "Training Epoch: 58 [29696/50000]\tLoss: 4.7255\tLR: 5.759079\n",
      "Training Epoch: 58 [29824/50000]\tLoss: 4.6505\tLR: 5.759335\n",
      "Training Epoch: 58 [29952/50000]\tLoss: 4.7081\tLR: 5.759591\n",
      "Training Epoch: 58 [30080/50000]\tLoss: 4.7464\tLR: 5.759847\n",
      "Training Epoch: 58 [30208/50000]\tLoss: 4.6737\tLR: 5.760102\n",
      "Training Epoch: 58 [30336/50000]\tLoss: 4.6662\tLR: 5.760358\n",
      "Training Epoch: 58 [30464/50000]\tLoss: 4.7342\tLR: 5.760614\n",
      "Training Epoch: 58 [30592/50000]\tLoss: 4.7419\tLR: 5.760870\n",
      "Training Epoch: 58 [30720/50000]\tLoss: 4.6421\tLR: 5.761125\n",
      "Training Epoch: 58 [30848/50000]\tLoss: 4.7163\tLR: 5.761381\n",
      "Training Epoch: 58 [30976/50000]\tLoss: 4.6359\tLR: 5.761637\n",
      "Training Epoch: 58 [31104/50000]\tLoss: 4.6997\tLR: 5.761893\n",
      "Training Epoch: 58 [31232/50000]\tLoss: 4.7009\tLR: 5.762148\n",
      "Training Epoch: 58 [31360/50000]\tLoss: 4.7336\tLR: 5.762404\n",
      "Training Epoch: 58 [31488/50000]\tLoss: 4.7327\tLR: 5.762660\n",
      "Training Epoch: 58 [31616/50000]\tLoss: 4.6894\tLR: 5.762916\n",
      "Training Epoch: 58 [31744/50000]\tLoss: 4.6941\tLR: 5.763171\n",
      "Training Epoch: 58 [31872/50000]\tLoss: 4.7746\tLR: 5.763427\n",
      "Training Epoch: 58 [32000/50000]\tLoss: 4.7457\tLR: 5.763683\n",
      "Training Epoch: 58 [32128/50000]\tLoss: 4.8400\tLR: 5.763939\n",
      "Training Epoch: 58 [32256/50000]\tLoss: 4.7018\tLR: 5.764194\n",
      "Training Epoch: 58 [32384/50000]\tLoss: 4.6389\tLR: 5.764450\n",
      "Training Epoch: 58 [32512/50000]\tLoss: 4.7542\tLR: 5.764706\n",
      "Training Epoch: 58 [32640/50000]\tLoss: 4.7196\tLR: 5.764962\n",
      "Training Epoch: 58 [32768/50000]\tLoss: 4.7843\tLR: 5.765217\n",
      "Training Epoch: 58 [32896/50000]\tLoss: 4.7037\tLR: 5.765473\n",
      "Training Epoch: 58 [33024/50000]\tLoss: 4.7310\tLR: 5.765729\n",
      "Training Epoch: 58 [33152/50000]\tLoss: 4.6552\tLR: 5.765985\n",
      "Training Epoch: 58 [33280/50000]\tLoss: 4.6638\tLR: 5.766240\n",
      "Training Epoch: 58 [33408/50000]\tLoss: 4.7176\tLR: 5.766496\n",
      "Training Epoch: 58 [33536/50000]\tLoss: 4.6795\tLR: 5.766752\n",
      "Training Epoch: 58 [33664/50000]\tLoss: 4.7069\tLR: 5.767008\n",
      "Training Epoch: 58 [33792/50000]\tLoss: 4.7351\tLR: 5.767263\n",
      "Training Epoch: 58 [33920/50000]\tLoss: 4.6616\tLR: 5.767519\n",
      "Training Epoch: 58 [34048/50000]\tLoss: 4.7085\tLR: 5.767775\n",
      "Training Epoch: 58 [34176/50000]\tLoss: 4.6651\tLR: 5.768031\n",
      "Training Epoch: 58 [34304/50000]\tLoss: 4.6787\tLR: 5.768286\n",
      "Training Epoch: 58 [34432/50000]\tLoss: 4.7246\tLR: 5.768542\n",
      "Training Epoch: 58 [34560/50000]\tLoss: 4.7381\tLR: 5.768798\n",
      "Training Epoch: 58 [34688/50000]\tLoss: 4.6993\tLR: 5.769054\n",
      "Training Epoch: 58 [34816/50000]\tLoss: 4.7534\tLR: 5.769309\n",
      "Training Epoch: 58 [34944/50000]\tLoss: 4.7696\tLR: 5.769565\n",
      "Training Epoch: 58 [35072/50000]\tLoss: 4.7258\tLR: 5.769821\n",
      "Training Epoch: 58 [35200/50000]\tLoss: 4.7044\tLR: 5.770077\n",
      "Training Epoch: 58 [35328/50000]\tLoss: 4.7064\tLR: 5.770332\n",
      "Training Epoch: 58 [35456/50000]\tLoss: 4.7740\tLR: 5.770588\n",
      "Training Epoch: 58 [35584/50000]\tLoss: 4.6879\tLR: 5.770844\n",
      "Training Epoch: 58 [35712/50000]\tLoss: 4.6348\tLR: 5.771100\n",
      "Training Epoch: 58 [35840/50000]\tLoss: 4.7768\tLR: 5.771355\n",
      "Training Epoch: 58 [35968/50000]\tLoss: 4.6928\tLR: 5.771611\n",
      "Training Epoch: 58 [36096/50000]\tLoss: 4.6970\tLR: 5.771867\n",
      "Training Epoch: 58 [36224/50000]\tLoss: 4.6740\tLR: 5.772123\n",
      "Training Epoch: 58 [36352/50000]\tLoss: 4.6266\tLR: 5.772379\n",
      "Training Epoch: 58 [36480/50000]\tLoss: 4.7006\tLR: 5.772634\n",
      "Training Epoch: 58 [36608/50000]\tLoss: 4.7679\tLR: 5.772890\n",
      "Training Epoch: 58 [36736/50000]\tLoss: 4.7566\tLR: 5.773146\n",
      "Training Epoch: 58 [36864/50000]\tLoss: 4.7641\tLR: 5.773402\n",
      "Training Epoch: 58 [36992/50000]\tLoss: 4.7340\tLR: 5.773657\n",
      "Training Epoch: 58 [37120/50000]\tLoss: 4.7309\tLR: 5.773913\n",
      "Training Epoch: 58 [37248/50000]\tLoss: 4.6381\tLR: 5.774169\n",
      "Training Epoch: 58 [37376/50000]\tLoss: 4.6571\tLR: 5.774425\n",
      "Training Epoch: 58 [37504/50000]\tLoss: 4.7215\tLR: 5.774680\n",
      "Training Epoch: 58 [37632/50000]\tLoss: 4.7065\tLR: 5.774936\n",
      "Training Epoch: 58 [37760/50000]\tLoss: 4.7651\tLR: 5.775192\n",
      "Training Epoch: 58 [37888/50000]\tLoss: 4.6884\tLR: 5.775448\n",
      "Training Epoch: 58 [38016/50000]\tLoss: 4.7099\tLR: 5.775703\n",
      "Training Epoch: 58 [38144/50000]\tLoss: 4.7646\tLR: 5.775959\n",
      "Training Epoch: 58 [38272/50000]\tLoss: 4.7574\tLR: 5.776215\n",
      "Training Epoch: 58 [38400/50000]\tLoss: 4.7098\tLR: 5.776471\n",
      "Training Epoch: 58 [38528/50000]\tLoss: 4.8143\tLR: 5.776726\n",
      "Training Epoch: 58 [38656/50000]\tLoss: 4.7786\tLR: 5.776982\n",
      "Training Epoch: 58 [38784/50000]\tLoss: 4.7904\tLR: 5.777238\n",
      "Training Epoch: 58 [38912/50000]\tLoss: 4.7047\tLR: 5.777494\n",
      "Training Epoch: 58 [39040/50000]\tLoss: 4.7162\tLR: 5.777749\n",
      "Training Epoch: 58 [39168/50000]\tLoss: 4.7502\tLR: 5.778005\n",
      "Training Epoch: 58 [39296/50000]\tLoss: 4.7083\tLR: 5.778261\n",
      "Training Epoch: 58 [39424/50000]\tLoss: 4.7129\tLR: 5.778517\n",
      "Training Epoch: 58 [39552/50000]\tLoss: 4.7302\tLR: 5.778772\n",
      "Training Epoch: 58 [39680/50000]\tLoss: 4.6136\tLR: 5.779028\n",
      "Training Epoch: 58 [39808/50000]\tLoss: 4.7314\tLR: 5.779284\n",
      "Training Epoch: 58 [39936/50000]\tLoss: 4.6714\tLR: 5.779540\n",
      "Training Epoch: 58 [40064/50000]\tLoss: 4.7951\tLR: 5.779795\n",
      "Training Epoch: 58 [40192/50000]\tLoss: 4.7142\tLR: 5.780051\n",
      "Training Epoch: 58 [40320/50000]\tLoss: 4.7282\tLR: 5.780307\n",
      "Training Epoch: 58 [40448/50000]\tLoss: 4.7462\tLR: 5.780563\n",
      "Training Epoch: 58 [40576/50000]\tLoss: 4.7086\tLR: 5.780818\n",
      "Training Epoch: 58 [40704/50000]\tLoss: 4.6683\tLR: 5.781074\n",
      "Training Epoch: 58 [40832/50000]\tLoss: 4.7393\tLR: 5.781330\n",
      "Training Epoch: 58 [40960/50000]\tLoss: 4.7443\tLR: 5.781586\n",
      "Training Epoch: 58 [41088/50000]\tLoss: 4.7513\tLR: 5.781841\n",
      "Training Epoch: 58 [41216/50000]\tLoss: 4.7336\tLR: 5.782097\n",
      "Training Epoch: 58 [41344/50000]\tLoss: 4.7282\tLR: 5.782353\n",
      "Training Epoch: 58 [41472/50000]\tLoss: 4.7412\tLR: 5.782609\n",
      "Training Epoch: 58 [41600/50000]\tLoss: 4.6678\tLR: 5.782864\n",
      "Training Epoch: 58 [41728/50000]\tLoss: 4.7127\tLR: 5.783120\n",
      "Training Epoch: 58 [41856/50000]\tLoss: 4.6538\tLR: 5.783376\n",
      "Training Epoch: 58 [41984/50000]\tLoss: 4.7505\tLR: 5.783632\n",
      "Training Epoch: 58 [42112/50000]\tLoss: 4.7785\tLR: 5.783887\n",
      "Training Epoch: 58 [42240/50000]\tLoss: 4.7308\tLR: 5.784143\n",
      "Training Epoch: 58 [42368/50000]\tLoss: 4.6824\tLR: 5.784399\n",
      "Training Epoch: 58 [42496/50000]\tLoss: 4.7130\tLR: 5.784655\n",
      "Training Epoch: 58 [42624/50000]\tLoss: 4.6277\tLR: 5.784910\n",
      "Training Epoch: 58 [42752/50000]\tLoss: 4.7771\tLR: 5.785166\n",
      "Training Epoch: 58 [42880/50000]\tLoss: 4.6920\tLR: 5.785422\n",
      "Training Epoch: 58 [43008/50000]\tLoss: 4.7287\tLR: 5.785678\n",
      "Training Epoch: 58 [43136/50000]\tLoss: 4.7766\tLR: 5.785934\n",
      "Training Epoch: 58 [43264/50000]\tLoss: 4.6969\tLR: 5.786189\n",
      "Training Epoch: 58 [43392/50000]\tLoss: 4.6957\tLR: 5.786445\n",
      "Training Epoch: 58 [43520/50000]\tLoss: 4.7011\tLR: 5.786701\n",
      "Training Epoch: 58 [43648/50000]\tLoss: 4.7191\tLR: 5.786957\n",
      "Training Epoch: 58 [43776/50000]\tLoss: 4.7120\tLR: 5.787212\n",
      "Training Epoch: 58 [43904/50000]\tLoss: 4.7802\tLR: 5.787468\n",
      "Training Epoch: 58 [44032/50000]\tLoss: 4.6924\tLR: 5.787724\n",
      "Training Epoch: 58 [44160/50000]\tLoss: 4.7106\tLR: 5.787980\n",
      "Training Epoch: 58 [44288/50000]\tLoss: 4.6735\tLR: 5.788235\n",
      "Training Epoch: 58 [44416/50000]\tLoss: 4.7271\tLR: 5.788491\n",
      "Training Epoch: 58 [44544/50000]\tLoss: 4.7335\tLR: 5.788747\n",
      "Training Epoch: 58 [44672/50000]\tLoss: 4.7059\tLR: 5.789003\n",
      "Training Epoch: 58 [44800/50000]\tLoss: 4.6679\tLR: 5.789258\n",
      "Training Epoch: 58 [44928/50000]\tLoss: 4.6841\tLR: 5.789514\n",
      "Training Epoch: 58 [45056/50000]\tLoss: 4.6659\tLR: 5.789770\n",
      "Training Epoch: 58 [45184/50000]\tLoss: 4.7053\tLR: 5.790026\n",
      "Training Epoch: 58 [45312/50000]\tLoss: 4.7100\tLR: 5.790281\n",
      "Training Epoch: 58 [45440/50000]\tLoss: 4.6859\tLR: 5.790537\n",
      "Training Epoch: 58 [45568/50000]\tLoss: 4.6991\tLR: 5.790793\n",
      "Training Epoch: 58 [45696/50000]\tLoss: 4.7066\tLR: 5.791049\n",
      "Training Epoch: 58 [45824/50000]\tLoss: 4.7961\tLR: 5.791304\n",
      "Training Epoch: 58 [45952/50000]\tLoss: 4.6987\tLR: 5.791560\n",
      "Training Epoch: 58 [46080/50000]\tLoss: 4.6545\tLR: 5.791816\n",
      "Training Epoch: 58 [46208/50000]\tLoss: 4.6704\tLR: 5.792072\n",
      "Training Epoch: 58 [46336/50000]\tLoss: 4.6719\tLR: 5.792327\n",
      "Training Epoch: 58 [46464/50000]\tLoss: 4.6530\tLR: 5.792583\n",
      "Training Epoch: 58 [46592/50000]\tLoss: 4.7249\tLR: 5.792839\n",
      "Training Epoch: 58 [46720/50000]\tLoss: 4.7297\tLR: 5.793095\n",
      "Training Epoch: 58 [46848/50000]\tLoss: 4.6725\tLR: 5.793350\n",
      "Training Epoch: 58 [46976/50000]\tLoss: 4.8514\tLR: 5.793606\n",
      "Training Epoch: 58 [47104/50000]\tLoss: 4.6795\tLR: 5.793862\n",
      "Training Epoch: 58 [47232/50000]\tLoss: 4.7816\tLR: 5.794118\n",
      "Training Epoch: 58 [47360/50000]\tLoss: 4.7856\tLR: 5.794373\n",
      "Training Epoch: 58 [47488/50000]\tLoss: 4.6603\tLR: 5.794629\n",
      "Training Epoch: 58 [47616/50000]\tLoss: 4.7615\tLR: 5.794885\n",
      "Training Epoch: 58 [47744/50000]\tLoss: 4.6865\tLR: 5.795141\n",
      "Training Epoch: 58 [47872/50000]\tLoss: 4.6921\tLR: 5.795396\n",
      "Training Epoch: 58 [48000/50000]\tLoss: 4.6980\tLR: 5.795652\n",
      "Training Epoch: 58 [48128/50000]\tLoss: 4.6842\tLR: 5.795908\n",
      "Training Epoch: 58 [48256/50000]\tLoss: 4.7130\tLR: 5.796164\n",
      "Training Epoch: 58 [48384/50000]\tLoss: 4.7054\tLR: 5.796419\n",
      "Training Epoch: 58 [48512/50000]\tLoss: 4.6477\tLR: 5.796675\n",
      "Training Epoch: 58 [48640/50000]\tLoss: 4.6469\tLR: 5.796931\n",
      "Training Epoch: 58 [48768/50000]\tLoss: 4.8246\tLR: 5.797187\n",
      "Training Epoch: 58 [48896/50000]\tLoss: 4.7685\tLR: 5.797442\n",
      "Training Epoch: 58 [49024/50000]\tLoss: 4.7470\tLR: 5.797698\n",
      "Training Epoch: 58 [49152/50000]\tLoss: 4.7389\tLR: 5.797954\n",
      "Training Epoch: 58 [49280/50000]\tLoss: 4.6709\tLR: 5.798210\n",
      "Training Epoch: 58 [49408/50000]\tLoss: 4.6794\tLR: 5.798465\n",
      "Training Epoch: 58 [49536/50000]\tLoss: 4.6913\tLR: 5.798721\n",
      "Training Epoch: 58 [49664/50000]\tLoss: 4.7006\tLR: 5.798977\n",
      "Training Epoch: 58 [49792/50000]\tLoss: 4.7159\tLR: 5.799233\n",
      "Training Epoch: 58 [49920/50000]\tLoss: 4.7170\tLR: 5.799488\n",
      "Training Epoch: 58 [50000/50000]\tLoss: 4.8194\tLR: 5.799744\n",
      "epoch 58 training time consumed: 489.10s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   81312 GB |   81312 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   81063 GB |   81062 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     249 GB |     249 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   81312 GB |   81312 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   81063 GB |   81062 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     249 GB |     249 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   80168 GB |   80168 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   79919 GB |   79919 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     249 GB |     249 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    8622 K  |    8621 K  |\n",
      "|       from large pool |      24    |      65    |    3675 K  |    3675 K  |\n",
      "|       from small pool |     231    |     274    |    4946 K  |    4946 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    8622 K  |    8621 K  |\n",
      "|       from large pool |      24    |      65    |    3675 K  |    3675 K  |\n",
      "|       from small pool |     231    |     274    |    4946 K  |    4946 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      47    |    4996 K  |    4996 K  |\n",
      "|       from large pool |      10    |      23    |    1766 K  |    1766 K  |\n",
      "|       from small pool |      26    |      35    |    3229 K  |    3229 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 58, Average loss: 0.0372, Accuracy: 0.0100, Time consumed:31.11s\n",
      "\n",
      "Training Epoch: 59 [128/50000]\tLoss: 4.7548\tLR: 0.100000\n",
      "Training Epoch: 59 [256/50000]\tLoss: 4.6670\tLR: 5.800256\n",
      "Training Epoch: 59 [384/50000]\tLoss: 4.7389\tLR: 5.800512\n",
      "Training Epoch: 59 [512/50000]\tLoss: 4.6805\tLR: 5.800767\n",
      "Training Epoch: 59 [640/50000]\tLoss: 4.7506\tLR: 5.801023\n",
      "Training Epoch: 59 [768/50000]\tLoss: 4.6914\tLR: 5.801279\n",
      "Training Epoch: 59 [896/50000]\tLoss: 4.6293\tLR: 5.801535\n",
      "Training Epoch: 59 [1024/50000]\tLoss: 4.7498\tLR: 5.801790\n",
      "Training Epoch: 59 [1152/50000]\tLoss: 4.6007\tLR: 5.802046\n",
      "Training Epoch: 59 [1280/50000]\tLoss: 4.6996\tLR: 5.802302\n",
      "Training Epoch: 59 [1408/50000]\tLoss: 4.6869\tLR: 5.802558\n",
      "Training Epoch: 59 [1536/50000]\tLoss: 4.7273\tLR: 5.802813\n",
      "Training Epoch: 59 [1664/50000]\tLoss: 4.6789\tLR: 5.803069\n",
      "Training Epoch: 59 [1792/50000]\tLoss: 4.6837\tLR: 5.803325\n",
      "Training Epoch: 59 [1920/50000]\tLoss: 4.7342\tLR: 5.803581\n",
      "Training Epoch: 59 [2048/50000]\tLoss: 4.7536\tLR: 5.803836\n",
      "Training Epoch: 59 [2176/50000]\tLoss: 4.7026\tLR: 5.804092\n",
      "Training Epoch: 59 [2304/50000]\tLoss: 4.6551\tLR: 5.804348\n",
      "Training Epoch: 59 [2432/50000]\tLoss: 4.6438\tLR: 5.804604\n",
      "Training Epoch: 59 [2560/50000]\tLoss: 4.6381\tLR: 5.804859\n",
      "Training Epoch: 59 [2688/50000]\tLoss: 4.7280\tLR: 5.805115\n",
      "Training Epoch: 59 [2816/50000]\tLoss: 4.6570\tLR: 5.805371\n",
      "Training Epoch: 59 [2944/50000]\tLoss: 4.6660\tLR: 5.805627\n",
      "Training Epoch: 59 [3072/50000]\tLoss: 4.7539\tLR: 5.805882\n",
      "Training Epoch: 59 [3200/50000]\tLoss: 4.7519\tLR: 5.806138\n",
      "Training Epoch: 59 [3328/50000]\tLoss: 4.6660\tLR: 5.806394\n",
      "Training Epoch: 59 [3456/50000]\tLoss: 4.6642\tLR: 5.806650\n",
      "Training Epoch: 59 [3584/50000]\tLoss: 4.6993\tLR: 5.806905\n",
      "Training Epoch: 59 [3712/50000]\tLoss: 4.7256\tLR: 5.807161\n",
      "Training Epoch: 59 [3840/50000]\tLoss: 4.7481\tLR: 5.807417\n",
      "Training Epoch: 59 [3968/50000]\tLoss: 4.7195\tLR: 5.807673\n",
      "Training Epoch: 59 [4096/50000]\tLoss: 4.6828\tLR: 5.807928\n",
      "Training Epoch: 59 [4224/50000]\tLoss: 4.6821\tLR: 5.808184\n",
      "Training Epoch: 59 [4352/50000]\tLoss: 4.7493\tLR: 5.808440\n",
      "Training Epoch: 59 [4480/50000]\tLoss: 4.6593\tLR: 5.808696\n",
      "Training Epoch: 59 [4608/50000]\tLoss: 4.7681\tLR: 5.808951\n",
      "Training Epoch: 59 [4736/50000]\tLoss: 4.7412\tLR: 5.809207\n",
      "Training Epoch: 59 [4864/50000]\tLoss: 4.7108\tLR: 5.809463\n",
      "Training Epoch: 59 [4992/50000]\tLoss: 4.6445\tLR: 5.809719\n",
      "Training Epoch: 59 [5120/50000]\tLoss: 4.6788\tLR: 5.809974\n",
      "Training Epoch: 59 [5248/50000]\tLoss: 4.7338\tLR: 5.810230\n",
      "Training Epoch: 59 [5376/50000]\tLoss: 4.7010\tLR: 5.810486\n",
      "Training Epoch: 59 [5504/50000]\tLoss: 4.8147\tLR: 5.810742\n",
      "Training Epoch: 59 [5632/50000]\tLoss: 4.7245\tLR: 5.810997\n",
      "Training Epoch: 59 [5760/50000]\tLoss: 4.6967\tLR: 5.811253\n",
      "Training Epoch: 59 [5888/50000]\tLoss: 4.7588\tLR: 5.811509\n",
      "Training Epoch: 59 [6016/50000]\tLoss: 4.7007\tLR: 5.811765\n",
      "Training Epoch: 59 [6144/50000]\tLoss: 4.6959\tLR: 5.812020\n",
      "Training Epoch: 59 [6272/50000]\tLoss: 4.6731\tLR: 5.812276\n",
      "Training Epoch: 59 [6400/50000]\tLoss: 4.7175\tLR: 5.812532\n",
      "Training Epoch: 59 [6528/50000]\tLoss: 4.6058\tLR: 5.812788\n",
      "Training Epoch: 59 [6656/50000]\tLoss: 4.6317\tLR: 5.813043\n",
      "Training Epoch: 59 [6784/50000]\tLoss: 4.7934\tLR: 5.813299\n",
      "Training Epoch: 59 [6912/50000]\tLoss: 4.6961\tLR: 5.813555\n",
      "Training Epoch: 59 [7040/50000]\tLoss: 4.7062\tLR: 5.813811\n",
      "Training Epoch: 59 [7168/50000]\tLoss: 4.7422\tLR: 5.814066\n",
      "Training Epoch: 59 [7296/50000]\tLoss: 4.6623\tLR: 5.814322\n",
      "Training Epoch: 59 [7424/50000]\tLoss: 4.6892\tLR: 5.814578\n",
      "Training Epoch: 59 [7552/50000]\tLoss: 4.7611\tLR: 5.814834\n",
      "Training Epoch: 59 [7680/50000]\tLoss: 4.7191\tLR: 5.815090\n",
      "Training Epoch: 59 [7808/50000]\tLoss: 4.6858\tLR: 5.815345\n",
      "Training Epoch: 59 [7936/50000]\tLoss: 4.7056\tLR: 5.815601\n",
      "Training Epoch: 59 [8064/50000]\tLoss: 4.6792\tLR: 5.815857\n",
      "Training Epoch: 59 [8192/50000]\tLoss: 4.6801\tLR: 5.816113\n",
      "Training Epoch: 59 [8320/50000]\tLoss: 4.6692\tLR: 5.816368\n",
      "Training Epoch: 59 [8448/50000]\tLoss: 4.7135\tLR: 5.816624\n",
      "Training Epoch: 59 [8576/50000]\tLoss: 4.7038\tLR: 5.816880\n",
      "Training Epoch: 59 [8704/50000]\tLoss: 4.6821\tLR: 5.817136\n",
      "Training Epoch: 59 [8832/50000]\tLoss: 4.8002\tLR: 5.817391\n",
      "Training Epoch: 59 [8960/50000]\tLoss: 4.7287\tLR: 5.817647\n",
      "Training Epoch: 59 [9088/50000]\tLoss: 4.6740\tLR: 5.817903\n",
      "Training Epoch: 59 [9216/50000]\tLoss: 4.6919\tLR: 5.818159\n",
      "Training Epoch: 59 [9344/50000]\tLoss: 4.6405\tLR: 5.818414\n",
      "Training Epoch: 59 [9472/50000]\tLoss: 4.7569\tLR: 5.818670\n",
      "Training Epoch: 59 [9600/50000]\tLoss: 4.7391\tLR: 5.818926\n",
      "Training Epoch: 59 [9728/50000]\tLoss: 4.7486\tLR: 5.819182\n",
      "Training Epoch: 59 [9856/50000]\tLoss: 4.7048\tLR: 5.819437\n",
      "Training Epoch: 59 [9984/50000]\tLoss: 4.7064\tLR: 5.819693\n",
      "Training Epoch: 59 [10112/50000]\tLoss: 4.7209\tLR: 5.819949\n",
      "Training Epoch: 59 [10240/50000]\tLoss: 4.7586\tLR: 5.820205\n",
      "Training Epoch: 59 [10368/50000]\tLoss: 4.7609\tLR: 5.820460\n",
      "Training Epoch: 59 [10496/50000]\tLoss: 4.6648\tLR: 5.820716\n",
      "Training Epoch: 59 [10624/50000]\tLoss: 4.7740\tLR: 5.820972\n",
      "Training Epoch: 59 [10752/50000]\tLoss: 4.6269\tLR: 5.821228\n",
      "Training Epoch: 59 [10880/50000]\tLoss: 4.6374\tLR: 5.821483\n",
      "Training Epoch: 59 [11008/50000]\tLoss: 4.7203\tLR: 5.821739\n",
      "Training Epoch: 59 [11136/50000]\tLoss: 4.7798\tLR: 5.821995\n",
      "Training Epoch: 59 [11264/50000]\tLoss: 4.6253\tLR: 5.822251\n",
      "Training Epoch: 59 [11392/50000]\tLoss: 4.7419\tLR: 5.822506\n",
      "Training Epoch: 59 [11520/50000]\tLoss: 4.6755\tLR: 5.822762\n",
      "Training Epoch: 59 [11648/50000]\tLoss: 4.6485\tLR: 5.823018\n",
      "Training Epoch: 59 [11776/50000]\tLoss: 4.7307\tLR: 5.823274\n",
      "Training Epoch: 59 [11904/50000]\tLoss: 4.7150\tLR: 5.823529\n",
      "Training Epoch: 59 [12032/50000]\tLoss: 4.7281\tLR: 5.823785\n",
      "Training Epoch: 59 [12160/50000]\tLoss: 4.6932\tLR: 5.824041\n",
      "Training Epoch: 59 [12288/50000]\tLoss: 4.6865\tLR: 5.824297\n",
      "Training Epoch: 59 [12416/50000]\tLoss: 4.7406\tLR: 5.824552\n",
      "Training Epoch: 59 [12544/50000]\tLoss: 4.7064\tLR: 5.824808\n",
      "Training Epoch: 59 [12672/50000]\tLoss: 4.7182\tLR: 5.825064\n",
      "Training Epoch: 59 [12800/50000]\tLoss: 4.7057\tLR: 5.825320\n",
      "Training Epoch: 59 [12928/50000]\tLoss: 4.6980\tLR: 5.825575\n",
      "Training Epoch: 59 [13056/50000]\tLoss: 4.7866\tLR: 5.825831\n",
      "Training Epoch: 59 [13184/50000]\tLoss: 4.7488\tLR: 5.826087\n",
      "Training Epoch: 59 [13312/50000]\tLoss: 4.8222\tLR: 5.826343\n",
      "Training Epoch: 59 [13440/50000]\tLoss: 4.6577\tLR: 5.826598\n",
      "Training Epoch: 59 [13568/50000]\tLoss: 4.6683\tLR: 5.826854\n",
      "Training Epoch: 59 [13696/50000]\tLoss: 4.7114\tLR: 5.827110\n",
      "Training Epoch: 59 [13824/50000]\tLoss: 4.7357\tLR: 5.827366\n",
      "Training Epoch: 59 [13952/50000]\tLoss: 4.7524\tLR: 5.827621\n",
      "Training Epoch: 59 [14080/50000]\tLoss: 4.7385\tLR: 5.827877\n",
      "Training Epoch: 59 [14208/50000]\tLoss: 4.6863\tLR: 5.828133\n",
      "Training Epoch: 59 [14336/50000]\tLoss: 4.6998\tLR: 5.828389\n",
      "Training Epoch: 59 [14464/50000]\tLoss: 4.7638\tLR: 5.828645\n",
      "Training Epoch: 59 [14592/50000]\tLoss: 4.6495\tLR: 5.828900\n",
      "Training Epoch: 59 [14720/50000]\tLoss: 4.7358\tLR: 5.829156\n",
      "Training Epoch: 59 [14848/50000]\tLoss: 4.7287\tLR: 5.829412\n",
      "Training Epoch: 59 [14976/50000]\tLoss: 4.6980\tLR: 5.829668\n",
      "Training Epoch: 59 [15104/50000]\tLoss: 4.7298\tLR: 5.829923\n",
      "Training Epoch: 59 [15232/50000]\tLoss: 4.7060\tLR: 5.830179\n",
      "Training Epoch: 59 [15360/50000]\tLoss: 4.7251\tLR: 5.830435\n",
      "Training Epoch: 59 [15488/50000]\tLoss: 4.6930\tLR: 5.830691\n",
      "Training Epoch: 59 [15616/50000]\tLoss: 4.7022\tLR: 5.830946\n",
      "Training Epoch: 59 [15744/50000]\tLoss: 4.6652\tLR: 5.831202\n",
      "Training Epoch: 59 [15872/50000]\tLoss: 4.7104\tLR: 5.831458\n",
      "Training Epoch: 59 [16000/50000]\tLoss: 4.7696\tLR: 5.831714\n",
      "Training Epoch: 59 [16128/50000]\tLoss: 4.6932\tLR: 5.831969\n",
      "Training Epoch: 59 [16256/50000]\tLoss: 4.7237\tLR: 5.832225\n",
      "Training Epoch: 59 [16384/50000]\tLoss: 4.7342\tLR: 5.832481\n",
      "Training Epoch: 59 [16512/50000]\tLoss: 4.6880\tLR: 5.832737\n",
      "Training Epoch: 59 [16640/50000]\tLoss: 4.7628\tLR: 5.832992\n",
      "Training Epoch: 59 [16768/50000]\tLoss: 4.8134\tLR: 5.833248\n",
      "Training Epoch: 59 [16896/50000]\tLoss: 4.7672\tLR: 5.833504\n",
      "Training Epoch: 59 [17024/50000]\tLoss: 4.7110\tLR: 5.833760\n",
      "Training Epoch: 59 [17152/50000]\tLoss: 4.6409\tLR: 5.834015\n",
      "Training Epoch: 59 [17280/50000]\tLoss: 4.6481\tLR: 5.834271\n",
      "Training Epoch: 59 [17408/50000]\tLoss: 4.6306\tLR: 5.834527\n",
      "Training Epoch: 59 [17536/50000]\tLoss: 4.7434\tLR: 5.834783\n",
      "Training Epoch: 59 [17664/50000]\tLoss: 4.7245\tLR: 5.835038\n",
      "Training Epoch: 59 [17792/50000]\tLoss: 4.7633\tLR: 5.835294\n",
      "Training Epoch: 59 [17920/50000]\tLoss: 4.7501\tLR: 5.835550\n",
      "Training Epoch: 59 [18048/50000]\tLoss: 4.7585\tLR: 5.835806\n",
      "Training Epoch: 59 [18176/50000]\tLoss: 4.7103\tLR: 5.836061\n",
      "Training Epoch: 59 [18304/50000]\tLoss: 4.7203\tLR: 5.836317\n",
      "Training Epoch: 59 [18432/50000]\tLoss: 4.6681\tLR: 5.836573\n",
      "Training Epoch: 59 [18560/50000]\tLoss: 4.6998\tLR: 5.836829\n",
      "Training Epoch: 59 [18688/50000]\tLoss: 4.7390\tLR: 5.837084\n",
      "Training Epoch: 59 [18816/50000]\tLoss: 4.6828\tLR: 5.837340\n",
      "Training Epoch: 59 [18944/50000]\tLoss: 4.6582\tLR: 5.837596\n",
      "Training Epoch: 59 [19072/50000]\tLoss: 4.6475\tLR: 5.837852\n",
      "Training Epoch: 59 [19200/50000]\tLoss: 4.7592\tLR: 5.838107\n",
      "Training Epoch: 59 [19328/50000]\tLoss: 4.6766\tLR: 5.838363\n",
      "Training Epoch: 59 [19456/50000]\tLoss: 4.6592\tLR: 5.838619\n",
      "Training Epoch: 59 [19584/50000]\tLoss: 4.7365\tLR: 5.838875\n",
      "Training Epoch: 59 [19712/50000]\tLoss: 4.7062\tLR: 5.839130\n",
      "Training Epoch: 59 [19840/50000]\tLoss: 4.6779\tLR: 5.839386\n",
      "Training Epoch: 59 [19968/50000]\tLoss: 4.6884\tLR: 5.839642\n",
      "Training Epoch: 59 [20096/50000]\tLoss: 4.7244\tLR: 5.839898\n",
      "Training Epoch: 59 [20224/50000]\tLoss: 4.6422\tLR: 5.840153\n",
      "Training Epoch: 59 [20352/50000]\tLoss: 4.6490\tLR: 5.840409\n",
      "Training Epoch: 59 [20480/50000]\tLoss: 4.5800\tLR: 5.840665\n",
      "Training Epoch: 59 [20608/50000]\tLoss: 4.7630\tLR: 5.840921\n",
      "Training Epoch: 59 [20736/50000]\tLoss: 4.7441\tLR: 5.841176\n",
      "Training Epoch: 59 [20864/50000]\tLoss: 4.6839\tLR: 5.841432\n",
      "Training Epoch: 59 [20992/50000]\tLoss: 4.6841\tLR: 5.841688\n",
      "Training Epoch: 59 [21120/50000]\tLoss: 4.7391\tLR: 5.841944\n",
      "Training Epoch: 59 [21248/50000]\tLoss: 4.7488\tLR: 5.842199\n",
      "Training Epoch: 59 [21376/50000]\tLoss: 4.7222\tLR: 5.842455\n",
      "Training Epoch: 59 [21504/50000]\tLoss: 4.7102\tLR: 5.842711\n",
      "Training Epoch: 59 [21632/50000]\tLoss: 4.7141\tLR: 5.842967\n",
      "Training Epoch: 59 [21760/50000]\tLoss: 4.7193\tLR: 5.843223\n",
      "Training Epoch: 59 [21888/50000]\tLoss: 4.6900\tLR: 5.843478\n",
      "Training Epoch: 59 [22016/50000]\tLoss: 4.7166\tLR: 5.843734\n",
      "Training Epoch: 59 [22144/50000]\tLoss: 4.6756\tLR: 5.843990\n",
      "Training Epoch: 59 [22272/50000]\tLoss: 4.7156\tLR: 5.844246\n",
      "Training Epoch: 59 [22400/50000]\tLoss: 4.7723\tLR: 5.844501\n",
      "Training Epoch: 59 [22528/50000]\tLoss: 4.7839\tLR: 5.844757\n",
      "Training Epoch: 59 [22656/50000]\tLoss: 4.7819\tLR: 5.845013\n",
      "Training Epoch: 59 [22784/50000]\tLoss: 4.7286\tLR: 5.845269\n",
      "Training Epoch: 59 [22912/50000]\tLoss: 4.7090\tLR: 5.845524\n",
      "Training Epoch: 59 [23040/50000]\tLoss: 4.7334\tLR: 5.845780\n",
      "Training Epoch: 59 [23168/50000]\tLoss: 4.6790\tLR: 5.846036\n",
      "Training Epoch: 59 [23296/50000]\tLoss: 4.7298\tLR: 5.846292\n",
      "Training Epoch: 59 [23424/50000]\tLoss: 4.7645\tLR: 5.846547\n",
      "Training Epoch: 59 [23552/50000]\tLoss: 4.7631\tLR: 5.846803\n",
      "Training Epoch: 59 [23680/50000]\tLoss: 4.7981\tLR: 5.847059\n",
      "Training Epoch: 59 [23808/50000]\tLoss: 4.6859\tLR: 5.847315\n",
      "Training Epoch: 59 [23936/50000]\tLoss: 4.6798\tLR: 5.847570\n",
      "Training Epoch: 59 [24064/50000]\tLoss: 4.7373\tLR: 5.847826\n",
      "Training Epoch: 59 [24192/50000]\tLoss: 4.7558\tLR: 5.848082\n",
      "Training Epoch: 59 [24320/50000]\tLoss: 4.6584\tLR: 5.848338\n",
      "Training Epoch: 59 [24448/50000]\tLoss: 4.7961\tLR: 5.848593\n",
      "Training Epoch: 59 [24576/50000]\tLoss: 4.7585\tLR: 5.848849\n",
      "Training Epoch: 59 [24704/50000]\tLoss: 4.6720\tLR: 5.849105\n",
      "Training Epoch: 59 [24832/50000]\tLoss: 4.6809\tLR: 5.849361\n",
      "Training Epoch: 59 [24960/50000]\tLoss: 4.7155\tLR: 5.849616\n",
      "Training Epoch: 59 [25088/50000]\tLoss: 4.6876\tLR: 5.849872\n",
      "Training Epoch: 59 [25216/50000]\tLoss: 4.8008\tLR: 5.850128\n",
      "Training Epoch: 59 [25344/50000]\tLoss: 4.7472\tLR: 5.850384\n",
      "Training Epoch: 59 [25472/50000]\tLoss: 4.7421\tLR: 5.850639\n",
      "Training Epoch: 59 [25600/50000]\tLoss: 4.7466\tLR: 5.850895\n",
      "Training Epoch: 59 [25728/50000]\tLoss: 4.6994\tLR: 5.851151\n",
      "Training Epoch: 59 [25856/50000]\tLoss: 4.7072\tLR: 5.851407\n",
      "Training Epoch: 59 [25984/50000]\tLoss: 4.6444\tLR: 5.851662\n",
      "Training Epoch: 59 [26112/50000]\tLoss: 4.7099\tLR: 5.851918\n",
      "Training Epoch: 59 [26240/50000]\tLoss: 4.6981\tLR: 5.852174\n",
      "Training Epoch: 59 [26368/50000]\tLoss: 4.7970\tLR: 5.852430\n",
      "Training Epoch: 59 [26496/50000]\tLoss: 4.7527\tLR: 5.852685\n",
      "Training Epoch: 59 [26624/50000]\tLoss: 4.7418\tLR: 5.852941\n",
      "Training Epoch: 59 [26752/50000]\tLoss: 4.6837\tLR: 5.853197\n",
      "Training Epoch: 59 [26880/50000]\tLoss: 4.7556\tLR: 5.853453\n",
      "Training Epoch: 59 [27008/50000]\tLoss: 4.7014\tLR: 5.853708\n",
      "Training Epoch: 59 [27136/50000]\tLoss: 4.7573\tLR: 5.853964\n",
      "Training Epoch: 59 [27264/50000]\tLoss: 4.7276\tLR: 5.854220\n",
      "Training Epoch: 59 [27392/50000]\tLoss: 4.8113\tLR: 5.854476\n",
      "Training Epoch: 59 [27520/50000]\tLoss: 4.6907\tLR: 5.854731\n",
      "Training Epoch: 59 [27648/50000]\tLoss: 4.6812\tLR: 5.854987\n",
      "Training Epoch: 59 [27776/50000]\tLoss: 4.7731\tLR: 5.855243\n",
      "Training Epoch: 59 [27904/50000]\tLoss: 4.7093\tLR: 5.855499\n",
      "Training Epoch: 59 [28032/50000]\tLoss: 4.6909\tLR: 5.855754\n",
      "Training Epoch: 59 [28160/50000]\tLoss: 4.7997\tLR: 5.856010\n",
      "Training Epoch: 59 [28288/50000]\tLoss: 4.8253\tLR: 5.856266\n",
      "Training Epoch: 59 [28416/50000]\tLoss: 4.8166\tLR: 5.856522\n",
      "Training Epoch: 59 [28544/50000]\tLoss: 4.7621\tLR: 5.856777\n",
      "Training Epoch: 59 [28672/50000]\tLoss: 4.7018\tLR: 5.857033\n",
      "Training Epoch: 59 [28800/50000]\tLoss: 4.6819\tLR: 5.857289\n",
      "Training Epoch: 59 [28928/50000]\tLoss: 4.7097\tLR: 5.857545\n",
      "Training Epoch: 59 [29056/50000]\tLoss: 4.7716\tLR: 5.857801\n",
      "Training Epoch: 59 [29184/50000]\tLoss: 4.7850\tLR: 5.858056\n",
      "Training Epoch: 59 [29312/50000]\tLoss: 4.6945\tLR: 5.858312\n",
      "Training Epoch: 59 [29440/50000]\tLoss: 4.7001\tLR: 5.858568\n",
      "Training Epoch: 59 [29568/50000]\tLoss: 4.6950\tLR: 5.858824\n",
      "Training Epoch: 59 [29696/50000]\tLoss: 4.7213\tLR: 5.859079\n",
      "Training Epoch: 59 [29824/50000]\tLoss: 4.8218\tLR: 5.859335\n",
      "Training Epoch: 59 [29952/50000]\tLoss: 4.7582\tLR: 5.859591\n",
      "Training Epoch: 59 [30080/50000]\tLoss: 4.7478\tLR: 5.859847\n",
      "Training Epoch: 59 [30208/50000]\tLoss: 4.7210\tLR: 5.860102\n",
      "Training Epoch: 59 [30336/50000]\tLoss: 4.6695\tLR: 5.860358\n",
      "Training Epoch: 59 [30464/50000]\tLoss: 4.6695\tLR: 5.860614\n",
      "Training Epoch: 59 [30592/50000]\tLoss: 4.7522\tLR: 5.860870\n",
      "Training Epoch: 59 [30720/50000]\tLoss: 4.7088\tLR: 5.861125\n",
      "Training Epoch: 59 [30848/50000]\tLoss: 4.7433\tLR: 5.861381\n",
      "Training Epoch: 59 [30976/50000]\tLoss: 4.6870\tLR: 5.861637\n",
      "Training Epoch: 59 [31104/50000]\tLoss: 4.6741\tLR: 5.861893\n",
      "Training Epoch: 59 [31232/50000]\tLoss: 4.6730\tLR: 5.862148\n",
      "Training Epoch: 59 [31360/50000]\tLoss: 4.7742\tLR: 5.862404\n",
      "Training Epoch: 59 [31488/50000]\tLoss: 4.7431\tLR: 5.862660\n",
      "Training Epoch: 59 [31616/50000]\tLoss: 4.7355\tLR: 5.862916\n",
      "Training Epoch: 59 [31744/50000]\tLoss: 4.7341\tLR: 5.863171\n",
      "Training Epoch: 59 [31872/50000]\tLoss: 4.7517\tLR: 5.863427\n",
      "Training Epoch: 59 [32000/50000]\tLoss: 4.7479\tLR: 5.863683\n",
      "Training Epoch: 59 [32128/50000]\tLoss: 4.7454\tLR: 5.863939\n",
      "Training Epoch: 59 [32256/50000]\tLoss: 4.6641\tLR: 5.864194\n",
      "Training Epoch: 59 [32384/50000]\tLoss: 4.6882\tLR: 5.864450\n",
      "Training Epoch: 59 [32512/50000]\tLoss: 4.6709\tLR: 5.864706\n",
      "Training Epoch: 59 [32640/50000]\tLoss: 4.7999\tLR: 5.864962\n",
      "Training Epoch: 59 [32768/50000]\tLoss: 4.7537\tLR: 5.865217\n",
      "Training Epoch: 59 [32896/50000]\tLoss: 4.7144\tLR: 5.865473\n",
      "Training Epoch: 59 [33024/50000]\tLoss: 4.7245\tLR: 5.865729\n",
      "Training Epoch: 59 [33152/50000]\tLoss: 4.7727\tLR: 5.865985\n",
      "Training Epoch: 59 [33280/50000]\tLoss: 4.7940\tLR: 5.866240\n",
      "Training Epoch: 59 [33408/50000]\tLoss: 4.6841\tLR: 5.866496\n",
      "Training Epoch: 59 [33536/50000]\tLoss: 4.6892\tLR: 5.866752\n",
      "Training Epoch: 59 [33664/50000]\tLoss: 4.7349\tLR: 5.867008\n",
      "Training Epoch: 59 [33792/50000]\tLoss: 4.7456\tLR: 5.867263\n",
      "Training Epoch: 59 [33920/50000]\tLoss: 4.6877\tLR: 5.867519\n",
      "Training Epoch: 59 [34048/50000]\tLoss: 4.7299\tLR: 5.867775\n",
      "Training Epoch: 59 [34176/50000]\tLoss: 4.6357\tLR: 5.868031\n",
      "Training Epoch: 59 [34304/50000]\tLoss: 4.6625\tLR: 5.868286\n",
      "Training Epoch: 59 [34432/50000]\tLoss: 4.7716\tLR: 5.868542\n",
      "Training Epoch: 59 [34560/50000]\tLoss: 4.6751\tLR: 5.868798\n",
      "Training Epoch: 59 [34688/50000]\tLoss: 4.7466\tLR: 5.869054\n",
      "Training Epoch: 59 [34816/50000]\tLoss: 4.6721\tLR: 5.869309\n",
      "Training Epoch: 59 [34944/50000]\tLoss: 4.6971\tLR: 5.869565\n",
      "Training Epoch: 59 [35072/50000]\tLoss: 4.7253\tLR: 5.869821\n",
      "Training Epoch: 59 [35200/50000]\tLoss: 4.6898\tLR: 5.870077\n",
      "Training Epoch: 59 [35328/50000]\tLoss: 4.7595\tLR: 5.870332\n",
      "Training Epoch: 59 [35456/50000]\tLoss: 4.7604\tLR: 5.870588\n",
      "Training Epoch: 59 [35584/50000]\tLoss: 4.6979\tLR: 5.870844\n",
      "Training Epoch: 59 [35712/50000]\tLoss: 4.8198\tLR: 5.871100\n",
      "Training Epoch: 59 [35840/50000]\tLoss: 4.6977\tLR: 5.871355\n",
      "Training Epoch: 59 [35968/50000]\tLoss: 4.7087\tLR: 5.871611\n",
      "Training Epoch: 59 [36096/50000]\tLoss: 4.7043\tLR: 5.871867\n",
      "Training Epoch: 59 [36224/50000]\tLoss: 4.6851\tLR: 5.872123\n",
      "Training Epoch: 59 [36352/50000]\tLoss: 4.6664\tLR: 5.872379\n",
      "Training Epoch: 59 [36480/50000]\tLoss: 4.6875\tLR: 5.872634\n",
      "Training Epoch: 59 [36608/50000]\tLoss: 4.7233\tLR: 5.872890\n",
      "Training Epoch: 59 [36736/50000]\tLoss: 4.6900\tLR: 5.873146\n",
      "Training Epoch: 59 [36864/50000]\tLoss: 4.7690\tLR: 5.873402\n",
      "Training Epoch: 59 [36992/50000]\tLoss: 4.7595\tLR: 5.873657\n",
      "Training Epoch: 59 [37120/50000]\tLoss: 4.7591\tLR: 5.873913\n",
      "Training Epoch: 59 [37248/50000]\tLoss: 4.7964\tLR: 5.874169\n",
      "Training Epoch: 59 [37376/50000]\tLoss: 4.7474\tLR: 5.874425\n",
      "Training Epoch: 59 [37504/50000]\tLoss: 4.6750\tLR: 5.874680\n",
      "Training Epoch: 59 [37632/50000]\tLoss: 4.7238\tLR: 5.874936\n",
      "Training Epoch: 59 [37760/50000]\tLoss: 4.7044\tLR: 5.875192\n",
      "Training Epoch: 59 [37888/50000]\tLoss: 4.7142\tLR: 5.875448\n",
      "Training Epoch: 59 [38016/50000]\tLoss: 4.6980\tLR: 5.875703\n",
      "Training Epoch: 59 [38144/50000]\tLoss: 4.7229\tLR: 5.875959\n",
      "Training Epoch: 59 [38272/50000]\tLoss: 4.7431\tLR: 5.876215\n",
      "Training Epoch: 59 [38400/50000]\tLoss: 4.7952\tLR: 5.876471\n",
      "Training Epoch: 59 [38528/50000]\tLoss: 4.6890\tLR: 5.876726\n",
      "Training Epoch: 59 [38656/50000]\tLoss: 4.7428\tLR: 5.876982\n",
      "Training Epoch: 59 [38784/50000]\tLoss: 4.7416\tLR: 5.877238\n",
      "Training Epoch: 59 [38912/50000]\tLoss: 4.6925\tLR: 5.877494\n",
      "Training Epoch: 59 [39040/50000]\tLoss: 4.6893\tLR: 5.877749\n",
      "Training Epoch: 59 [39168/50000]\tLoss: 4.6739\tLR: 5.878005\n",
      "Training Epoch: 59 [39296/50000]\tLoss: 4.7150\tLR: 5.878261\n",
      "Training Epoch: 59 [39424/50000]\tLoss: 4.6883\tLR: 5.878517\n",
      "Training Epoch: 59 [39552/50000]\tLoss: 4.7615\tLR: 5.878772\n",
      "Training Epoch: 59 [39680/50000]\tLoss: 4.7008\tLR: 5.879028\n",
      "Training Epoch: 59 [39808/50000]\tLoss: 4.6278\tLR: 5.879284\n",
      "Training Epoch: 59 [39936/50000]\tLoss: 4.8026\tLR: 5.879540\n",
      "Training Epoch: 59 [40064/50000]\tLoss: 4.7137\tLR: 5.879795\n",
      "Training Epoch: 59 [40192/50000]\tLoss: 4.7071\tLR: 5.880051\n",
      "Training Epoch: 59 [40320/50000]\tLoss: 4.7664\tLR: 5.880307\n",
      "Training Epoch: 59 [40448/50000]\tLoss: 4.6972\tLR: 5.880563\n",
      "Training Epoch: 59 [40576/50000]\tLoss: 4.5955\tLR: 5.880818\n",
      "Training Epoch: 59 [40704/50000]\tLoss: 4.6995\tLR: 5.881074\n",
      "Training Epoch: 59 [40832/50000]\tLoss: 4.7173\tLR: 5.881330\n",
      "Training Epoch: 59 [40960/50000]\tLoss: 4.7431\tLR: 5.881586\n",
      "Training Epoch: 59 [41088/50000]\tLoss: 4.7016\tLR: 5.881841\n",
      "Training Epoch: 59 [41216/50000]\tLoss: 4.7310\tLR: 5.882097\n",
      "Training Epoch: 59 [41344/50000]\tLoss: 4.7250\tLR: 5.882353\n",
      "Training Epoch: 59 [41472/50000]\tLoss: 4.7374\tLR: 5.882609\n",
      "Training Epoch: 59 [41600/50000]\tLoss: 4.6795\tLR: 5.882864\n",
      "Training Epoch: 59 [41728/50000]\tLoss: 4.7040\tLR: 5.883120\n",
      "Training Epoch: 59 [41856/50000]\tLoss: 4.6616\tLR: 5.883376\n",
      "Training Epoch: 59 [41984/50000]\tLoss: 4.6830\tLR: 5.883632\n",
      "Training Epoch: 59 [42112/50000]\tLoss: 4.6640\tLR: 5.883887\n",
      "Training Epoch: 59 [42240/50000]\tLoss: 4.7583\tLR: 5.884143\n",
      "Training Epoch: 59 [42368/50000]\tLoss: 4.7766\tLR: 5.884399\n",
      "Training Epoch: 59 [42496/50000]\tLoss: 4.6449\tLR: 5.884655\n",
      "Training Epoch: 59 [42624/50000]\tLoss: 4.6948\tLR: 5.884910\n",
      "Training Epoch: 59 [42752/50000]\tLoss: 4.8010\tLR: 5.885166\n",
      "Training Epoch: 59 [42880/50000]\tLoss: 4.7210\tLR: 5.885422\n",
      "Training Epoch: 59 [43008/50000]\tLoss: 4.7831\tLR: 5.885678\n",
      "Training Epoch: 59 [43136/50000]\tLoss: 4.7492\tLR: 5.885934\n",
      "Training Epoch: 59 [43264/50000]\tLoss: 4.6733\tLR: 5.886189\n",
      "Training Epoch: 59 [43392/50000]\tLoss: 4.7560\tLR: 5.886445\n",
      "Training Epoch: 59 [43520/50000]\tLoss: 4.7174\tLR: 5.886701\n",
      "Training Epoch: 59 [43648/50000]\tLoss: 4.7720\tLR: 5.886957\n",
      "Training Epoch: 59 [43776/50000]\tLoss: 4.7055\tLR: 5.887212\n",
      "Training Epoch: 59 [43904/50000]\tLoss: 4.7359\tLR: 5.887468\n",
      "Training Epoch: 59 [44032/50000]\tLoss: 4.7250\tLR: 5.887724\n",
      "Training Epoch: 59 [44160/50000]\tLoss: 4.7143\tLR: 5.887980\n",
      "Training Epoch: 59 [44288/50000]\tLoss: 4.7206\tLR: 5.888235\n",
      "Training Epoch: 59 [44416/50000]\tLoss: 4.7517\tLR: 5.888491\n",
      "Training Epoch: 59 [44544/50000]\tLoss: 4.6972\tLR: 5.888747\n",
      "Training Epoch: 59 [44672/50000]\tLoss: 4.8305\tLR: 5.889003\n",
      "Training Epoch: 59 [44800/50000]\tLoss: 4.7622\tLR: 5.889258\n",
      "Training Epoch: 59 [44928/50000]\tLoss: 4.7171\tLR: 5.889514\n",
      "Training Epoch: 59 [45056/50000]\tLoss: 4.6862\tLR: 5.889770\n",
      "Training Epoch: 59 [45184/50000]\tLoss: 4.7595\tLR: 5.890026\n",
      "Training Epoch: 59 [45312/50000]\tLoss: 4.6811\tLR: 5.890281\n",
      "Training Epoch: 59 [45440/50000]\tLoss: 4.7632\tLR: 5.890537\n",
      "Training Epoch: 59 [45568/50000]\tLoss: 4.8124\tLR: 5.890793\n",
      "Training Epoch: 59 [45696/50000]\tLoss: 4.7007\tLR: 5.891049\n",
      "Training Epoch: 59 [45824/50000]\tLoss: 4.6819\tLR: 5.891304\n",
      "Training Epoch: 59 [45952/50000]\tLoss: 4.6943\tLR: 5.891560\n",
      "Training Epoch: 59 [46080/50000]\tLoss: 4.7757\tLR: 5.891816\n",
      "Training Epoch: 59 [46208/50000]\tLoss: 4.7176\tLR: 5.892072\n",
      "Training Epoch: 59 [46336/50000]\tLoss: 4.8532\tLR: 5.892327\n",
      "Training Epoch: 59 [46464/50000]\tLoss: 4.7454\tLR: 5.892583\n",
      "Training Epoch: 59 [46592/50000]\tLoss: 4.7442\tLR: 5.892839\n",
      "Training Epoch: 59 [46720/50000]\tLoss: 4.7314\tLR: 5.893095\n",
      "Training Epoch: 59 [46848/50000]\tLoss: 4.7320\tLR: 5.893350\n",
      "Training Epoch: 59 [46976/50000]\tLoss: 4.6999\tLR: 5.893606\n",
      "Training Epoch: 59 [47104/50000]\tLoss: 4.7020\tLR: 5.893862\n",
      "Training Epoch: 59 [47232/50000]\tLoss: 4.7291\tLR: 5.894118\n",
      "Training Epoch: 59 [47360/50000]\tLoss: 4.7568\tLR: 5.894373\n",
      "Training Epoch: 59 [47488/50000]\tLoss: 4.7096\tLR: 5.894629\n",
      "Training Epoch: 59 [47616/50000]\tLoss: 4.7708\tLR: 5.894885\n",
      "Training Epoch: 59 [47744/50000]\tLoss: 4.6723\tLR: 5.895141\n",
      "Training Epoch: 59 [47872/50000]\tLoss: 4.7797\tLR: 5.895396\n",
      "Training Epoch: 59 [48000/50000]\tLoss: 4.7126\tLR: 5.895652\n",
      "Training Epoch: 59 [48128/50000]\tLoss: 4.7118\tLR: 5.895908\n",
      "Training Epoch: 59 [48256/50000]\tLoss: 4.7611\tLR: 5.896164\n",
      "Training Epoch: 59 [48384/50000]\tLoss: 4.7979\tLR: 5.896419\n",
      "Training Epoch: 59 [48512/50000]\tLoss: 4.7600\tLR: 5.896675\n",
      "Training Epoch: 59 [48640/50000]\tLoss: 4.7548\tLR: 5.896931\n",
      "Training Epoch: 59 [48768/50000]\tLoss: 4.7430\tLR: 5.897187\n",
      "Training Epoch: 59 [48896/50000]\tLoss: 4.8045\tLR: 5.897442\n",
      "Training Epoch: 59 [49024/50000]\tLoss: 4.8093\tLR: 5.897698\n",
      "Training Epoch: 59 [49152/50000]\tLoss: 4.6839\tLR: 5.897954\n",
      "Training Epoch: 59 [49280/50000]\tLoss: 4.6478\tLR: 5.898210\n",
      "Training Epoch: 59 [49408/50000]\tLoss: 4.7196\tLR: 5.898465\n",
      "Training Epoch: 59 [49536/50000]\tLoss: 4.6906\tLR: 5.898721\n",
      "Training Epoch: 59 [49664/50000]\tLoss: 4.7099\tLR: 5.898977\n",
      "Training Epoch: 59 [49792/50000]\tLoss: 4.7361\tLR: 5.899233\n",
      "Training Epoch: 59 [49920/50000]\tLoss: 4.7715\tLR: 5.899488\n",
      "Training Epoch: 59 [50000/50000]\tLoss: 4.6947\tLR: 5.899744\n",
      "epoch 59 training time consumed: 489.98s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   82714 GB |   82714 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   82460 GB |   82460 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     253 GB |     253 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   82714 GB |   82714 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   82460 GB |   82460 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     253 GB |     253 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   81551 GB |   81551 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   81297 GB |   81297 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     253 GB |     253 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    8770 K  |    8770 K  |\n",
      "|       from large pool |      24    |      65    |    3738 K  |    3738 K  |\n",
      "|       from small pool |     231    |     274    |    5031 K  |    5031 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    8770 K  |    8770 K  |\n",
      "|       from large pool |      24    |      65    |    3738 K  |    3738 K  |\n",
      "|       from small pool |     231    |     274    |    5031 K  |    5031 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      47    |    5083 K  |    5083 K  |\n",
      "|       from large pool |      10    |      23    |    1797 K  |    1797 K  |\n",
      "|       from small pool |      27    |      35    |    3286 K  |    3286 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 59, Average loss: 0.0373, Accuracy: 0.0100, Time consumed:31.20s\n",
      "\n",
      "Training Epoch: 60 [128/50000]\tLoss: 4.7240\tLR: 0.020000\n",
      "Training Epoch: 60 [256/50000]\tLoss: 4.7514\tLR: 5.900256\n",
      "Training Epoch: 60 [384/50000]\tLoss: 4.7166\tLR: 5.900512\n",
      "Training Epoch: 60 [512/50000]\tLoss: 4.7770\tLR: 5.900767\n",
      "Training Epoch: 60 [640/50000]\tLoss: 4.6967\tLR: 5.901023\n",
      "Training Epoch: 60 [768/50000]\tLoss: 4.7335\tLR: 5.901279\n",
      "Training Epoch: 60 [896/50000]\tLoss: 4.6964\tLR: 5.901535\n",
      "Training Epoch: 60 [1024/50000]\tLoss: 4.6986\tLR: 5.901790\n",
      "Training Epoch: 60 [1152/50000]\tLoss: 4.7322\tLR: 5.902046\n",
      "Training Epoch: 60 [1280/50000]\tLoss: 4.6804\tLR: 5.902302\n",
      "Training Epoch: 60 [1408/50000]\tLoss: 4.7189\tLR: 5.902558\n",
      "Training Epoch: 60 [1536/50000]\tLoss: 4.7320\tLR: 5.902813\n",
      "Training Epoch: 60 [1664/50000]\tLoss: 4.7265\tLR: 5.903069\n",
      "Training Epoch: 60 [1792/50000]\tLoss: 4.7104\tLR: 5.903325\n",
      "Training Epoch: 60 [1920/50000]\tLoss: 4.7740\tLR: 5.903581\n",
      "Training Epoch: 60 [2048/50000]\tLoss: 4.8206\tLR: 5.903836\n",
      "Training Epoch: 60 [2176/50000]\tLoss: 4.7198\tLR: 5.904092\n",
      "Training Epoch: 60 [2304/50000]\tLoss: 4.7792\tLR: 5.904348\n",
      "Training Epoch: 60 [2432/50000]\tLoss: 4.7957\tLR: 5.904604\n",
      "Training Epoch: 60 [2560/50000]\tLoss: 4.6619\tLR: 5.904859\n",
      "Training Epoch: 60 [2688/50000]\tLoss: 4.7376\tLR: 5.905115\n",
      "Training Epoch: 60 [2816/50000]\tLoss: 4.6715\tLR: 5.905371\n",
      "Training Epoch: 60 [2944/50000]\tLoss: 4.7842\tLR: 5.905627\n",
      "Training Epoch: 60 [3072/50000]\tLoss: 4.6767\tLR: 5.905882\n",
      "Training Epoch: 60 [3200/50000]\tLoss: 4.6641\tLR: 5.906138\n",
      "Training Epoch: 60 [3328/50000]\tLoss: 4.7299\tLR: 5.906394\n",
      "Training Epoch: 60 [3456/50000]\tLoss: 4.6229\tLR: 5.906650\n",
      "Training Epoch: 60 [3584/50000]\tLoss: 4.7428\tLR: 5.906905\n",
      "Training Epoch: 60 [3712/50000]\tLoss: 4.7245\tLR: 5.907161\n",
      "Training Epoch: 60 [3840/50000]\tLoss: 4.7815\tLR: 5.907417\n",
      "Training Epoch: 60 [3968/50000]\tLoss: 4.7770\tLR: 5.907673\n",
      "Training Epoch: 60 [4096/50000]\tLoss: 4.6231\tLR: 5.907928\n",
      "Training Epoch: 60 [4224/50000]\tLoss: 4.7536\tLR: 5.908184\n",
      "Training Epoch: 60 [4352/50000]\tLoss: 4.7646\tLR: 5.908440\n",
      "Training Epoch: 60 [4480/50000]\tLoss: 4.7475\tLR: 5.908696\n",
      "Training Epoch: 60 [4608/50000]\tLoss: 4.7141\tLR: 5.908951\n",
      "Training Epoch: 60 [4736/50000]\tLoss: 4.6806\tLR: 5.909207\n",
      "Training Epoch: 60 [4864/50000]\tLoss: 4.7237\tLR: 5.909463\n",
      "Training Epoch: 60 [4992/50000]\tLoss: 4.6886\tLR: 5.909719\n",
      "Training Epoch: 60 [5120/50000]\tLoss: 4.8007\tLR: 5.909974\n",
      "Training Epoch: 60 [5248/50000]\tLoss: 4.6533\tLR: 5.910230\n",
      "Training Epoch: 60 [5376/50000]\tLoss: 4.6806\tLR: 5.910486\n",
      "Training Epoch: 60 [5504/50000]\tLoss: 4.6919\tLR: 5.910742\n",
      "Training Epoch: 60 [5632/50000]\tLoss: 4.7368\tLR: 5.910997\n",
      "Training Epoch: 60 [5760/50000]\tLoss: 4.7341\tLR: 5.911253\n",
      "Training Epoch: 60 [5888/50000]\tLoss: 4.7773\tLR: 5.911509\n",
      "Training Epoch: 60 [6016/50000]\tLoss: 4.7361\tLR: 5.911765\n",
      "Training Epoch: 60 [6144/50000]\tLoss: 4.7591\tLR: 5.912020\n",
      "Training Epoch: 60 [6272/50000]\tLoss: 4.7211\tLR: 5.912276\n",
      "Training Epoch: 60 [6400/50000]\tLoss: 4.7307\tLR: 5.912532\n",
      "Training Epoch: 60 [6528/50000]\tLoss: 4.6996\tLR: 5.912788\n",
      "Training Epoch: 60 [6656/50000]\tLoss: 4.7527\tLR: 5.913043\n",
      "Training Epoch: 60 [6784/50000]\tLoss: 4.6473\tLR: 5.913299\n",
      "Training Epoch: 60 [6912/50000]\tLoss: 4.7494\tLR: 5.913555\n",
      "Training Epoch: 60 [7040/50000]\tLoss: 4.7634\tLR: 5.913811\n",
      "Training Epoch: 60 [7168/50000]\tLoss: 4.7021\tLR: 5.914066\n",
      "Training Epoch: 60 [7296/50000]\tLoss: 4.7864\tLR: 5.914322\n",
      "Training Epoch: 60 [7424/50000]\tLoss: 4.7665\tLR: 5.914578\n",
      "Training Epoch: 60 [7552/50000]\tLoss: 4.6559\tLR: 5.914834\n",
      "Training Epoch: 60 [7680/50000]\tLoss: 4.7041\tLR: 5.915090\n",
      "Training Epoch: 60 [7808/50000]\tLoss: 4.6713\tLR: 5.915345\n",
      "Training Epoch: 60 [7936/50000]\tLoss: 4.7130\tLR: 5.915601\n",
      "Training Epoch: 60 [8064/50000]\tLoss: 4.7726\tLR: 5.915857\n",
      "Training Epoch: 60 [8192/50000]\tLoss: 4.7596\tLR: 5.916113\n",
      "Training Epoch: 60 [8320/50000]\tLoss: 4.6002\tLR: 5.916368\n",
      "Training Epoch: 60 [8448/50000]\tLoss: 4.7223\tLR: 5.916624\n",
      "Training Epoch: 60 [8576/50000]\tLoss: 4.7096\tLR: 5.916880\n",
      "Training Epoch: 60 [8704/50000]\tLoss: 4.6585\tLR: 5.917136\n",
      "Training Epoch: 60 [8832/50000]\tLoss: 4.6711\tLR: 5.917391\n",
      "Training Epoch: 60 [8960/50000]\tLoss: 4.7031\tLR: 5.917647\n",
      "Training Epoch: 60 [9088/50000]\tLoss: 4.7955\tLR: 5.917903\n",
      "Training Epoch: 60 [9216/50000]\tLoss: 4.7367\tLR: 5.918159\n",
      "Training Epoch: 60 [9344/50000]\tLoss: 4.6994\tLR: 5.918414\n",
      "Training Epoch: 60 [9472/50000]\tLoss: 4.7052\tLR: 5.918670\n",
      "Training Epoch: 60 [9600/50000]\tLoss: 4.7344\tLR: 5.918926\n",
      "Training Epoch: 60 [9728/50000]\tLoss: 4.7000\tLR: 5.919182\n",
      "Training Epoch: 60 [9856/50000]\tLoss: 4.8259\tLR: 5.919437\n",
      "Training Epoch: 60 [9984/50000]\tLoss: 4.7246\tLR: 5.919693\n",
      "Training Epoch: 60 [10112/50000]\tLoss: 4.7727\tLR: 5.919949\n",
      "Training Epoch: 60 [10240/50000]\tLoss: 4.7098\tLR: 5.920205\n",
      "Training Epoch: 60 [10368/50000]\tLoss: 4.7344\tLR: 5.920460\n",
      "Training Epoch: 60 [10496/50000]\tLoss: 4.7071\tLR: 5.920716\n",
      "Training Epoch: 60 [10624/50000]\tLoss: 4.7797\tLR: 5.920972\n",
      "Training Epoch: 60 [10752/50000]\tLoss: 4.6804\tLR: 5.921228\n",
      "Training Epoch: 60 [10880/50000]\tLoss: 4.6709\tLR: 5.921483\n",
      "Training Epoch: 60 [11008/50000]\tLoss: 4.7966\tLR: 5.921739\n",
      "Training Epoch: 60 [11136/50000]\tLoss: 4.6663\tLR: 5.921995\n",
      "Training Epoch: 60 [11264/50000]\tLoss: 4.7439\tLR: 5.922251\n",
      "Training Epoch: 60 [11392/50000]\tLoss: 4.7025\tLR: 5.922506\n",
      "Training Epoch: 60 [11520/50000]\tLoss: 4.7319\tLR: 5.922762\n",
      "Training Epoch: 60 [11648/50000]\tLoss: 4.6530\tLR: 5.923018\n",
      "Training Epoch: 60 [11776/50000]\tLoss: 4.7489\tLR: 5.923274\n",
      "Training Epoch: 60 [11904/50000]\tLoss: 4.7054\tLR: 5.923529\n",
      "Training Epoch: 60 [12032/50000]\tLoss: 4.7095\tLR: 5.923785\n",
      "Training Epoch: 60 [12160/50000]\tLoss: 4.7177\tLR: 5.924041\n",
      "Training Epoch: 60 [12288/50000]\tLoss: 4.7411\tLR: 5.924297\n",
      "Training Epoch: 60 [12416/50000]\tLoss: 4.6681\tLR: 5.924552\n",
      "Training Epoch: 60 [12544/50000]\tLoss: 4.7011\tLR: 5.924808\n",
      "Training Epoch: 60 [12672/50000]\tLoss: 4.7938\tLR: 5.925064\n",
      "Training Epoch: 60 [12800/50000]\tLoss: 4.7073\tLR: 5.925320\n",
      "Training Epoch: 60 [12928/50000]\tLoss: 4.7887\tLR: 5.925575\n",
      "Training Epoch: 60 [13056/50000]\tLoss: 4.6724\tLR: 5.925831\n",
      "Training Epoch: 60 [13184/50000]\tLoss: 4.7490\tLR: 5.926087\n",
      "Training Epoch: 60 [13312/50000]\tLoss: 4.6887\tLR: 5.926343\n",
      "Training Epoch: 60 [13440/50000]\tLoss: 4.6930\tLR: 5.926598\n",
      "Training Epoch: 60 [13568/50000]\tLoss: 4.7779\tLR: 5.926854\n",
      "Training Epoch: 60 [13696/50000]\tLoss: 4.8132\tLR: 5.927110\n",
      "Training Epoch: 60 [13824/50000]\tLoss: 4.7372\tLR: 5.927366\n",
      "Training Epoch: 60 [13952/50000]\tLoss: 4.6625\tLR: 5.927621\n",
      "Training Epoch: 60 [14080/50000]\tLoss: 4.7460\tLR: 5.927877\n",
      "Training Epoch: 60 [14208/50000]\tLoss: 4.6639\tLR: 5.928133\n",
      "Training Epoch: 60 [14336/50000]\tLoss: 4.7355\tLR: 5.928389\n",
      "Training Epoch: 60 [14464/50000]\tLoss: 4.7251\tLR: 5.928645\n",
      "Training Epoch: 60 [14592/50000]\tLoss: 4.7700\tLR: 5.928900\n",
      "Training Epoch: 60 [14720/50000]\tLoss: 4.7554\tLR: 5.929156\n",
      "Training Epoch: 60 [14848/50000]\tLoss: 4.8103\tLR: 5.929412\n",
      "Training Epoch: 60 [14976/50000]\tLoss: 4.7562\tLR: 5.929668\n",
      "Training Epoch: 60 [15104/50000]\tLoss: 4.7711\tLR: 5.929923\n",
      "Training Epoch: 60 [15232/50000]\tLoss: 4.6438\tLR: 5.930179\n",
      "Training Epoch: 60 [15360/50000]\tLoss: 4.8224\tLR: 5.930435\n",
      "Training Epoch: 60 [15488/50000]\tLoss: 4.6827\tLR: 5.930691\n",
      "Training Epoch: 60 [15616/50000]\tLoss: 4.7089\tLR: 5.930946\n",
      "Training Epoch: 60 [15744/50000]\tLoss: 4.7319\tLR: 5.931202\n",
      "Training Epoch: 60 [15872/50000]\tLoss: 4.6697\tLR: 5.931458\n",
      "Training Epoch: 60 [16000/50000]\tLoss: 4.6958\tLR: 5.931714\n",
      "Training Epoch: 60 [16128/50000]\tLoss: 4.7370\tLR: 5.931969\n",
      "Training Epoch: 60 [16256/50000]\tLoss: 4.7029\tLR: 5.932225\n",
      "Training Epoch: 60 [16384/50000]\tLoss: 4.6823\tLR: 5.932481\n",
      "Training Epoch: 60 [16512/50000]\tLoss: 4.7251\tLR: 5.932737\n",
      "Training Epoch: 60 [16640/50000]\tLoss: 4.7012\tLR: 5.932992\n",
      "Training Epoch: 60 [16768/50000]\tLoss: 4.7181\tLR: 5.933248\n",
      "Training Epoch: 60 [16896/50000]\tLoss: 4.7689\tLR: 5.933504\n",
      "Training Epoch: 60 [17024/50000]\tLoss: 4.7215\tLR: 5.933760\n",
      "Training Epoch: 60 [17152/50000]\tLoss: 4.6704\tLR: 5.934015\n",
      "Training Epoch: 60 [17280/50000]\tLoss: 4.7783\tLR: 5.934271\n",
      "Training Epoch: 60 [17408/50000]\tLoss: 4.6571\tLR: 5.934527\n",
      "Training Epoch: 60 [17536/50000]\tLoss: 4.7534\tLR: 5.934783\n",
      "Training Epoch: 60 [17664/50000]\tLoss: 4.7210\tLR: 5.935038\n",
      "Training Epoch: 60 [17792/50000]\tLoss: 4.6749\tLR: 5.935294\n",
      "Training Epoch: 60 [17920/50000]\tLoss: 4.7348\tLR: 5.935550\n",
      "Training Epoch: 60 [18048/50000]\tLoss: 4.7129\tLR: 5.935806\n",
      "Training Epoch: 60 [18176/50000]\tLoss: 4.7580\tLR: 5.936061\n",
      "Training Epoch: 60 [18304/50000]\tLoss: 4.6232\tLR: 5.936317\n",
      "Training Epoch: 60 [18432/50000]\tLoss: 4.6531\tLR: 5.936573\n",
      "Training Epoch: 60 [18560/50000]\tLoss: 4.6965\tLR: 5.936829\n",
      "Training Epoch: 60 [18688/50000]\tLoss: 4.7146\tLR: 5.937084\n",
      "Training Epoch: 60 [18816/50000]\tLoss: 4.8103\tLR: 5.937340\n",
      "Training Epoch: 60 [18944/50000]\tLoss: 4.7088\tLR: 5.937596\n",
      "Training Epoch: 60 [19072/50000]\tLoss: 4.7703\tLR: 5.937852\n",
      "Training Epoch: 60 [19200/50000]\tLoss: 4.7153\tLR: 5.938107\n",
      "Training Epoch: 60 [19328/50000]\tLoss: 4.7573\tLR: 5.938363\n",
      "Training Epoch: 60 [19456/50000]\tLoss: 4.7062\tLR: 5.938619\n",
      "Training Epoch: 60 [19584/50000]\tLoss: 4.6214\tLR: 5.938875\n",
      "Training Epoch: 60 [19712/50000]\tLoss: 4.7099\tLR: 5.939130\n",
      "Training Epoch: 60 [19840/50000]\tLoss: 4.6805\tLR: 5.939386\n",
      "Training Epoch: 60 [19968/50000]\tLoss: 4.6908\tLR: 5.939642\n",
      "Training Epoch: 60 [20096/50000]\tLoss: 4.6954\tLR: 5.939898\n",
      "Training Epoch: 60 [20224/50000]\tLoss: 4.7414\tLR: 5.940153\n",
      "Training Epoch: 60 [20352/50000]\tLoss: 4.6947\tLR: 5.940409\n",
      "Training Epoch: 60 [20480/50000]\tLoss: 4.7217\tLR: 5.940665\n",
      "Training Epoch: 60 [20608/50000]\tLoss: 4.7244\tLR: 5.940921\n",
      "Training Epoch: 60 [20736/50000]\tLoss: 4.6944\tLR: 5.941176\n",
      "Training Epoch: 60 [20864/50000]\tLoss: 4.7360\tLR: 5.941432\n",
      "Training Epoch: 60 [20992/50000]\tLoss: 4.6919\tLR: 5.941688\n",
      "Training Epoch: 60 [21120/50000]\tLoss: 4.6745\tLR: 5.941944\n",
      "Training Epoch: 60 [21248/50000]\tLoss: 4.7782\tLR: 5.942199\n",
      "Training Epoch: 60 [21376/50000]\tLoss: 4.7377\tLR: 5.942455\n",
      "Training Epoch: 60 [21504/50000]\tLoss: 4.7355\tLR: 5.942711\n",
      "Training Epoch: 60 [21632/50000]\tLoss: 4.6960\tLR: 5.942967\n",
      "Training Epoch: 60 [21760/50000]\tLoss: 4.6750\tLR: 5.943223\n",
      "Training Epoch: 60 [21888/50000]\tLoss: 4.7154\tLR: 5.943478\n",
      "Training Epoch: 60 [22016/50000]\tLoss: 4.7175\tLR: 5.943734\n",
      "Training Epoch: 60 [22144/50000]\tLoss: 4.7042\tLR: 5.943990\n",
      "Training Epoch: 60 [22272/50000]\tLoss: 4.6771\tLR: 5.944246\n",
      "Training Epoch: 60 [22400/50000]\tLoss: 4.7329\tLR: 5.944501\n",
      "Training Epoch: 60 [22528/50000]\tLoss: 4.7016\tLR: 5.944757\n",
      "Training Epoch: 60 [22656/50000]\tLoss: 4.7671\tLR: 5.945013\n",
      "Training Epoch: 60 [22784/50000]\tLoss: 4.7605\tLR: 5.945269\n",
      "Training Epoch: 60 [22912/50000]\tLoss: 4.7321\tLR: 5.945524\n",
      "Training Epoch: 60 [23040/50000]\tLoss: 4.7134\tLR: 5.945780\n",
      "Training Epoch: 60 [23168/50000]\tLoss: 4.7131\tLR: 5.946036\n",
      "Training Epoch: 60 [23296/50000]\tLoss: 4.6592\tLR: 5.946292\n",
      "Training Epoch: 60 [23424/50000]\tLoss: 4.6790\tLR: 5.946547\n",
      "Training Epoch: 60 [23552/50000]\tLoss: 4.6768\tLR: 5.946803\n",
      "Training Epoch: 60 [23680/50000]\tLoss: 4.7696\tLR: 5.947059\n",
      "Training Epoch: 60 [23808/50000]\tLoss: 4.6884\tLR: 5.947315\n",
      "Training Epoch: 60 [23936/50000]\tLoss: 4.6896\tLR: 5.947570\n",
      "Training Epoch: 60 [24064/50000]\tLoss: 4.7867\tLR: 5.947826\n",
      "Training Epoch: 60 [24192/50000]\tLoss: 4.6639\tLR: 5.948082\n",
      "Training Epoch: 60 [24320/50000]\tLoss: 4.7834\tLR: 5.948338\n",
      "Training Epoch: 60 [24448/50000]\tLoss: 4.7139\tLR: 5.948593\n",
      "Training Epoch: 60 [24576/50000]\tLoss: 4.6592\tLR: 5.948849\n",
      "Training Epoch: 60 [24704/50000]\tLoss: 4.7200\tLR: 5.949105\n",
      "Training Epoch: 60 [24832/50000]\tLoss: 4.6801\tLR: 5.949361\n",
      "Training Epoch: 60 [24960/50000]\tLoss: 4.6780\tLR: 5.949616\n",
      "Training Epoch: 60 [25088/50000]\tLoss: 4.7242\tLR: 5.949872\n",
      "Training Epoch: 60 [25216/50000]\tLoss: 4.7468\tLR: 5.950128\n",
      "Training Epoch: 60 [25344/50000]\tLoss: 4.7177\tLR: 5.950384\n",
      "Training Epoch: 60 [25472/50000]\tLoss: 4.6519\tLR: 5.950639\n",
      "Training Epoch: 60 [25600/50000]\tLoss: 4.7452\tLR: 5.950895\n",
      "Training Epoch: 60 [25728/50000]\tLoss: 4.7331\tLR: 5.951151\n",
      "Training Epoch: 60 [25856/50000]\tLoss: 4.7101\tLR: 5.951407\n",
      "Training Epoch: 60 [25984/50000]\tLoss: 4.7427\tLR: 5.951662\n",
      "Training Epoch: 60 [26112/50000]\tLoss: 4.7269\tLR: 5.951918\n",
      "Training Epoch: 60 [26240/50000]\tLoss: 4.7556\tLR: 5.952174\n",
      "Training Epoch: 60 [26368/50000]\tLoss: 4.7700\tLR: 5.952430\n",
      "Training Epoch: 60 [26496/50000]\tLoss: 4.6228\tLR: 5.952685\n",
      "Training Epoch: 60 [26624/50000]\tLoss: 4.7449\tLR: 5.952941\n",
      "Training Epoch: 60 [26752/50000]\tLoss: 4.6934\tLR: 5.953197\n",
      "Training Epoch: 60 [26880/50000]\tLoss: 4.7363\tLR: 5.953453\n",
      "Training Epoch: 60 [27008/50000]\tLoss: 4.7450\tLR: 5.953708\n",
      "Training Epoch: 60 [27136/50000]\tLoss: 4.7386\tLR: 5.953964\n",
      "Training Epoch: 60 [27264/50000]\tLoss: 4.7087\tLR: 5.954220\n",
      "Training Epoch: 60 [27392/50000]\tLoss: 4.6582\tLR: 5.954476\n",
      "Training Epoch: 60 [27520/50000]\tLoss: 4.6816\tLR: 5.954731\n",
      "Training Epoch: 60 [27648/50000]\tLoss: 4.8211\tLR: 5.954987\n",
      "Training Epoch: 60 [27776/50000]\tLoss: 4.7662\tLR: 5.955243\n",
      "Training Epoch: 60 [27904/50000]\tLoss: 4.7031\tLR: 5.955499\n",
      "Training Epoch: 60 [28032/50000]\tLoss: 4.8138\tLR: 5.955754\n",
      "Training Epoch: 60 [28160/50000]\tLoss: 4.7981\tLR: 5.956010\n",
      "Training Epoch: 60 [28288/50000]\tLoss: 4.7212\tLR: 5.956266\n",
      "Training Epoch: 60 [28416/50000]\tLoss: 4.7007\tLR: 5.956522\n",
      "Training Epoch: 60 [28544/50000]\tLoss: 4.7091\tLR: 5.956777\n",
      "Training Epoch: 60 [28672/50000]\tLoss: 4.6745\tLR: 5.957033\n",
      "Training Epoch: 60 [28800/50000]\tLoss: 4.7618\tLR: 5.957289\n",
      "Training Epoch: 60 [28928/50000]\tLoss: 4.7381\tLR: 5.957545\n",
      "Training Epoch: 60 [29056/50000]\tLoss: 4.7200\tLR: 5.957801\n",
      "Training Epoch: 60 [29184/50000]\tLoss: 4.7087\tLR: 5.958056\n",
      "Training Epoch: 60 [29312/50000]\tLoss: 4.7123\tLR: 5.958312\n",
      "Training Epoch: 60 [29440/50000]\tLoss: 4.7453\tLR: 5.958568\n",
      "Training Epoch: 60 [29568/50000]\tLoss: 4.7118\tLR: 5.958824\n",
      "Training Epoch: 60 [29696/50000]\tLoss: 4.6736\tLR: 5.959079\n",
      "Training Epoch: 60 [29824/50000]\tLoss: 4.7668\tLR: 5.959335\n",
      "Training Epoch: 60 [29952/50000]\tLoss: 4.7073\tLR: 5.959591\n",
      "Training Epoch: 60 [30080/50000]\tLoss: 4.6389\tLR: 5.959847\n",
      "Training Epoch: 60 [30208/50000]\tLoss: 4.7264\tLR: 5.960102\n",
      "Training Epoch: 60 [30336/50000]\tLoss: 4.6575\tLR: 5.960358\n",
      "Training Epoch: 60 [30464/50000]\tLoss: 4.7309\tLR: 5.960614\n",
      "Training Epoch: 60 [30592/50000]\tLoss: 4.7609\tLR: 5.960870\n",
      "Training Epoch: 60 [30720/50000]\tLoss: 4.7017\tLR: 5.961125\n",
      "Training Epoch: 60 [30848/50000]\tLoss: 4.7084\tLR: 5.961381\n",
      "Training Epoch: 60 [30976/50000]\tLoss: 4.6731\tLR: 5.961637\n",
      "Training Epoch: 60 [31104/50000]\tLoss: 4.7186\tLR: 5.961893\n",
      "Training Epoch: 60 [31232/50000]\tLoss: 4.7073\tLR: 5.962148\n",
      "Training Epoch: 60 [31360/50000]\tLoss: 4.7269\tLR: 5.962404\n",
      "Training Epoch: 60 [31488/50000]\tLoss: 4.7061\tLR: 5.962660\n",
      "Training Epoch: 60 [31616/50000]\tLoss: 4.6129\tLR: 5.962916\n",
      "Training Epoch: 60 [31744/50000]\tLoss: 4.6346\tLR: 5.963171\n",
      "Training Epoch: 60 [31872/50000]\tLoss: 4.6680\tLR: 5.963427\n",
      "Training Epoch: 60 [32000/50000]\tLoss: 4.6602\tLR: 5.963683\n",
      "Training Epoch: 60 [32128/50000]\tLoss: 4.7066\tLR: 5.963939\n",
      "Training Epoch: 60 [32256/50000]\tLoss: 4.7009\tLR: 5.964194\n",
      "Training Epoch: 60 [32384/50000]\tLoss: 4.7679\tLR: 5.964450\n",
      "Training Epoch: 60 [32512/50000]\tLoss: 4.6494\tLR: 5.964706\n",
      "Training Epoch: 60 [32640/50000]\tLoss: 4.7316\tLR: 5.964962\n",
      "Training Epoch: 60 [32768/50000]\tLoss: 4.7097\tLR: 5.965217\n",
      "Training Epoch: 60 [32896/50000]\tLoss: 4.8112\tLR: 5.965473\n",
      "Training Epoch: 60 [33024/50000]\tLoss: 4.7544\tLR: 5.965729\n",
      "Training Epoch: 60 [33152/50000]\tLoss: 4.7404\tLR: 5.965985\n",
      "Training Epoch: 60 [33280/50000]\tLoss: 4.6804\tLR: 5.966240\n",
      "Training Epoch: 60 [33408/50000]\tLoss: 4.6986\tLR: 5.966496\n",
      "Training Epoch: 60 [33536/50000]\tLoss: 4.6682\tLR: 5.966752\n",
      "Training Epoch: 60 [33664/50000]\tLoss: 4.7264\tLR: 5.967008\n",
      "Training Epoch: 60 [33792/50000]\tLoss: 4.6438\tLR: 5.967263\n",
      "Training Epoch: 60 [33920/50000]\tLoss: 4.7386\tLR: 5.967519\n",
      "Training Epoch: 60 [34048/50000]\tLoss: 4.7065\tLR: 5.967775\n",
      "Training Epoch: 60 [34176/50000]\tLoss: 4.6905\tLR: 5.968031\n",
      "Training Epoch: 60 [34304/50000]\tLoss: 4.6939\tLR: 5.968286\n",
      "Training Epoch: 60 [34432/50000]\tLoss: 4.7440\tLR: 5.968542\n",
      "Training Epoch: 60 [34560/50000]\tLoss: 4.7752\tLR: 5.968798\n",
      "Training Epoch: 60 [34688/50000]\tLoss: 4.6561\tLR: 5.969054\n",
      "Training Epoch: 60 [34816/50000]\tLoss: 4.7041\tLR: 5.969309\n",
      "Training Epoch: 60 [34944/50000]\tLoss: 4.6734\tLR: 5.969565\n",
      "Training Epoch: 60 [35072/50000]\tLoss: 4.7555\tLR: 5.969821\n",
      "Training Epoch: 60 [35200/50000]\tLoss: 4.7290\tLR: 5.970077\n",
      "Training Epoch: 60 [35328/50000]\tLoss: 4.7144\tLR: 5.970332\n",
      "Training Epoch: 60 [35456/50000]\tLoss: 4.7255\tLR: 5.970588\n",
      "Training Epoch: 60 [35584/50000]\tLoss: 4.7373\tLR: 5.970844\n",
      "Training Epoch: 60 [35712/50000]\tLoss: 4.7152\tLR: 5.971100\n",
      "Training Epoch: 60 [35840/50000]\tLoss: 4.7910\tLR: 5.971355\n",
      "Training Epoch: 60 [35968/50000]\tLoss: 4.7046\tLR: 5.971611\n",
      "Training Epoch: 60 [36096/50000]\tLoss: 4.7745\tLR: 5.971867\n",
      "Training Epoch: 60 [36224/50000]\tLoss: 4.7868\tLR: 5.972123\n",
      "Training Epoch: 60 [36352/50000]\tLoss: 4.7600\tLR: 5.972379\n",
      "Training Epoch: 60 [36480/50000]\tLoss: 4.7457\tLR: 5.972634\n",
      "Training Epoch: 60 [36608/50000]\tLoss: 4.7763\tLR: 5.972890\n",
      "Training Epoch: 60 [36736/50000]\tLoss: 4.7081\tLR: 5.973146\n",
      "Training Epoch: 60 [36864/50000]\tLoss: 4.7521\tLR: 5.973402\n",
      "Training Epoch: 60 [36992/50000]\tLoss: 4.6829\tLR: 5.973657\n",
      "Training Epoch: 60 [37120/50000]\tLoss: 4.6565\tLR: 5.973913\n",
      "Training Epoch: 60 [37248/50000]\tLoss: 4.6925\tLR: 5.974169\n",
      "Training Epoch: 60 [37376/50000]\tLoss: 4.7330\tLR: 5.974425\n",
      "Training Epoch: 60 [37504/50000]\tLoss: 4.6313\tLR: 5.974680\n",
      "Training Epoch: 60 [37632/50000]\tLoss: 4.7461\tLR: 5.974936\n",
      "Training Epoch: 60 [37760/50000]\tLoss: 4.6571\tLR: 5.975192\n",
      "Training Epoch: 60 [37888/50000]\tLoss: 4.7932\tLR: 5.975448\n",
      "Training Epoch: 60 [38016/50000]\tLoss: 4.6804\tLR: 5.975703\n",
      "Training Epoch: 60 [38144/50000]\tLoss: 4.7443\tLR: 5.975959\n",
      "Training Epoch: 60 [38272/50000]\tLoss: 4.7077\tLR: 5.976215\n",
      "Training Epoch: 60 [38400/50000]\tLoss: 4.7416\tLR: 5.976471\n",
      "Training Epoch: 60 [38528/50000]\tLoss: 4.6287\tLR: 5.976726\n",
      "Training Epoch: 60 [38656/50000]\tLoss: 4.6785\tLR: 5.976982\n",
      "Training Epoch: 60 [38784/50000]\tLoss: 4.7832\tLR: 5.977238\n",
      "Training Epoch: 60 [38912/50000]\tLoss: 4.6884\tLR: 5.977494\n",
      "Training Epoch: 60 [39040/50000]\tLoss: 4.7040\tLR: 5.977749\n",
      "Training Epoch: 60 [39168/50000]\tLoss: 4.7418\tLR: 5.978005\n",
      "Training Epoch: 60 [39296/50000]\tLoss: 4.6993\tLR: 5.978261\n",
      "Training Epoch: 60 [39424/50000]\tLoss: 4.7536\tLR: 5.978517\n",
      "Training Epoch: 60 [39552/50000]\tLoss: 4.8334\tLR: 5.978772\n",
      "Training Epoch: 60 [39680/50000]\tLoss: 4.7560\tLR: 5.979028\n",
      "Training Epoch: 60 [39808/50000]\tLoss: 4.7945\tLR: 5.979284\n",
      "Training Epoch: 60 [39936/50000]\tLoss: 4.6966\tLR: 5.979540\n",
      "Training Epoch: 60 [40064/50000]\tLoss: 4.6725\tLR: 5.979795\n",
      "Training Epoch: 60 [40192/50000]\tLoss: 4.6675\tLR: 5.980051\n",
      "Training Epoch: 60 [40320/50000]\tLoss: 4.7636\tLR: 5.980307\n",
      "Training Epoch: 60 [40448/50000]\tLoss: 4.7577\tLR: 5.980563\n",
      "Training Epoch: 60 [40576/50000]\tLoss: 4.7070\tLR: 5.980818\n",
      "Training Epoch: 60 [40704/50000]\tLoss: 4.6825\tLR: 5.981074\n",
      "Training Epoch: 60 [40832/50000]\tLoss: 4.7059\tLR: 5.981330\n",
      "Training Epoch: 60 [40960/50000]\tLoss: 4.7527\tLR: 5.981586\n",
      "Training Epoch: 60 [41088/50000]\tLoss: 4.6978\tLR: 5.981841\n",
      "Training Epoch: 60 [41216/50000]\tLoss: 4.7553\tLR: 5.982097\n",
      "Training Epoch: 60 [41344/50000]\tLoss: 4.7019\tLR: 5.982353\n",
      "Training Epoch: 60 [41472/50000]\tLoss: 4.7208\tLR: 5.982609\n",
      "Training Epoch: 60 [41600/50000]\tLoss: 4.6219\tLR: 5.982864\n",
      "Training Epoch: 60 [41728/50000]\tLoss: 4.6501\tLR: 5.983120\n",
      "Training Epoch: 60 [41856/50000]\tLoss: 4.7315\tLR: 5.983376\n",
      "Training Epoch: 60 [41984/50000]\tLoss: 4.7527\tLR: 5.983632\n",
      "Training Epoch: 60 [42112/50000]\tLoss: 4.7259\tLR: 5.983887\n",
      "Training Epoch: 60 [42240/50000]\tLoss: 4.6749\tLR: 5.984143\n",
      "Training Epoch: 60 [42368/50000]\tLoss: 4.7228\tLR: 5.984399\n",
      "Training Epoch: 60 [42496/50000]\tLoss: 4.8221\tLR: 5.984655\n",
      "Training Epoch: 60 [42624/50000]\tLoss: 4.7548\tLR: 5.984910\n",
      "Training Epoch: 60 [42752/50000]\tLoss: 4.6556\tLR: 5.985166\n",
      "Training Epoch: 60 [42880/50000]\tLoss: 4.7405\tLR: 5.985422\n",
      "Training Epoch: 60 [43008/50000]\tLoss: 4.6785\tLR: 5.985678\n",
      "Training Epoch: 60 [43136/50000]\tLoss: 4.6542\tLR: 5.985934\n",
      "Training Epoch: 60 [43264/50000]\tLoss: 4.6893\tLR: 5.986189\n",
      "Training Epoch: 60 [43392/50000]\tLoss: 4.7736\tLR: 5.986445\n",
      "Training Epoch: 60 [43520/50000]\tLoss: 4.6853\tLR: 5.986701\n",
      "Training Epoch: 60 [43648/50000]\tLoss: 4.6959\tLR: 5.986957\n",
      "Training Epoch: 60 [43776/50000]\tLoss: 4.7546\tLR: 5.987212\n",
      "Training Epoch: 60 [43904/50000]\tLoss: 4.7670\tLR: 5.987468\n",
      "Training Epoch: 60 [44032/50000]\tLoss: 4.7159\tLR: 5.987724\n",
      "Training Epoch: 60 [44160/50000]\tLoss: 4.7383\tLR: 5.987980\n",
      "Training Epoch: 60 [44288/50000]\tLoss: 4.7188\tLR: 5.988235\n",
      "Training Epoch: 60 [44416/50000]\tLoss: 4.7471\tLR: 5.988491\n",
      "Training Epoch: 60 [44544/50000]\tLoss: 4.6741\tLR: 5.988747\n",
      "Training Epoch: 60 [44672/50000]\tLoss: 4.7378\tLR: 5.989003\n",
      "Training Epoch: 60 [44800/50000]\tLoss: 4.7365\tLR: 5.989258\n",
      "Training Epoch: 60 [44928/50000]\tLoss: 4.7209\tLR: 5.989514\n",
      "Training Epoch: 60 [45056/50000]\tLoss: 4.7705\tLR: 5.989770\n",
      "Training Epoch: 60 [45184/50000]\tLoss: 4.6935\tLR: 5.990026\n",
      "Training Epoch: 60 [45312/50000]\tLoss: 4.7753\tLR: 5.990281\n",
      "Training Epoch: 60 [45440/50000]\tLoss: 4.7740\tLR: 5.990537\n",
      "Training Epoch: 60 [45568/50000]\tLoss: 4.7081\tLR: 5.990793\n",
      "Training Epoch: 60 [45696/50000]\tLoss: 4.7101\tLR: 5.991049\n",
      "Training Epoch: 60 [45824/50000]\tLoss: 4.6934\tLR: 5.991304\n",
      "Training Epoch: 60 [45952/50000]\tLoss: 4.8154\tLR: 5.991560\n",
      "Training Epoch: 60 [46080/50000]\tLoss: 4.7526\tLR: 5.991816\n",
      "Training Epoch: 60 [46208/50000]\tLoss: 4.7914\tLR: 5.992072\n",
      "Training Epoch: 60 [46336/50000]\tLoss: 4.7739\tLR: 5.992327\n",
      "Training Epoch: 60 [46464/50000]\tLoss: 4.8231\tLR: 5.992583\n",
      "Training Epoch: 60 [46592/50000]\tLoss: 4.6915\tLR: 5.992839\n",
      "Training Epoch: 60 [46720/50000]\tLoss: 4.6683\tLR: 5.993095\n",
      "Training Epoch: 60 [46848/50000]\tLoss: 4.6812\tLR: 5.993350\n",
      "Training Epoch: 60 [46976/50000]\tLoss: 4.6788\tLR: 5.993606\n",
      "Training Epoch: 60 [47104/50000]\tLoss: 4.7549\tLR: 5.993862\n",
      "Training Epoch: 60 [47232/50000]\tLoss: 4.7494\tLR: 5.994118\n",
      "Training Epoch: 60 [47360/50000]\tLoss: 4.7455\tLR: 5.994373\n",
      "Training Epoch: 60 [47488/50000]\tLoss: 4.7107\tLR: 5.994629\n",
      "Training Epoch: 60 [47616/50000]\tLoss: 4.7283\tLR: 5.994885\n",
      "Training Epoch: 60 [47744/50000]\tLoss: 4.7581\tLR: 5.995141\n",
      "Training Epoch: 60 [47872/50000]\tLoss: 4.6813\tLR: 5.995396\n",
      "Training Epoch: 60 [48000/50000]\tLoss: 4.7577\tLR: 5.995652\n",
      "Training Epoch: 60 [48128/50000]\tLoss: 4.7478\tLR: 5.995908\n",
      "Training Epoch: 60 [48256/50000]\tLoss: 4.7657\tLR: 5.996164\n",
      "Training Epoch: 60 [48384/50000]\tLoss: 4.6591\tLR: 5.996419\n",
      "Training Epoch: 60 [48512/50000]\tLoss: 4.7047\tLR: 5.996675\n",
      "Training Epoch: 60 [48640/50000]\tLoss: 4.7144\tLR: 5.996931\n",
      "Training Epoch: 60 [48768/50000]\tLoss: 4.7476\tLR: 5.997187\n",
      "Training Epoch: 60 [48896/50000]\tLoss: 4.6934\tLR: 5.997442\n",
      "Training Epoch: 60 [49024/50000]\tLoss: 4.7935\tLR: 5.997698\n",
      "Training Epoch: 60 [49152/50000]\tLoss: 4.7306\tLR: 5.997954\n",
      "Training Epoch: 60 [49280/50000]\tLoss: 4.7143\tLR: 5.998210\n",
      "Training Epoch: 60 [49408/50000]\tLoss: 4.7351\tLR: 5.998465\n",
      "Training Epoch: 60 [49536/50000]\tLoss: 4.7505\tLR: 5.998721\n",
      "Training Epoch: 60 [49664/50000]\tLoss: 4.7572\tLR: 5.998977\n",
      "Training Epoch: 60 [49792/50000]\tLoss: 4.6627\tLR: 5.999233\n",
      "Training Epoch: 60 [49920/50000]\tLoss: 4.7165\tLR: 5.999488\n",
      "Training Epoch: 60 [50000/50000]\tLoss: 4.8105\tLR: 5.999744\n",
      "epoch 60 training time consumed: 489.00s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   84116 GB |   84116 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   83858 GB |   83858 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     258 GB |     258 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   84116 GB |   84116 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   83858 GB |   83858 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     258 GB |     258 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   82933 GB |   82933 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   82675 GB |   82675 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     258 GB |     258 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    8919 K  |    8919 K  |\n",
      "|       from large pool |      24    |      65    |    3802 K  |    3802 K  |\n",
      "|       from small pool |     231    |     274    |    5117 K  |    5116 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    8919 K  |    8919 K  |\n",
      "|       from large pool |      24    |      65    |    3802 K  |    3802 K  |\n",
      "|       from small pool |     231    |     274    |    5117 K  |    5116 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      47    |    5169 K  |    5169 K  |\n",
      "|       from large pool |      10    |      23    |    1827 K  |    1827 K  |\n",
      "|       from small pool |      25    |      35    |    3342 K  |    3342 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 60, Average loss: 0.0376, Accuracy: 0.0100, Time consumed:31.28s\n",
      "\n",
      "saving weights file to checkpoint\\resnet18\\Monday_25_July_2022_16h_56m_47s\\resnet18-60-regular.pth\n",
      "Training Epoch: 61 [128/50000]\tLoss: 4.7675\tLR: 0.020000\n",
      "Training Epoch: 61 [256/50000]\tLoss: 4.8036\tLR: 6.000256\n",
      "Training Epoch: 61 [384/50000]\tLoss: 4.7943\tLR: 6.000512\n",
      "Training Epoch: 61 [512/50000]\tLoss: 4.7620\tLR: 6.000767\n",
      "Training Epoch: 61 [640/50000]\tLoss: 4.6749\tLR: 6.001023\n",
      "Training Epoch: 61 [768/50000]\tLoss: 4.6925\tLR: 6.001279\n",
      "Training Epoch: 61 [896/50000]\tLoss: 4.8027\tLR: 6.001535\n",
      "Training Epoch: 61 [1024/50000]\tLoss: 4.7005\tLR: 6.001790\n",
      "Training Epoch: 61 [1152/50000]\tLoss: 4.7722\tLR: 6.002046\n",
      "Training Epoch: 61 [1280/50000]\tLoss: 4.6870\tLR: 6.002302\n",
      "Training Epoch: 61 [1408/50000]\tLoss: 4.7531\tLR: 6.002558\n",
      "Training Epoch: 61 [1536/50000]\tLoss: 4.7521\tLR: 6.002813\n",
      "Training Epoch: 61 [1664/50000]\tLoss: 4.7523\tLR: 6.003069\n",
      "Training Epoch: 61 [1792/50000]\tLoss: 4.7228\tLR: 6.003325\n",
      "Training Epoch: 61 [1920/50000]\tLoss: 4.7320\tLR: 6.003581\n",
      "Training Epoch: 61 [2048/50000]\tLoss: 4.7278\tLR: 6.003836\n",
      "Training Epoch: 61 [2176/50000]\tLoss: 4.7347\tLR: 6.004092\n",
      "Training Epoch: 61 [2304/50000]\tLoss: 4.6635\tLR: 6.004348\n",
      "Training Epoch: 61 [2432/50000]\tLoss: 4.8108\tLR: 6.004604\n",
      "Training Epoch: 61 [2560/50000]\tLoss: 4.7141\tLR: 6.004859\n",
      "Training Epoch: 61 [2688/50000]\tLoss: 4.6836\tLR: 6.005115\n",
      "Training Epoch: 61 [2816/50000]\tLoss: 4.6973\tLR: 6.005371\n",
      "Training Epoch: 61 [2944/50000]\tLoss: 4.7596\tLR: 6.005627\n",
      "Training Epoch: 61 [3072/50000]\tLoss: 4.7564\tLR: 6.005882\n",
      "Training Epoch: 61 [3200/50000]\tLoss: 4.7214\tLR: 6.006138\n",
      "Training Epoch: 61 [3328/50000]\tLoss: 4.7086\tLR: 6.006394\n",
      "Training Epoch: 61 [3456/50000]\tLoss: 4.7498\tLR: 6.006650\n",
      "Training Epoch: 61 [3584/50000]\tLoss: 4.7982\tLR: 6.006905\n",
      "Training Epoch: 61 [3712/50000]\tLoss: 4.8179\tLR: 6.007161\n",
      "Training Epoch: 61 [3840/50000]\tLoss: 4.7697\tLR: 6.007417\n",
      "Training Epoch: 61 [3968/50000]\tLoss: 4.6573\tLR: 6.007673\n",
      "Training Epoch: 61 [4096/50000]\tLoss: 4.7044\tLR: 6.007928\n",
      "Training Epoch: 61 [4224/50000]\tLoss: 4.7426\tLR: 6.008184\n",
      "Training Epoch: 61 [4352/50000]\tLoss: 4.7244\tLR: 6.008440\n",
      "Training Epoch: 61 [4480/50000]\tLoss: 4.6598\tLR: 6.008696\n",
      "Training Epoch: 61 [4608/50000]\tLoss: 4.7372\tLR: 6.008951\n",
      "Training Epoch: 61 [4736/50000]\tLoss: 4.7308\tLR: 6.009207\n",
      "Training Epoch: 61 [4864/50000]\tLoss: 4.7090\tLR: 6.009463\n",
      "Training Epoch: 61 [4992/50000]\tLoss: 4.6673\tLR: 6.009719\n",
      "Training Epoch: 61 [5120/50000]\tLoss: 4.8221\tLR: 6.009974\n",
      "Training Epoch: 61 [5248/50000]\tLoss: 4.7414\tLR: 6.010230\n",
      "Training Epoch: 61 [5376/50000]\tLoss: 4.7645\tLR: 6.010486\n",
      "Training Epoch: 61 [5504/50000]\tLoss: 4.7064\tLR: 6.010742\n",
      "Training Epoch: 61 [5632/50000]\tLoss: 4.7181\tLR: 6.010997\n",
      "Training Epoch: 61 [5760/50000]\tLoss: 4.7110\tLR: 6.011253\n",
      "Training Epoch: 61 [5888/50000]\tLoss: 4.7824\tLR: 6.011509\n",
      "Training Epoch: 61 [6016/50000]\tLoss: 4.8117\tLR: 6.011765\n",
      "Training Epoch: 61 [6144/50000]\tLoss: 4.7409\tLR: 6.012020\n",
      "Training Epoch: 61 [6272/50000]\tLoss: 4.6677\tLR: 6.012276\n",
      "Training Epoch: 61 [6400/50000]\tLoss: 4.7603\tLR: 6.012532\n",
      "Training Epoch: 61 [6528/50000]\tLoss: 4.7679\tLR: 6.012788\n",
      "Training Epoch: 61 [6656/50000]\tLoss: 4.7316\tLR: 6.013043\n",
      "Training Epoch: 61 [6784/50000]\tLoss: 4.7097\tLR: 6.013299\n",
      "Training Epoch: 61 [6912/50000]\tLoss: 4.7671\tLR: 6.013555\n",
      "Training Epoch: 61 [7040/50000]\tLoss: 4.6962\tLR: 6.013811\n",
      "Training Epoch: 61 [7168/50000]\tLoss: 4.7335\tLR: 6.014066\n",
      "Training Epoch: 61 [7296/50000]\tLoss: 4.7315\tLR: 6.014322\n",
      "Training Epoch: 61 [7424/50000]\tLoss: 4.6567\tLR: 6.014578\n",
      "Training Epoch: 61 [7552/50000]\tLoss: 4.7306\tLR: 6.014834\n",
      "Training Epoch: 61 [7680/50000]\tLoss: 4.6733\tLR: 6.015090\n",
      "Training Epoch: 61 [7808/50000]\tLoss: 4.7583\tLR: 6.015345\n",
      "Training Epoch: 61 [7936/50000]\tLoss: 4.7417\tLR: 6.015601\n",
      "Training Epoch: 61 [8064/50000]\tLoss: 4.7286\tLR: 6.015857\n",
      "Training Epoch: 61 [8192/50000]\tLoss: 4.7359\tLR: 6.016113\n",
      "Training Epoch: 61 [8320/50000]\tLoss: 4.6902\tLR: 6.016368\n",
      "Training Epoch: 61 [8448/50000]\tLoss: 4.7007\tLR: 6.016624\n",
      "Training Epoch: 61 [8576/50000]\tLoss: 4.7501\tLR: 6.016880\n",
      "Training Epoch: 61 [8704/50000]\tLoss: 4.7636\tLR: 6.017136\n",
      "Training Epoch: 61 [8832/50000]\tLoss: 4.6942\tLR: 6.017391\n",
      "Training Epoch: 61 [8960/50000]\tLoss: 4.7274\tLR: 6.017647\n",
      "Training Epoch: 61 [9088/50000]\tLoss: 4.7842\tLR: 6.017903\n",
      "Training Epoch: 61 [9216/50000]\tLoss: 4.6291\tLR: 6.018159\n",
      "Training Epoch: 61 [9344/50000]\tLoss: 4.7047\tLR: 6.018414\n",
      "Training Epoch: 61 [9472/50000]\tLoss: 4.7012\tLR: 6.018670\n",
      "Training Epoch: 61 [9600/50000]\tLoss: 4.6510\tLR: 6.018926\n",
      "Training Epoch: 61 [9728/50000]\tLoss: 4.6746\tLR: 6.019182\n",
      "Training Epoch: 61 [9856/50000]\tLoss: 4.6990\tLR: 6.019437\n",
      "Training Epoch: 61 [9984/50000]\tLoss: 4.6888\tLR: 6.019693\n",
      "Training Epoch: 61 [10112/50000]\tLoss: 4.7512\tLR: 6.019949\n",
      "Training Epoch: 61 [10240/50000]\tLoss: 4.6977\tLR: 6.020205\n",
      "Training Epoch: 61 [10368/50000]\tLoss: 4.6872\tLR: 6.020460\n",
      "Training Epoch: 61 [10496/50000]\tLoss: 4.7680\tLR: 6.020716\n",
      "Training Epoch: 61 [10624/50000]\tLoss: 4.8064\tLR: 6.020972\n",
      "Training Epoch: 61 [10752/50000]\tLoss: 4.7601\tLR: 6.021228\n",
      "Training Epoch: 61 [10880/50000]\tLoss: 4.7064\tLR: 6.021483\n",
      "Training Epoch: 61 [11008/50000]\tLoss: 4.6675\tLR: 6.021739\n",
      "Training Epoch: 61 [11136/50000]\tLoss: 4.7269\tLR: 6.021995\n",
      "Training Epoch: 61 [11264/50000]\tLoss: 4.7030\tLR: 6.022251\n",
      "Training Epoch: 61 [11392/50000]\tLoss: 4.6868\tLR: 6.022506\n",
      "Training Epoch: 61 [11520/50000]\tLoss: 4.6633\tLR: 6.022762\n",
      "Training Epoch: 61 [11648/50000]\tLoss: 4.6646\tLR: 6.023018\n",
      "Training Epoch: 61 [11776/50000]\tLoss: 4.7243\tLR: 6.023274\n",
      "Training Epoch: 61 [11904/50000]\tLoss: 4.5913\tLR: 6.023529\n",
      "Training Epoch: 61 [12032/50000]\tLoss: 4.6384\tLR: 6.023785\n",
      "Training Epoch: 61 [12160/50000]\tLoss: 4.6651\tLR: 6.024041\n",
      "Training Epoch: 61 [12288/50000]\tLoss: 4.8688\tLR: 6.024297\n",
      "Training Epoch: 61 [12416/50000]\tLoss: 4.6914\tLR: 6.024552\n",
      "Training Epoch: 61 [12544/50000]\tLoss: 4.7302\tLR: 6.024808\n",
      "Training Epoch: 61 [12672/50000]\tLoss: 4.7449\tLR: 6.025064\n",
      "Training Epoch: 61 [12800/50000]\tLoss: 4.7694\tLR: 6.025320\n",
      "Training Epoch: 61 [12928/50000]\tLoss: 4.7485\tLR: 6.025575\n",
      "Training Epoch: 61 [13056/50000]\tLoss: 4.7036\tLR: 6.025831\n",
      "Training Epoch: 61 [13184/50000]\tLoss: 4.7063\tLR: 6.026087\n",
      "Training Epoch: 61 [13312/50000]\tLoss: 4.6349\tLR: 6.026343\n",
      "Training Epoch: 61 [13440/50000]\tLoss: 4.7162\tLR: 6.026598\n",
      "Training Epoch: 61 [13568/50000]\tLoss: 4.7031\tLR: 6.026854\n",
      "Training Epoch: 61 [13696/50000]\tLoss: 4.7009\tLR: 6.027110\n",
      "Training Epoch: 61 [13824/50000]\tLoss: 4.6986\tLR: 6.027366\n",
      "Training Epoch: 61 [13952/50000]\tLoss: 4.7333\tLR: 6.027621\n",
      "Training Epoch: 61 [14080/50000]\tLoss: 4.7102\tLR: 6.027877\n",
      "Training Epoch: 61 [14208/50000]\tLoss: 4.6927\tLR: 6.028133\n",
      "Training Epoch: 61 [14336/50000]\tLoss: 4.6943\tLR: 6.028389\n",
      "Training Epoch: 61 [14464/50000]\tLoss: 4.7319\tLR: 6.028645\n",
      "Training Epoch: 61 [14592/50000]\tLoss: 4.7710\tLR: 6.028900\n",
      "Training Epoch: 61 [14720/50000]\tLoss: 4.6502\tLR: 6.029156\n",
      "Training Epoch: 61 [14848/50000]\tLoss: 4.7310\tLR: 6.029412\n",
      "Training Epoch: 61 [14976/50000]\tLoss: 4.7474\tLR: 6.029668\n",
      "Training Epoch: 61 [15104/50000]\tLoss: 4.7671\tLR: 6.029923\n",
      "Training Epoch: 61 [15232/50000]\tLoss: 4.7407\tLR: 6.030179\n",
      "Training Epoch: 61 [15360/50000]\tLoss: 4.7482\tLR: 6.030435\n",
      "Training Epoch: 61 [15488/50000]\tLoss: 4.7234\tLR: 6.030691\n",
      "Training Epoch: 61 [15616/50000]\tLoss: 4.6818\tLR: 6.030946\n",
      "Training Epoch: 61 [15744/50000]\tLoss: 4.6876\tLR: 6.031202\n",
      "Training Epoch: 61 [15872/50000]\tLoss: 4.7109\tLR: 6.031458\n",
      "Training Epoch: 61 [16000/50000]\tLoss: 4.7695\tLR: 6.031714\n",
      "Training Epoch: 61 [16128/50000]\tLoss: 4.8234\tLR: 6.031969\n",
      "Training Epoch: 61 [16256/50000]\tLoss: 4.6673\tLR: 6.032225\n",
      "Training Epoch: 61 [16384/50000]\tLoss: 4.7711\tLR: 6.032481\n",
      "Training Epoch: 61 [16512/50000]\tLoss: 4.6888\tLR: 6.032737\n",
      "Training Epoch: 61 [16640/50000]\tLoss: 4.7413\tLR: 6.032992\n",
      "Training Epoch: 61 [16768/50000]\tLoss: 4.7183\tLR: 6.033248\n",
      "Training Epoch: 61 [16896/50000]\tLoss: 4.6479\tLR: 6.033504\n",
      "Training Epoch: 61 [17024/50000]\tLoss: 4.6624\tLR: 6.033760\n",
      "Training Epoch: 61 [17152/50000]\tLoss: 4.7391\tLR: 6.034015\n",
      "Training Epoch: 61 [17280/50000]\tLoss: 4.8569\tLR: 6.034271\n",
      "Training Epoch: 61 [17408/50000]\tLoss: 4.7133\tLR: 6.034527\n",
      "Training Epoch: 61 [17536/50000]\tLoss: 4.6846\tLR: 6.034783\n",
      "Training Epoch: 61 [17664/50000]\tLoss: 4.6338\tLR: 6.035038\n",
      "Training Epoch: 61 [17792/50000]\tLoss: 4.7217\tLR: 6.035294\n",
      "Training Epoch: 61 [17920/50000]\tLoss: 4.7110\tLR: 6.035550\n",
      "Training Epoch: 61 [18048/50000]\tLoss: 4.6507\tLR: 6.035806\n",
      "Training Epoch: 61 [18176/50000]\tLoss: 4.6686\tLR: 6.036061\n",
      "Training Epoch: 61 [18304/50000]\tLoss: 4.6443\tLR: 6.036317\n",
      "Training Epoch: 61 [18432/50000]\tLoss: 4.7234\tLR: 6.036573\n",
      "Training Epoch: 61 [18560/50000]\tLoss: 4.7199\tLR: 6.036829\n",
      "Training Epoch: 61 [18688/50000]\tLoss: 4.6982\tLR: 6.037084\n",
      "Training Epoch: 61 [18816/50000]\tLoss: 4.6998\tLR: 6.037340\n",
      "Training Epoch: 61 [18944/50000]\tLoss: 4.8155\tLR: 6.037596\n",
      "Training Epoch: 61 [19072/50000]\tLoss: 4.6869\tLR: 6.037852\n",
      "Training Epoch: 61 [19200/50000]\tLoss: 4.6407\tLR: 6.038107\n",
      "Training Epoch: 61 [19328/50000]\tLoss: 4.6947\tLR: 6.038363\n",
      "Training Epoch: 61 [19456/50000]\tLoss: 4.7562\tLR: 6.038619\n",
      "Training Epoch: 61 [19584/50000]\tLoss: 4.7745\tLR: 6.038875\n",
      "Training Epoch: 61 [19712/50000]\tLoss: 4.7440\tLR: 6.039130\n",
      "Training Epoch: 61 [19840/50000]\tLoss: 4.6441\tLR: 6.039386\n",
      "Training Epoch: 61 [19968/50000]\tLoss: 4.7352\tLR: 6.039642\n",
      "Training Epoch: 61 [20096/50000]\tLoss: 4.6537\tLR: 6.039898\n",
      "Training Epoch: 61 [20224/50000]\tLoss: 4.7463\tLR: 6.040153\n",
      "Training Epoch: 61 [20352/50000]\tLoss: 4.7827\tLR: 6.040409\n",
      "Training Epoch: 61 [20480/50000]\tLoss: 4.7320\tLR: 6.040665\n",
      "Training Epoch: 61 [20608/50000]\tLoss: 4.7249\tLR: 6.040921\n",
      "Training Epoch: 61 [20736/50000]\tLoss: 4.7309\tLR: 6.041176\n",
      "Training Epoch: 61 [20864/50000]\tLoss: 4.7157\tLR: 6.041432\n",
      "Training Epoch: 61 [20992/50000]\tLoss: 4.6969\tLR: 6.041688\n",
      "Training Epoch: 61 [21120/50000]\tLoss: 4.8315\tLR: 6.041944\n",
      "Training Epoch: 61 [21248/50000]\tLoss: 4.7312\tLR: 6.042199\n",
      "Training Epoch: 61 [21376/50000]\tLoss: 4.6946\tLR: 6.042455\n",
      "Training Epoch: 61 [21504/50000]\tLoss: 4.7660\tLR: 6.042711\n",
      "Training Epoch: 61 [21632/50000]\tLoss: 4.7209\tLR: 6.042967\n",
      "Training Epoch: 61 [21760/50000]\tLoss: 4.6876\tLR: 6.043223\n",
      "Training Epoch: 61 [21888/50000]\tLoss: 4.6720\tLR: 6.043478\n",
      "Training Epoch: 61 [22016/50000]\tLoss: 4.7067\tLR: 6.043734\n",
      "Training Epoch: 61 [22144/50000]\tLoss: 4.6714\tLR: 6.043990\n",
      "Training Epoch: 61 [22272/50000]\tLoss: 4.6861\tLR: 6.044246\n",
      "Training Epoch: 61 [22400/50000]\tLoss: 4.7061\tLR: 6.044501\n",
      "Training Epoch: 61 [22528/50000]\tLoss: 4.7333\tLR: 6.044757\n",
      "Training Epoch: 61 [22656/50000]\tLoss: 4.7394\tLR: 6.045013\n",
      "Training Epoch: 61 [22784/50000]\tLoss: 4.6646\tLR: 6.045269\n",
      "Training Epoch: 61 [22912/50000]\tLoss: 4.6995\tLR: 6.045524\n",
      "Training Epoch: 61 [23040/50000]\tLoss: 4.7271\tLR: 6.045780\n",
      "Training Epoch: 61 [23168/50000]\tLoss: 4.6849\tLR: 6.046036\n",
      "Training Epoch: 61 [23296/50000]\tLoss: 4.7065\tLR: 6.046292\n",
      "Training Epoch: 61 [23424/50000]\tLoss: 4.7137\tLR: 6.046547\n",
      "Training Epoch: 61 [23552/50000]\tLoss: 4.6875\tLR: 6.046803\n",
      "Training Epoch: 61 [23680/50000]\tLoss: 4.7031\tLR: 6.047059\n",
      "Training Epoch: 61 [23808/50000]\tLoss: 4.7035\tLR: 6.047315\n",
      "Training Epoch: 61 [23936/50000]\tLoss: 4.6947\tLR: 6.047570\n",
      "Training Epoch: 61 [24064/50000]\tLoss: 4.7073\tLR: 6.047826\n",
      "Training Epoch: 61 [24192/50000]\tLoss: 4.8447\tLR: 6.048082\n",
      "Training Epoch: 61 [24320/50000]\tLoss: 4.6528\tLR: 6.048338\n",
      "Training Epoch: 61 [24448/50000]\tLoss: 4.6727\tLR: 6.048593\n",
      "Training Epoch: 61 [24576/50000]\tLoss: 4.7549\tLR: 6.048849\n",
      "Training Epoch: 61 [24704/50000]\tLoss: 4.7430\tLR: 6.049105\n",
      "Training Epoch: 61 [24832/50000]\tLoss: 4.5677\tLR: 6.049361\n",
      "Training Epoch: 61 [24960/50000]\tLoss: 4.7742\tLR: 6.049616\n",
      "Training Epoch: 61 [25088/50000]\tLoss: 4.7526\tLR: 6.049872\n",
      "Training Epoch: 61 [25216/50000]\tLoss: 4.7375\tLR: 6.050128\n",
      "Training Epoch: 61 [25344/50000]\tLoss: 4.7924\tLR: 6.050384\n",
      "Training Epoch: 61 [25472/50000]\tLoss: 4.6754\tLR: 6.050639\n",
      "Training Epoch: 61 [25600/50000]\tLoss: 4.6989\tLR: 6.050895\n",
      "Training Epoch: 61 [25728/50000]\tLoss: 4.6403\tLR: 6.051151\n",
      "Training Epoch: 61 [25856/50000]\tLoss: 4.7156\tLR: 6.051407\n",
      "Training Epoch: 61 [25984/50000]\tLoss: 4.7507\tLR: 6.051662\n",
      "Training Epoch: 61 [26112/50000]\tLoss: 4.7290\tLR: 6.051918\n",
      "Training Epoch: 61 [26240/50000]\tLoss: 4.6987\tLR: 6.052174\n",
      "Training Epoch: 61 [26368/50000]\tLoss: 4.6589\tLR: 6.052430\n",
      "Training Epoch: 61 [26496/50000]\tLoss: 4.6971\tLR: 6.052685\n",
      "Training Epoch: 61 [26624/50000]\tLoss: 4.7830\tLR: 6.052941\n",
      "Training Epoch: 61 [26752/50000]\tLoss: 4.7158\tLR: 6.053197\n",
      "Training Epoch: 61 [26880/50000]\tLoss: 4.7123\tLR: 6.053453\n",
      "Training Epoch: 61 [27008/50000]\tLoss: 4.7440\tLR: 6.053708\n",
      "Training Epoch: 61 [27136/50000]\tLoss: 4.8004\tLR: 6.053964\n",
      "Training Epoch: 61 [27264/50000]\tLoss: 4.7205\tLR: 6.054220\n",
      "Training Epoch: 61 [27392/50000]\tLoss: 4.7980\tLR: 6.054476\n",
      "Training Epoch: 61 [27520/50000]\tLoss: 4.6156\tLR: 6.054731\n",
      "Training Epoch: 61 [27648/50000]\tLoss: 4.7048\tLR: 6.054987\n",
      "Training Epoch: 61 [27776/50000]\tLoss: 4.7859\tLR: 6.055243\n",
      "Training Epoch: 61 [27904/50000]\tLoss: 4.7366\tLR: 6.055499\n",
      "Training Epoch: 61 [28032/50000]\tLoss: 4.7443\tLR: 6.055754\n",
      "Training Epoch: 61 [28160/50000]\tLoss: 4.6830\tLR: 6.056010\n",
      "Training Epoch: 61 [28288/50000]\tLoss: 4.7308\tLR: 6.056266\n",
      "Training Epoch: 61 [28416/50000]\tLoss: 4.7320\tLR: 6.056522\n",
      "Training Epoch: 61 [28544/50000]\tLoss: 4.6763\tLR: 6.056777\n",
      "Training Epoch: 61 [28672/50000]\tLoss: 4.6810\tLR: 6.057033\n",
      "Training Epoch: 61 [28800/50000]\tLoss: 4.8121\tLR: 6.057289\n",
      "Training Epoch: 61 [28928/50000]\tLoss: 4.6638\tLR: 6.057545\n",
      "Training Epoch: 61 [29056/50000]\tLoss: 4.7037\tLR: 6.057801\n",
      "Training Epoch: 61 [29184/50000]\tLoss: 4.7211\tLR: 6.058056\n",
      "Training Epoch: 61 [29312/50000]\tLoss: 4.7402\tLR: 6.058312\n",
      "Training Epoch: 61 [29440/50000]\tLoss: 4.6855\tLR: 6.058568\n",
      "Training Epoch: 61 [29568/50000]\tLoss: 4.7332\tLR: 6.058824\n",
      "Training Epoch: 61 [29696/50000]\tLoss: 4.7880\tLR: 6.059079\n",
      "Training Epoch: 61 [29824/50000]\tLoss: 4.7336\tLR: 6.059335\n",
      "Training Epoch: 61 [29952/50000]\tLoss: 4.7843\tLR: 6.059591\n",
      "Training Epoch: 61 [30080/50000]\tLoss: 4.7227\tLR: 6.059847\n",
      "Training Epoch: 61 [30208/50000]\tLoss: 4.7087\tLR: 6.060102\n",
      "Training Epoch: 61 [30336/50000]\tLoss: 4.7185\tLR: 6.060358\n",
      "Training Epoch: 61 [30464/50000]\tLoss: 4.7298\tLR: 6.060614\n",
      "Training Epoch: 61 [30592/50000]\tLoss: 4.7424\tLR: 6.060870\n",
      "Training Epoch: 61 [30720/50000]\tLoss: 4.7243\tLR: 6.061125\n",
      "Training Epoch: 61 [30848/50000]\tLoss: 4.7262\tLR: 6.061381\n",
      "Training Epoch: 61 [30976/50000]\tLoss: 4.7607\tLR: 6.061637\n",
      "Training Epoch: 61 [31104/50000]\tLoss: 4.7163\tLR: 6.061893\n",
      "Training Epoch: 61 [31232/50000]\tLoss: 4.7159\tLR: 6.062148\n",
      "Training Epoch: 61 [31360/50000]\tLoss: 4.6343\tLR: 6.062404\n",
      "Training Epoch: 61 [31488/50000]\tLoss: 4.7335\tLR: 6.062660\n",
      "Training Epoch: 61 [31616/50000]\tLoss: 4.6797\tLR: 6.062916\n",
      "Training Epoch: 61 [31744/50000]\tLoss: 4.7416\tLR: 6.063171\n",
      "Training Epoch: 61 [31872/50000]\tLoss: 4.7589\tLR: 6.063427\n",
      "Training Epoch: 61 [32000/50000]\tLoss: 4.7082\tLR: 6.063683\n",
      "Training Epoch: 61 [32128/50000]\tLoss: 4.7169\tLR: 6.063939\n",
      "Training Epoch: 61 [32256/50000]\tLoss: 4.7501\tLR: 6.064194\n",
      "Training Epoch: 61 [32384/50000]\tLoss: 4.7062\tLR: 6.064450\n",
      "Training Epoch: 61 [32512/50000]\tLoss: 4.7059\tLR: 6.064706\n",
      "Training Epoch: 61 [32640/50000]\tLoss: 4.7091\tLR: 6.064962\n",
      "Training Epoch: 61 [32768/50000]\tLoss: 4.6894\tLR: 6.065217\n",
      "Training Epoch: 61 [32896/50000]\tLoss: 4.6916\tLR: 6.065473\n",
      "Training Epoch: 61 [33024/50000]\tLoss: 4.7472\tLR: 6.065729\n",
      "Training Epoch: 61 [33152/50000]\tLoss: 4.7515\tLR: 6.065985\n",
      "Training Epoch: 61 [33280/50000]\tLoss: 4.7392\tLR: 6.066240\n",
      "Training Epoch: 61 [33408/50000]\tLoss: 4.7207\tLR: 6.066496\n",
      "Training Epoch: 61 [33536/50000]\tLoss: 4.7131\tLR: 6.066752\n",
      "Training Epoch: 61 [33664/50000]\tLoss: 4.6841\tLR: 6.067008\n",
      "Training Epoch: 61 [33792/50000]\tLoss: 4.7840\tLR: 6.067263\n",
      "Training Epoch: 61 [33920/50000]\tLoss: 4.6888\tLR: 6.067519\n",
      "Training Epoch: 61 [34048/50000]\tLoss: 4.6950\tLR: 6.067775\n",
      "Training Epoch: 61 [34176/50000]\tLoss: 4.6202\tLR: 6.068031\n",
      "Training Epoch: 61 [34304/50000]\tLoss: 4.6744\tLR: 6.068286\n",
      "Training Epoch: 61 [34432/50000]\tLoss: 4.7208\tLR: 6.068542\n",
      "Training Epoch: 61 [34560/50000]\tLoss: 4.6841\tLR: 6.068798\n",
      "Training Epoch: 61 [34688/50000]\tLoss: 4.7557\tLR: 6.069054\n",
      "Training Epoch: 61 [34816/50000]\tLoss: 4.7286\tLR: 6.069309\n",
      "Training Epoch: 61 [34944/50000]\tLoss: 4.6845\tLR: 6.069565\n",
      "Training Epoch: 61 [35072/50000]\tLoss: 4.6582\tLR: 6.069821\n",
      "Training Epoch: 61 [35200/50000]\tLoss: 4.7286\tLR: 6.070077\n",
      "Training Epoch: 61 [35328/50000]\tLoss: 4.6306\tLR: 6.070332\n",
      "Training Epoch: 61 [35456/50000]\tLoss: 4.6895\tLR: 6.070588\n",
      "Training Epoch: 61 [35584/50000]\tLoss: 4.7068\tLR: 6.070844\n",
      "Training Epoch: 61 [35712/50000]\tLoss: 4.7062\tLR: 6.071100\n",
      "Training Epoch: 61 [35840/50000]\tLoss: 4.6465\tLR: 6.071355\n",
      "Training Epoch: 61 [35968/50000]\tLoss: 4.7074\tLR: 6.071611\n",
      "Training Epoch: 61 [36096/50000]\tLoss: 4.7168\tLR: 6.071867\n",
      "Training Epoch: 61 [36224/50000]\tLoss: 4.7220\tLR: 6.072123\n",
      "Training Epoch: 61 [36352/50000]\tLoss: 4.7136\tLR: 6.072379\n",
      "Training Epoch: 61 [36480/50000]\tLoss: 4.6670\tLR: 6.072634\n",
      "Training Epoch: 61 [36608/50000]\tLoss: 4.7235\tLR: 6.072890\n",
      "Training Epoch: 61 [36736/50000]\tLoss: 4.7206\tLR: 6.073146\n",
      "Training Epoch: 61 [36864/50000]\tLoss: 4.6591\tLR: 6.073402\n",
      "Training Epoch: 61 [36992/50000]\tLoss: 4.7161\tLR: 6.073657\n",
      "Training Epoch: 61 [37120/50000]\tLoss: 4.7473\tLR: 6.073913\n",
      "Training Epoch: 61 [37248/50000]\tLoss: 4.7187\tLR: 6.074169\n",
      "Training Epoch: 61 [37376/50000]\tLoss: 4.6701\tLR: 6.074425\n",
      "Training Epoch: 61 [37504/50000]\tLoss: 4.7309\tLR: 6.074680\n",
      "Training Epoch: 61 [37632/50000]\tLoss: 4.6399\tLR: 6.074936\n",
      "Training Epoch: 61 [37760/50000]\tLoss: 4.7478\tLR: 6.075192\n",
      "Training Epoch: 61 [37888/50000]\tLoss: 4.6672\tLR: 6.075448\n",
      "Training Epoch: 61 [38016/50000]\tLoss: 4.7958\tLR: 6.075703\n",
      "Training Epoch: 61 [38144/50000]\tLoss: 4.6404\tLR: 6.075959\n",
      "Training Epoch: 61 [38272/50000]\tLoss: 4.6668\tLR: 6.076215\n",
      "Training Epoch: 61 [38400/50000]\tLoss: 4.7389\tLR: 6.076471\n",
      "Training Epoch: 61 [38528/50000]\tLoss: 4.8312\tLR: 6.076726\n",
      "Training Epoch: 61 [38656/50000]\tLoss: 4.6923\tLR: 6.076982\n",
      "Training Epoch: 61 [38784/50000]\tLoss: 4.7621\tLR: 6.077238\n",
      "Training Epoch: 61 [38912/50000]\tLoss: 4.7145\tLR: 6.077494\n",
      "Training Epoch: 61 [39040/50000]\tLoss: 4.7408\tLR: 6.077749\n",
      "Training Epoch: 61 [39168/50000]\tLoss: 4.7631\tLR: 6.078005\n",
      "Training Epoch: 61 [39296/50000]\tLoss: 4.7115\tLR: 6.078261\n",
      "Training Epoch: 61 [39424/50000]\tLoss: 4.6530\tLR: 6.078517\n",
      "Training Epoch: 61 [39552/50000]\tLoss: 4.6834\tLR: 6.078772\n",
      "Training Epoch: 61 [39680/50000]\tLoss: 4.7287\tLR: 6.079028\n",
      "Training Epoch: 61 [39808/50000]\tLoss: 4.6603\tLR: 6.079284\n",
      "Training Epoch: 61 [39936/50000]\tLoss: 4.7728\tLR: 6.079540\n",
      "Training Epoch: 61 [40064/50000]\tLoss: 4.6300\tLR: 6.079795\n",
      "Training Epoch: 61 [40192/50000]\tLoss: 4.7255\tLR: 6.080051\n",
      "Training Epoch: 61 [40320/50000]\tLoss: 4.7847\tLR: 6.080307\n",
      "Training Epoch: 61 [40448/50000]\tLoss: 4.7555\tLR: 6.080563\n",
      "Training Epoch: 61 [40576/50000]\tLoss: 4.7045\tLR: 6.080818\n",
      "Training Epoch: 61 [40704/50000]\tLoss: 4.7005\tLR: 6.081074\n",
      "Training Epoch: 61 [40832/50000]\tLoss: 4.7111\tLR: 6.081330\n",
      "Training Epoch: 61 [40960/50000]\tLoss: 4.6945\tLR: 6.081586\n",
      "Training Epoch: 61 [41088/50000]\tLoss: 4.7120\tLR: 6.081841\n",
      "Training Epoch: 61 [41216/50000]\tLoss: 4.7262\tLR: 6.082097\n",
      "Training Epoch: 61 [41344/50000]\tLoss: 4.6857\tLR: 6.082353\n",
      "Training Epoch: 61 [41472/50000]\tLoss: 4.7943\tLR: 6.082609\n",
      "Training Epoch: 61 [41600/50000]\tLoss: 4.7198\tLR: 6.082864\n",
      "Training Epoch: 61 [41728/50000]\tLoss: 4.5935\tLR: 6.083120\n",
      "Training Epoch: 61 [41856/50000]\tLoss: 4.7395\tLR: 6.083376\n",
      "Training Epoch: 61 [41984/50000]\tLoss: 4.7611\tLR: 6.083632\n",
      "Training Epoch: 61 [42112/50000]\tLoss: 4.7609\tLR: 6.083887\n",
      "Training Epoch: 61 [42240/50000]\tLoss: 4.6639\tLR: 6.084143\n",
      "Training Epoch: 61 [42368/50000]\tLoss: 4.7151\tLR: 6.084399\n",
      "Training Epoch: 61 [42496/50000]\tLoss: 4.6886\tLR: 6.084655\n",
      "Training Epoch: 61 [42624/50000]\tLoss: 4.7144\tLR: 6.084910\n",
      "Training Epoch: 61 [42752/50000]\tLoss: 4.7615\tLR: 6.085166\n",
      "Training Epoch: 61 [42880/50000]\tLoss: 4.7259\tLR: 6.085422\n",
      "Training Epoch: 61 [43008/50000]\tLoss: 4.7321\tLR: 6.085678\n",
      "Training Epoch: 61 [43136/50000]\tLoss: 4.7515\tLR: 6.085934\n",
      "Training Epoch: 61 [43264/50000]\tLoss: 4.6319\tLR: 6.086189\n",
      "Training Epoch: 61 [43392/50000]\tLoss: 4.6945\tLR: 6.086445\n",
      "Training Epoch: 61 [43520/50000]\tLoss: 4.7730\tLR: 6.086701\n",
      "Training Epoch: 61 [43648/50000]\tLoss: 4.7576\tLR: 6.086957\n",
      "Training Epoch: 61 [43776/50000]\tLoss: 4.6371\tLR: 6.087212\n",
      "Training Epoch: 61 [43904/50000]\tLoss: 4.7737\tLR: 6.087468\n",
      "Training Epoch: 61 [44032/50000]\tLoss: 4.8008\tLR: 6.087724\n",
      "Training Epoch: 61 [44160/50000]\tLoss: 4.7218\tLR: 6.087980\n",
      "Training Epoch: 61 [44288/50000]\tLoss: 4.6196\tLR: 6.088235\n",
      "Training Epoch: 61 [44416/50000]\tLoss: 4.7242\tLR: 6.088491\n",
      "Training Epoch: 61 [44544/50000]\tLoss: 4.6834\tLR: 6.088747\n",
      "Training Epoch: 61 [44672/50000]\tLoss: 4.7130\tLR: 6.089003\n",
      "Training Epoch: 61 [44800/50000]\tLoss: 4.7748\tLR: 6.089258\n",
      "Training Epoch: 61 [44928/50000]\tLoss: 4.6548\tLR: 6.089514\n",
      "Training Epoch: 61 [45056/50000]\tLoss: 4.6964\tLR: 6.089770\n",
      "Training Epoch: 61 [45184/50000]\tLoss: 4.6964\tLR: 6.090026\n",
      "Training Epoch: 61 [45312/50000]\tLoss: 4.7152\tLR: 6.090281\n",
      "Training Epoch: 61 [45440/50000]\tLoss: 4.7311\tLR: 6.090537\n",
      "Training Epoch: 61 [45568/50000]\tLoss: 4.6939\tLR: 6.090793\n",
      "Training Epoch: 61 [45696/50000]\tLoss: 4.7038\tLR: 6.091049\n",
      "Training Epoch: 61 [45824/50000]\tLoss: 4.7474\tLR: 6.091304\n",
      "Training Epoch: 61 [45952/50000]\tLoss: 4.6756\tLR: 6.091560\n",
      "Training Epoch: 61 [46080/50000]\tLoss: 4.7364\tLR: 6.091816\n",
      "Training Epoch: 61 [46208/50000]\tLoss: 4.7216\tLR: 6.092072\n",
      "Training Epoch: 61 [46336/50000]\tLoss: 4.6856\tLR: 6.092327\n",
      "Training Epoch: 61 [46464/50000]\tLoss: 4.7097\tLR: 6.092583\n",
      "Training Epoch: 61 [46592/50000]\tLoss: 4.7483\tLR: 6.092839\n",
      "Training Epoch: 61 [46720/50000]\tLoss: 4.6959\tLR: 6.093095\n",
      "Training Epoch: 61 [46848/50000]\tLoss: 4.6910\tLR: 6.093350\n",
      "Training Epoch: 61 [46976/50000]\tLoss: 4.7020\tLR: 6.093606\n",
      "Training Epoch: 61 [47104/50000]\tLoss: 4.6360\tLR: 6.093862\n",
      "Training Epoch: 61 [47232/50000]\tLoss: 4.7369\tLR: 6.094118\n",
      "Training Epoch: 61 [47360/50000]\tLoss: 4.7227\tLR: 6.094373\n",
      "Training Epoch: 61 [47488/50000]\tLoss: 4.8056\tLR: 6.094629\n",
      "Training Epoch: 61 [47616/50000]\tLoss: 4.6659\tLR: 6.094885\n",
      "Training Epoch: 61 [47744/50000]\tLoss: 4.7897\tLR: 6.095141\n",
      "Training Epoch: 61 [47872/50000]\tLoss: 4.7267\tLR: 6.095396\n",
      "Training Epoch: 61 [48000/50000]\tLoss: 4.7095\tLR: 6.095652\n",
      "Training Epoch: 61 [48128/50000]\tLoss: 4.7239\tLR: 6.095908\n",
      "Training Epoch: 61 [48256/50000]\tLoss: 4.6997\tLR: 6.096164\n",
      "Training Epoch: 61 [48384/50000]\tLoss: 4.7865\tLR: 6.096419\n",
      "Training Epoch: 61 [48512/50000]\tLoss: 4.7970\tLR: 6.096675\n",
      "Training Epoch: 61 [48640/50000]\tLoss: 4.6511\tLR: 6.096931\n",
      "Training Epoch: 61 [48768/50000]\tLoss: 4.7478\tLR: 6.097187\n",
      "Training Epoch: 61 [48896/50000]\tLoss: 4.6271\tLR: 6.097442\n",
      "Training Epoch: 61 [49024/50000]\tLoss: 4.7663\tLR: 6.097698\n",
      "Training Epoch: 61 [49152/50000]\tLoss: 4.7528\tLR: 6.097954\n",
      "Training Epoch: 61 [49280/50000]\tLoss: 4.8163\tLR: 6.098210\n",
      "Training Epoch: 61 [49408/50000]\tLoss: 4.6257\tLR: 6.098465\n",
      "Training Epoch: 61 [49536/50000]\tLoss: 4.7222\tLR: 6.098721\n",
      "Training Epoch: 61 [49664/50000]\tLoss: 4.7089\tLR: 6.098977\n",
      "Training Epoch: 61 [49792/50000]\tLoss: 4.7565\tLR: 6.099233\n",
      "Training Epoch: 61 [49920/50000]\tLoss: 4.6462\tLR: 6.099488\n",
      "Training Epoch: 61 [50000/50000]\tLoss: 4.6529\tLR: 6.099744\n",
      "epoch 61 training time consumed: 489.08s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   85518 GB |   85518 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   85255 GB |   85255 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     262 GB |     262 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   85518 GB |   85518 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   85255 GB |   85255 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     262 GB |     262 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   84315 GB |   84315 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   84053 GB |   84053 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     262 GB |     262 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    9068 K  |    9067 K  |\n",
      "|       from large pool |      24    |      65    |    3865 K  |    3865 K  |\n",
      "|       from small pool |     231    |     274    |    5202 K  |    5202 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    9068 K  |    9067 K  |\n",
      "|       from large pool |      24    |      65    |    3865 K  |    3865 K  |\n",
      "|       from small pool |     231    |     274    |    5202 K  |    5202 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      47    |    5256 K  |    5256 K  |\n",
      "|       from large pool |      10    |      23    |    1858 K  |    1858 K  |\n",
      "|       from small pool |      25    |      35    |    3398 K  |    3398 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 61, Average loss: 0.0375, Accuracy: 0.0100, Time consumed:31.16s\n",
      "\n",
      "Training Epoch: 62 [128/50000]\tLoss: 4.7327\tLR: 0.020000\n",
      "Training Epoch: 62 [256/50000]\tLoss: 4.6524\tLR: 6.100256\n",
      "Training Epoch: 62 [384/50000]\tLoss: 4.6873\tLR: 6.100512\n",
      "Training Epoch: 62 [512/50000]\tLoss: 4.6901\tLR: 6.100767\n",
      "Training Epoch: 62 [640/50000]\tLoss: 4.7104\tLR: 6.101023\n",
      "Training Epoch: 62 [768/50000]\tLoss: 4.7169\tLR: 6.101279\n",
      "Training Epoch: 62 [896/50000]\tLoss: 4.7505\tLR: 6.101535\n",
      "Training Epoch: 62 [1024/50000]\tLoss: 4.6607\tLR: 6.101790\n",
      "Training Epoch: 62 [1152/50000]\tLoss: 4.7586\tLR: 6.102046\n",
      "Training Epoch: 62 [1280/50000]\tLoss: 4.6609\tLR: 6.102302\n",
      "Training Epoch: 62 [1408/50000]\tLoss: 4.7704\tLR: 6.102558\n",
      "Training Epoch: 62 [1536/50000]\tLoss: 4.7711\tLR: 6.102813\n",
      "Training Epoch: 62 [1664/50000]\tLoss: 4.6949\tLR: 6.103069\n",
      "Training Epoch: 62 [1792/50000]\tLoss: 4.7265\tLR: 6.103325\n",
      "Training Epoch: 62 [1920/50000]\tLoss: 4.7166\tLR: 6.103581\n",
      "Training Epoch: 62 [2048/50000]\tLoss: 4.7120\tLR: 6.103836\n",
      "Training Epoch: 62 [2176/50000]\tLoss: 4.6828\tLR: 6.104092\n",
      "Training Epoch: 62 [2304/50000]\tLoss: 4.6714\tLR: 6.104348\n",
      "Training Epoch: 62 [2432/50000]\tLoss: 4.6935\tLR: 6.104604\n",
      "Training Epoch: 62 [2560/50000]\tLoss: 4.6880\tLR: 6.104859\n",
      "Training Epoch: 62 [2688/50000]\tLoss: 4.7302\tLR: 6.105115\n",
      "Training Epoch: 62 [2816/50000]\tLoss: 4.7248\tLR: 6.105371\n",
      "Training Epoch: 62 [2944/50000]\tLoss: 4.7467\tLR: 6.105627\n",
      "Training Epoch: 62 [3072/50000]\tLoss: 4.7611\tLR: 6.105882\n",
      "Training Epoch: 62 [3200/50000]\tLoss: 4.7836\tLR: 6.106138\n",
      "Training Epoch: 62 [3328/50000]\tLoss: 4.7698\tLR: 6.106394\n",
      "Training Epoch: 62 [3456/50000]\tLoss: 4.6487\tLR: 6.106650\n",
      "Training Epoch: 62 [3584/50000]\tLoss: 4.7358\tLR: 6.106905\n",
      "Training Epoch: 62 [3712/50000]\tLoss: 4.7319\tLR: 6.107161\n",
      "Training Epoch: 62 [3840/50000]\tLoss: 4.7082\tLR: 6.107417\n",
      "Training Epoch: 62 [3968/50000]\tLoss: 4.7210\tLR: 6.107673\n",
      "Training Epoch: 62 [4096/50000]\tLoss: 4.7144\tLR: 6.107928\n",
      "Training Epoch: 62 [4224/50000]\tLoss: 4.6415\tLR: 6.108184\n",
      "Training Epoch: 62 [4352/50000]\tLoss: 4.7866\tLR: 6.108440\n",
      "Training Epoch: 62 [4480/50000]\tLoss: 4.7203\tLR: 6.108696\n",
      "Training Epoch: 62 [4608/50000]\tLoss: 4.6206\tLR: 6.108951\n",
      "Training Epoch: 62 [4736/50000]\tLoss: 4.7279\tLR: 6.109207\n",
      "Training Epoch: 62 [4864/50000]\tLoss: 4.7287\tLR: 6.109463\n",
      "Training Epoch: 62 [4992/50000]\tLoss: 4.7368\tLR: 6.109719\n",
      "Training Epoch: 62 [5120/50000]\tLoss: 4.6759\tLR: 6.109974\n",
      "Training Epoch: 62 [5248/50000]\tLoss: 4.7880\tLR: 6.110230\n",
      "Training Epoch: 62 [5376/50000]\tLoss: 4.7235\tLR: 6.110486\n",
      "Training Epoch: 62 [5504/50000]\tLoss: 4.6828\tLR: 6.110742\n",
      "Training Epoch: 62 [5632/50000]\tLoss: 4.6870\tLR: 6.110997\n",
      "Training Epoch: 62 [5760/50000]\tLoss: 4.7385\tLR: 6.111253\n",
      "Training Epoch: 62 [5888/50000]\tLoss: 4.7516\tLR: 6.111509\n",
      "Training Epoch: 62 [6016/50000]\tLoss: 4.8159\tLR: 6.111765\n",
      "Training Epoch: 62 [6144/50000]\tLoss: 4.7162\tLR: 6.112020\n",
      "Training Epoch: 62 [6272/50000]\tLoss: 4.6849\tLR: 6.112276\n",
      "Training Epoch: 62 [6400/50000]\tLoss: 4.7229\tLR: 6.112532\n",
      "Training Epoch: 62 [6528/50000]\tLoss: 4.7434\tLR: 6.112788\n",
      "Training Epoch: 62 [6656/50000]\tLoss: 4.6969\tLR: 6.113043\n",
      "Training Epoch: 62 [6784/50000]\tLoss: 4.7638\tLR: 6.113299\n",
      "Training Epoch: 62 [6912/50000]\tLoss: 4.7756\tLR: 6.113555\n",
      "Training Epoch: 62 [7040/50000]\tLoss: 4.7656\tLR: 6.113811\n",
      "Training Epoch: 62 [7168/50000]\tLoss: 4.7738\tLR: 6.114066\n",
      "Training Epoch: 62 [7296/50000]\tLoss: 4.7685\tLR: 6.114322\n",
      "Training Epoch: 62 [7424/50000]\tLoss: 4.7313\tLR: 6.114578\n",
      "Training Epoch: 62 [7552/50000]\tLoss: 4.6880\tLR: 6.114834\n",
      "Training Epoch: 62 [7680/50000]\tLoss: 4.7305\tLR: 6.115090\n",
      "Training Epoch: 62 [7808/50000]\tLoss: 4.7135\tLR: 6.115345\n",
      "Training Epoch: 62 [7936/50000]\tLoss: 4.7733\tLR: 6.115601\n",
      "Training Epoch: 62 [8064/50000]\tLoss: 4.7065\tLR: 6.115857\n",
      "Training Epoch: 62 [8192/50000]\tLoss: 4.7362\tLR: 6.116113\n",
      "Training Epoch: 62 [8320/50000]\tLoss: 4.7162\tLR: 6.116368\n",
      "Training Epoch: 62 [8448/50000]\tLoss: 4.6887\tLR: 6.116624\n",
      "Training Epoch: 62 [8576/50000]\tLoss: 4.6432\tLR: 6.116880\n",
      "Training Epoch: 62 [8704/50000]\tLoss: 4.7305\tLR: 6.117136\n",
      "Training Epoch: 62 [8832/50000]\tLoss: 4.6617\tLR: 6.117391\n",
      "Training Epoch: 62 [8960/50000]\tLoss: 4.7218\tLR: 6.117647\n",
      "Training Epoch: 62 [9088/50000]\tLoss: 4.7487\tLR: 6.117903\n",
      "Training Epoch: 62 [9216/50000]\tLoss: 4.7079\tLR: 6.118159\n",
      "Training Epoch: 62 [9344/50000]\tLoss: 4.6974\tLR: 6.118414\n",
      "Training Epoch: 62 [9472/50000]\tLoss: 4.6673\tLR: 6.118670\n",
      "Training Epoch: 62 [9600/50000]\tLoss: 4.6715\tLR: 6.118926\n",
      "Training Epoch: 62 [9728/50000]\tLoss: 4.7955\tLR: 6.119182\n",
      "Training Epoch: 62 [9856/50000]\tLoss: 4.7165\tLR: 6.119437\n",
      "Training Epoch: 62 [9984/50000]\tLoss: 4.6784\tLR: 6.119693\n",
      "Training Epoch: 62 [10112/50000]\tLoss: 4.6866\tLR: 6.119949\n",
      "Training Epoch: 62 [10240/50000]\tLoss: 4.6381\tLR: 6.120205\n",
      "Training Epoch: 62 [10368/50000]\tLoss: 4.7263\tLR: 6.120460\n",
      "Training Epoch: 62 [10496/50000]\tLoss: 4.7264\tLR: 6.120716\n",
      "Training Epoch: 62 [10624/50000]\tLoss: 4.6924\tLR: 6.120972\n",
      "Training Epoch: 62 [10752/50000]\tLoss: 4.7028\tLR: 6.121228\n",
      "Training Epoch: 62 [10880/50000]\tLoss: 4.6680\tLR: 6.121483\n",
      "Training Epoch: 62 [11008/50000]\tLoss: 4.6844\tLR: 6.121739\n",
      "Training Epoch: 62 [11136/50000]\tLoss: 4.6360\tLR: 6.121995\n",
      "Training Epoch: 62 [11264/50000]\tLoss: 4.7013\tLR: 6.122251\n",
      "Training Epoch: 62 [11392/50000]\tLoss: 4.7209\tLR: 6.122506\n",
      "Training Epoch: 62 [11520/50000]\tLoss: 4.7430\tLR: 6.122762\n",
      "Training Epoch: 62 [11648/50000]\tLoss: 4.7681\tLR: 6.123018\n",
      "Training Epoch: 62 [11776/50000]\tLoss: 4.6897\tLR: 6.123274\n",
      "Training Epoch: 62 [11904/50000]\tLoss: 4.7192\tLR: 6.123529\n",
      "Training Epoch: 62 [12032/50000]\tLoss: 4.7471\tLR: 6.123785\n",
      "Training Epoch: 62 [12160/50000]\tLoss: 4.7720\tLR: 6.124041\n",
      "Training Epoch: 62 [12288/50000]\tLoss: 4.7719\tLR: 6.124297\n",
      "Training Epoch: 62 [12416/50000]\tLoss: 4.6989\tLR: 6.124552\n",
      "Training Epoch: 62 [12544/50000]\tLoss: 4.6779\tLR: 6.124808\n",
      "Training Epoch: 62 [12672/50000]\tLoss: 4.6821\tLR: 6.125064\n",
      "Training Epoch: 62 [12800/50000]\tLoss: 4.6333\tLR: 6.125320\n",
      "Training Epoch: 62 [12928/50000]\tLoss: 4.7113\tLR: 6.125575\n",
      "Training Epoch: 62 [13056/50000]\tLoss: 4.6470\tLR: 6.125831\n",
      "Training Epoch: 62 [13184/50000]\tLoss: 4.7415\tLR: 6.126087\n",
      "Training Epoch: 62 [13312/50000]\tLoss: 4.6948\tLR: 6.126343\n",
      "Training Epoch: 62 [13440/50000]\tLoss: 4.7525\tLR: 6.126598\n",
      "Training Epoch: 62 [13568/50000]\tLoss: 4.7055\tLR: 6.126854\n",
      "Training Epoch: 62 [13696/50000]\tLoss: 4.7098\tLR: 6.127110\n",
      "Training Epoch: 62 [13824/50000]\tLoss: 4.7625\tLR: 6.127366\n",
      "Training Epoch: 62 [13952/50000]\tLoss: 4.7764\tLR: 6.127621\n",
      "Training Epoch: 62 [14080/50000]\tLoss: 4.7695\tLR: 6.127877\n",
      "Training Epoch: 62 [14208/50000]\tLoss: 4.7733\tLR: 6.128133\n",
      "Training Epoch: 62 [14336/50000]\tLoss: 4.6823\tLR: 6.128389\n",
      "Training Epoch: 62 [14464/50000]\tLoss: 4.6678\tLR: 6.128645\n",
      "Training Epoch: 62 [14592/50000]\tLoss: 4.6762\tLR: 6.128900\n",
      "Training Epoch: 62 [14720/50000]\tLoss: 4.6464\tLR: 6.129156\n",
      "Training Epoch: 62 [14848/50000]\tLoss: 4.6476\tLR: 6.129412\n",
      "Training Epoch: 62 [14976/50000]\tLoss: 4.7698\tLR: 6.129668\n",
      "Training Epoch: 62 [15104/50000]\tLoss: 4.7484\tLR: 6.129923\n",
      "Training Epoch: 62 [15232/50000]\tLoss: 4.7287\tLR: 6.130179\n",
      "Training Epoch: 62 [15360/50000]\tLoss: 4.7678\tLR: 6.130435\n",
      "Training Epoch: 62 [15488/50000]\tLoss: 4.6731\tLR: 6.130691\n",
      "Training Epoch: 62 [15616/50000]\tLoss: 4.7104\tLR: 6.130946\n",
      "Training Epoch: 62 [15744/50000]\tLoss: 4.7888\tLR: 6.131202\n",
      "Training Epoch: 62 [15872/50000]\tLoss: 4.7784\tLR: 6.131458\n",
      "Training Epoch: 62 [16000/50000]\tLoss: 4.7224\tLR: 6.131714\n",
      "Training Epoch: 62 [16128/50000]\tLoss: 4.6909\tLR: 6.131969\n",
      "Training Epoch: 62 [16256/50000]\tLoss: 4.7530\tLR: 6.132225\n",
      "Training Epoch: 62 [16384/50000]\tLoss: 4.7131\tLR: 6.132481\n",
      "Training Epoch: 62 [16512/50000]\tLoss: 4.7649\tLR: 6.132737\n",
      "Training Epoch: 62 [16640/50000]\tLoss: 4.6492\tLR: 6.132992\n",
      "Training Epoch: 62 [16768/50000]\tLoss: 4.7745\tLR: 6.133248\n",
      "Training Epoch: 62 [16896/50000]\tLoss: 4.7594\tLR: 6.133504\n",
      "Training Epoch: 62 [17024/50000]\tLoss: 4.7521\tLR: 6.133760\n",
      "Training Epoch: 62 [17152/50000]\tLoss: 4.7719\tLR: 6.134015\n",
      "Training Epoch: 62 [17280/50000]\tLoss: 4.7292\tLR: 6.134271\n",
      "Training Epoch: 62 [17408/50000]\tLoss: 4.7746\tLR: 6.134527\n",
      "Training Epoch: 62 [17536/50000]\tLoss: 4.7017\tLR: 6.134783\n",
      "Training Epoch: 62 [17664/50000]\tLoss: 4.7773\tLR: 6.135038\n",
      "Training Epoch: 62 [17792/50000]\tLoss: 4.7828\tLR: 6.135294\n",
      "Training Epoch: 62 [17920/50000]\tLoss: 4.6306\tLR: 6.135550\n",
      "Training Epoch: 62 [18048/50000]\tLoss: 4.7996\tLR: 6.135806\n",
      "Training Epoch: 62 [18176/50000]\tLoss: 4.7292\tLR: 6.136061\n",
      "Training Epoch: 62 [18304/50000]\tLoss: 4.7751\tLR: 6.136317\n",
      "Training Epoch: 62 [18432/50000]\tLoss: 4.7496\tLR: 6.136573\n",
      "Training Epoch: 62 [18560/50000]\tLoss: 4.7267\tLR: 6.136829\n",
      "Training Epoch: 62 [18688/50000]\tLoss: 4.7342\tLR: 6.137084\n",
      "Training Epoch: 62 [18816/50000]\tLoss: 4.6873\tLR: 6.137340\n",
      "Training Epoch: 62 [18944/50000]\tLoss: 4.6368\tLR: 6.137596\n",
      "Training Epoch: 62 [19072/50000]\tLoss: 4.6859\tLR: 6.137852\n",
      "Training Epoch: 62 [19200/50000]\tLoss: 4.7277\tLR: 6.138107\n",
      "Training Epoch: 62 [19328/50000]\tLoss: 4.7363\tLR: 6.138363\n",
      "Training Epoch: 62 [19456/50000]\tLoss: 4.7568\tLR: 6.138619\n",
      "Training Epoch: 62 [19584/50000]\tLoss: 4.7448\tLR: 6.138875\n",
      "Training Epoch: 62 [19712/50000]\tLoss: 4.6559\tLR: 6.139130\n",
      "Training Epoch: 62 [19840/50000]\tLoss: 4.7081\tLR: 6.139386\n",
      "Training Epoch: 62 [19968/50000]\tLoss: 4.7593\tLR: 6.139642\n",
      "Training Epoch: 62 [20096/50000]\tLoss: 4.6754\tLR: 6.139898\n",
      "Training Epoch: 62 [20224/50000]\tLoss: 4.7111\tLR: 6.140153\n",
      "Training Epoch: 62 [20352/50000]\tLoss: 4.7234\tLR: 6.140409\n",
      "Training Epoch: 62 [20480/50000]\tLoss: 4.7492\tLR: 6.140665\n",
      "Training Epoch: 62 [20608/50000]\tLoss: 4.7101\tLR: 6.140921\n",
      "Training Epoch: 62 [20736/50000]\tLoss: 4.8302\tLR: 6.141176\n",
      "Training Epoch: 62 [20864/50000]\tLoss: 4.6476\tLR: 6.141432\n",
      "Training Epoch: 62 [20992/50000]\tLoss: 4.7754\tLR: 6.141688\n",
      "Training Epoch: 62 [21120/50000]\tLoss: 4.8324\tLR: 6.141944\n",
      "Training Epoch: 62 [21248/50000]\tLoss: 4.7690\tLR: 6.142199\n",
      "Training Epoch: 62 [21376/50000]\tLoss: 4.7072\tLR: 6.142455\n",
      "Training Epoch: 62 [21504/50000]\tLoss: 4.6642\tLR: 6.142711\n",
      "Training Epoch: 62 [21632/50000]\tLoss: 4.7716\tLR: 6.142967\n",
      "Training Epoch: 62 [21760/50000]\tLoss: 4.7115\tLR: 6.143223\n",
      "Training Epoch: 62 [21888/50000]\tLoss: 4.7027\tLR: 6.143478\n",
      "Training Epoch: 62 [22016/50000]\tLoss: 4.7237\tLR: 6.143734\n",
      "Training Epoch: 62 [22144/50000]\tLoss: 4.7872\tLR: 6.143990\n",
      "Training Epoch: 62 [22272/50000]\tLoss: 4.8440\tLR: 6.144246\n",
      "Training Epoch: 62 [22400/50000]\tLoss: 4.7593\tLR: 6.144501\n",
      "Training Epoch: 62 [22528/50000]\tLoss: 4.6935\tLR: 6.144757\n",
      "Training Epoch: 62 [22656/50000]\tLoss: 4.7102\tLR: 6.145013\n",
      "Training Epoch: 62 [22784/50000]\tLoss: 4.6705\tLR: 6.145269\n",
      "Training Epoch: 62 [22912/50000]\tLoss: 4.7156\tLR: 6.145524\n",
      "Training Epoch: 62 [23040/50000]\tLoss: 4.8092\tLR: 6.145780\n",
      "Training Epoch: 62 [23168/50000]\tLoss: 4.7259\tLR: 6.146036\n",
      "Training Epoch: 62 [23296/50000]\tLoss: 4.7378\tLR: 6.146292\n",
      "Training Epoch: 62 [23424/50000]\tLoss: 4.6724\tLR: 6.146547\n",
      "Training Epoch: 62 [23552/50000]\tLoss: 4.7496\tLR: 6.146803\n",
      "Training Epoch: 62 [23680/50000]\tLoss: 4.7271\tLR: 6.147059\n",
      "Training Epoch: 62 [23808/50000]\tLoss: 4.7761\tLR: 6.147315\n",
      "Training Epoch: 62 [23936/50000]\tLoss: 4.7402\tLR: 6.147570\n",
      "Training Epoch: 62 [24064/50000]\tLoss: 4.7558\tLR: 6.147826\n",
      "Training Epoch: 62 [24192/50000]\tLoss: 4.7183\tLR: 6.148082\n",
      "Training Epoch: 62 [24320/50000]\tLoss: 4.8199\tLR: 6.148338\n",
      "Training Epoch: 62 [24448/50000]\tLoss: 4.6683\tLR: 6.148593\n",
      "Training Epoch: 62 [24576/50000]\tLoss: 4.7275\tLR: 6.148849\n",
      "Training Epoch: 62 [24704/50000]\tLoss: 4.7137\tLR: 6.149105\n",
      "Training Epoch: 62 [24832/50000]\tLoss: 4.7484\tLR: 6.149361\n",
      "Training Epoch: 62 [24960/50000]\tLoss: 4.7104\tLR: 6.149616\n",
      "Training Epoch: 62 [25088/50000]\tLoss: 4.6948\tLR: 6.149872\n",
      "Training Epoch: 62 [25216/50000]\tLoss: 4.7758\tLR: 6.150128\n",
      "Training Epoch: 62 [25344/50000]\tLoss: 4.7721\tLR: 6.150384\n",
      "Training Epoch: 62 [25472/50000]\tLoss: 4.6928\tLR: 6.150639\n",
      "Training Epoch: 62 [25600/50000]\tLoss: 4.6699\tLR: 6.150895\n",
      "Training Epoch: 62 [25728/50000]\tLoss: 4.7279\tLR: 6.151151\n",
      "Training Epoch: 62 [25856/50000]\tLoss: 4.6196\tLR: 6.151407\n",
      "Training Epoch: 62 [25984/50000]\tLoss: 4.7217\tLR: 6.151662\n",
      "Training Epoch: 62 [26112/50000]\tLoss: 4.7467\tLR: 6.151918\n",
      "Training Epoch: 62 [26240/50000]\tLoss: 4.6157\tLR: 6.152174\n",
      "Training Epoch: 62 [26368/50000]\tLoss: 4.6972\tLR: 6.152430\n",
      "Training Epoch: 62 [26496/50000]\tLoss: 4.7150\tLR: 6.152685\n",
      "Training Epoch: 62 [26624/50000]\tLoss: 4.7623\tLR: 6.152941\n",
      "Training Epoch: 62 [26752/50000]\tLoss: 4.6707\tLR: 6.153197\n",
      "Training Epoch: 62 [26880/50000]\tLoss: 4.7114\tLR: 6.153453\n",
      "Training Epoch: 62 [27008/50000]\tLoss: 4.7116\tLR: 6.153708\n",
      "Training Epoch: 62 [27136/50000]\tLoss: 4.7355\tLR: 6.153964\n",
      "Training Epoch: 62 [27264/50000]\tLoss: 4.6893\tLR: 6.154220\n",
      "Training Epoch: 62 [27392/50000]\tLoss: 4.7006\tLR: 6.154476\n",
      "Training Epoch: 62 [27520/50000]\tLoss: 4.6484\tLR: 6.154731\n",
      "Training Epoch: 62 [27648/50000]\tLoss: 4.6344\tLR: 6.154987\n",
      "Training Epoch: 62 [27776/50000]\tLoss: 4.7177\tLR: 6.155243\n",
      "Training Epoch: 62 [27904/50000]\tLoss: 4.7921\tLR: 6.155499\n",
      "Training Epoch: 62 [28032/50000]\tLoss: 4.7354\tLR: 6.155754\n",
      "Training Epoch: 62 [28160/50000]\tLoss: 4.6992\tLR: 6.156010\n",
      "Training Epoch: 62 [28288/50000]\tLoss: 4.7410\tLR: 6.156266\n",
      "Training Epoch: 62 [28416/50000]\tLoss: 4.6780\tLR: 6.156522\n",
      "Training Epoch: 62 [28544/50000]\tLoss: 4.7194\tLR: 6.156777\n",
      "Training Epoch: 62 [28672/50000]\tLoss: 4.6761\tLR: 6.157033\n",
      "Training Epoch: 62 [28800/50000]\tLoss: 4.7602\tLR: 6.157289\n",
      "Training Epoch: 62 [28928/50000]\tLoss: 4.7624\tLR: 6.157545\n",
      "Training Epoch: 62 [29056/50000]\tLoss: 4.7126\tLR: 6.157801\n",
      "Training Epoch: 62 [29184/50000]\tLoss: 4.7186\tLR: 6.158056\n",
      "Training Epoch: 62 [29312/50000]\tLoss: 4.7298\tLR: 6.158312\n",
      "Training Epoch: 62 [29440/50000]\tLoss: 4.6674\tLR: 6.158568\n",
      "Training Epoch: 62 [29568/50000]\tLoss: 4.6680\tLR: 6.158824\n",
      "Training Epoch: 62 [29696/50000]\tLoss: 4.6937\tLR: 6.159079\n",
      "Training Epoch: 62 [29824/50000]\tLoss: 4.6993\tLR: 6.159335\n",
      "Training Epoch: 62 [29952/50000]\tLoss: 4.6996\tLR: 6.159591\n",
      "Training Epoch: 62 [30080/50000]\tLoss: 4.6704\tLR: 6.159847\n",
      "Training Epoch: 62 [30208/50000]\tLoss: 4.8043\tLR: 6.160102\n",
      "Training Epoch: 62 [30336/50000]\tLoss: 4.8069\tLR: 6.160358\n",
      "Training Epoch: 62 [30464/50000]\tLoss: 4.8073\tLR: 6.160614\n",
      "Training Epoch: 62 [30592/50000]\tLoss: 4.7065\tLR: 6.160870\n",
      "Training Epoch: 62 [30720/50000]\tLoss: 4.7018\tLR: 6.161125\n",
      "Training Epoch: 62 [30848/50000]\tLoss: 4.6672\tLR: 6.161381\n",
      "Training Epoch: 62 [30976/50000]\tLoss: 4.7486\tLR: 6.161637\n",
      "Training Epoch: 62 [31104/50000]\tLoss: 4.6625\tLR: 6.161893\n",
      "Training Epoch: 62 [31232/50000]\tLoss: 4.7304\tLR: 6.162148\n",
      "Training Epoch: 62 [31360/50000]\tLoss: 4.7156\tLR: 6.162404\n",
      "Training Epoch: 62 [31488/50000]\tLoss: 4.7121\tLR: 6.162660\n",
      "Training Epoch: 62 [31616/50000]\tLoss: 4.8134\tLR: 6.162916\n",
      "Training Epoch: 62 [31744/50000]\tLoss: 4.7431\tLR: 6.163171\n",
      "Training Epoch: 62 [31872/50000]\tLoss: 4.7939\tLR: 6.163427\n",
      "Training Epoch: 62 [32000/50000]\tLoss: 4.7180\tLR: 6.163683\n",
      "Training Epoch: 62 [32128/50000]\tLoss: 4.6805\tLR: 6.163939\n",
      "Training Epoch: 62 [32256/50000]\tLoss: 4.6476\tLR: 6.164194\n",
      "Training Epoch: 62 [32384/50000]\tLoss: 4.7459\tLR: 6.164450\n",
      "Training Epoch: 62 [32512/50000]\tLoss: 4.7850\tLR: 6.164706\n",
      "Training Epoch: 62 [32640/50000]\tLoss: 4.8236\tLR: 6.164962\n",
      "Training Epoch: 62 [32768/50000]\tLoss: 4.7716\tLR: 6.165217\n",
      "Training Epoch: 62 [32896/50000]\tLoss: 4.8046\tLR: 6.165473\n",
      "Training Epoch: 62 [33024/50000]\tLoss: 4.6389\tLR: 6.165729\n",
      "Training Epoch: 62 [33152/50000]\tLoss: 4.7206\tLR: 6.165985\n",
      "Training Epoch: 62 [33280/50000]\tLoss: 4.7293\tLR: 6.166240\n",
      "Training Epoch: 62 [33408/50000]\tLoss: 4.7697\tLR: 6.166496\n",
      "Training Epoch: 62 [33536/50000]\tLoss: 4.6628\tLR: 6.166752\n",
      "Training Epoch: 62 [33664/50000]\tLoss: 4.8092\tLR: 6.167008\n",
      "Training Epoch: 62 [33792/50000]\tLoss: 4.7149\tLR: 6.167263\n",
      "Training Epoch: 62 [33920/50000]\tLoss: 4.8002\tLR: 6.167519\n",
      "Training Epoch: 62 [34048/50000]\tLoss: 4.7534\tLR: 6.167775\n",
      "Training Epoch: 62 [34176/50000]\tLoss: 4.6089\tLR: 6.168031\n",
      "Training Epoch: 62 [34304/50000]\tLoss: 4.7965\tLR: 6.168286\n",
      "Training Epoch: 62 [34432/50000]\tLoss: 4.6718\tLR: 6.168542\n",
      "Training Epoch: 62 [34560/50000]\tLoss: 4.6612\tLR: 6.168798\n",
      "Training Epoch: 62 [34688/50000]\tLoss: 4.7412\tLR: 6.169054\n",
      "Training Epoch: 62 [34816/50000]\tLoss: 4.7904\tLR: 6.169309\n",
      "Training Epoch: 62 [34944/50000]\tLoss: 4.7201\tLR: 6.169565\n",
      "Training Epoch: 62 [35072/50000]\tLoss: 4.7788\tLR: 6.169821\n",
      "Training Epoch: 62 [35200/50000]\tLoss: 4.7410\tLR: 6.170077\n",
      "Training Epoch: 62 [35328/50000]\tLoss: 4.7935\tLR: 6.170332\n",
      "Training Epoch: 62 [35456/50000]\tLoss: 4.7098\tLR: 6.170588\n",
      "Training Epoch: 62 [35584/50000]\tLoss: 4.7440\tLR: 6.170844\n",
      "Training Epoch: 62 [35712/50000]\tLoss: 4.6913\tLR: 6.171100\n",
      "Training Epoch: 62 [35840/50000]\tLoss: 4.6936\tLR: 6.171355\n",
      "Training Epoch: 62 [35968/50000]\tLoss: 4.7622\tLR: 6.171611\n",
      "Training Epoch: 62 [36096/50000]\tLoss: 4.7313\tLR: 6.171867\n",
      "Training Epoch: 62 [36224/50000]\tLoss: 4.7782\tLR: 6.172123\n",
      "Training Epoch: 62 [36352/50000]\tLoss: 4.7512\tLR: 6.172379\n",
      "Training Epoch: 62 [36480/50000]\tLoss: 4.7047\tLR: 6.172634\n",
      "Training Epoch: 62 [36608/50000]\tLoss: 4.6315\tLR: 6.172890\n",
      "Training Epoch: 62 [36736/50000]\tLoss: 4.6798\tLR: 6.173146\n",
      "Training Epoch: 62 [36864/50000]\tLoss: 4.7892\tLR: 6.173402\n",
      "Training Epoch: 62 [36992/50000]\tLoss: 4.6829\tLR: 6.173657\n",
      "Training Epoch: 62 [37120/50000]\tLoss: 4.7566\tLR: 6.173913\n",
      "Training Epoch: 62 [37248/50000]\tLoss: 4.7453\tLR: 6.174169\n",
      "Training Epoch: 62 [37376/50000]\tLoss: 4.7473\tLR: 6.174425\n",
      "Training Epoch: 62 [37504/50000]\tLoss: 4.7038\tLR: 6.174680\n",
      "Training Epoch: 62 [37632/50000]\tLoss: 4.6694\tLR: 6.174936\n",
      "Training Epoch: 62 [37760/50000]\tLoss: 4.7185\tLR: 6.175192\n",
      "Training Epoch: 62 [37888/50000]\tLoss: 4.7622\tLR: 6.175448\n",
      "Training Epoch: 62 [38016/50000]\tLoss: 4.6964\tLR: 6.175703\n",
      "Training Epoch: 62 [38144/50000]\tLoss: 4.6808\tLR: 6.175959\n",
      "Training Epoch: 62 [38272/50000]\tLoss: 4.7187\tLR: 6.176215\n",
      "Training Epoch: 62 [38400/50000]\tLoss: 4.6752\tLR: 6.176471\n",
      "Training Epoch: 62 [38528/50000]\tLoss: 4.6495\tLR: 6.176726\n",
      "Training Epoch: 62 [38656/50000]\tLoss: 4.6290\tLR: 6.176982\n",
      "Training Epoch: 62 [38784/50000]\tLoss: 4.7369\tLR: 6.177238\n",
      "Training Epoch: 62 [38912/50000]\tLoss: 4.7498\tLR: 6.177494\n",
      "Training Epoch: 62 [39040/50000]\tLoss: 4.7578\tLR: 6.177749\n",
      "Training Epoch: 62 [39168/50000]\tLoss: 4.8539\tLR: 6.178005\n",
      "Training Epoch: 62 [39296/50000]\tLoss: 4.7293\tLR: 6.178261\n",
      "Training Epoch: 62 [39424/50000]\tLoss: 4.6724\tLR: 6.178517\n",
      "Training Epoch: 62 [39552/50000]\tLoss: 4.7542\tLR: 6.178772\n",
      "Training Epoch: 62 [39680/50000]\tLoss: 4.7097\tLR: 6.179028\n",
      "Training Epoch: 62 [39808/50000]\tLoss: 4.6638\tLR: 6.179284\n",
      "Training Epoch: 62 [39936/50000]\tLoss: 4.7954\tLR: 6.179540\n",
      "Training Epoch: 62 [40064/50000]\tLoss: 4.7005\tLR: 6.179795\n",
      "Training Epoch: 62 [40192/50000]\tLoss: 4.7320\tLR: 6.180051\n",
      "Training Epoch: 62 [40320/50000]\tLoss: 4.7490\tLR: 6.180307\n",
      "Training Epoch: 62 [40448/50000]\tLoss: 4.7365\tLR: 6.180563\n",
      "Training Epoch: 62 [40576/50000]\tLoss: 4.6677\tLR: 6.180818\n",
      "Training Epoch: 62 [40704/50000]\tLoss: 4.7383\tLR: 6.181074\n",
      "Training Epoch: 62 [40832/50000]\tLoss: 4.7961\tLR: 6.181330\n",
      "Training Epoch: 62 [40960/50000]\tLoss: 4.7283\tLR: 6.181586\n",
      "Training Epoch: 62 [41088/50000]\tLoss: 4.7898\tLR: 6.181841\n",
      "Training Epoch: 62 [41216/50000]\tLoss: 4.7202\tLR: 6.182097\n",
      "Training Epoch: 62 [41344/50000]\tLoss: 4.7393\tLR: 6.182353\n",
      "Training Epoch: 62 [41472/50000]\tLoss: 4.7451\tLR: 6.182609\n",
      "Training Epoch: 62 [41600/50000]\tLoss: 4.7430\tLR: 6.182864\n",
      "Training Epoch: 62 [41728/50000]\tLoss: 4.7245\tLR: 6.183120\n",
      "Training Epoch: 62 [41856/50000]\tLoss: 4.6995\tLR: 6.183376\n",
      "Training Epoch: 62 [41984/50000]\tLoss: 4.7318\tLR: 6.183632\n",
      "Training Epoch: 62 [42112/50000]\tLoss: 4.6775\tLR: 6.183887\n",
      "Training Epoch: 62 [42240/50000]\tLoss: 4.7219\tLR: 6.184143\n",
      "Training Epoch: 62 [42368/50000]\tLoss: 4.6368\tLR: 6.184399\n",
      "Training Epoch: 62 [42496/50000]\tLoss: 4.7482\tLR: 6.184655\n",
      "Training Epoch: 62 [42624/50000]\tLoss: 4.7758\tLR: 6.184910\n",
      "Training Epoch: 62 [42752/50000]\tLoss: 4.7728\tLR: 6.185166\n",
      "Training Epoch: 62 [42880/50000]\tLoss: 4.7682\tLR: 6.185422\n",
      "Training Epoch: 62 [43008/50000]\tLoss: 4.7943\tLR: 6.185678\n",
      "Training Epoch: 62 [43136/50000]\tLoss: 4.8209\tLR: 6.185934\n",
      "Training Epoch: 62 [43264/50000]\tLoss: 4.6545\tLR: 6.186189\n",
      "Training Epoch: 62 [43392/50000]\tLoss: 4.7050\tLR: 6.186445\n",
      "Training Epoch: 62 [43520/50000]\tLoss: 4.7595\tLR: 6.186701\n",
      "Training Epoch: 62 [43648/50000]\tLoss: 4.7072\tLR: 6.186957\n",
      "Training Epoch: 62 [43776/50000]\tLoss: 4.7466\tLR: 6.187212\n",
      "Training Epoch: 62 [43904/50000]\tLoss: 4.7594\tLR: 6.187468\n",
      "Training Epoch: 62 [44032/50000]\tLoss: 4.7378\tLR: 6.187724\n",
      "Training Epoch: 62 [44160/50000]\tLoss: 4.6588\tLR: 6.187980\n",
      "Training Epoch: 62 [44288/50000]\tLoss: 4.6729\tLR: 6.188235\n",
      "Training Epoch: 62 [44416/50000]\tLoss: 4.7451\tLR: 6.188491\n",
      "Training Epoch: 62 [44544/50000]\tLoss: 4.7149\tLR: 6.188747\n",
      "Training Epoch: 62 [44672/50000]\tLoss: 4.7420\tLR: 6.189003\n",
      "Training Epoch: 62 [44800/50000]\tLoss: 4.7455\tLR: 6.189258\n",
      "Training Epoch: 62 [44928/50000]\tLoss: 4.7129\tLR: 6.189514\n",
      "Training Epoch: 62 [45056/50000]\tLoss: 4.8011\tLR: 6.189770\n",
      "Training Epoch: 62 [45184/50000]\tLoss: 4.7150\tLR: 6.190026\n",
      "Training Epoch: 62 [45312/50000]\tLoss: 4.7779\tLR: 6.190281\n",
      "Training Epoch: 62 [45440/50000]\tLoss: 4.7463\tLR: 6.190537\n",
      "Training Epoch: 62 [45568/50000]\tLoss: 4.7065\tLR: 6.190793\n",
      "Training Epoch: 62 [45696/50000]\tLoss: 4.7735\tLR: 6.191049\n",
      "Training Epoch: 62 [45824/50000]\tLoss: 4.7684\tLR: 6.191304\n",
      "Training Epoch: 62 [45952/50000]\tLoss: 4.6543\tLR: 6.191560\n",
      "Training Epoch: 62 [46080/50000]\tLoss: 4.7778\tLR: 6.191816\n",
      "Training Epoch: 62 [46208/50000]\tLoss: 4.7028\tLR: 6.192072\n",
      "Training Epoch: 62 [46336/50000]\tLoss: 4.6877\tLR: 6.192327\n",
      "Training Epoch: 62 [46464/50000]\tLoss: 4.6704\tLR: 6.192583\n",
      "Training Epoch: 62 [46592/50000]\tLoss: 4.7138\tLR: 6.192839\n",
      "Training Epoch: 62 [46720/50000]\tLoss: 4.7209\tLR: 6.193095\n",
      "Training Epoch: 62 [46848/50000]\tLoss: 4.6794\tLR: 6.193350\n",
      "Training Epoch: 62 [46976/50000]\tLoss: 4.6585\tLR: 6.193606\n",
      "Training Epoch: 62 [47104/50000]\tLoss: 4.7519\tLR: 6.193862\n",
      "Training Epoch: 62 [47232/50000]\tLoss: 4.6895\tLR: 6.194118\n",
      "Training Epoch: 62 [47360/50000]\tLoss: 4.7662\tLR: 6.194373\n",
      "Training Epoch: 62 [47488/50000]\tLoss: 4.6981\tLR: 6.194629\n",
      "Training Epoch: 62 [47616/50000]\tLoss: 4.7390\tLR: 6.194885\n",
      "Training Epoch: 62 [47744/50000]\tLoss: 4.6949\tLR: 6.195141\n",
      "Training Epoch: 62 [47872/50000]\tLoss: 4.6749\tLR: 6.195396\n",
      "Training Epoch: 62 [48000/50000]\tLoss: 4.7286\tLR: 6.195652\n",
      "Training Epoch: 62 [48128/50000]\tLoss: 4.6931\tLR: 6.195908\n",
      "Training Epoch: 62 [48256/50000]\tLoss: 4.7155\tLR: 6.196164\n",
      "Training Epoch: 62 [48384/50000]\tLoss: 4.6866\tLR: 6.196419\n",
      "Training Epoch: 62 [48512/50000]\tLoss: 4.7592\tLR: 6.196675\n",
      "Training Epoch: 62 [48640/50000]\tLoss: 4.6408\tLR: 6.196931\n",
      "Training Epoch: 62 [48768/50000]\tLoss: 4.6949\tLR: 6.197187\n",
      "Training Epoch: 62 [48896/50000]\tLoss: 4.7159\tLR: 6.197442\n",
      "Training Epoch: 62 [49024/50000]\tLoss: 4.7528\tLR: 6.197698\n",
      "Training Epoch: 62 [49152/50000]\tLoss: 4.7158\tLR: 6.197954\n",
      "Training Epoch: 62 [49280/50000]\tLoss: 4.7607\tLR: 6.198210\n",
      "Training Epoch: 62 [49408/50000]\tLoss: 4.6967\tLR: 6.198465\n",
      "Training Epoch: 62 [49536/50000]\tLoss: 4.7340\tLR: 6.198721\n",
      "Training Epoch: 62 [49664/50000]\tLoss: 4.7021\tLR: 6.198977\n",
      "Training Epoch: 62 [49792/50000]\tLoss: 4.7183\tLR: 6.199233\n",
      "Training Epoch: 62 [49920/50000]\tLoss: 4.6486\tLR: 6.199488\n",
      "Training Epoch: 62 [50000/50000]\tLoss: 4.6635\tLR: 6.199744\n",
      "epoch 62 training time consumed: 489.08s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   86920 GB |   86920 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   86653 GB |   86653 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     266 GB |     266 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   86920 GB |   86920 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   86653 GB |   86653 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     266 GB |     266 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   85697 GB |   85697 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   85431 GB |   85430 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     266 GB |     266 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    9216 K  |    9216 K  |\n",
      "|       from large pool |      24    |      65    |    3928 K  |    3928 K  |\n",
      "|       from small pool |     231    |     274    |    5287 K  |    5287 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    9216 K  |    9216 K  |\n",
      "|       from large pool |      24    |      65    |    3928 K  |    3928 K  |\n",
      "|       from small pool |     231    |     274    |    5287 K  |    5287 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      47    |    5342 K  |    5342 K  |\n",
      "|       from large pool |      10    |      23    |    1888 K  |    1888 K  |\n",
      "|       from small pool |      27    |      35    |    3453 K  |    3453 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 62, Average loss: 0.0372, Accuracy: 0.0100, Time consumed:31.09s\n",
      "\n",
      "Training Epoch: 63 [128/50000]\tLoss: 4.7866\tLR: 0.020000\n",
      "Training Epoch: 63 [256/50000]\tLoss: 4.5943\tLR: 6.200256\n",
      "Training Epoch: 63 [384/50000]\tLoss: 4.6904\tLR: 6.200512\n",
      "Training Epoch: 63 [512/50000]\tLoss: 4.7551\tLR: 6.200767\n",
      "Training Epoch: 63 [640/50000]\tLoss: 4.7689\tLR: 6.201023\n",
      "Training Epoch: 63 [768/50000]\tLoss: 4.7168\tLR: 6.201279\n",
      "Training Epoch: 63 [896/50000]\tLoss: 4.6249\tLR: 6.201535\n",
      "Training Epoch: 63 [1024/50000]\tLoss: 4.7305\tLR: 6.201790\n",
      "Training Epoch: 63 [1152/50000]\tLoss: 4.7500\tLR: 6.202046\n",
      "Training Epoch: 63 [1280/50000]\tLoss: 4.6451\tLR: 6.202302\n",
      "Training Epoch: 63 [1408/50000]\tLoss: 4.6950\tLR: 6.202558\n",
      "Training Epoch: 63 [1536/50000]\tLoss: 4.7500\tLR: 6.202813\n",
      "Training Epoch: 63 [1664/50000]\tLoss: 4.6691\tLR: 6.203069\n",
      "Training Epoch: 63 [1792/50000]\tLoss: 4.6866\tLR: 6.203325\n",
      "Training Epoch: 63 [1920/50000]\tLoss: 4.6637\tLR: 6.203581\n",
      "Training Epoch: 63 [2048/50000]\tLoss: 4.7290\tLR: 6.203836\n",
      "Training Epoch: 63 [2176/50000]\tLoss: 4.7281\tLR: 6.204092\n",
      "Training Epoch: 63 [2304/50000]\tLoss: 4.7835\tLR: 6.204348\n",
      "Training Epoch: 63 [2432/50000]\tLoss: 4.7648\tLR: 6.204604\n",
      "Training Epoch: 63 [2560/50000]\tLoss: 4.6756\tLR: 6.204859\n",
      "Training Epoch: 63 [2688/50000]\tLoss: 4.6782\tLR: 6.205115\n",
      "Training Epoch: 63 [2816/50000]\tLoss: 4.6467\tLR: 6.205371\n",
      "Training Epoch: 63 [2944/50000]\tLoss: 4.6792\tLR: 6.205627\n",
      "Training Epoch: 63 [3072/50000]\tLoss: 4.7032\tLR: 6.205882\n",
      "Training Epoch: 63 [3200/50000]\tLoss: 4.7621\tLR: 6.206138\n",
      "Training Epoch: 63 [3328/50000]\tLoss: 4.7113\tLR: 6.206394\n",
      "Training Epoch: 63 [3456/50000]\tLoss: 4.7286\tLR: 6.206650\n",
      "Training Epoch: 63 [3584/50000]\tLoss: 4.7045\tLR: 6.206905\n",
      "Training Epoch: 63 [3712/50000]\tLoss: 4.7862\tLR: 6.207161\n",
      "Training Epoch: 63 [3840/50000]\tLoss: 4.6464\tLR: 6.207417\n",
      "Training Epoch: 63 [3968/50000]\tLoss: 4.6436\tLR: 6.207673\n",
      "Training Epoch: 63 [4096/50000]\tLoss: 4.6789\tLR: 6.207928\n",
      "Training Epoch: 63 [4224/50000]\tLoss: 4.7461\tLR: 6.208184\n",
      "Training Epoch: 63 [4352/50000]\tLoss: 4.7625\tLR: 6.208440\n",
      "Training Epoch: 63 [4480/50000]\tLoss: 4.7060\tLR: 6.208696\n",
      "Training Epoch: 63 [4608/50000]\tLoss: 4.6478\tLR: 6.208951\n",
      "Training Epoch: 63 [4736/50000]\tLoss: 4.7070\tLR: 6.209207\n",
      "Training Epoch: 63 [4864/50000]\tLoss: 4.7487\tLR: 6.209463\n",
      "Training Epoch: 63 [4992/50000]\tLoss: 4.8163\tLR: 6.209719\n",
      "Training Epoch: 63 [5120/50000]\tLoss: 4.7150\tLR: 6.209974\n",
      "Training Epoch: 63 [5248/50000]\tLoss: 4.6523\tLR: 6.210230\n",
      "Training Epoch: 63 [5376/50000]\tLoss: 4.7101\tLR: 6.210486\n",
      "Training Epoch: 63 [5504/50000]\tLoss: 4.7231\tLR: 6.210742\n",
      "Training Epoch: 63 [5632/50000]\tLoss: 4.7330\tLR: 6.210997\n",
      "Training Epoch: 63 [5760/50000]\tLoss: 4.7098\tLR: 6.211253\n",
      "Training Epoch: 63 [5888/50000]\tLoss: 4.6705\tLR: 6.211509\n",
      "Training Epoch: 63 [6016/50000]\tLoss: 4.6908\tLR: 6.211765\n",
      "Training Epoch: 63 [6144/50000]\tLoss: 4.6711\tLR: 6.212020\n",
      "Training Epoch: 63 [6272/50000]\tLoss: 4.6671\tLR: 6.212276\n",
      "Training Epoch: 63 [6400/50000]\tLoss: 4.6398\tLR: 6.212532\n",
      "Training Epoch: 63 [6528/50000]\tLoss: 4.7685\tLR: 6.212788\n",
      "Training Epoch: 63 [6656/50000]\tLoss: 4.7149\tLR: 6.213043\n",
      "Training Epoch: 63 [6784/50000]\tLoss: 4.7134\tLR: 6.213299\n",
      "Training Epoch: 63 [6912/50000]\tLoss: 4.7336\tLR: 6.213555\n",
      "Training Epoch: 63 [7040/50000]\tLoss: 4.7032\tLR: 6.213811\n",
      "Training Epoch: 63 [7168/50000]\tLoss: 4.7524\tLR: 6.214066\n",
      "Training Epoch: 63 [7296/50000]\tLoss: 4.7957\tLR: 6.214322\n",
      "Training Epoch: 63 [7424/50000]\tLoss: 4.7764\tLR: 6.214578\n",
      "Training Epoch: 63 [7552/50000]\tLoss: 4.7481\tLR: 6.214834\n",
      "Training Epoch: 63 [7680/50000]\tLoss: 4.6996\tLR: 6.215090\n",
      "Training Epoch: 63 [7808/50000]\tLoss: 4.6610\tLR: 6.215345\n",
      "Training Epoch: 63 [7936/50000]\tLoss: 4.7015\tLR: 6.215601\n",
      "Training Epoch: 63 [8064/50000]\tLoss: 4.7039\tLR: 6.215857\n",
      "Training Epoch: 63 [8192/50000]\tLoss: 4.7656\tLR: 6.216113\n",
      "Training Epoch: 63 [8320/50000]\tLoss: 4.7202\tLR: 6.216368\n",
      "Training Epoch: 63 [8448/50000]\tLoss: 4.7821\tLR: 6.216624\n",
      "Training Epoch: 63 [8576/50000]\tLoss: 4.7405\tLR: 6.216880\n",
      "Training Epoch: 63 [8704/50000]\tLoss: 4.8139\tLR: 6.217136\n",
      "Training Epoch: 63 [8832/50000]\tLoss: 4.7540\tLR: 6.217391\n",
      "Training Epoch: 63 [8960/50000]\tLoss: 4.6321\tLR: 6.217647\n",
      "Training Epoch: 63 [9088/50000]\tLoss: 4.7435\tLR: 6.217903\n",
      "Training Epoch: 63 [9216/50000]\tLoss: 4.7884\tLR: 6.218159\n",
      "Training Epoch: 63 [9344/50000]\tLoss: 4.7011\tLR: 6.218414\n",
      "Training Epoch: 63 [9472/50000]\tLoss: 4.6416\tLR: 6.218670\n",
      "Training Epoch: 63 [9600/50000]\tLoss: 4.7533\tLR: 6.218926\n",
      "Training Epoch: 63 [9728/50000]\tLoss: 4.7663\tLR: 6.219182\n",
      "Training Epoch: 63 [9856/50000]\tLoss: 4.7446\tLR: 6.219437\n",
      "Training Epoch: 63 [9984/50000]\tLoss: 4.7483\tLR: 6.219693\n",
      "Training Epoch: 63 [10112/50000]\tLoss: 4.6400\tLR: 6.219949\n",
      "Training Epoch: 63 [10240/50000]\tLoss: 4.6844\tLR: 6.220205\n",
      "Training Epoch: 63 [10368/50000]\tLoss: 4.7338\tLR: 6.220460\n",
      "Training Epoch: 63 [10496/50000]\tLoss: 4.7829\tLR: 6.220716\n",
      "Training Epoch: 63 [10624/50000]\tLoss: 4.7064\tLR: 6.220972\n",
      "Training Epoch: 63 [10752/50000]\tLoss: 4.6768\tLR: 6.221228\n",
      "Training Epoch: 63 [10880/50000]\tLoss: 4.6674\tLR: 6.221483\n",
      "Training Epoch: 63 [11008/50000]\tLoss: 4.7521\tLR: 6.221739\n",
      "Training Epoch: 63 [11136/50000]\tLoss: 4.6499\tLR: 6.221995\n",
      "Training Epoch: 63 [11264/50000]\tLoss: 4.7397\tLR: 6.222251\n",
      "Training Epoch: 63 [11392/50000]\tLoss: 4.6762\tLR: 6.222506\n",
      "Training Epoch: 63 [11520/50000]\tLoss: 4.7090\tLR: 6.222762\n",
      "Training Epoch: 63 [11648/50000]\tLoss: 4.7124\tLR: 6.223018\n",
      "Training Epoch: 63 [11776/50000]\tLoss: 4.8317\tLR: 6.223274\n",
      "Training Epoch: 63 [11904/50000]\tLoss: 4.8029\tLR: 6.223529\n",
      "Training Epoch: 63 [12032/50000]\tLoss: 4.7880\tLR: 6.223785\n",
      "Training Epoch: 63 [12160/50000]\tLoss: 4.7000\tLR: 6.224041\n",
      "Training Epoch: 63 [12288/50000]\tLoss: 4.7239\tLR: 6.224297\n",
      "Training Epoch: 63 [12416/50000]\tLoss: 4.6974\tLR: 6.224552\n",
      "Training Epoch: 63 [12544/50000]\tLoss: 4.6901\tLR: 6.224808\n",
      "Training Epoch: 63 [12672/50000]\tLoss: 4.7419\tLR: 6.225064\n",
      "Training Epoch: 63 [12800/50000]\tLoss: 4.6791\tLR: 6.225320\n",
      "Training Epoch: 63 [12928/50000]\tLoss: 4.7560\tLR: 6.225575\n",
      "Training Epoch: 63 [13056/50000]\tLoss: 4.7312\tLR: 6.225831\n",
      "Training Epoch: 63 [13184/50000]\tLoss: 4.7547\tLR: 6.226087\n",
      "Training Epoch: 63 [13312/50000]\tLoss: 4.6339\tLR: 6.226343\n",
      "Training Epoch: 63 [13440/50000]\tLoss: 4.6964\tLR: 6.226598\n",
      "Training Epoch: 63 [13568/50000]\tLoss: 4.8229\tLR: 6.226854\n",
      "Training Epoch: 63 [13696/50000]\tLoss: 4.7984\tLR: 6.227110\n",
      "Training Epoch: 63 [13824/50000]\tLoss: 4.7500\tLR: 6.227366\n",
      "Training Epoch: 63 [13952/50000]\tLoss: 4.7418\tLR: 6.227621\n",
      "Training Epoch: 63 [14080/50000]\tLoss: 4.7271\tLR: 6.227877\n",
      "Training Epoch: 63 [14208/50000]\tLoss: 4.7024\tLR: 6.228133\n",
      "Training Epoch: 63 [14336/50000]\tLoss: 4.6659\tLR: 6.228389\n",
      "Training Epoch: 63 [14464/50000]\tLoss: 4.7201\tLR: 6.228645\n",
      "Training Epoch: 63 [14592/50000]\tLoss: 4.6168\tLR: 6.228900\n",
      "Training Epoch: 63 [14720/50000]\tLoss: 4.7600\tLR: 6.229156\n",
      "Training Epoch: 63 [14848/50000]\tLoss: 4.7256\tLR: 6.229412\n",
      "Training Epoch: 63 [14976/50000]\tLoss: 4.7017\tLR: 6.229668\n",
      "Training Epoch: 63 [15104/50000]\tLoss: 4.7396\tLR: 6.229923\n",
      "Training Epoch: 63 [15232/50000]\tLoss: 4.7205\tLR: 6.230179\n",
      "Training Epoch: 63 [15360/50000]\tLoss: 4.6657\tLR: 6.230435\n",
      "Training Epoch: 63 [15488/50000]\tLoss: 4.7213\tLR: 6.230691\n",
      "Training Epoch: 63 [15616/50000]\tLoss: 4.7936\tLR: 6.230946\n",
      "Training Epoch: 63 [15744/50000]\tLoss: 4.7729\tLR: 6.231202\n",
      "Training Epoch: 63 [15872/50000]\tLoss: 4.7211\tLR: 6.231458\n",
      "Training Epoch: 63 [16000/50000]\tLoss: 4.6686\tLR: 6.231714\n",
      "Training Epoch: 63 [16128/50000]\tLoss: 4.7146\tLR: 6.231969\n",
      "Training Epoch: 63 [16256/50000]\tLoss: 4.7611\tLR: 6.232225\n",
      "Training Epoch: 63 [16384/50000]\tLoss: 4.7038\tLR: 6.232481\n",
      "Training Epoch: 63 [16512/50000]\tLoss: 4.7948\tLR: 6.232737\n",
      "Training Epoch: 63 [16640/50000]\tLoss: 4.7255\tLR: 6.232992\n",
      "Training Epoch: 63 [16768/50000]\tLoss: 4.7181\tLR: 6.233248\n",
      "Training Epoch: 63 [16896/50000]\tLoss: 4.6779\tLR: 6.233504\n",
      "Training Epoch: 63 [17024/50000]\tLoss: 4.7193\tLR: 6.233760\n",
      "Training Epoch: 63 [17152/50000]\tLoss: 4.6938\tLR: 6.234015\n",
      "Training Epoch: 63 [17280/50000]\tLoss: 4.7429\tLR: 6.234271\n",
      "Training Epoch: 63 [17408/50000]\tLoss: 4.6348\tLR: 6.234527\n",
      "Training Epoch: 63 [17536/50000]\tLoss: 4.7541\tLR: 6.234783\n",
      "Training Epoch: 63 [17664/50000]\tLoss: 4.7464\tLR: 6.235038\n",
      "Training Epoch: 63 [17792/50000]\tLoss: 4.7552\tLR: 6.235294\n",
      "Training Epoch: 63 [17920/50000]\tLoss: 4.6629\tLR: 6.235550\n",
      "Training Epoch: 63 [18048/50000]\tLoss: 4.7768\tLR: 6.235806\n",
      "Training Epoch: 63 [18176/50000]\tLoss: 4.7529\tLR: 6.236061\n",
      "Training Epoch: 63 [18304/50000]\tLoss: 4.7325\tLR: 6.236317\n",
      "Training Epoch: 63 [18432/50000]\tLoss: 4.7451\tLR: 6.236573\n",
      "Training Epoch: 63 [18560/50000]\tLoss: 4.6862\tLR: 6.236829\n",
      "Training Epoch: 63 [18688/50000]\tLoss: 4.7266\tLR: 6.237084\n",
      "Training Epoch: 63 [18816/50000]\tLoss: 4.7331\tLR: 6.237340\n",
      "Training Epoch: 63 [18944/50000]\tLoss: 4.7988\tLR: 6.237596\n",
      "Training Epoch: 63 [19072/50000]\tLoss: 4.7688\tLR: 6.237852\n",
      "Training Epoch: 63 [19200/50000]\tLoss: 4.7592\tLR: 6.238107\n",
      "Training Epoch: 63 [19328/50000]\tLoss: 4.7016\tLR: 6.238363\n",
      "Training Epoch: 63 [19456/50000]\tLoss: 4.7382\tLR: 6.238619\n",
      "Training Epoch: 63 [19584/50000]\tLoss: 4.6996\tLR: 6.238875\n",
      "Training Epoch: 63 [19712/50000]\tLoss: 4.6819\tLR: 6.239130\n",
      "Training Epoch: 63 [19840/50000]\tLoss: 4.7596\tLR: 6.239386\n",
      "Training Epoch: 63 [19968/50000]\tLoss: 4.7810\tLR: 6.239642\n",
      "Training Epoch: 63 [20096/50000]\tLoss: 4.7823\tLR: 6.239898\n",
      "Training Epoch: 63 [20224/50000]\tLoss: 4.7306\tLR: 6.240153\n",
      "Training Epoch: 63 [20352/50000]\tLoss: 4.7154\tLR: 6.240409\n",
      "Training Epoch: 63 [20480/50000]\tLoss: 4.7096\tLR: 6.240665\n",
      "Training Epoch: 63 [20608/50000]\tLoss: 4.6531\tLR: 6.240921\n",
      "Training Epoch: 63 [20736/50000]\tLoss: 4.7882\tLR: 6.241176\n",
      "Training Epoch: 63 [20864/50000]\tLoss: 4.7133\tLR: 6.241432\n",
      "Training Epoch: 63 [20992/50000]\tLoss: 4.6515\tLR: 6.241688\n",
      "Training Epoch: 63 [21120/50000]\tLoss: 4.6991\tLR: 6.241944\n",
      "Training Epoch: 63 [21248/50000]\tLoss: 4.7527\tLR: 6.242199\n",
      "Training Epoch: 63 [21376/50000]\tLoss: 4.7599\tLR: 6.242455\n",
      "Training Epoch: 63 [21504/50000]\tLoss: 4.7160\tLR: 6.242711\n",
      "Training Epoch: 63 [21632/50000]\tLoss: 4.7275\tLR: 6.242967\n",
      "Training Epoch: 63 [21760/50000]\tLoss: 4.7919\tLR: 6.243223\n",
      "Training Epoch: 63 [21888/50000]\tLoss: 4.6752\tLR: 6.243478\n",
      "Training Epoch: 63 [22016/50000]\tLoss: 4.7075\tLR: 6.243734\n",
      "Training Epoch: 63 [22144/50000]\tLoss: 4.7118\tLR: 6.243990\n",
      "Training Epoch: 63 [22272/50000]\tLoss: 4.7080\tLR: 6.244246\n",
      "Training Epoch: 63 [22400/50000]\tLoss: 4.7633\tLR: 6.244501\n",
      "Training Epoch: 63 [22528/50000]\tLoss: 4.6933\tLR: 6.244757\n",
      "Training Epoch: 63 [22656/50000]\tLoss: 4.7458\tLR: 6.245013\n",
      "Training Epoch: 63 [22784/50000]\tLoss: 4.7588\tLR: 6.245269\n",
      "Training Epoch: 63 [22912/50000]\tLoss: 4.7019\tLR: 6.245524\n",
      "Training Epoch: 63 [23040/50000]\tLoss: 4.6457\tLR: 6.245780\n",
      "Training Epoch: 63 [23168/50000]\tLoss: 4.7165\tLR: 6.246036\n",
      "Training Epoch: 63 [23296/50000]\tLoss: 4.7768\tLR: 6.246292\n",
      "Training Epoch: 63 [23424/50000]\tLoss: 4.6906\tLR: 6.246547\n",
      "Training Epoch: 63 [23552/50000]\tLoss: 4.7336\tLR: 6.246803\n",
      "Training Epoch: 63 [23680/50000]\tLoss: 4.7136\tLR: 6.247059\n",
      "Training Epoch: 63 [23808/50000]\tLoss: 4.7569\tLR: 6.247315\n",
      "Training Epoch: 63 [23936/50000]\tLoss: 4.7192\tLR: 6.247570\n",
      "Training Epoch: 63 [24064/50000]\tLoss: 4.7283\tLR: 6.247826\n",
      "Training Epoch: 63 [24192/50000]\tLoss: 4.6710\tLR: 6.248082\n",
      "Training Epoch: 63 [24320/50000]\tLoss: 4.7162\tLR: 6.248338\n",
      "Training Epoch: 63 [24448/50000]\tLoss: 4.7136\tLR: 6.248593\n",
      "Training Epoch: 63 [24576/50000]\tLoss: 4.7170\tLR: 6.248849\n",
      "Training Epoch: 63 [24704/50000]\tLoss: 4.7480\tLR: 6.249105\n",
      "Training Epoch: 63 [24832/50000]\tLoss: 4.6776\tLR: 6.249361\n",
      "Training Epoch: 63 [24960/50000]\tLoss: 4.7650\tLR: 6.249616\n",
      "Training Epoch: 63 [25088/50000]\tLoss: 4.7740\tLR: 6.249872\n",
      "Training Epoch: 63 [25216/50000]\tLoss: 4.6194\tLR: 6.250128\n",
      "Training Epoch: 63 [25344/50000]\tLoss: 4.8024\tLR: 6.250384\n",
      "Training Epoch: 63 [25472/50000]\tLoss: 4.7160\tLR: 6.250639\n",
      "Training Epoch: 63 [25600/50000]\tLoss: 4.6955\tLR: 6.250895\n",
      "Training Epoch: 63 [25728/50000]\tLoss: 4.7920\tLR: 6.251151\n",
      "Training Epoch: 63 [25856/50000]\tLoss: 4.6715\tLR: 6.251407\n",
      "Training Epoch: 63 [25984/50000]\tLoss: 4.6979\tLR: 6.251662\n",
      "Training Epoch: 63 [26112/50000]\tLoss: 4.6725\tLR: 6.251918\n",
      "Training Epoch: 63 [26240/50000]\tLoss: 4.7479\tLR: 6.252174\n",
      "Training Epoch: 63 [26368/50000]\tLoss: 4.7937\tLR: 6.252430\n",
      "Training Epoch: 63 [26496/50000]\tLoss: 4.7473\tLR: 6.252685\n",
      "Training Epoch: 63 [26624/50000]\tLoss: 4.7150\tLR: 6.252941\n",
      "Training Epoch: 63 [26752/50000]\tLoss: 4.7036\tLR: 6.253197\n",
      "Training Epoch: 63 [26880/50000]\tLoss: 4.7142\tLR: 6.253453\n",
      "Training Epoch: 63 [27008/50000]\tLoss: 4.7619\tLR: 6.253708\n",
      "Training Epoch: 63 [27136/50000]\tLoss: 4.7929\tLR: 6.253964\n",
      "Training Epoch: 63 [27264/50000]\tLoss: 4.7430\tLR: 6.254220\n",
      "Training Epoch: 63 [27392/50000]\tLoss: 4.7347\tLR: 6.254476\n",
      "Training Epoch: 63 [27520/50000]\tLoss: 4.8004\tLR: 6.254731\n",
      "Training Epoch: 63 [27648/50000]\tLoss: 4.7250\tLR: 6.254987\n",
      "Training Epoch: 63 [27776/50000]\tLoss: 4.6987\tLR: 6.255243\n",
      "Training Epoch: 63 [27904/50000]\tLoss: 4.6424\tLR: 6.255499\n",
      "Training Epoch: 63 [28032/50000]\tLoss: 4.7643\tLR: 6.255754\n",
      "Training Epoch: 63 [28160/50000]\tLoss: 4.6875\tLR: 6.256010\n",
      "Training Epoch: 63 [28288/50000]\tLoss: 4.7317\tLR: 6.256266\n",
      "Training Epoch: 63 [28416/50000]\tLoss: 4.7317\tLR: 6.256522\n",
      "Training Epoch: 63 [28544/50000]\tLoss: 4.7378\tLR: 6.256777\n",
      "Training Epoch: 63 [28672/50000]\tLoss: 4.7180\tLR: 6.257033\n",
      "Training Epoch: 63 [28800/50000]\tLoss: 4.6552\tLR: 6.257289\n",
      "Training Epoch: 63 [28928/50000]\tLoss: 4.8012\tLR: 6.257545\n",
      "Training Epoch: 63 [29056/50000]\tLoss: 4.7416\tLR: 6.257801\n",
      "Training Epoch: 63 [29184/50000]\tLoss: 4.6875\tLR: 6.258056\n",
      "Training Epoch: 63 [29312/50000]\tLoss: 4.6493\tLR: 6.258312\n",
      "Training Epoch: 63 [29440/50000]\tLoss: 4.7357\tLR: 6.258568\n",
      "Training Epoch: 63 [29568/50000]\tLoss: 4.7682\tLR: 6.258824\n",
      "Training Epoch: 63 [29696/50000]\tLoss: 4.7504\tLR: 6.259079\n",
      "Training Epoch: 63 [29824/50000]\tLoss: 4.6907\tLR: 6.259335\n",
      "Training Epoch: 63 [29952/50000]\tLoss: 4.6547\tLR: 6.259591\n",
      "Training Epoch: 63 [30080/50000]\tLoss: 4.6936\tLR: 6.259847\n",
      "Training Epoch: 63 [30208/50000]\tLoss: 4.7272\tLR: 6.260102\n",
      "Training Epoch: 63 [30336/50000]\tLoss: 4.8128\tLR: 6.260358\n",
      "Training Epoch: 63 [30464/50000]\tLoss: 4.6970\tLR: 6.260614\n",
      "Training Epoch: 63 [30592/50000]\tLoss: 4.7552\tLR: 6.260870\n",
      "Training Epoch: 63 [30720/50000]\tLoss: 4.8695\tLR: 6.261125\n",
      "Training Epoch: 63 [30848/50000]\tLoss: 4.8298\tLR: 6.261381\n",
      "Training Epoch: 63 [30976/50000]\tLoss: 4.7042\tLR: 6.261637\n",
      "Training Epoch: 63 [31104/50000]\tLoss: 4.7752\tLR: 6.261893\n",
      "Training Epoch: 63 [31232/50000]\tLoss: 4.6393\tLR: 6.262148\n",
      "Training Epoch: 63 [31360/50000]\tLoss: 4.7600\tLR: 6.262404\n",
      "Training Epoch: 63 [31488/50000]\tLoss: 4.7992\tLR: 6.262660\n",
      "Training Epoch: 63 [31616/50000]\tLoss: 4.7049\tLR: 6.262916\n",
      "Training Epoch: 63 [31744/50000]\tLoss: 4.7096\tLR: 6.263171\n",
      "Training Epoch: 63 [31872/50000]\tLoss: 4.6815\tLR: 6.263427\n",
      "Training Epoch: 63 [32000/50000]\tLoss: 4.8271\tLR: 6.263683\n",
      "Training Epoch: 63 [32128/50000]\tLoss: 4.7949\tLR: 6.263939\n",
      "Training Epoch: 63 [32256/50000]\tLoss: 4.6360\tLR: 6.264194\n",
      "Training Epoch: 63 [32384/50000]\tLoss: 4.7427\tLR: 6.264450\n",
      "Training Epoch: 63 [32512/50000]\tLoss: 4.7833\tLR: 6.264706\n",
      "Training Epoch: 63 [32640/50000]\tLoss: 4.7644\tLR: 6.264962\n",
      "Training Epoch: 63 [32768/50000]\tLoss: 4.7045\tLR: 6.265217\n",
      "Training Epoch: 63 [32896/50000]\tLoss: 4.6743\tLR: 6.265473\n",
      "Training Epoch: 63 [33024/50000]\tLoss: 4.7793\tLR: 6.265729\n",
      "Training Epoch: 63 [33152/50000]\tLoss: 4.7014\tLR: 6.265985\n",
      "Training Epoch: 63 [33280/50000]\tLoss: 4.7186\tLR: 6.266240\n",
      "Training Epoch: 63 [33408/50000]\tLoss: 4.7171\tLR: 6.266496\n",
      "Training Epoch: 63 [33536/50000]\tLoss: 4.7040\tLR: 6.266752\n",
      "Training Epoch: 63 [33664/50000]\tLoss: 4.6830\tLR: 6.267008\n",
      "Training Epoch: 63 [33792/50000]\tLoss: 4.7659\tLR: 6.267263\n",
      "Training Epoch: 63 [33920/50000]\tLoss: 4.7423\tLR: 6.267519\n",
      "Training Epoch: 63 [34048/50000]\tLoss: 4.7277\tLR: 6.267775\n",
      "Training Epoch: 63 [34176/50000]\tLoss: 4.7631\tLR: 6.268031\n",
      "Training Epoch: 63 [34304/50000]\tLoss: 4.7075\tLR: 6.268286\n",
      "Training Epoch: 63 [34432/50000]\tLoss: 4.7062\tLR: 6.268542\n",
      "Training Epoch: 63 [34560/50000]\tLoss: 4.7825\tLR: 6.268798\n",
      "Training Epoch: 63 [34688/50000]\tLoss: 4.7788\tLR: 6.269054\n",
      "Training Epoch: 63 [34816/50000]\tLoss: 4.6261\tLR: 6.269309\n",
      "Training Epoch: 63 [34944/50000]\tLoss: 4.6842\tLR: 6.269565\n",
      "Training Epoch: 63 [35072/50000]\tLoss: 4.7798\tLR: 6.269821\n",
      "Training Epoch: 63 [35200/50000]\tLoss: 4.7349\tLR: 6.270077\n",
      "Training Epoch: 63 [35328/50000]\tLoss: 4.7525\tLR: 6.270332\n",
      "Training Epoch: 63 [35456/50000]\tLoss: 4.7057\tLR: 6.270588\n",
      "Training Epoch: 63 [35584/50000]\tLoss: 4.6547\tLR: 6.270844\n",
      "Training Epoch: 63 [35712/50000]\tLoss: 4.6852\tLR: 6.271100\n",
      "Training Epoch: 63 [35840/50000]\tLoss: 4.6723\tLR: 6.271355\n",
      "Training Epoch: 63 [35968/50000]\tLoss: 4.6650\tLR: 6.271611\n",
      "Training Epoch: 63 [36096/50000]\tLoss: 4.7476\tLR: 6.271867\n",
      "Training Epoch: 63 [36224/50000]\tLoss: 4.7511\tLR: 6.272123\n",
      "Training Epoch: 63 [36352/50000]\tLoss: 4.7126\tLR: 6.272379\n",
      "Training Epoch: 63 [36480/50000]\tLoss: 4.6856\tLR: 6.272634\n",
      "Training Epoch: 63 [36608/50000]\tLoss: 4.6896\tLR: 6.272890\n",
      "Training Epoch: 63 [36736/50000]\tLoss: 4.6510\tLR: 6.273146\n",
      "Training Epoch: 63 [36864/50000]\tLoss: 4.7783\tLR: 6.273402\n",
      "Training Epoch: 63 [36992/50000]\tLoss: 4.8225\tLR: 6.273657\n",
      "Training Epoch: 63 [37120/50000]\tLoss: 4.7608\tLR: 6.273913\n",
      "Training Epoch: 63 [37248/50000]\tLoss: 4.6874\tLR: 6.274169\n",
      "Training Epoch: 63 [37376/50000]\tLoss: 4.6713\tLR: 6.274425\n",
      "Training Epoch: 63 [37504/50000]\tLoss: 4.7402\tLR: 6.274680\n",
      "Training Epoch: 63 [37632/50000]\tLoss: 4.6601\tLR: 6.274936\n",
      "Training Epoch: 63 [37760/50000]\tLoss: 4.8241\tLR: 6.275192\n",
      "Training Epoch: 63 [37888/50000]\tLoss: 4.7646\tLR: 6.275448\n",
      "Training Epoch: 63 [38016/50000]\tLoss: 4.6962\tLR: 6.275703\n",
      "Training Epoch: 63 [38144/50000]\tLoss: 4.7394\tLR: 6.275959\n",
      "Training Epoch: 63 [38272/50000]\tLoss: 4.7872\tLR: 6.276215\n",
      "Training Epoch: 63 [38400/50000]\tLoss: 4.6855\tLR: 6.276471\n",
      "Training Epoch: 63 [38528/50000]\tLoss: 4.6738\tLR: 6.276726\n",
      "Training Epoch: 63 [38656/50000]\tLoss: 4.7768\tLR: 6.276982\n",
      "Training Epoch: 63 [38784/50000]\tLoss: 4.7043\tLR: 6.277238\n",
      "Training Epoch: 63 [38912/50000]\tLoss: 4.7600\tLR: 6.277494\n",
      "Training Epoch: 63 [39040/50000]\tLoss: 4.7423\tLR: 6.277749\n",
      "Training Epoch: 63 [39168/50000]\tLoss: 4.7636\tLR: 6.278005\n",
      "Training Epoch: 63 [39296/50000]\tLoss: 4.8093\tLR: 6.278261\n",
      "Training Epoch: 63 [39424/50000]\tLoss: 4.6951\tLR: 6.278517\n",
      "Training Epoch: 63 [39552/50000]\tLoss: 4.7308\tLR: 6.278772\n",
      "Training Epoch: 63 [39680/50000]\tLoss: 4.7223\tLR: 6.279028\n",
      "Training Epoch: 63 [39808/50000]\tLoss: 4.6788\tLR: 6.279284\n",
      "Training Epoch: 63 [39936/50000]\tLoss: 4.6435\tLR: 6.279540\n",
      "Training Epoch: 63 [40064/50000]\tLoss: 4.7461\tLR: 6.279795\n",
      "Training Epoch: 63 [40192/50000]\tLoss: 4.6817\tLR: 6.280051\n",
      "Training Epoch: 63 [40320/50000]\tLoss: 4.8370\tLR: 6.280307\n",
      "Training Epoch: 63 [40448/50000]\tLoss: 4.6736\tLR: 6.280563\n",
      "Training Epoch: 63 [40576/50000]\tLoss: 4.8040\tLR: 6.280818\n",
      "Training Epoch: 63 [40704/50000]\tLoss: 4.7190\tLR: 6.281074\n",
      "Training Epoch: 63 [40832/50000]\tLoss: 4.6910\tLR: 6.281330\n",
      "Training Epoch: 63 [40960/50000]\tLoss: 4.7988\tLR: 6.281586\n",
      "Training Epoch: 63 [41088/50000]\tLoss: 4.7053\tLR: 6.281841\n",
      "Training Epoch: 63 [41216/50000]\tLoss: 4.7243\tLR: 6.282097\n",
      "Training Epoch: 63 [41344/50000]\tLoss: 4.6923\tLR: 6.282353\n",
      "Training Epoch: 63 [41472/50000]\tLoss: 4.7444\tLR: 6.282609\n",
      "Training Epoch: 63 [41600/50000]\tLoss: 4.7824\tLR: 6.282864\n",
      "Training Epoch: 63 [41728/50000]\tLoss: 4.7976\tLR: 6.283120\n",
      "Training Epoch: 63 [41856/50000]\tLoss: 4.7149\tLR: 6.283376\n",
      "Training Epoch: 63 [41984/50000]\tLoss: 4.7791\tLR: 6.283632\n",
      "Training Epoch: 63 [42112/50000]\tLoss: 4.7089\tLR: 6.283887\n",
      "Training Epoch: 63 [42240/50000]\tLoss: 4.6558\tLR: 6.284143\n",
      "Training Epoch: 63 [42368/50000]\tLoss: 4.6820\tLR: 6.284399\n",
      "Training Epoch: 63 [42496/50000]\tLoss: 4.7535\tLR: 6.284655\n",
      "Training Epoch: 63 [42624/50000]\tLoss: 4.7875\tLR: 6.284910\n",
      "Training Epoch: 63 [42752/50000]\tLoss: 4.6937\tLR: 6.285166\n",
      "Training Epoch: 63 [42880/50000]\tLoss: 4.7332\tLR: 6.285422\n",
      "Training Epoch: 63 [43008/50000]\tLoss: 4.7493\tLR: 6.285678\n",
      "Training Epoch: 63 [43136/50000]\tLoss: 4.7015\tLR: 6.285934\n",
      "Training Epoch: 63 [43264/50000]\tLoss: 4.7364\tLR: 6.286189\n",
      "Training Epoch: 63 [43392/50000]\tLoss: 4.6829\tLR: 6.286445\n",
      "Training Epoch: 63 [43520/50000]\tLoss: 4.7490\tLR: 6.286701\n",
      "Training Epoch: 63 [43648/50000]\tLoss: 4.7253\tLR: 6.286957\n",
      "Training Epoch: 63 [43776/50000]\tLoss: 4.7067\tLR: 6.287212\n",
      "Training Epoch: 63 [43904/50000]\tLoss: 4.6916\tLR: 6.287468\n",
      "Training Epoch: 63 [44032/50000]\tLoss: 4.8526\tLR: 6.287724\n",
      "Training Epoch: 63 [44160/50000]\tLoss: 4.7363\tLR: 6.287980\n",
      "Training Epoch: 63 [44288/50000]\tLoss: 4.6750\tLR: 6.288235\n",
      "Training Epoch: 63 [44416/50000]\tLoss: 4.7091\tLR: 6.288491\n",
      "Training Epoch: 63 [44544/50000]\tLoss: 4.6819\tLR: 6.288747\n",
      "Training Epoch: 63 [44672/50000]\tLoss: 4.7541\tLR: 6.289003\n",
      "Training Epoch: 63 [44800/50000]\tLoss: 4.7486\tLR: 6.289258\n",
      "Training Epoch: 63 [44928/50000]\tLoss: 4.7275\tLR: 6.289514\n",
      "Training Epoch: 63 [45056/50000]\tLoss: 4.6874\tLR: 6.289770\n",
      "Training Epoch: 63 [45184/50000]\tLoss: 4.7705\tLR: 6.290026\n",
      "Training Epoch: 63 [45312/50000]\tLoss: 4.7716\tLR: 6.290281\n",
      "Training Epoch: 63 [45440/50000]\tLoss: 4.7322\tLR: 6.290537\n",
      "Training Epoch: 63 [45568/50000]\tLoss: 4.6761\tLR: 6.290793\n",
      "Training Epoch: 63 [45696/50000]\tLoss: 4.6745\tLR: 6.291049\n",
      "Training Epoch: 63 [45824/50000]\tLoss: 4.6837\tLR: 6.291304\n",
      "Training Epoch: 63 [45952/50000]\tLoss: 4.7620\tLR: 6.291560\n",
      "Training Epoch: 63 [46080/50000]\tLoss: 4.7158\tLR: 6.291816\n",
      "Training Epoch: 63 [46208/50000]\tLoss: 4.7288\tLR: 6.292072\n",
      "Training Epoch: 63 [46336/50000]\tLoss: 4.8340\tLR: 6.292327\n",
      "Training Epoch: 63 [46464/50000]\tLoss: 4.7416\tLR: 6.292583\n",
      "Training Epoch: 63 [46592/50000]\tLoss: 4.7688\tLR: 6.292839\n",
      "Training Epoch: 63 [46720/50000]\tLoss: 4.6670\tLR: 6.293095\n",
      "Training Epoch: 63 [46848/50000]\tLoss: 4.6457\tLR: 6.293350\n",
      "Training Epoch: 63 [46976/50000]\tLoss: 4.6706\tLR: 6.293606\n",
      "Training Epoch: 63 [47104/50000]\tLoss: 4.6630\tLR: 6.293862\n",
      "Training Epoch: 63 [47232/50000]\tLoss: 4.7071\tLR: 6.294118\n",
      "Training Epoch: 63 [47360/50000]\tLoss: 4.7109\tLR: 6.294373\n",
      "Training Epoch: 63 [47488/50000]\tLoss: 4.6847\tLR: 6.294629\n",
      "Training Epoch: 63 [47616/50000]\tLoss: 4.6897\tLR: 6.294885\n",
      "Training Epoch: 63 [47744/50000]\tLoss: 4.7753\tLR: 6.295141\n",
      "Training Epoch: 63 [47872/50000]\tLoss: 4.7179\tLR: 6.295396\n",
      "Training Epoch: 63 [48000/50000]\tLoss: 4.7293\tLR: 6.295652\n",
      "Training Epoch: 63 [48128/50000]\tLoss: 4.7096\tLR: 6.295908\n",
      "Training Epoch: 63 [48256/50000]\tLoss: 4.7025\tLR: 6.296164\n",
      "Training Epoch: 63 [48384/50000]\tLoss: 4.8079\tLR: 6.296419\n",
      "Training Epoch: 63 [48512/50000]\tLoss: 4.7089\tLR: 6.296675\n",
      "Training Epoch: 63 [48640/50000]\tLoss: 4.7944\tLR: 6.296931\n",
      "Training Epoch: 63 [48768/50000]\tLoss: 4.6441\tLR: 6.297187\n",
      "Training Epoch: 63 [48896/50000]\tLoss: 4.6936\tLR: 6.297442\n",
      "Training Epoch: 63 [49024/50000]\tLoss: 4.6905\tLR: 6.297698\n",
      "Training Epoch: 63 [49152/50000]\tLoss: 4.6447\tLR: 6.297954\n",
      "Training Epoch: 63 [49280/50000]\tLoss: 4.6995\tLR: 6.298210\n",
      "Training Epoch: 63 [49408/50000]\tLoss: 4.7364\tLR: 6.298465\n",
      "Training Epoch: 63 [49536/50000]\tLoss: 4.6834\tLR: 6.298721\n",
      "Training Epoch: 63 [49664/50000]\tLoss: 4.7796\tLR: 6.298977\n",
      "Training Epoch: 63 [49792/50000]\tLoss: 4.8013\tLR: 6.299233\n",
      "Training Epoch: 63 [49920/50000]\tLoss: 4.7544\tLR: 6.299488\n",
      "Training Epoch: 63 [50000/50000]\tLoss: 4.6720\tLR: 6.299744\n",
      "epoch 63 training time consumed: 488.89s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   88322 GB |   88322 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   88051 GB |   88051 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     271 GB |     271 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   88322 GB |   88322 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   88051 GB |   88051 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     271 GB |     271 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   87080 GB |   87080 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   86808 GB |   86808 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     271 GB |     271 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    9365 K  |    9365 K  |\n",
      "|       from large pool |      24    |      65    |    3992 K  |    3992 K  |\n",
      "|       from small pool |     231    |     274    |    5373 K  |    5372 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    9365 K  |    9365 K  |\n",
      "|       from large pool |      24    |      65    |    3992 K  |    3992 K  |\n",
      "|       from small pool |     231    |     274    |    5373 K  |    5372 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      47    |    5428 K  |    5428 K  |\n",
      "|       from large pool |      10    |      23    |    1919 K  |    1918 K  |\n",
      "|       from small pool |      25    |      35    |    3509 K  |    3509 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 63, Average loss: 0.0372, Accuracy: 0.0100, Time consumed:31.14s\n",
      "\n",
      "Training Epoch: 64 [128/50000]\tLoss: 4.6993\tLR: 0.020000\n",
      "Training Epoch: 64 [256/50000]\tLoss: 4.6743\tLR: 6.300256\n",
      "Training Epoch: 64 [384/50000]\tLoss: 4.6603\tLR: 6.300512\n",
      "Training Epoch: 64 [512/50000]\tLoss: 4.7335\tLR: 6.300767\n",
      "Training Epoch: 64 [640/50000]\tLoss: 4.7390\tLR: 6.301023\n",
      "Training Epoch: 64 [768/50000]\tLoss: 4.6620\tLR: 6.301279\n",
      "Training Epoch: 64 [896/50000]\tLoss: 4.7483\tLR: 6.301535\n",
      "Training Epoch: 64 [1024/50000]\tLoss: 4.6394\tLR: 6.301790\n",
      "Training Epoch: 64 [1152/50000]\tLoss: 4.6965\tLR: 6.302046\n",
      "Training Epoch: 64 [1280/50000]\tLoss: 4.7287\tLR: 6.302302\n",
      "Training Epoch: 64 [1408/50000]\tLoss: 4.6477\tLR: 6.302558\n",
      "Training Epoch: 64 [1536/50000]\tLoss: 4.8087\tLR: 6.302813\n",
      "Training Epoch: 64 [1664/50000]\tLoss: 4.6738\tLR: 6.303069\n",
      "Training Epoch: 64 [1792/50000]\tLoss: 4.6676\tLR: 6.303325\n",
      "Training Epoch: 64 [1920/50000]\tLoss: 4.6782\tLR: 6.303581\n",
      "Training Epoch: 64 [2048/50000]\tLoss: 4.6891\tLR: 6.303836\n",
      "Training Epoch: 64 [2176/50000]\tLoss: 4.7250\tLR: 6.304092\n",
      "Training Epoch: 64 [2304/50000]\tLoss: 4.7012\tLR: 6.304348\n",
      "Training Epoch: 64 [2432/50000]\tLoss: 4.7374\tLR: 6.304604\n",
      "Training Epoch: 64 [2560/50000]\tLoss: 4.6894\tLR: 6.304859\n",
      "Training Epoch: 64 [2688/50000]\tLoss: 4.7288\tLR: 6.305115\n",
      "Training Epoch: 64 [2816/50000]\tLoss: 4.6827\tLR: 6.305371\n",
      "Training Epoch: 64 [2944/50000]\tLoss: 4.7171\tLR: 6.305627\n",
      "Training Epoch: 64 [3072/50000]\tLoss: 4.6476\tLR: 6.305882\n",
      "Training Epoch: 64 [3200/50000]\tLoss: 4.7076\tLR: 6.306138\n",
      "Training Epoch: 64 [3328/50000]\tLoss: 4.6852\tLR: 6.306394\n",
      "Training Epoch: 64 [3456/50000]\tLoss: 4.7432\tLR: 6.306650\n",
      "Training Epoch: 64 [3584/50000]\tLoss: 4.6754\tLR: 6.306905\n",
      "Training Epoch: 64 [3712/50000]\tLoss: 4.7790\tLR: 6.307161\n",
      "Training Epoch: 64 [3840/50000]\tLoss: 4.7282\tLR: 6.307417\n",
      "Training Epoch: 64 [3968/50000]\tLoss: 4.7616\tLR: 6.307673\n",
      "Training Epoch: 64 [4096/50000]\tLoss: 4.7617\tLR: 6.307928\n",
      "Training Epoch: 64 [4224/50000]\tLoss: 4.7462\tLR: 6.308184\n",
      "Training Epoch: 64 [4352/50000]\tLoss: 4.6818\tLR: 6.308440\n",
      "Training Epoch: 64 [4480/50000]\tLoss: 4.7791\tLR: 6.308696\n",
      "Training Epoch: 64 [4608/50000]\tLoss: 4.7489\tLR: 6.308951\n",
      "Training Epoch: 64 [4736/50000]\tLoss: 4.7193\tLR: 6.309207\n",
      "Training Epoch: 64 [4864/50000]\tLoss: 4.7438\tLR: 6.309463\n",
      "Training Epoch: 64 [4992/50000]\tLoss: 4.7097\tLR: 6.309719\n",
      "Training Epoch: 64 [5120/50000]\tLoss: 4.7711\tLR: 6.309974\n",
      "Training Epoch: 64 [5248/50000]\tLoss: 4.7134\tLR: 6.310230\n",
      "Training Epoch: 64 [5376/50000]\tLoss: 4.7385\tLR: 6.310486\n",
      "Training Epoch: 64 [5504/50000]\tLoss: 4.7100\tLR: 6.310742\n",
      "Training Epoch: 64 [5632/50000]\tLoss: 4.7284\tLR: 6.310997\n",
      "Training Epoch: 64 [5760/50000]\tLoss: 4.6541\tLR: 6.311253\n",
      "Training Epoch: 64 [5888/50000]\tLoss: 4.7908\tLR: 6.311509\n",
      "Training Epoch: 64 [6016/50000]\tLoss: 4.6613\tLR: 6.311765\n",
      "Training Epoch: 64 [6144/50000]\tLoss: 4.7152\tLR: 6.312020\n",
      "Training Epoch: 64 [6272/50000]\tLoss: 4.7091\tLR: 6.312276\n",
      "Training Epoch: 64 [6400/50000]\tLoss: 4.7082\tLR: 6.312532\n",
      "Training Epoch: 64 [6528/50000]\tLoss: 4.6936\tLR: 6.312788\n",
      "Training Epoch: 64 [6656/50000]\tLoss: 4.7415\tLR: 6.313043\n",
      "Training Epoch: 64 [6784/50000]\tLoss: 4.6747\tLR: 6.313299\n",
      "Training Epoch: 64 [6912/50000]\tLoss: 4.6908\tLR: 6.313555\n",
      "Training Epoch: 64 [7040/50000]\tLoss: 4.7515\tLR: 6.313811\n",
      "Training Epoch: 64 [7168/50000]\tLoss: 4.7768\tLR: 6.314066\n",
      "Training Epoch: 64 [7296/50000]\tLoss: 4.7367\tLR: 6.314322\n",
      "Training Epoch: 64 [7424/50000]\tLoss: 4.6994\tLR: 6.314578\n",
      "Training Epoch: 64 [7552/50000]\tLoss: 4.7042\tLR: 6.314834\n",
      "Training Epoch: 64 [7680/50000]\tLoss: 4.7829\tLR: 6.315090\n",
      "Training Epoch: 64 [7808/50000]\tLoss: 4.7039\tLR: 6.315345\n",
      "Training Epoch: 64 [7936/50000]\tLoss: 4.7313\tLR: 6.315601\n",
      "Training Epoch: 64 [8064/50000]\tLoss: 4.6496\tLR: 6.315857\n",
      "Training Epoch: 64 [8192/50000]\tLoss: 4.7294\tLR: 6.316113\n",
      "Training Epoch: 64 [8320/50000]\tLoss: 4.7143\tLR: 6.316368\n",
      "Training Epoch: 64 [8448/50000]\tLoss: 4.6504\tLR: 6.316624\n",
      "Training Epoch: 64 [8576/50000]\tLoss: 4.6721\tLR: 6.316880\n",
      "Training Epoch: 64 [8704/50000]\tLoss: 4.7143\tLR: 6.317136\n",
      "Training Epoch: 64 [8832/50000]\tLoss: 4.7136\tLR: 6.317391\n",
      "Training Epoch: 64 [8960/50000]\tLoss: 4.7750\tLR: 6.317647\n",
      "Training Epoch: 64 [9088/50000]\tLoss: 4.7091\tLR: 6.317903\n",
      "Training Epoch: 64 [9216/50000]\tLoss: 4.7092\tLR: 6.318159\n",
      "Training Epoch: 64 [9344/50000]\tLoss: 4.6915\tLR: 6.318414\n",
      "Training Epoch: 64 [9472/50000]\tLoss: 4.7092\tLR: 6.318670\n",
      "Training Epoch: 64 [9600/50000]\tLoss: 4.6661\tLR: 6.318926\n",
      "Training Epoch: 64 [9728/50000]\tLoss: 4.6928\tLR: 6.319182\n",
      "Training Epoch: 64 [9856/50000]\tLoss: 4.7702\tLR: 6.319437\n",
      "Training Epoch: 64 [9984/50000]\tLoss: 4.7043\tLR: 6.319693\n",
      "Training Epoch: 64 [10112/50000]\tLoss: 4.7345\tLR: 6.319949\n",
      "Training Epoch: 64 [10240/50000]\tLoss: 4.7092\tLR: 6.320205\n",
      "Training Epoch: 64 [10368/50000]\tLoss: 4.7473\tLR: 6.320460\n",
      "Training Epoch: 64 [10496/50000]\tLoss: 4.7063\tLR: 6.320716\n",
      "Training Epoch: 64 [10624/50000]\tLoss: 4.6231\tLR: 6.320972\n",
      "Training Epoch: 64 [10752/50000]\tLoss: 4.7518\tLR: 6.321228\n",
      "Training Epoch: 64 [10880/50000]\tLoss: 4.7900\tLR: 6.321483\n",
      "Training Epoch: 64 [11008/50000]\tLoss: 4.6881\tLR: 6.321739\n",
      "Training Epoch: 64 [11136/50000]\tLoss: 4.7850\tLR: 6.321995\n",
      "Training Epoch: 64 [11264/50000]\tLoss: 4.6334\tLR: 6.322251\n",
      "Training Epoch: 64 [11392/50000]\tLoss: 4.7964\tLR: 6.322506\n",
      "Training Epoch: 64 [11520/50000]\tLoss: 4.6337\tLR: 6.322762\n",
      "Training Epoch: 64 [11648/50000]\tLoss: 4.7650\tLR: 6.323018\n",
      "Training Epoch: 64 [11776/50000]\tLoss: 4.6685\tLR: 6.323274\n",
      "Training Epoch: 64 [11904/50000]\tLoss: 4.7594\tLR: 6.323529\n",
      "Training Epoch: 64 [12032/50000]\tLoss: 4.7696\tLR: 6.323785\n",
      "Training Epoch: 64 [12160/50000]\tLoss: 4.6893\tLR: 6.324041\n",
      "Training Epoch: 64 [12288/50000]\tLoss: 4.6926\tLR: 6.324297\n",
      "Training Epoch: 64 [12416/50000]\tLoss: 4.7033\tLR: 6.324552\n",
      "Training Epoch: 64 [12544/50000]\tLoss: 4.7703\tLR: 6.324808\n",
      "Training Epoch: 64 [12672/50000]\tLoss: 4.7777\tLR: 6.325064\n",
      "Training Epoch: 64 [12800/50000]\tLoss: 4.6665\tLR: 6.325320\n",
      "Training Epoch: 64 [12928/50000]\tLoss: 4.7874\tLR: 6.325575\n",
      "Training Epoch: 64 [13056/50000]\tLoss: 4.7494\tLR: 6.325831\n",
      "Training Epoch: 64 [13184/50000]\tLoss: 4.7498\tLR: 6.326087\n",
      "Training Epoch: 64 [13312/50000]\tLoss: 4.6957\tLR: 6.326343\n",
      "Training Epoch: 64 [13440/50000]\tLoss: 4.7428\tLR: 6.326598\n",
      "Training Epoch: 64 [13568/50000]\tLoss: 4.6353\tLR: 6.326854\n",
      "Training Epoch: 64 [13696/50000]\tLoss: 4.6781\tLR: 6.327110\n",
      "Training Epoch: 64 [13824/50000]\tLoss: 4.7916\tLR: 6.327366\n",
      "Training Epoch: 64 [13952/50000]\tLoss: 4.7100\tLR: 6.327621\n",
      "Training Epoch: 64 [14080/50000]\tLoss: 4.7166\tLR: 6.327877\n",
      "Training Epoch: 64 [14208/50000]\tLoss: 4.8416\tLR: 6.328133\n",
      "Training Epoch: 64 [14336/50000]\tLoss: 4.6897\tLR: 6.328389\n",
      "Training Epoch: 64 [14464/50000]\tLoss: 4.7698\tLR: 6.328645\n",
      "Training Epoch: 64 [14592/50000]\tLoss: 4.8057\tLR: 6.328900\n",
      "Training Epoch: 64 [14720/50000]\tLoss: 4.6735\tLR: 6.329156\n",
      "Training Epoch: 64 [14848/50000]\tLoss: 4.6825\tLR: 6.329412\n",
      "Training Epoch: 64 [14976/50000]\tLoss: 4.7514\tLR: 6.329668\n",
      "Training Epoch: 64 [15104/50000]\tLoss: 4.7277\tLR: 6.329923\n",
      "Training Epoch: 64 [15232/50000]\tLoss: 4.7221\tLR: 6.330179\n",
      "Training Epoch: 64 [15360/50000]\tLoss: 4.6733\tLR: 6.330435\n",
      "Training Epoch: 64 [15488/50000]\tLoss: 4.6910\tLR: 6.330691\n",
      "Training Epoch: 64 [15616/50000]\tLoss: 4.7957\tLR: 6.330946\n",
      "Training Epoch: 64 [15744/50000]\tLoss: 4.8744\tLR: 6.331202\n",
      "Training Epoch: 64 [15872/50000]\tLoss: 4.7120\tLR: 6.331458\n",
      "Training Epoch: 64 [16000/50000]\tLoss: 4.7513\tLR: 6.331714\n",
      "Training Epoch: 64 [16128/50000]\tLoss: 4.7164\tLR: 6.331969\n",
      "Training Epoch: 64 [16256/50000]\tLoss: 4.7316\tLR: 6.332225\n",
      "Training Epoch: 64 [16384/50000]\tLoss: 4.7249\tLR: 6.332481\n",
      "Training Epoch: 64 [16512/50000]\tLoss: 4.7220\tLR: 6.332737\n",
      "Training Epoch: 64 [16640/50000]\tLoss: 4.7474\tLR: 6.332992\n",
      "Training Epoch: 64 [16768/50000]\tLoss: 4.7486\tLR: 6.333248\n",
      "Training Epoch: 64 [16896/50000]\tLoss: 4.7344\tLR: 6.333504\n",
      "Training Epoch: 64 [17024/50000]\tLoss: 4.7244\tLR: 6.333760\n",
      "Training Epoch: 64 [17152/50000]\tLoss: 4.7463\tLR: 6.334015\n",
      "Training Epoch: 64 [17280/50000]\tLoss: 4.7779\tLR: 6.334271\n",
      "Training Epoch: 64 [17408/50000]\tLoss: 4.7622\tLR: 6.334527\n",
      "Training Epoch: 64 [17536/50000]\tLoss: 4.6942\tLR: 6.334783\n",
      "Training Epoch: 64 [17664/50000]\tLoss: 4.6989\tLR: 6.335038\n",
      "Training Epoch: 64 [17792/50000]\tLoss: 4.6974\tLR: 6.335294\n",
      "Training Epoch: 64 [17920/50000]\tLoss: 4.7246\tLR: 6.335550\n",
      "Training Epoch: 64 [18048/50000]\tLoss: 4.7305\tLR: 6.335806\n",
      "Training Epoch: 64 [18176/50000]\tLoss: 4.7566\tLR: 6.336061\n",
      "Training Epoch: 64 [18304/50000]\tLoss: 4.7367\tLR: 6.336317\n",
      "Training Epoch: 64 [18432/50000]\tLoss: 4.7143\tLR: 6.336573\n",
      "Training Epoch: 64 [18560/50000]\tLoss: 4.7590\tLR: 6.336829\n",
      "Training Epoch: 64 [18688/50000]\tLoss: 4.6894\tLR: 6.337084\n",
      "Training Epoch: 64 [18816/50000]\tLoss: 4.8147\tLR: 6.337340\n",
      "Training Epoch: 64 [18944/50000]\tLoss: 4.7400\tLR: 6.337596\n",
      "Training Epoch: 64 [19072/50000]\tLoss: 4.6560\tLR: 6.337852\n",
      "Training Epoch: 64 [19200/50000]\tLoss: 4.7161\tLR: 6.338107\n",
      "Training Epoch: 64 [19328/50000]\tLoss: 4.7255\tLR: 6.338363\n",
      "Training Epoch: 64 [19456/50000]\tLoss: 4.7610\tLR: 6.338619\n",
      "Training Epoch: 64 [19584/50000]\tLoss: 4.7554\tLR: 6.338875\n",
      "Training Epoch: 64 [19712/50000]\tLoss: 4.6925\tLR: 6.339130\n",
      "Training Epoch: 64 [19840/50000]\tLoss: 4.7369\tLR: 6.339386\n",
      "Training Epoch: 64 [19968/50000]\tLoss: 4.8079\tLR: 6.339642\n",
      "Training Epoch: 64 [20096/50000]\tLoss: 4.6639\tLR: 6.339898\n",
      "Training Epoch: 64 [20224/50000]\tLoss: 4.7478\tLR: 6.340153\n",
      "Training Epoch: 64 [20352/50000]\tLoss: 4.6840\tLR: 6.340409\n",
      "Training Epoch: 64 [20480/50000]\tLoss: 4.6472\tLR: 6.340665\n",
      "Training Epoch: 64 [20608/50000]\tLoss: 4.6768\tLR: 6.340921\n",
      "Training Epoch: 64 [20736/50000]\tLoss: 4.6894\tLR: 6.341176\n",
      "Training Epoch: 64 [20864/50000]\tLoss: 4.6800\tLR: 6.341432\n",
      "Training Epoch: 64 [20992/50000]\tLoss: 4.7351\tLR: 6.341688\n",
      "Training Epoch: 64 [21120/50000]\tLoss: 4.7616\tLR: 6.341944\n",
      "Training Epoch: 64 [21248/50000]\tLoss: 4.7134\tLR: 6.342199\n",
      "Training Epoch: 64 [21376/50000]\tLoss: 4.6728\tLR: 6.342455\n",
      "Training Epoch: 64 [21504/50000]\tLoss: 4.7080\tLR: 6.342711\n",
      "Training Epoch: 64 [21632/50000]\tLoss: 4.7248\tLR: 6.342967\n",
      "Training Epoch: 64 [21760/50000]\tLoss: 4.6863\tLR: 6.343223\n",
      "Training Epoch: 64 [21888/50000]\tLoss: 4.7138\tLR: 6.343478\n",
      "Training Epoch: 64 [22016/50000]\tLoss: 4.7144\tLR: 6.343734\n",
      "Training Epoch: 64 [22144/50000]\tLoss: 4.7713\tLR: 6.343990\n",
      "Training Epoch: 64 [22272/50000]\tLoss: 4.6608\tLR: 6.344246\n",
      "Training Epoch: 64 [22400/50000]\tLoss: 4.7376\tLR: 6.344501\n",
      "Training Epoch: 64 [22528/50000]\tLoss: 4.7505\tLR: 6.344757\n",
      "Training Epoch: 64 [22656/50000]\tLoss: 4.7370\tLR: 6.345013\n",
      "Training Epoch: 64 [22784/50000]\tLoss: 4.7297\tLR: 6.345269\n",
      "Training Epoch: 64 [22912/50000]\tLoss: 4.6692\tLR: 6.345524\n",
      "Training Epoch: 64 [23040/50000]\tLoss: 4.7302\tLR: 6.345780\n",
      "Training Epoch: 64 [23168/50000]\tLoss: 4.7263\tLR: 6.346036\n",
      "Training Epoch: 64 [23296/50000]\tLoss: 4.7537\tLR: 6.346292\n",
      "Training Epoch: 64 [23424/50000]\tLoss: 4.5899\tLR: 6.346547\n",
      "Training Epoch: 64 [23552/50000]\tLoss: 4.6850\tLR: 6.346803\n",
      "Training Epoch: 64 [23680/50000]\tLoss: 4.7558\tLR: 6.347059\n",
      "Training Epoch: 64 [23808/50000]\tLoss: 4.7048\tLR: 6.347315\n",
      "Training Epoch: 64 [23936/50000]\tLoss: 4.7089\tLR: 6.347570\n",
      "Training Epoch: 64 [24064/50000]\tLoss: 4.8195\tLR: 6.347826\n",
      "Training Epoch: 64 [24192/50000]\tLoss: 4.7070\tLR: 6.348082\n",
      "Training Epoch: 64 [24320/50000]\tLoss: 4.6547\tLR: 6.348338\n",
      "Training Epoch: 64 [24448/50000]\tLoss: 4.7650\tLR: 6.348593\n",
      "Training Epoch: 64 [24576/50000]\tLoss: 4.7226\tLR: 6.348849\n",
      "Training Epoch: 64 [24704/50000]\tLoss: 4.7508\tLR: 6.349105\n",
      "Training Epoch: 64 [24832/50000]\tLoss: 4.8007\tLR: 6.349361\n",
      "Training Epoch: 64 [24960/50000]\tLoss: 4.7603\tLR: 6.349616\n",
      "Training Epoch: 64 [25088/50000]\tLoss: 4.6901\tLR: 6.349872\n",
      "Training Epoch: 64 [25216/50000]\tLoss: 4.6553\tLR: 6.350128\n",
      "Training Epoch: 64 [25344/50000]\tLoss: 4.6919\tLR: 6.350384\n",
      "Training Epoch: 64 [25472/50000]\tLoss: 4.6743\tLR: 6.350639\n",
      "Training Epoch: 64 [25600/50000]\tLoss: 4.7125\tLR: 6.350895\n",
      "Training Epoch: 64 [25728/50000]\tLoss: 4.7770\tLR: 6.351151\n",
      "Training Epoch: 64 [25856/50000]\tLoss: 4.7518\tLR: 6.351407\n",
      "Training Epoch: 64 [25984/50000]\tLoss: 4.6842\tLR: 6.351662\n",
      "Training Epoch: 64 [26112/50000]\tLoss: 4.6947\tLR: 6.351918\n",
      "Training Epoch: 64 [26240/50000]\tLoss: 4.7399\tLR: 6.352174\n",
      "Training Epoch: 64 [26368/50000]\tLoss: 4.6708\tLR: 6.352430\n",
      "Training Epoch: 64 [26496/50000]\tLoss: 4.7466\tLR: 6.352685\n",
      "Training Epoch: 64 [26624/50000]\tLoss: 4.7330\tLR: 6.352941\n",
      "Training Epoch: 64 [26752/50000]\tLoss: 4.7455\tLR: 6.353197\n",
      "Training Epoch: 64 [26880/50000]\tLoss: 4.7016\tLR: 6.353453\n",
      "Training Epoch: 64 [27008/50000]\tLoss: 4.6905\tLR: 6.353708\n",
      "Training Epoch: 64 [27136/50000]\tLoss: 4.6563\tLR: 6.353964\n",
      "Training Epoch: 64 [27264/50000]\tLoss: 4.7316\tLR: 6.354220\n",
      "Training Epoch: 64 [27392/50000]\tLoss: 4.8552\tLR: 6.354476\n",
      "Training Epoch: 64 [27520/50000]\tLoss: 4.6970\tLR: 6.354731\n",
      "Training Epoch: 64 [27648/50000]\tLoss: 4.7920\tLR: 6.354987\n",
      "Training Epoch: 64 [27776/50000]\tLoss: 4.7348\tLR: 6.355243\n",
      "Training Epoch: 64 [27904/50000]\tLoss: 4.7455\tLR: 6.355499\n",
      "Training Epoch: 64 [28032/50000]\tLoss: 4.7440\tLR: 6.355754\n",
      "Training Epoch: 64 [28160/50000]\tLoss: 4.6838\tLR: 6.356010\n",
      "Training Epoch: 64 [28288/50000]\tLoss: 4.7263\tLR: 6.356266\n",
      "Training Epoch: 64 [28416/50000]\tLoss: 4.7071\tLR: 6.356522\n",
      "Training Epoch: 64 [28544/50000]\tLoss: 4.6823\tLR: 6.356777\n",
      "Training Epoch: 64 [28672/50000]\tLoss: 4.7064\tLR: 6.357033\n",
      "Training Epoch: 64 [28800/50000]\tLoss: 4.7428\tLR: 6.357289\n",
      "Training Epoch: 64 [28928/50000]\tLoss: 4.7658\tLR: 6.357545\n",
      "Training Epoch: 64 [29056/50000]\tLoss: 4.7203\tLR: 6.357801\n",
      "Training Epoch: 64 [29184/50000]\tLoss: 4.8581\tLR: 6.358056\n",
      "Training Epoch: 64 [29312/50000]\tLoss: 4.6831\tLR: 6.358312\n",
      "Training Epoch: 64 [29440/50000]\tLoss: 4.7218\tLR: 6.358568\n",
      "Training Epoch: 64 [29568/50000]\tLoss: 4.6639\tLR: 6.358824\n",
      "Training Epoch: 64 [29696/50000]\tLoss: 4.7147\tLR: 6.359079\n",
      "Training Epoch: 64 [29824/50000]\tLoss: 4.6490\tLR: 6.359335\n",
      "Training Epoch: 64 [29952/50000]\tLoss: 4.7117\tLR: 6.359591\n",
      "Training Epoch: 64 [30080/50000]\tLoss: 4.7641\tLR: 6.359847\n",
      "Training Epoch: 64 [30208/50000]\tLoss: 4.7125\tLR: 6.360102\n",
      "Training Epoch: 64 [30336/50000]\tLoss: 4.7239\tLR: 6.360358\n",
      "Training Epoch: 64 [30464/50000]\tLoss: 4.8506\tLR: 6.360614\n",
      "Training Epoch: 64 [30592/50000]\tLoss: 4.8113\tLR: 6.360870\n",
      "Training Epoch: 64 [30720/50000]\tLoss: 4.6904\tLR: 6.361125\n",
      "Training Epoch: 64 [30848/50000]\tLoss: 4.7344\tLR: 6.361381\n",
      "Training Epoch: 64 [30976/50000]\tLoss: 4.7009\tLR: 6.361637\n",
      "Training Epoch: 64 [31104/50000]\tLoss: 4.6979\tLR: 6.361893\n",
      "Training Epoch: 64 [31232/50000]\tLoss: 4.7234\tLR: 6.362148\n",
      "Training Epoch: 64 [31360/50000]\tLoss: 4.6585\tLR: 6.362404\n",
      "Training Epoch: 64 [31488/50000]\tLoss: 4.6715\tLR: 6.362660\n",
      "Training Epoch: 64 [31616/50000]\tLoss: 4.7393\tLR: 6.362916\n",
      "Training Epoch: 64 [31744/50000]\tLoss: 4.7336\tLR: 6.363171\n",
      "Training Epoch: 64 [31872/50000]\tLoss: 4.7223\tLR: 6.363427\n",
      "Training Epoch: 64 [32000/50000]\tLoss: 4.6604\tLR: 6.363683\n",
      "Training Epoch: 64 [32128/50000]\tLoss: 4.6876\tLR: 6.363939\n",
      "Training Epoch: 64 [32256/50000]\tLoss: 4.7242\tLR: 6.364194\n",
      "Training Epoch: 64 [32384/50000]\tLoss: 4.7358\tLR: 6.364450\n",
      "Training Epoch: 64 [32512/50000]\tLoss: 4.6337\tLR: 6.364706\n",
      "Training Epoch: 64 [32640/50000]\tLoss: 4.6676\tLR: 6.364962\n",
      "Training Epoch: 64 [32768/50000]\tLoss: 4.7065\tLR: 6.365217\n",
      "Training Epoch: 64 [32896/50000]\tLoss: 4.7241\tLR: 6.365473\n",
      "Training Epoch: 64 [33024/50000]\tLoss: 4.6673\tLR: 6.365729\n",
      "Training Epoch: 64 [33152/50000]\tLoss: 4.7635\tLR: 6.365985\n",
      "Training Epoch: 64 [33280/50000]\tLoss: 4.6623\tLR: 6.366240\n",
      "Training Epoch: 64 [33408/50000]\tLoss: 4.7979\tLR: 6.366496\n",
      "Training Epoch: 64 [33536/50000]\tLoss: 4.6992\tLR: 6.366752\n",
      "Training Epoch: 64 [33664/50000]\tLoss: 4.7065\tLR: 6.367008\n",
      "Training Epoch: 64 [33792/50000]\tLoss: 4.6902\tLR: 6.367263\n",
      "Training Epoch: 64 [33920/50000]\tLoss: 4.8464\tLR: 6.367519\n",
      "Training Epoch: 64 [34048/50000]\tLoss: 4.8353\tLR: 6.367775\n",
      "Training Epoch: 64 [34176/50000]\tLoss: 4.7167\tLR: 6.368031\n",
      "Training Epoch: 64 [34304/50000]\tLoss: 4.7104\tLR: 6.368286\n",
      "Training Epoch: 64 [34432/50000]\tLoss: 4.6936\tLR: 6.368542\n",
      "Training Epoch: 64 [34560/50000]\tLoss: 4.7611\tLR: 6.368798\n",
      "Training Epoch: 64 [34688/50000]\tLoss: 4.7073\tLR: 6.369054\n",
      "Training Epoch: 64 [34816/50000]\tLoss: 4.8599\tLR: 6.369309\n",
      "Training Epoch: 64 [34944/50000]\tLoss: 4.7348\tLR: 6.369565\n",
      "Training Epoch: 64 [35072/50000]\tLoss: 4.7326\tLR: 6.369821\n",
      "Training Epoch: 64 [35200/50000]\tLoss: 4.7287\tLR: 6.370077\n",
      "Training Epoch: 64 [35328/50000]\tLoss: 4.7214\tLR: 6.370332\n",
      "Training Epoch: 64 [35456/50000]\tLoss: 4.6825\tLR: 6.370588\n",
      "Training Epoch: 64 [35584/50000]\tLoss: 4.8742\tLR: 6.370844\n",
      "Training Epoch: 64 [35712/50000]\tLoss: 4.7352\tLR: 6.371100\n",
      "Training Epoch: 64 [35840/50000]\tLoss: 4.7819\tLR: 6.371355\n",
      "Training Epoch: 64 [35968/50000]\tLoss: 4.6991\tLR: 6.371611\n",
      "Training Epoch: 64 [36096/50000]\tLoss: 4.6921\tLR: 6.371867\n",
      "Training Epoch: 64 [36224/50000]\tLoss: 4.8217\tLR: 6.372123\n",
      "Training Epoch: 64 [36352/50000]\tLoss: 4.7083\tLR: 6.372379\n",
      "Training Epoch: 64 [36480/50000]\tLoss: 4.8044\tLR: 6.372634\n",
      "Training Epoch: 64 [36608/50000]\tLoss: 4.7771\tLR: 6.372890\n",
      "Training Epoch: 64 [36736/50000]\tLoss: 4.8220\tLR: 6.373146\n",
      "Training Epoch: 64 [36864/50000]\tLoss: 4.7746\tLR: 6.373402\n",
      "Training Epoch: 64 [36992/50000]\tLoss: 4.7166\tLR: 6.373657\n",
      "Training Epoch: 64 [37120/50000]\tLoss: 4.7267\tLR: 6.373913\n",
      "Training Epoch: 64 [37248/50000]\tLoss: 4.7461\tLR: 6.374169\n",
      "Training Epoch: 64 [37376/50000]\tLoss: 4.8474\tLR: 6.374425\n",
      "Training Epoch: 64 [37504/50000]\tLoss: 4.7590\tLR: 6.374680\n",
      "Training Epoch: 64 [37632/50000]\tLoss: 4.6963\tLR: 6.374936\n",
      "Training Epoch: 64 [37760/50000]\tLoss: 4.6927\tLR: 6.375192\n",
      "Training Epoch: 64 [37888/50000]\tLoss: 4.8127\tLR: 6.375448\n",
      "Training Epoch: 64 [38016/50000]\tLoss: 4.7096\tLR: 6.375703\n",
      "Training Epoch: 64 [38144/50000]\tLoss: 4.7623\tLR: 6.375959\n",
      "Training Epoch: 64 [38272/50000]\tLoss: 4.7003\tLR: 6.376215\n",
      "Training Epoch: 64 [38400/50000]\tLoss: 4.6882\tLR: 6.376471\n",
      "Training Epoch: 64 [38528/50000]\tLoss: 4.7648\tLR: 6.376726\n",
      "Training Epoch: 64 [38656/50000]\tLoss: 4.6601\tLR: 6.376982\n",
      "Training Epoch: 64 [38784/50000]\tLoss: 4.8918\tLR: 6.377238\n",
      "Training Epoch: 64 [38912/50000]\tLoss: 4.7929\tLR: 6.377494\n",
      "Training Epoch: 64 [39040/50000]\tLoss: 4.7084\tLR: 6.377749\n",
      "Training Epoch: 64 [39168/50000]\tLoss: 4.7607\tLR: 6.378005\n",
      "Training Epoch: 64 [39296/50000]\tLoss: 4.8024\tLR: 6.378261\n",
      "Training Epoch: 64 [39424/50000]\tLoss: 4.7270\tLR: 6.378517\n",
      "Training Epoch: 64 [39552/50000]\tLoss: 4.8013\tLR: 6.378772\n",
      "Training Epoch: 64 [39680/50000]\tLoss: 4.7359\tLR: 6.379028\n",
      "Training Epoch: 64 [39808/50000]\tLoss: 4.7242\tLR: 6.379284\n",
      "Training Epoch: 64 [39936/50000]\tLoss: 4.7054\tLR: 6.379540\n",
      "Training Epoch: 64 [40064/50000]\tLoss: 4.6506\tLR: 6.379795\n",
      "Training Epoch: 64 [40192/50000]\tLoss: 4.7163\tLR: 6.380051\n",
      "Training Epoch: 64 [40320/50000]\tLoss: 4.7690\tLR: 6.380307\n",
      "Training Epoch: 64 [40448/50000]\tLoss: 4.6769\tLR: 6.380563\n",
      "Training Epoch: 64 [40576/50000]\tLoss: 4.6945\tLR: 6.380818\n",
      "Training Epoch: 64 [40704/50000]\tLoss: 4.6689\tLR: 6.381074\n",
      "Training Epoch: 64 [40832/50000]\tLoss: 4.7843\tLR: 6.381330\n",
      "Training Epoch: 64 [40960/50000]\tLoss: 4.7766\tLR: 6.381586\n",
      "Training Epoch: 64 [41088/50000]\tLoss: 4.6803\tLR: 6.381841\n",
      "Training Epoch: 64 [41216/50000]\tLoss: 4.7135\tLR: 6.382097\n",
      "Training Epoch: 64 [41344/50000]\tLoss: 4.7390\tLR: 6.382353\n",
      "Training Epoch: 64 [41472/50000]\tLoss: 4.7626\tLR: 6.382609\n",
      "Training Epoch: 64 [41600/50000]\tLoss: 4.7004\tLR: 6.382864\n",
      "Training Epoch: 64 [41728/50000]\tLoss: 4.7244\tLR: 6.383120\n",
      "Training Epoch: 64 [41856/50000]\tLoss: 4.7210\tLR: 6.383376\n",
      "Training Epoch: 64 [41984/50000]\tLoss: 4.6707\tLR: 6.383632\n",
      "Training Epoch: 64 [42112/50000]\tLoss: 4.7772\tLR: 6.383887\n",
      "Training Epoch: 64 [42240/50000]\tLoss: 4.6525\tLR: 6.384143\n",
      "Training Epoch: 64 [42368/50000]\tLoss: 4.7267\tLR: 6.384399\n",
      "Training Epoch: 64 [42496/50000]\tLoss: 4.7561\tLR: 6.384655\n",
      "Training Epoch: 64 [42624/50000]\tLoss: 4.7077\tLR: 6.384910\n",
      "Training Epoch: 64 [42752/50000]\tLoss: 4.7354\tLR: 6.385166\n",
      "Training Epoch: 64 [42880/50000]\tLoss: 4.7171\tLR: 6.385422\n",
      "Training Epoch: 64 [43008/50000]\tLoss: 4.7156\tLR: 6.385678\n",
      "Training Epoch: 64 [43136/50000]\tLoss: 4.7133\tLR: 6.385934\n",
      "Training Epoch: 64 [43264/50000]\tLoss: 4.7033\tLR: 6.386189\n",
      "Training Epoch: 64 [43392/50000]\tLoss: 4.6825\tLR: 6.386445\n",
      "Training Epoch: 64 [43520/50000]\tLoss: 4.7084\tLR: 6.386701\n",
      "Training Epoch: 64 [43648/50000]\tLoss: 4.8010\tLR: 6.386957\n",
      "Training Epoch: 64 [43776/50000]\tLoss: 4.7686\tLR: 6.387212\n",
      "Training Epoch: 64 [43904/50000]\tLoss: 4.7219\tLR: 6.387468\n",
      "Training Epoch: 64 [44032/50000]\tLoss: 4.7548\tLR: 6.387724\n",
      "Training Epoch: 64 [44160/50000]\tLoss: 4.7167\tLR: 6.387980\n",
      "Training Epoch: 64 [44288/50000]\tLoss: 4.6869\tLR: 6.388235\n",
      "Training Epoch: 64 [44416/50000]\tLoss: 4.6616\tLR: 6.388491\n",
      "Training Epoch: 64 [44544/50000]\tLoss: 4.7161\tLR: 6.388747\n",
      "Training Epoch: 64 [44672/50000]\tLoss: 4.7194\tLR: 6.389003\n",
      "Training Epoch: 64 [44800/50000]\tLoss: 4.7457\tLR: 6.389258\n",
      "Training Epoch: 64 [44928/50000]\tLoss: 4.7641\tLR: 6.389514\n",
      "Training Epoch: 64 [45056/50000]\tLoss: 4.7226\tLR: 6.389770\n",
      "Training Epoch: 64 [45184/50000]\tLoss: 4.7695\tLR: 6.390026\n",
      "Training Epoch: 64 [45312/50000]\tLoss: 4.7802\tLR: 6.390281\n",
      "Training Epoch: 64 [45440/50000]\tLoss: 4.7747\tLR: 6.390537\n",
      "Training Epoch: 64 [45568/50000]\tLoss: 4.7703\tLR: 6.390793\n",
      "Training Epoch: 64 [45696/50000]\tLoss: 4.7302\tLR: 6.391049\n",
      "Training Epoch: 64 [45824/50000]\tLoss: 4.7201\tLR: 6.391304\n",
      "Training Epoch: 64 [45952/50000]\tLoss: 4.7281\tLR: 6.391560\n",
      "Training Epoch: 64 [46080/50000]\tLoss: 4.6730\tLR: 6.391816\n",
      "Training Epoch: 64 [46208/50000]\tLoss: 4.6986\tLR: 6.392072\n",
      "Training Epoch: 64 [46336/50000]\tLoss: 4.8084\tLR: 6.392327\n",
      "Training Epoch: 64 [46464/50000]\tLoss: 4.6453\tLR: 6.392583\n",
      "Training Epoch: 64 [46592/50000]\tLoss: 4.7089\tLR: 6.392839\n",
      "Training Epoch: 64 [46720/50000]\tLoss: 4.6285\tLR: 6.393095\n",
      "Training Epoch: 64 [46848/50000]\tLoss: 4.7660\tLR: 6.393350\n",
      "Training Epoch: 64 [46976/50000]\tLoss: 4.8602\tLR: 6.393606\n",
      "Training Epoch: 64 [47104/50000]\tLoss: 4.7898\tLR: 6.393862\n",
      "Training Epoch: 64 [47232/50000]\tLoss: 4.7097\tLR: 6.394118\n",
      "Training Epoch: 64 [47360/50000]\tLoss: 4.6379\tLR: 6.394373\n",
      "Training Epoch: 64 [47488/50000]\tLoss: 4.7843\tLR: 6.394629\n",
      "Training Epoch: 64 [47616/50000]\tLoss: 4.6983\tLR: 6.394885\n",
      "Training Epoch: 64 [47744/50000]\tLoss: 4.6346\tLR: 6.395141\n",
      "Training Epoch: 64 [47872/50000]\tLoss: 4.6797\tLR: 6.395396\n",
      "Training Epoch: 64 [48000/50000]\tLoss: 4.7110\tLR: 6.395652\n",
      "Training Epoch: 64 [48128/50000]\tLoss: 4.6752\tLR: 6.395908\n",
      "Training Epoch: 64 [48256/50000]\tLoss: 4.7139\tLR: 6.396164\n",
      "Training Epoch: 64 [48384/50000]\tLoss: 4.6871\tLR: 6.396419\n",
      "Training Epoch: 64 [48512/50000]\tLoss: 4.7504\tLR: 6.396675\n",
      "Training Epoch: 64 [48640/50000]\tLoss: 4.7470\tLR: 6.396931\n",
      "Training Epoch: 64 [48768/50000]\tLoss: 4.8527\tLR: 6.397187\n",
      "Training Epoch: 64 [48896/50000]\tLoss: 4.8252\tLR: 6.397442\n",
      "Training Epoch: 64 [49024/50000]\tLoss: 4.6299\tLR: 6.397698\n",
      "Training Epoch: 64 [49152/50000]\tLoss: 4.7179\tLR: 6.397954\n",
      "Training Epoch: 64 [49280/50000]\tLoss: 4.7031\tLR: 6.398210\n",
      "Training Epoch: 64 [49408/50000]\tLoss: 4.7392\tLR: 6.398465\n",
      "Training Epoch: 64 [49536/50000]\tLoss: 4.8498\tLR: 6.398721\n",
      "Training Epoch: 64 [49664/50000]\tLoss: 4.7984\tLR: 6.398977\n",
      "Training Epoch: 64 [49792/50000]\tLoss: 4.7336\tLR: 6.399233\n",
      "Training Epoch: 64 [49920/50000]\tLoss: 4.7668\tLR: 6.399488\n",
      "Training Epoch: 64 [50000/50000]\tLoss: 4.8256\tLR: 6.399744\n",
      "epoch 64 training time consumed: 488.94s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   89724 GB |   89724 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   89448 GB |   89448 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     275 GB |     275 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   89724 GB |   89724 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   89448 GB |   89448 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     275 GB |     275 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   88462 GB |   88462 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   88186 GB |   88186 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     275 GB |     275 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    9513 K  |    9513 K  |\n",
      "|       from large pool |      24    |      65    |    4055 K  |    4055 K  |\n",
      "|       from small pool |     231    |     274    |    5458 K  |    5458 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    9513 K  |    9513 K  |\n",
      "|       from large pool |      24    |      65    |    4055 K  |    4055 K  |\n",
      "|       from small pool |     231    |     274    |    5458 K  |    5458 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      39    |      47    |    5514 K  |    5514 K  |\n",
      "|       from large pool |      10    |      23    |    1949 K  |    1949 K  |\n",
      "|       from small pool |      29    |      35    |    3565 K  |    3565 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 64, Average loss: 0.0375, Accuracy: 0.0100, Time consumed:31.21s\n",
      "\n",
      "Training Epoch: 65 [128/50000]\tLoss: 4.7116\tLR: 0.020000\n",
      "Training Epoch: 65 [256/50000]\tLoss: 4.7373\tLR: 6.400256\n",
      "Training Epoch: 65 [384/50000]\tLoss: 4.6734\tLR: 6.400512\n",
      "Training Epoch: 65 [512/50000]\tLoss: 4.7761\tLR: 6.400767\n",
      "Training Epoch: 65 [640/50000]\tLoss: 4.7347\tLR: 6.401023\n",
      "Training Epoch: 65 [768/50000]\tLoss: 4.6981\tLR: 6.401279\n",
      "Training Epoch: 65 [896/50000]\tLoss: 4.7246\tLR: 6.401535\n",
      "Training Epoch: 65 [1024/50000]\tLoss: 4.6655\tLR: 6.401790\n",
      "Training Epoch: 65 [1152/50000]\tLoss: 4.7551\tLR: 6.402046\n",
      "Training Epoch: 65 [1280/50000]\tLoss: 4.7342\tLR: 6.402302\n",
      "Training Epoch: 65 [1408/50000]\tLoss: 4.7990\tLR: 6.402558\n",
      "Training Epoch: 65 [1536/50000]\tLoss: 4.7605\tLR: 6.402813\n",
      "Training Epoch: 65 [1664/50000]\tLoss: 4.7546\tLR: 6.403069\n",
      "Training Epoch: 65 [1792/50000]\tLoss: 4.6734\tLR: 6.403325\n",
      "Training Epoch: 65 [1920/50000]\tLoss: 4.6675\tLR: 6.403581\n",
      "Training Epoch: 65 [2048/50000]\tLoss: 4.7816\tLR: 6.403836\n",
      "Training Epoch: 65 [2176/50000]\tLoss: 4.7828\tLR: 6.404092\n",
      "Training Epoch: 65 [2304/50000]\tLoss: 4.8821\tLR: 6.404348\n",
      "Training Epoch: 65 [2432/50000]\tLoss: 4.7162\tLR: 6.404604\n",
      "Training Epoch: 65 [2560/50000]\tLoss: 4.7378\tLR: 6.404859\n",
      "Training Epoch: 65 [2688/50000]\tLoss: 4.7706\tLR: 6.405115\n",
      "Training Epoch: 65 [2816/50000]\tLoss: 4.8601\tLR: 6.405371\n",
      "Training Epoch: 65 [2944/50000]\tLoss: 4.7761\tLR: 6.405627\n",
      "Training Epoch: 65 [3072/50000]\tLoss: 4.7510\tLR: 6.405882\n",
      "Training Epoch: 65 [3200/50000]\tLoss: 4.6544\tLR: 6.406138\n",
      "Training Epoch: 65 [3328/50000]\tLoss: 4.8518\tLR: 6.406394\n",
      "Training Epoch: 65 [3456/50000]\tLoss: 4.7264\tLR: 6.406650\n",
      "Training Epoch: 65 [3584/50000]\tLoss: 4.7245\tLR: 6.406905\n",
      "Training Epoch: 65 [3712/50000]\tLoss: 4.7822\tLR: 6.407161\n",
      "Training Epoch: 65 [3840/50000]\tLoss: 4.8122\tLR: 6.407417\n",
      "Training Epoch: 65 [3968/50000]\tLoss: 4.7960\tLR: 6.407673\n",
      "Training Epoch: 65 [4096/50000]\tLoss: 4.7657\tLR: 6.407928\n",
      "Training Epoch: 65 [4224/50000]\tLoss: 4.7069\tLR: 6.408184\n",
      "Training Epoch: 65 [4352/50000]\tLoss: 4.6647\tLR: 6.408440\n",
      "Training Epoch: 65 [4480/50000]\tLoss: 4.7778\tLR: 6.408696\n",
      "Training Epoch: 65 [4608/50000]\tLoss: 4.7796\tLR: 6.408951\n",
      "Training Epoch: 65 [4736/50000]\tLoss: 4.8515\tLR: 6.409207\n",
      "Training Epoch: 65 [4864/50000]\tLoss: 4.7054\tLR: 6.409463\n",
      "Training Epoch: 65 [4992/50000]\tLoss: 4.7863\tLR: 6.409719\n",
      "Training Epoch: 65 [5120/50000]\tLoss: 4.8110\tLR: 6.409974\n",
      "Training Epoch: 65 [5248/50000]\tLoss: 4.7348\tLR: 6.410230\n",
      "Training Epoch: 65 [5376/50000]\tLoss: 4.8103\tLR: 6.410486\n",
      "Training Epoch: 65 [5504/50000]\tLoss: 4.6461\tLR: 6.410742\n",
      "Training Epoch: 65 [5632/50000]\tLoss: 4.7697\tLR: 6.410997\n",
      "Training Epoch: 65 [5760/50000]\tLoss: 4.7102\tLR: 6.411253\n",
      "Training Epoch: 65 [5888/50000]\tLoss: 4.7929\tLR: 6.411509\n",
      "Training Epoch: 65 [6016/50000]\tLoss: 4.7579\tLR: 6.411765\n",
      "Training Epoch: 65 [6144/50000]\tLoss: 4.7409\tLR: 6.412020\n",
      "Training Epoch: 65 [6272/50000]\tLoss: 4.7865\tLR: 6.412276\n",
      "Training Epoch: 65 [6400/50000]\tLoss: 4.7206\tLR: 6.412532\n",
      "Training Epoch: 65 [6528/50000]\tLoss: 4.7626\tLR: 6.412788\n",
      "Training Epoch: 65 [6656/50000]\tLoss: 4.6891\tLR: 6.413043\n",
      "Training Epoch: 65 [6784/50000]\tLoss: 4.8136\tLR: 6.413299\n",
      "Training Epoch: 65 [6912/50000]\tLoss: 4.7051\tLR: 6.413555\n",
      "Training Epoch: 65 [7040/50000]\tLoss: 4.6463\tLR: 6.413811\n",
      "Training Epoch: 65 [7168/50000]\tLoss: 4.7694\tLR: 6.414066\n",
      "Training Epoch: 65 [7296/50000]\tLoss: 4.7001\tLR: 6.414322\n",
      "Training Epoch: 65 [7424/50000]\tLoss: 4.7300\tLR: 6.414578\n",
      "Training Epoch: 65 [7552/50000]\tLoss: 4.6651\tLR: 6.414834\n",
      "Training Epoch: 65 [7680/50000]\tLoss: 4.6763\tLR: 6.415090\n",
      "Training Epoch: 65 [7808/50000]\tLoss: 4.7329\tLR: 6.415345\n",
      "Training Epoch: 65 [7936/50000]\tLoss: 4.7090\tLR: 6.415601\n",
      "Training Epoch: 65 [8064/50000]\tLoss: 4.7880\tLR: 6.415857\n",
      "Training Epoch: 65 [8192/50000]\tLoss: 4.9101\tLR: 6.416113\n",
      "Training Epoch: 65 [8320/50000]\tLoss: 4.6941\tLR: 6.416368\n",
      "Training Epoch: 65 [8448/50000]\tLoss: 4.7230\tLR: 6.416624\n",
      "Training Epoch: 65 [8576/50000]\tLoss: 4.7046\tLR: 6.416880\n",
      "Training Epoch: 65 [8704/50000]\tLoss: 4.7764\tLR: 6.417136\n",
      "Training Epoch: 65 [8832/50000]\tLoss: 4.6863\tLR: 6.417391\n",
      "Training Epoch: 65 [8960/50000]\tLoss: 4.7364\tLR: 6.417647\n",
      "Training Epoch: 65 [9088/50000]\tLoss: 4.8035\tLR: 6.417903\n",
      "Training Epoch: 65 [9216/50000]\tLoss: 4.7052\tLR: 6.418159\n",
      "Training Epoch: 65 [9344/50000]\tLoss: 4.7500\tLR: 6.418414\n",
      "Training Epoch: 65 [9472/50000]\tLoss: 4.6577\tLR: 6.418670\n",
      "Training Epoch: 65 [9600/50000]\tLoss: 4.7206\tLR: 6.418926\n",
      "Training Epoch: 65 [9728/50000]\tLoss: 4.7512\tLR: 6.419182\n",
      "Training Epoch: 65 [9856/50000]\tLoss: 4.7575\tLR: 6.419437\n",
      "Training Epoch: 65 [9984/50000]\tLoss: 4.7598\tLR: 6.419693\n",
      "Training Epoch: 65 [10112/50000]\tLoss: 4.7466\tLR: 6.419949\n",
      "Training Epoch: 65 [10240/50000]\tLoss: 4.7818\tLR: 6.420205\n",
      "Training Epoch: 65 [10368/50000]\tLoss: 4.7222\tLR: 6.420460\n",
      "Training Epoch: 65 [10496/50000]\tLoss: 4.7800\tLR: 6.420716\n",
      "Training Epoch: 65 [10624/50000]\tLoss: 4.7605\tLR: 6.420972\n",
      "Training Epoch: 65 [10752/50000]\tLoss: 4.6817\tLR: 6.421228\n",
      "Training Epoch: 65 [10880/50000]\tLoss: 4.8218\tLR: 6.421483\n",
      "Training Epoch: 65 [11008/50000]\tLoss: 4.7149\tLR: 6.421739\n",
      "Training Epoch: 65 [11136/50000]\tLoss: 4.7499\tLR: 6.421995\n",
      "Training Epoch: 65 [11264/50000]\tLoss: 4.6924\tLR: 6.422251\n",
      "Training Epoch: 65 [11392/50000]\tLoss: 4.7422\tLR: 6.422506\n",
      "Training Epoch: 65 [11520/50000]\tLoss: 4.6406\tLR: 6.422762\n",
      "Training Epoch: 65 [11648/50000]\tLoss: 4.7011\tLR: 6.423018\n",
      "Training Epoch: 65 [11776/50000]\tLoss: 4.8012\tLR: 6.423274\n",
      "Training Epoch: 65 [11904/50000]\tLoss: 4.7625\tLR: 6.423529\n",
      "Training Epoch: 65 [12032/50000]\tLoss: 4.8024\tLR: 6.423785\n",
      "Training Epoch: 65 [12160/50000]\tLoss: 4.7237\tLR: 6.424041\n",
      "Training Epoch: 65 [12288/50000]\tLoss: 4.7824\tLR: 6.424297\n",
      "Training Epoch: 65 [12416/50000]\tLoss: 4.7777\tLR: 6.424552\n",
      "Training Epoch: 65 [12544/50000]\tLoss: 4.7183\tLR: 6.424808\n",
      "Training Epoch: 65 [12672/50000]\tLoss: 4.7327\tLR: 6.425064\n",
      "Training Epoch: 65 [12800/50000]\tLoss: 4.7077\tLR: 6.425320\n",
      "Training Epoch: 65 [12928/50000]\tLoss: 4.7858\tLR: 6.425575\n",
      "Training Epoch: 65 [13056/50000]\tLoss: 4.6590\tLR: 6.425831\n",
      "Training Epoch: 65 [13184/50000]\tLoss: 4.7699\tLR: 6.426087\n",
      "Training Epoch: 65 [13312/50000]\tLoss: 4.6790\tLR: 6.426343\n",
      "Training Epoch: 65 [13440/50000]\tLoss: 4.7415\tLR: 6.426598\n",
      "Training Epoch: 65 [13568/50000]\tLoss: 4.7653\tLR: 6.426854\n",
      "Training Epoch: 65 [13696/50000]\tLoss: 4.7033\tLR: 6.427110\n",
      "Training Epoch: 65 [13824/50000]\tLoss: 4.7897\tLR: 6.427366\n",
      "Training Epoch: 65 [13952/50000]\tLoss: 4.6362\tLR: 6.427621\n",
      "Training Epoch: 65 [14080/50000]\tLoss: 4.7226\tLR: 6.427877\n",
      "Training Epoch: 65 [14208/50000]\tLoss: 4.6819\tLR: 6.428133\n",
      "Training Epoch: 65 [14336/50000]\tLoss: 4.7637\tLR: 6.428389\n",
      "Training Epoch: 65 [14464/50000]\tLoss: 4.6446\tLR: 6.428645\n",
      "Training Epoch: 65 [14592/50000]\tLoss: 4.7707\tLR: 6.428900\n",
      "Training Epoch: 65 [14720/50000]\tLoss: 4.6982\tLR: 6.429156\n",
      "Training Epoch: 65 [14848/50000]\tLoss: 4.8072\tLR: 6.429412\n",
      "Training Epoch: 65 [14976/50000]\tLoss: 4.7446\tLR: 6.429668\n",
      "Training Epoch: 65 [15104/50000]\tLoss: 4.7187\tLR: 6.429923\n",
      "Training Epoch: 65 [15232/50000]\tLoss: 4.7389\tLR: 6.430179\n",
      "Training Epoch: 65 [15360/50000]\tLoss: 4.7543\tLR: 6.430435\n",
      "Training Epoch: 65 [15488/50000]\tLoss: 4.7283\tLR: 6.430691\n",
      "Training Epoch: 65 [15616/50000]\tLoss: 4.7558\tLR: 6.430946\n",
      "Training Epoch: 65 [15744/50000]\tLoss: 4.6343\tLR: 6.431202\n",
      "Training Epoch: 65 [15872/50000]\tLoss: 4.7026\tLR: 6.431458\n",
      "Training Epoch: 65 [16000/50000]\tLoss: 4.7690\tLR: 6.431714\n",
      "Training Epoch: 65 [16128/50000]\tLoss: 4.6861\tLR: 6.431969\n",
      "Training Epoch: 65 [16256/50000]\tLoss: 4.7832\tLR: 6.432225\n",
      "Training Epoch: 65 [16384/50000]\tLoss: 4.7745\tLR: 6.432481\n",
      "Training Epoch: 65 [16512/50000]\tLoss: 4.7189\tLR: 6.432737\n",
      "Training Epoch: 65 [16640/50000]\tLoss: 4.9174\tLR: 6.432992\n",
      "Training Epoch: 65 [16768/50000]\tLoss: 4.6193\tLR: 6.433248\n",
      "Training Epoch: 65 [16896/50000]\tLoss: 4.7198\tLR: 6.433504\n",
      "Training Epoch: 65 [17024/50000]\tLoss: 4.7422\tLR: 6.433760\n",
      "Training Epoch: 65 [17152/50000]\tLoss: 4.7009\tLR: 6.434015\n",
      "Training Epoch: 65 [17280/50000]\tLoss: 4.7395\tLR: 6.434271\n",
      "Training Epoch: 65 [17408/50000]\tLoss: 4.7092\tLR: 6.434527\n",
      "Training Epoch: 65 [17536/50000]\tLoss: 4.7416\tLR: 6.434783\n",
      "Training Epoch: 65 [17664/50000]\tLoss: 4.7682\tLR: 6.435038\n",
      "Training Epoch: 65 [17792/50000]\tLoss: 4.7766\tLR: 6.435294\n",
      "Training Epoch: 65 [17920/50000]\tLoss: 4.6634\tLR: 6.435550\n",
      "Training Epoch: 65 [18048/50000]\tLoss: 4.7423\tLR: 6.435806\n",
      "Training Epoch: 65 [18176/50000]\tLoss: 4.6972\tLR: 6.436061\n",
      "Training Epoch: 65 [18304/50000]\tLoss: 4.8163\tLR: 6.436317\n",
      "Training Epoch: 65 [18432/50000]\tLoss: 4.7501\tLR: 6.436573\n",
      "Training Epoch: 65 [18560/50000]\tLoss: 4.7855\tLR: 6.436829\n",
      "Training Epoch: 65 [18688/50000]\tLoss: 4.6984\tLR: 6.437084\n",
      "Training Epoch: 65 [18816/50000]\tLoss: 4.6929\tLR: 6.437340\n",
      "Training Epoch: 65 [18944/50000]\tLoss: 4.7178\tLR: 6.437596\n",
      "Training Epoch: 65 [19072/50000]\tLoss: 4.6578\tLR: 6.437852\n",
      "Training Epoch: 65 [19200/50000]\tLoss: 4.7703\tLR: 6.438107\n",
      "Training Epoch: 65 [19328/50000]\tLoss: 4.7056\tLR: 6.438363\n",
      "Training Epoch: 65 [19456/50000]\tLoss: 4.7938\tLR: 6.438619\n",
      "Training Epoch: 65 [19584/50000]\tLoss: 4.7346\tLR: 6.438875\n",
      "Training Epoch: 65 [19712/50000]\tLoss: 4.7098\tLR: 6.439130\n",
      "Training Epoch: 65 [19840/50000]\tLoss: 4.7697\tLR: 6.439386\n",
      "Training Epoch: 65 [19968/50000]\tLoss: 4.6465\tLR: 6.439642\n",
      "Training Epoch: 65 [20096/50000]\tLoss: 4.7268\tLR: 6.439898\n",
      "Training Epoch: 65 [20224/50000]\tLoss: 4.7679\tLR: 6.440153\n",
      "Training Epoch: 65 [20352/50000]\tLoss: 4.6934\tLR: 6.440409\n",
      "Training Epoch: 65 [20480/50000]\tLoss: 4.6941\tLR: 6.440665\n",
      "Training Epoch: 65 [20608/50000]\tLoss: 4.6982\tLR: 6.440921\n",
      "Training Epoch: 65 [20736/50000]\tLoss: 4.7344\tLR: 6.441176\n",
      "Training Epoch: 65 [20864/50000]\tLoss: 4.6976\tLR: 6.441432\n",
      "Training Epoch: 65 [20992/50000]\tLoss: 4.7578\tLR: 6.441688\n",
      "Training Epoch: 65 [21120/50000]\tLoss: 4.7653\tLR: 6.441944\n",
      "Training Epoch: 65 [21248/50000]\tLoss: 4.8686\tLR: 6.442199\n",
      "Training Epoch: 65 [21376/50000]\tLoss: 4.7599\tLR: 6.442455\n",
      "Training Epoch: 65 [21504/50000]\tLoss: 4.7822\tLR: 6.442711\n",
      "Training Epoch: 65 [21632/50000]\tLoss: 4.8451\tLR: 6.442967\n",
      "Training Epoch: 65 [21760/50000]\tLoss: 4.7184\tLR: 6.443223\n",
      "Training Epoch: 65 [21888/50000]\tLoss: 4.6874\tLR: 6.443478\n",
      "Training Epoch: 65 [22016/50000]\tLoss: 4.8094\tLR: 6.443734\n",
      "Training Epoch: 65 [22144/50000]\tLoss: 4.7698\tLR: 6.443990\n",
      "Training Epoch: 65 [22272/50000]\tLoss: 4.7811\tLR: 6.444246\n",
      "Training Epoch: 65 [22400/50000]\tLoss: 4.8266\tLR: 6.444501\n",
      "Training Epoch: 65 [22528/50000]\tLoss: 4.7784\tLR: 6.444757\n",
      "Training Epoch: 65 [22656/50000]\tLoss: 4.7795\tLR: 6.445013\n",
      "Training Epoch: 65 [22784/50000]\tLoss: 4.8062\tLR: 6.445269\n",
      "Training Epoch: 65 [22912/50000]\tLoss: 4.7147\tLR: 6.445524\n",
      "Training Epoch: 65 [23040/50000]\tLoss: 4.7805\tLR: 6.445780\n",
      "Training Epoch: 65 [23168/50000]\tLoss: 4.7527\tLR: 6.446036\n",
      "Training Epoch: 65 [23296/50000]\tLoss: 4.7321\tLR: 6.446292\n",
      "Training Epoch: 65 [23424/50000]\tLoss: 4.7096\tLR: 6.446547\n",
      "Training Epoch: 65 [23552/50000]\tLoss: 4.7601\tLR: 6.446803\n",
      "Training Epoch: 65 [23680/50000]\tLoss: 4.7011\tLR: 6.447059\n",
      "Training Epoch: 65 [23808/50000]\tLoss: 4.7528\tLR: 6.447315\n",
      "Training Epoch: 65 [23936/50000]\tLoss: 4.6785\tLR: 6.447570\n",
      "Training Epoch: 65 [24064/50000]\tLoss: 4.7854\tLR: 6.447826\n",
      "Training Epoch: 65 [24192/50000]\tLoss: 4.7068\tLR: 6.448082\n",
      "Training Epoch: 65 [24320/50000]\tLoss: 4.7046\tLR: 6.448338\n",
      "Training Epoch: 65 [24448/50000]\tLoss: 4.7597\tLR: 6.448593\n",
      "Training Epoch: 65 [24576/50000]\tLoss: 4.8069\tLR: 6.448849\n",
      "Training Epoch: 65 [24704/50000]\tLoss: 4.7808\tLR: 6.449105\n",
      "Training Epoch: 65 [24832/50000]\tLoss: 4.7640\tLR: 6.449361\n",
      "Training Epoch: 65 [24960/50000]\tLoss: 4.7263\tLR: 6.449616\n",
      "Training Epoch: 65 [25088/50000]\tLoss: 4.6738\tLR: 6.449872\n",
      "Training Epoch: 65 [25216/50000]\tLoss: 4.6457\tLR: 6.450128\n",
      "Training Epoch: 65 [25344/50000]\tLoss: 4.8047\tLR: 6.450384\n",
      "Training Epoch: 65 [25472/50000]\tLoss: 4.7494\tLR: 6.450639\n",
      "Training Epoch: 65 [25600/50000]\tLoss: 4.7033\tLR: 6.450895\n",
      "Training Epoch: 65 [25728/50000]\tLoss: 4.7279\tLR: 6.451151\n",
      "Training Epoch: 65 [25856/50000]\tLoss: 4.7419\tLR: 6.451407\n",
      "Training Epoch: 65 [25984/50000]\tLoss: 4.7186\tLR: 6.451662\n",
      "Training Epoch: 65 [26112/50000]\tLoss: 4.8042\tLR: 6.451918\n",
      "Training Epoch: 65 [26240/50000]\tLoss: 4.7308\tLR: 6.452174\n",
      "Training Epoch: 65 [26368/50000]\tLoss: 4.7089\tLR: 6.452430\n",
      "Training Epoch: 65 [26496/50000]\tLoss: 4.6561\tLR: 6.452685\n",
      "Training Epoch: 65 [26624/50000]\tLoss: 4.7132\tLR: 6.452941\n",
      "Training Epoch: 65 [26752/50000]\tLoss: 4.7064\tLR: 6.453197\n",
      "Training Epoch: 65 [26880/50000]\tLoss: 4.6695\tLR: 6.453453\n",
      "Training Epoch: 65 [27008/50000]\tLoss: 4.7805\tLR: 6.453708\n",
      "Training Epoch: 65 [27136/50000]\tLoss: 4.7966\tLR: 6.453964\n",
      "Training Epoch: 65 [27264/50000]\tLoss: 4.7967\tLR: 6.454220\n",
      "Training Epoch: 65 [27392/50000]\tLoss: 4.6665\tLR: 6.454476\n",
      "Training Epoch: 65 [27520/50000]\tLoss: 4.6453\tLR: 6.454731\n",
      "Training Epoch: 65 [27648/50000]\tLoss: 4.6045\tLR: 6.454987\n",
      "Training Epoch: 65 [27776/50000]\tLoss: 4.7201\tLR: 6.455243\n",
      "Training Epoch: 65 [27904/50000]\tLoss: 4.6948\tLR: 6.455499\n",
      "Training Epoch: 65 [28032/50000]\tLoss: 4.7216\tLR: 6.455754\n",
      "Training Epoch: 65 [28160/50000]\tLoss: 4.7537\tLR: 6.456010\n",
      "Training Epoch: 65 [28288/50000]\tLoss: 4.7930\tLR: 6.456266\n",
      "Training Epoch: 65 [28416/50000]\tLoss: 4.6563\tLR: 6.456522\n",
      "Training Epoch: 65 [28544/50000]\tLoss: 4.8167\tLR: 6.456777\n",
      "Training Epoch: 65 [28672/50000]\tLoss: 4.6909\tLR: 6.457033\n",
      "Training Epoch: 65 [28800/50000]\tLoss: 4.6523\tLR: 6.457289\n",
      "Training Epoch: 65 [28928/50000]\tLoss: 4.6622\tLR: 6.457545\n",
      "Training Epoch: 65 [29056/50000]\tLoss: 4.6473\tLR: 6.457801\n",
      "Training Epoch: 65 [29184/50000]\tLoss: 4.7126\tLR: 6.458056\n",
      "Training Epoch: 65 [29312/50000]\tLoss: 4.7908\tLR: 6.458312\n",
      "Training Epoch: 65 [29440/50000]\tLoss: 4.6302\tLR: 6.458568\n",
      "Training Epoch: 65 [29568/50000]\tLoss: 4.7940\tLR: 6.458824\n",
      "Training Epoch: 65 [29696/50000]\tLoss: 4.7146\tLR: 6.459079\n",
      "Training Epoch: 65 [29824/50000]\tLoss: 4.7232\tLR: 6.459335\n",
      "Training Epoch: 65 [29952/50000]\tLoss: 4.7589\tLR: 6.459591\n",
      "Training Epoch: 65 [30080/50000]\tLoss: 4.7009\tLR: 6.459847\n",
      "Training Epoch: 65 [30208/50000]\tLoss: 4.7049\tLR: 6.460102\n",
      "Training Epoch: 65 [30336/50000]\tLoss: 4.7580\tLR: 6.460358\n",
      "Training Epoch: 65 [30464/50000]\tLoss: 4.7045\tLR: 6.460614\n",
      "Training Epoch: 65 [30592/50000]\tLoss: 4.6837\tLR: 6.460870\n",
      "Training Epoch: 65 [30720/50000]\tLoss: 4.7009\tLR: 6.461125\n",
      "Training Epoch: 65 [30848/50000]\tLoss: 4.7973\tLR: 6.461381\n",
      "Training Epoch: 65 [30976/50000]\tLoss: 4.6980\tLR: 6.461637\n",
      "Training Epoch: 65 [31104/50000]\tLoss: 4.7153\tLR: 6.461893\n",
      "Training Epoch: 65 [31232/50000]\tLoss: 4.7621\tLR: 6.462148\n",
      "Training Epoch: 65 [31360/50000]\tLoss: 4.7571\tLR: 6.462404\n",
      "Training Epoch: 65 [31488/50000]\tLoss: 4.6930\tLR: 6.462660\n",
      "Training Epoch: 65 [31616/50000]\tLoss: 4.6796\tLR: 6.462916\n",
      "Training Epoch: 65 [31744/50000]\tLoss: 4.7084\tLR: 6.463171\n",
      "Training Epoch: 65 [31872/50000]\tLoss: 4.7389\tLR: 6.463427\n",
      "Training Epoch: 65 [32000/50000]\tLoss: 4.6306\tLR: 6.463683\n",
      "Training Epoch: 65 [32128/50000]\tLoss: 4.7229\tLR: 6.463939\n",
      "Training Epoch: 65 [32256/50000]\tLoss: 4.7266\tLR: 6.464194\n",
      "Training Epoch: 65 [32384/50000]\tLoss: 4.7283\tLR: 6.464450\n",
      "Training Epoch: 65 [32512/50000]\tLoss: 4.6979\tLR: 6.464706\n",
      "Training Epoch: 65 [32640/50000]\tLoss: 4.7019\tLR: 6.464962\n",
      "Training Epoch: 65 [32768/50000]\tLoss: 4.7988\tLR: 6.465217\n",
      "Training Epoch: 65 [32896/50000]\tLoss: 4.7241\tLR: 6.465473\n",
      "Training Epoch: 65 [33024/50000]\tLoss: 4.6963\tLR: 6.465729\n",
      "Training Epoch: 65 [33152/50000]\tLoss: 4.6332\tLR: 6.465985\n",
      "Training Epoch: 65 [33280/50000]\tLoss: 4.7224\tLR: 6.466240\n",
      "Training Epoch: 65 [33408/50000]\tLoss: 4.6732\tLR: 6.466496\n",
      "Training Epoch: 65 [33536/50000]\tLoss: 4.7323\tLR: 6.466752\n",
      "Training Epoch: 65 [33664/50000]\tLoss: 4.7666\tLR: 6.467008\n",
      "Training Epoch: 65 [33792/50000]\tLoss: 4.7734\tLR: 6.467263\n",
      "Training Epoch: 65 [33920/50000]\tLoss: 4.6945\tLR: 6.467519\n",
      "Training Epoch: 65 [34048/50000]\tLoss: 4.6724\tLR: 6.467775\n",
      "Training Epoch: 65 [34176/50000]\tLoss: 4.7578\tLR: 6.468031\n",
      "Training Epoch: 65 [34304/50000]\tLoss: 4.7655\tLR: 6.468286\n",
      "Training Epoch: 65 [34432/50000]\tLoss: 4.7308\tLR: 6.468542\n",
      "Training Epoch: 65 [34560/50000]\tLoss: 4.6772\tLR: 6.468798\n",
      "Training Epoch: 65 [34688/50000]\tLoss: 4.6709\tLR: 6.469054\n",
      "Training Epoch: 65 [34816/50000]\tLoss: 4.7932\tLR: 6.469309\n",
      "Training Epoch: 65 [34944/50000]\tLoss: 4.7326\tLR: 6.469565\n",
      "Training Epoch: 65 [35072/50000]\tLoss: 4.7626\tLR: 6.469821\n",
      "Training Epoch: 65 [35200/50000]\tLoss: 4.7682\tLR: 6.470077\n",
      "Training Epoch: 65 [35328/50000]\tLoss: 4.7737\tLR: 6.470332\n",
      "Training Epoch: 65 [35456/50000]\tLoss: 4.7929\tLR: 6.470588\n",
      "Training Epoch: 65 [35584/50000]\tLoss: 4.7404\tLR: 6.470844\n",
      "Training Epoch: 65 [35712/50000]\tLoss: 4.6992\tLR: 6.471100\n",
      "Training Epoch: 65 [35840/50000]\tLoss: 4.7206\tLR: 6.471355\n",
      "Training Epoch: 65 [35968/50000]\tLoss: 4.7582\tLR: 6.471611\n",
      "Training Epoch: 65 [36096/50000]\tLoss: 4.6993\tLR: 6.471867\n",
      "Training Epoch: 65 [36224/50000]\tLoss: 4.7370\tLR: 6.472123\n",
      "Training Epoch: 65 [36352/50000]\tLoss: 4.7073\tLR: 6.472379\n",
      "Training Epoch: 65 [36480/50000]\tLoss: 4.7334\tLR: 6.472634\n",
      "Training Epoch: 65 [36608/50000]\tLoss: 4.8092\tLR: 6.472890\n",
      "Training Epoch: 65 [36736/50000]\tLoss: 4.6549\tLR: 6.473146\n",
      "Training Epoch: 65 [36864/50000]\tLoss: 4.8270\tLR: 6.473402\n",
      "Training Epoch: 65 [36992/50000]\tLoss: 4.8239\tLR: 6.473657\n",
      "Training Epoch: 65 [37120/50000]\tLoss: 4.7244\tLR: 6.473913\n",
      "Training Epoch: 65 [37248/50000]\tLoss: 4.7433\tLR: 6.474169\n",
      "Training Epoch: 65 [37376/50000]\tLoss: 4.7404\tLR: 6.474425\n",
      "Training Epoch: 65 [37504/50000]\tLoss: 4.7654\tLR: 6.474680\n",
      "Training Epoch: 65 [37632/50000]\tLoss: 4.7050\tLR: 6.474936\n",
      "Training Epoch: 65 [37760/50000]\tLoss: 4.6935\tLR: 6.475192\n",
      "Training Epoch: 65 [37888/50000]\tLoss: 4.7279\tLR: 6.475448\n",
      "Training Epoch: 65 [38016/50000]\tLoss: 4.7986\tLR: 6.475703\n",
      "Training Epoch: 65 [38144/50000]\tLoss: 4.7830\tLR: 6.475959\n",
      "Training Epoch: 65 [38272/50000]\tLoss: 4.8004\tLR: 6.476215\n",
      "Training Epoch: 65 [38400/50000]\tLoss: 4.7168\tLR: 6.476471\n",
      "Training Epoch: 65 [38528/50000]\tLoss: 4.6668\tLR: 6.476726\n",
      "Training Epoch: 65 [38656/50000]\tLoss: 4.7437\tLR: 6.476982\n",
      "Training Epoch: 65 [38784/50000]\tLoss: 4.7394\tLR: 6.477238\n",
      "Training Epoch: 65 [38912/50000]\tLoss: 4.7298\tLR: 6.477494\n",
      "Training Epoch: 65 [39040/50000]\tLoss: 4.7094\tLR: 6.477749\n",
      "Training Epoch: 65 [39168/50000]\tLoss: 4.8321\tLR: 6.478005\n",
      "Training Epoch: 65 [39296/50000]\tLoss: 4.7506\tLR: 6.478261\n",
      "Training Epoch: 65 [39424/50000]\tLoss: 4.7190\tLR: 6.478517\n",
      "Training Epoch: 65 [39552/50000]\tLoss: 4.6642\tLR: 6.478772\n",
      "Training Epoch: 65 [39680/50000]\tLoss: 4.7339\tLR: 6.479028\n",
      "Training Epoch: 65 [39808/50000]\tLoss: 4.7127\tLR: 6.479284\n",
      "Training Epoch: 65 [39936/50000]\tLoss: 4.6747\tLR: 6.479540\n",
      "Training Epoch: 65 [40064/50000]\tLoss: 4.7153\tLR: 6.479795\n",
      "Training Epoch: 65 [40192/50000]\tLoss: 4.7464\tLR: 6.480051\n",
      "Training Epoch: 65 [40320/50000]\tLoss: 4.7309\tLR: 6.480307\n",
      "Training Epoch: 65 [40448/50000]\tLoss: 4.7506\tLR: 6.480563\n",
      "Training Epoch: 65 [40576/50000]\tLoss: 4.8040\tLR: 6.480818\n",
      "Training Epoch: 65 [40704/50000]\tLoss: 4.7491\tLR: 6.481074\n",
      "Training Epoch: 65 [40832/50000]\tLoss: 4.7460\tLR: 6.481330\n",
      "Training Epoch: 65 [40960/50000]\tLoss: 4.6876\tLR: 6.481586\n",
      "Training Epoch: 65 [41088/50000]\tLoss: 4.7465\tLR: 6.481841\n",
      "Training Epoch: 65 [41216/50000]\tLoss: 4.7720\tLR: 6.482097\n",
      "Training Epoch: 65 [41344/50000]\tLoss: 4.7415\tLR: 6.482353\n",
      "Training Epoch: 65 [41472/50000]\tLoss: 4.6612\tLR: 6.482609\n",
      "Training Epoch: 65 [41600/50000]\tLoss: 4.7307\tLR: 6.482864\n",
      "Training Epoch: 65 [41728/50000]\tLoss: 4.6542\tLR: 6.483120\n",
      "Training Epoch: 65 [41856/50000]\tLoss: 4.7301\tLR: 6.483376\n",
      "Training Epoch: 65 [41984/50000]\tLoss: 4.6383\tLR: 6.483632\n",
      "Training Epoch: 65 [42112/50000]\tLoss: 4.7588\tLR: 6.483887\n",
      "Training Epoch: 65 [42240/50000]\tLoss: 4.7664\tLR: 6.484143\n",
      "Training Epoch: 65 [42368/50000]\tLoss: 4.7552\tLR: 6.484399\n",
      "Training Epoch: 65 [42496/50000]\tLoss: 4.7807\tLR: 6.484655\n",
      "Training Epoch: 65 [42624/50000]\tLoss: 4.7255\tLR: 6.484910\n",
      "Training Epoch: 65 [42752/50000]\tLoss: 4.7114\tLR: 6.485166\n",
      "Training Epoch: 65 [42880/50000]\tLoss: 4.7547\tLR: 6.485422\n",
      "Training Epoch: 65 [43008/50000]\tLoss: 4.7445\tLR: 6.485678\n",
      "Training Epoch: 65 [43136/50000]\tLoss: 4.7088\tLR: 6.485934\n",
      "Training Epoch: 65 [43264/50000]\tLoss: 4.7037\tLR: 6.486189\n",
      "Training Epoch: 65 [43392/50000]\tLoss: 4.7213\tLR: 6.486445\n",
      "Training Epoch: 65 [43520/50000]\tLoss: 4.8443\tLR: 6.486701\n",
      "Training Epoch: 65 [43648/50000]\tLoss: 4.6792\tLR: 6.486957\n",
      "Training Epoch: 65 [43776/50000]\tLoss: 4.7857\tLR: 6.487212\n",
      "Training Epoch: 65 [43904/50000]\tLoss: 4.6475\tLR: 6.487468\n",
      "Training Epoch: 65 [44032/50000]\tLoss: 4.7076\tLR: 6.487724\n",
      "Training Epoch: 65 [44160/50000]\tLoss: 4.7542\tLR: 6.487980\n",
      "Training Epoch: 65 [44288/50000]\tLoss: 4.6725\tLR: 6.488235\n",
      "Training Epoch: 65 [44416/50000]\tLoss: 4.8109\tLR: 6.488491\n",
      "Training Epoch: 65 [44544/50000]\tLoss: 4.7402\tLR: 6.488747\n",
      "Training Epoch: 65 [44672/50000]\tLoss: 4.7980\tLR: 6.489003\n",
      "Training Epoch: 65 [44800/50000]\tLoss: 4.7371\tLR: 6.489258\n",
      "Training Epoch: 65 [44928/50000]\tLoss: 4.7617\tLR: 6.489514\n",
      "Training Epoch: 65 [45056/50000]\tLoss: 4.7423\tLR: 6.489770\n",
      "Training Epoch: 65 [45184/50000]\tLoss: 4.7251\tLR: 6.490026\n",
      "Training Epoch: 65 [45312/50000]\tLoss: 4.6707\tLR: 6.490281\n",
      "Training Epoch: 65 [45440/50000]\tLoss: 4.7556\tLR: 6.490537\n",
      "Training Epoch: 65 [45568/50000]\tLoss: 4.7455\tLR: 6.490793\n",
      "Training Epoch: 65 [45696/50000]\tLoss: 4.8029\tLR: 6.491049\n",
      "Training Epoch: 65 [45824/50000]\tLoss: 4.7272\tLR: 6.491304\n",
      "Training Epoch: 65 [45952/50000]\tLoss: 4.7036\tLR: 6.491560\n",
      "Training Epoch: 65 [46080/50000]\tLoss: 4.7119\tLR: 6.491816\n",
      "Training Epoch: 65 [46208/50000]\tLoss: 4.7846\tLR: 6.492072\n",
      "Training Epoch: 65 [46336/50000]\tLoss: 4.9074\tLR: 6.492327\n",
      "Training Epoch: 65 [46464/50000]\tLoss: 4.7527\tLR: 6.492583\n",
      "Training Epoch: 65 [46592/50000]\tLoss: 4.7471\tLR: 6.492839\n",
      "Training Epoch: 65 [46720/50000]\tLoss: 4.6558\tLR: 6.493095\n",
      "Training Epoch: 65 [46848/50000]\tLoss: 4.7923\tLR: 6.493350\n",
      "Training Epoch: 65 [46976/50000]\tLoss: 4.7038\tLR: 6.493606\n",
      "Training Epoch: 65 [47104/50000]\tLoss: 4.7328\tLR: 6.493862\n",
      "Training Epoch: 65 [47232/50000]\tLoss: 4.6976\tLR: 6.494118\n",
      "Training Epoch: 65 [47360/50000]\tLoss: 4.7360\tLR: 6.494373\n",
      "Training Epoch: 65 [47488/50000]\tLoss: 4.7189\tLR: 6.494629\n",
      "Training Epoch: 65 [47616/50000]\tLoss: 4.8234\tLR: 6.494885\n",
      "Training Epoch: 65 [47744/50000]\tLoss: 4.7196\tLR: 6.495141\n",
      "Training Epoch: 65 [47872/50000]\tLoss: 4.7564\tLR: 6.495396\n",
      "Training Epoch: 65 [48000/50000]\tLoss: 4.6283\tLR: 6.495652\n",
      "Training Epoch: 65 [48128/50000]\tLoss: 4.6878\tLR: 6.495908\n",
      "Training Epoch: 65 [48256/50000]\tLoss: 4.8323\tLR: 6.496164\n",
      "Training Epoch: 65 [48384/50000]\tLoss: 4.7181\tLR: 6.496419\n",
      "Training Epoch: 65 [48512/50000]\tLoss: 4.7055\tLR: 6.496675\n",
      "Training Epoch: 65 [48640/50000]\tLoss: 4.7365\tLR: 6.496931\n",
      "Training Epoch: 65 [48768/50000]\tLoss: 4.6430\tLR: 6.497187\n",
      "Training Epoch: 65 [48896/50000]\tLoss: 4.7573\tLR: 6.497442\n",
      "Training Epoch: 65 [49024/50000]\tLoss: 4.6939\tLR: 6.497698\n",
      "Training Epoch: 65 [49152/50000]\tLoss: 4.6723\tLR: 6.497954\n",
      "Training Epoch: 65 [49280/50000]\tLoss: 4.7078\tLR: 6.498210\n",
      "Training Epoch: 65 [49408/50000]\tLoss: 4.7093\tLR: 6.498465\n",
      "Training Epoch: 65 [49536/50000]\tLoss: 4.6950\tLR: 6.498721\n",
      "Training Epoch: 65 [49664/50000]\tLoss: 4.8255\tLR: 6.498977\n",
      "Training Epoch: 65 [49792/50000]\tLoss: 4.7535\tLR: 6.499233\n",
      "Training Epoch: 65 [49920/50000]\tLoss: 4.7850\tLR: 6.499488\n",
      "Training Epoch: 65 [50000/50000]\tLoss: 4.7495\tLR: 6.499744\n",
      "epoch 65 training time consumed: 488.88s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   91126 GB |   91125 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   90846 GB |   90846 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     279 GB |     279 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   91126 GB |   91125 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   90846 GB |   90846 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     279 GB |     279 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   89844 GB |   89844 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   89564 GB |   89564 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     279 GB |     279 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    9662 K  |    9662 K  |\n",
      "|       from large pool |      24    |      65    |    4119 K  |    4119 K  |\n",
      "|       from small pool |     231    |     274    |    5543 K  |    5543 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    9662 K  |    9662 K  |\n",
      "|       from large pool |      24    |      65    |    4119 K  |    4119 K  |\n",
      "|       from small pool |     231    |     274    |    5543 K  |    5543 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      47    |    5600 K  |    5600 K  |\n",
      "|       from large pool |      10    |      23    |    1979 K  |    1979 K  |\n",
      "|       from small pool |      27    |      35    |    3620 K  |    3620 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 65, Average loss: 0.0374, Accuracy: 0.0100, Time consumed:31.19s\n",
      "\n",
      "Training Epoch: 66 [128/50000]\tLoss: 4.7840\tLR: 0.020000\n",
      "Training Epoch: 66 [256/50000]\tLoss: 4.7374\tLR: 6.500256\n",
      "Training Epoch: 66 [384/50000]\tLoss: 4.7596\tLR: 6.500512\n",
      "Training Epoch: 66 [512/50000]\tLoss: 4.7488\tLR: 6.500767\n",
      "Training Epoch: 66 [640/50000]\tLoss: 4.7491\tLR: 6.501023\n",
      "Training Epoch: 66 [768/50000]\tLoss: 4.7556\tLR: 6.501279\n",
      "Training Epoch: 66 [896/50000]\tLoss: 4.6765\tLR: 6.501535\n",
      "Training Epoch: 66 [1024/50000]\tLoss: 4.7389\tLR: 6.501790\n",
      "Training Epoch: 66 [1152/50000]\tLoss: 4.7686\tLR: 6.502046\n",
      "Training Epoch: 66 [1280/50000]\tLoss: 4.7289\tLR: 6.502302\n",
      "Training Epoch: 66 [1408/50000]\tLoss: 4.7746\tLR: 6.502558\n",
      "Training Epoch: 66 [1536/50000]\tLoss: 4.7623\tLR: 6.502813\n",
      "Training Epoch: 66 [1664/50000]\tLoss: 4.7473\tLR: 6.503069\n",
      "Training Epoch: 66 [1792/50000]\tLoss: 4.6235\tLR: 6.503325\n",
      "Training Epoch: 66 [1920/50000]\tLoss: 4.6624\tLR: 6.503581\n",
      "Training Epoch: 66 [2048/50000]\tLoss: 4.7310\tLR: 6.503836\n",
      "Training Epoch: 66 [2176/50000]\tLoss: 4.6824\tLR: 6.504092\n",
      "Training Epoch: 66 [2304/50000]\tLoss: 4.6328\tLR: 6.504348\n",
      "Training Epoch: 66 [2432/50000]\tLoss: 4.7681\tLR: 6.504604\n",
      "Training Epoch: 66 [2560/50000]\tLoss: 4.6997\tLR: 6.504859\n",
      "Training Epoch: 66 [2688/50000]\tLoss: 4.6625\tLR: 6.505115\n",
      "Training Epoch: 66 [2816/50000]\tLoss: 4.7293\tLR: 6.505371\n",
      "Training Epoch: 66 [2944/50000]\tLoss: 4.6781\tLR: 6.505627\n",
      "Training Epoch: 66 [3072/50000]\tLoss: 4.7514\tLR: 6.505882\n",
      "Training Epoch: 66 [3200/50000]\tLoss: 4.7318\tLR: 6.506138\n",
      "Training Epoch: 66 [3328/50000]\tLoss: 4.7882\tLR: 6.506394\n",
      "Training Epoch: 66 [3456/50000]\tLoss: 4.7299\tLR: 6.506650\n",
      "Training Epoch: 66 [3584/50000]\tLoss: 4.7033\tLR: 6.506905\n",
      "Training Epoch: 66 [3712/50000]\tLoss: 4.6641\tLR: 6.507161\n",
      "Training Epoch: 66 [3840/50000]\tLoss: 4.7037\tLR: 6.507417\n",
      "Training Epoch: 66 [3968/50000]\tLoss: 4.7014\tLR: 6.507673\n",
      "Training Epoch: 66 [4096/50000]\tLoss: 4.6193\tLR: 6.507928\n",
      "Training Epoch: 66 [4224/50000]\tLoss: 4.8621\tLR: 6.508184\n",
      "Training Epoch: 66 [4352/50000]\tLoss: 4.7369\tLR: 6.508440\n",
      "Training Epoch: 66 [4480/50000]\tLoss: 4.6929\tLR: 6.508696\n",
      "Training Epoch: 66 [4608/50000]\tLoss: 4.7183\tLR: 6.508951\n",
      "Training Epoch: 66 [4736/50000]\tLoss: 4.7289\tLR: 6.509207\n",
      "Training Epoch: 66 [4864/50000]\tLoss: 4.7126\tLR: 6.509463\n",
      "Training Epoch: 66 [4992/50000]\tLoss: 4.6908\tLR: 6.509719\n",
      "Training Epoch: 66 [5120/50000]\tLoss: 4.7449\tLR: 6.509974\n",
      "Training Epoch: 66 [5248/50000]\tLoss: 4.6746\tLR: 6.510230\n",
      "Training Epoch: 66 [5376/50000]\tLoss: 4.6923\tLR: 6.510486\n",
      "Training Epoch: 66 [5504/50000]\tLoss: 4.7549\tLR: 6.510742\n",
      "Training Epoch: 66 [5632/50000]\tLoss: 4.7442\tLR: 6.510997\n",
      "Training Epoch: 66 [5760/50000]\tLoss: 4.7930\tLR: 6.511253\n",
      "Training Epoch: 66 [5888/50000]\tLoss: 4.7213\tLR: 6.511509\n",
      "Training Epoch: 66 [6016/50000]\tLoss: 4.6961\tLR: 6.511765\n",
      "Training Epoch: 66 [6144/50000]\tLoss: 4.7614\tLR: 6.512020\n",
      "Training Epoch: 66 [6272/50000]\tLoss: 4.7593\tLR: 6.512276\n",
      "Training Epoch: 66 [6400/50000]\tLoss: 4.6707\tLR: 6.512532\n",
      "Training Epoch: 66 [6528/50000]\tLoss: 4.7539\tLR: 6.512788\n",
      "Training Epoch: 66 [6656/50000]\tLoss: 4.5971\tLR: 6.513043\n",
      "Training Epoch: 66 [6784/50000]\tLoss: 4.6776\tLR: 6.513299\n",
      "Training Epoch: 66 [6912/50000]\tLoss: 4.7953\tLR: 6.513555\n",
      "Training Epoch: 66 [7040/50000]\tLoss: 4.7701\tLR: 6.513811\n",
      "Training Epoch: 66 [7168/50000]\tLoss: 4.7686\tLR: 6.514066\n",
      "Training Epoch: 66 [7296/50000]\tLoss: 4.8232\tLR: 6.514322\n",
      "Training Epoch: 66 [7424/50000]\tLoss: 4.6968\tLR: 6.514578\n",
      "Training Epoch: 66 [7552/50000]\tLoss: 4.7078\tLR: 6.514834\n",
      "Training Epoch: 66 [7680/50000]\tLoss: 4.7536\tLR: 6.515090\n",
      "Training Epoch: 66 [7808/50000]\tLoss: 4.7699\tLR: 6.515345\n",
      "Training Epoch: 66 [7936/50000]\tLoss: 4.6884\tLR: 6.515601\n",
      "Training Epoch: 66 [8064/50000]\tLoss: 4.7833\tLR: 6.515857\n",
      "Training Epoch: 66 [8192/50000]\tLoss: 4.6871\tLR: 6.516113\n",
      "Training Epoch: 66 [8320/50000]\tLoss: 4.7576\tLR: 6.516368\n",
      "Training Epoch: 66 [8448/50000]\tLoss: 4.8905\tLR: 6.516624\n",
      "Training Epoch: 66 [8576/50000]\tLoss: 4.7073\tLR: 6.516880\n",
      "Training Epoch: 66 [8704/50000]\tLoss: 4.7393\tLR: 6.517136\n",
      "Training Epoch: 66 [8832/50000]\tLoss: 4.8636\tLR: 6.517391\n",
      "Training Epoch: 66 [8960/50000]\tLoss: 4.8317\tLR: 6.517647\n",
      "Training Epoch: 66 [9088/50000]\tLoss: 4.7190\tLR: 6.517903\n",
      "Training Epoch: 66 [9216/50000]\tLoss: 4.6537\tLR: 6.518159\n",
      "Training Epoch: 66 [9344/50000]\tLoss: 4.6918\tLR: 6.518414\n",
      "Training Epoch: 66 [9472/50000]\tLoss: 4.7602\tLR: 6.518670\n",
      "Training Epoch: 66 [9600/50000]\tLoss: 4.7403\tLR: 6.518926\n",
      "Training Epoch: 66 [9728/50000]\tLoss: 4.7873\tLR: 6.519182\n",
      "Training Epoch: 66 [9856/50000]\tLoss: 4.7401\tLR: 6.519437\n",
      "Training Epoch: 66 [9984/50000]\tLoss: 4.7286\tLR: 6.519693\n",
      "Training Epoch: 66 [10112/50000]\tLoss: 4.6879\tLR: 6.519949\n",
      "Training Epoch: 66 [10240/50000]\tLoss: 4.7808\tLR: 6.520205\n",
      "Training Epoch: 66 [10368/50000]\tLoss: 4.6829\tLR: 6.520460\n",
      "Training Epoch: 66 [10496/50000]\tLoss: 4.7971\tLR: 6.520716\n",
      "Training Epoch: 66 [10624/50000]\tLoss: 4.7865\tLR: 6.520972\n",
      "Training Epoch: 66 [10752/50000]\tLoss: 4.7478\tLR: 6.521228\n",
      "Training Epoch: 66 [10880/50000]\tLoss: 4.7225\tLR: 6.521483\n",
      "Training Epoch: 66 [11008/50000]\tLoss: 4.6903\tLR: 6.521739\n",
      "Training Epoch: 66 [11136/50000]\tLoss: 4.7779\tLR: 6.521995\n",
      "Training Epoch: 66 [11264/50000]\tLoss: 4.7761\tLR: 6.522251\n",
      "Training Epoch: 66 [11392/50000]\tLoss: 4.6829\tLR: 6.522506\n",
      "Training Epoch: 66 [11520/50000]\tLoss: 4.7199\tLR: 6.522762\n",
      "Training Epoch: 66 [11648/50000]\tLoss: 4.7552\tLR: 6.523018\n",
      "Training Epoch: 66 [11776/50000]\tLoss: 4.8084\tLR: 6.523274\n",
      "Training Epoch: 66 [11904/50000]\tLoss: 4.7482\tLR: 6.523529\n",
      "Training Epoch: 66 [12032/50000]\tLoss: 4.7991\tLR: 6.523785\n",
      "Training Epoch: 66 [12160/50000]\tLoss: 4.7430\tLR: 6.524041\n",
      "Training Epoch: 66 [12288/50000]\tLoss: 4.7234\tLR: 6.524297\n",
      "Training Epoch: 66 [12416/50000]\tLoss: 4.7619\tLR: 6.524552\n",
      "Training Epoch: 66 [12544/50000]\tLoss: 4.7541\tLR: 6.524808\n",
      "Training Epoch: 66 [12672/50000]\tLoss: 4.7326\tLR: 6.525064\n",
      "Training Epoch: 66 [12800/50000]\tLoss: 4.7095\tLR: 6.525320\n",
      "Training Epoch: 66 [12928/50000]\tLoss: 4.7612\tLR: 6.525575\n",
      "Training Epoch: 66 [13056/50000]\tLoss: 4.7135\tLR: 6.525831\n",
      "Training Epoch: 66 [13184/50000]\tLoss: 4.7840\tLR: 6.526087\n",
      "Training Epoch: 66 [13312/50000]\tLoss: 4.7483\tLR: 6.526343\n",
      "Training Epoch: 66 [13440/50000]\tLoss: 4.8822\tLR: 6.526598\n",
      "Training Epoch: 66 [13568/50000]\tLoss: 4.7652\tLR: 6.526854\n",
      "Training Epoch: 66 [13696/50000]\tLoss: 4.7634\tLR: 6.527110\n",
      "Training Epoch: 66 [13824/50000]\tLoss: 4.8166\tLR: 6.527366\n",
      "Training Epoch: 66 [13952/50000]\tLoss: 4.6980\tLR: 6.527621\n",
      "Training Epoch: 66 [14080/50000]\tLoss: 4.6941\tLR: 6.527877\n",
      "Training Epoch: 66 [14208/50000]\tLoss: 4.7156\tLR: 6.528133\n",
      "Training Epoch: 66 [14336/50000]\tLoss: 4.7245\tLR: 6.528389\n",
      "Training Epoch: 66 [14464/50000]\tLoss: 4.8058\tLR: 6.528645\n",
      "Training Epoch: 66 [14592/50000]\tLoss: 4.6980\tLR: 6.528900\n",
      "Training Epoch: 66 [14720/50000]\tLoss: 4.7631\tLR: 6.529156\n",
      "Training Epoch: 66 [14848/50000]\tLoss: 4.7303\tLR: 6.529412\n",
      "Training Epoch: 66 [14976/50000]\tLoss: 4.7521\tLR: 6.529668\n",
      "Training Epoch: 66 [15104/50000]\tLoss: 4.6761\tLR: 6.529923\n",
      "Training Epoch: 66 [15232/50000]\tLoss: 4.6829\tLR: 6.530179\n",
      "Training Epoch: 66 [15360/50000]\tLoss: 4.6724\tLR: 6.530435\n",
      "Training Epoch: 66 [15488/50000]\tLoss: 4.6957\tLR: 6.530691\n",
      "Training Epoch: 66 [15616/50000]\tLoss: 4.6638\tLR: 6.530946\n",
      "Training Epoch: 66 [15744/50000]\tLoss: 4.7803\tLR: 6.531202\n",
      "Training Epoch: 66 [15872/50000]\tLoss: 4.8248\tLR: 6.531458\n",
      "Training Epoch: 66 [16000/50000]\tLoss: 4.7423\tLR: 6.531714\n",
      "Training Epoch: 66 [16128/50000]\tLoss: 4.6995\tLR: 6.531969\n",
      "Training Epoch: 66 [16256/50000]\tLoss: 4.7859\tLR: 6.532225\n",
      "Training Epoch: 66 [16384/50000]\tLoss: 4.7005\tLR: 6.532481\n",
      "Training Epoch: 66 [16512/50000]\tLoss: 4.7281\tLR: 6.532737\n",
      "Training Epoch: 66 [16640/50000]\tLoss: 4.7092\tLR: 6.532992\n",
      "Training Epoch: 66 [16768/50000]\tLoss: 4.6865\tLR: 6.533248\n",
      "Training Epoch: 66 [16896/50000]\tLoss: 4.7093\tLR: 6.533504\n",
      "Training Epoch: 66 [17024/50000]\tLoss: 4.7083\tLR: 6.533760\n",
      "Training Epoch: 66 [17152/50000]\tLoss: 4.7422\tLR: 6.534015\n",
      "Training Epoch: 66 [17280/50000]\tLoss: 4.6703\tLR: 6.534271\n",
      "Training Epoch: 66 [17408/50000]\tLoss: 4.7241\tLR: 6.534527\n",
      "Training Epoch: 66 [17536/50000]\tLoss: 4.8095\tLR: 6.534783\n",
      "Training Epoch: 66 [17664/50000]\tLoss: 4.6915\tLR: 6.535038\n",
      "Training Epoch: 66 [17792/50000]\tLoss: 4.7390\tLR: 6.535294\n",
      "Training Epoch: 66 [17920/50000]\tLoss: 4.6619\tLR: 6.535550\n",
      "Training Epoch: 66 [18048/50000]\tLoss: 4.6298\tLR: 6.535806\n",
      "Training Epoch: 66 [18176/50000]\tLoss: 4.6909\tLR: 6.536061\n",
      "Training Epoch: 66 [18304/50000]\tLoss: 4.7207\tLR: 6.536317\n",
      "Training Epoch: 66 [18432/50000]\tLoss: 4.6798\tLR: 6.536573\n",
      "Training Epoch: 66 [18560/50000]\tLoss: 4.7225\tLR: 6.536829\n",
      "Training Epoch: 66 [18688/50000]\tLoss: 4.7194\tLR: 6.537084\n",
      "Training Epoch: 66 [18816/50000]\tLoss: 4.7062\tLR: 6.537340\n",
      "Training Epoch: 66 [18944/50000]\tLoss: 4.6500\tLR: 6.537596\n",
      "Training Epoch: 66 [19072/50000]\tLoss: 4.6880\tLR: 6.537852\n",
      "Training Epoch: 66 [19200/50000]\tLoss: 4.6690\tLR: 6.538107\n",
      "Training Epoch: 66 [19328/50000]\tLoss: 4.7102\tLR: 6.538363\n",
      "Training Epoch: 66 [19456/50000]\tLoss: 4.7010\tLR: 6.538619\n",
      "Training Epoch: 66 [19584/50000]\tLoss: 4.6364\tLR: 6.538875\n",
      "Training Epoch: 66 [19712/50000]\tLoss: 4.7425\tLR: 6.539130\n",
      "Training Epoch: 66 [19840/50000]\tLoss: 4.7559\tLR: 6.539386\n",
      "Training Epoch: 66 [19968/50000]\tLoss: 4.7607\tLR: 6.539642\n",
      "Training Epoch: 66 [20096/50000]\tLoss: 4.7759\tLR: 6.539898\n",
      "Training Epoch: 66 [20224/50000]\tLoss: 4.6402\tLR: 6.540153\n",
      "Training Epoch: 66 [20352/50000]\tLoss: 4.7779\tLR: 6.540409\n",
      "Training Epoch: 66 [20480/50000]\tLoss: 4.7232\tLR: 6.540665\n",
      "Training Epoch: 66 [20608/50000]\tLoss: 4.6998\tLR: 6.540921\n",
      "Training Epoch: 66 [20736/50000]\tLoss: 4.7138\tLR: 6.541176\n",
      "Training Epoch: 66 [20864/50000]\tLoss: 4.7887\tLR: 6.541432\n",
      "Training Epoch: 66 [20992/50000]\tLoss: 4.7733\tLR: 6.541688\n",
      "Training Epoch: 66 [21120/50000]\tLoss: 4.7995\tLR: 6.541944\n",
      "Training Epoch: 66 [21248/50000]\tLoss: 4.6954\tLR: 6.542199\n",
      "Training Epoch: 66 [21376/50000]\tLoss: 4.7790\tLR: 6.542455\n",
      "Training Epoch: 66 [21504/50000]\tLoss: 4.7093\tLR: 6.542711\n",
      "Training Epoch: 66 [21632/50000]\tLoss: 4.7008\tLR: 6.542967\n",
      "Training Epoch: 66 [21760/50000]\tLoss: 4.7308\tLR: 6.543223\n",
      "Training Epoch: 66 [21888/50000]\tLoss: 4.7590\tLR: 6.543478\n",
      "Training Epoch: 66 [22016/50000]\tLoss: 4.7203\tLR: 6.543734\n",
      "Training Epoch: 66 [22144/50000]\tLoss: 4.8107\tLR: 6.543990\n",
      "Training Epoch: 66 [22272/50000]\tLoss: 4.7217\tLR: 6.544246\n",
      "Training Epoch: 66 [22400/50000]\tLoss: 4.7477\tLR: 6.544501\n",
      "Training Epoch: 66 [22528/50000]\tLoss: 4.6597\tLR: 6.544757\n",
      "Training Epoch: 66 [22656/50000]\tLoss: 4.7126\tLR: 6.545013\n",
      "Training Epoch: 66 [22784/50000]\tLoss: 4.7615\tLR: 6.545269\n",
      "Training Epoch: 66 [22912/50000]\tLoss: 4.7063\tLR: 6.545524\n",
      "Training Epoch: 66 [23040/50000]\tLoss: 4.6954\tLR: 6.545780\n",
      "Training Epoch: 66 [23168/50000]\tLoss: 4.6753\tLR: 6.546036\n",
      "Training Epoch: 66 [23296/50000]\tLoss: 4.6938\tLR: 6.546292\n",
      "Training Epoch: 66 [23424/50000]\tLoss: 4.6991\tLR: 6.546547\n",
      "Training Epoch: 66 [23552/50000]\tLoss: 4.6675\tLR: 6.546803\n",
      "Training Epoch: 66 [23680/50000]\tLoss: 4.7550\tLR: 6.547059\n",
      "Training Epoch: 66 [23808/50000]\tLoss: 4.7929\tLR: 6.547315\n",
      "Training Epoch: 66 [23936/50000]\tLoss: 4.6932\tLR: 6.547570\n",
      "Training Epoch: 66 [24064/50000]\tLoss: 4.7195\tLR: 6.547826\n",
      "Training Epoch: 66 [24192/50000]\tLoss: 4.7294\tLR: 6.548082\n",
      "Training Epoch: 66 [24320/50000]\tLoss: 4.7817\tLR: 6.548338\n",
      "Training Epoch: 66 [24448/50000]\tLoss: 4.6645\tLR: 6.548593\n",
      "Training Epoch: 66 [24576/50000]\tLoss: 4.7272\tLR: 6.548849\n",
      "Training Epoch: 66 [24704/50000]\tLoss: 4.7294\tLR: 6.549105\n",
      "Training Epoch: 66 [24832/50000]\tLoss: 4.6660\tLR: 6.549361\n",
      "Training Epoch: 66 [24960/50000]\tLoss: 4.7612\tLR: 6.549616\n",
      "Training Epoch: 66 [25088/50000]\tLoss: 4.8017\tLR: 6.549872\n",
      "Training Epoch: 66 [25216/50000]\tLoss: 4.6469\tLR: 6.550128\n",
      "Training Epoch: 66 [25344/50000]\tLoss: 4.6148\tLR: 6.550384\n",
      "Training Epoch: 66 [25472/50000]\tLoss: 4.7673\tLR: 6.550639\n",
      "Training Epoch: 66 [25600/50000]\tLoss: 4.7411\tLR: 6.550895\n",
      "Training Epoch: 66 [25728/50000]\tLoss: 4.7673\tLR: 6.551151\n",
      "Training Epoch: 66 [25856/50000]\tLoss: 4.7879\tLR: 6.551407\n",
      "Training Epoch: 66 [25984/50000]\tLoss: 4.7507\tLR: 6.551662\n",
      "Training Epoch: 66 [26112/50000]\tLoss: 4.6960\tLR: 6.551918\n",
      "Training Epoch: 66 [26240/50000]\tLoss: 4.7763\tLR: 6.552174\n",
      "Training Epoch: 66 [26368/50000]\tLoss: 4.7194\tLR: 6.552430\n",
      "Training Epoch: 66 [26496/50000]\tLoss: 4.6474\tLR: 6.552685\n",
      "Training Epoch: 66 [26624/50000]\tLoss: 4.6676\tLR: 6.552941\n",
      "Training Epoch: 66 [26752/50000]\tLoss: 4.7535\tLR: 6.553197\n",
      "Training Epoch: 66 [26880/50000]\tLoss: 4.7233\tLR: 6.553453\n",
      "Training Epoch: 66 [27008/50000]\tLoss: 4.7801\tLR: 6.553708\n",
      "Training Epoch: 66 [27136/50000]\tLoss: 4.7787\tLR: 6.553964\n",
      "Training Epoch: 66 [27264/50000]\tLoss: 4.6969\tLR: 6.554220\n",
      "Training Epoch: 66 [27392/50000]\tLoss: 4.8289\tLR: 6.554476\n",
      "Training Epoch: 66 [27520/50000]\tLoss: 4.7616\tLR: 6.554731\n",
      "Training Epoch: 66 [27648/50000]\tLoss: 4.7123\tLR: 6.554987\n",
      "Training Epoch: 66 [27776/50000]\tLoss: 4.7571\tLR: 6.555243\n",
      "Training Epoch: 66 [27904/50000]\tLoss: 4.7292\tLR: 6.555499\n",
      "Training Epoch: 66 [28032/50000]\tLoss: 4.7003\tLR: 6.555754\n",
      "Training Epoch: 66 [28160/50000]\tLoss: 4.8136\tLR: 6.556010\n",
      "Training Epoch: 66 [28288/50000]\tLoss: 4.6323\tLR: 6.556266\n",
      "Training Epoch: 66 [28416/50000]\tLoss: 4.6102\tLR: 6.556522\n",
      "Training Epoch: 66 [28544/50000]\tLoss: 4.6921\tLR: 6.556777\n",
      "Training Epoch: 66 [28672/50000]\tLoss: 4.7965\tLR: 6.557033\n",
      "Training Epoch: 66 [28800/50000]\tLoss: 4.7210\tLR: 6.557289\n",
      "Training Epoch: 66 [28928/50000]\tLoss: 4.7108\tLR: 6.557545\n",
      "Training Epoch: 66 [29056/50000]\tLoss: 4.7362\tLR: 6.557801\n",
      "Training Epoch: 66 [29184/50000]\tLoss: 4.7000\tLR: 6.558056\n",
      "Training Epoch: 66 [29312/50000]\tLoss: 4.7635\tLR: 6.558312\n",
      "Training Epoch: 66 [29440/50000]\tLoss: 4.7436\tLR: 6.558568\n",
      "Training Epoch: 66 [29568/50000]\tLoss: 4.6624\tLR: 6.558824\n",
      "Training Epoch: 66 [29696/50000]\tLoss: 4.7935\tLR: 6.559079\n",
      "Training Epoch: 66 [29824/50000]\tLoss: 4.7042\tLR: 6.559335\n",
      "Training Epoch: 66 [29952/50000]\tLoss: 4.8110\tLR: 6.559591\n",
      "Training Epoch: 66 [30080/50000]\tLoss: 4.7161\tLR: 6.559847\n",
      "Training Epoch: 66 [30208/50000]\tLoss: 4.7238\tLR: 6.560102\n",
      "Training Epoch: 66 [30336/50000]\tLoss: 4.7492\tLR: 6.560358\n",
      "Training Epoch: 66 [30464/50000]\tLoss: 4.7358\tLR: 6.560614\n",
      "Training Epoch: 66 [30592/50000]\tLoss: 4.7105\tLR: 6.560870\n",
      "Training Epoch: 66 [30720/50000]\tLoss: 4.5935\tLR: 6.561125\n",
      "Training Epoch: 66 [30848/50000]\tLoss: 4.6877\tLR: 6.561381\n",
      "Training Epoch: 66 [30976/50000]\tLoss: 4.8077\tLR: 6.561637\n",
      "Training Epoch: 66 [31104/50000]\tLoss: 4.7027\tLR: 6.561893\n",
      "Training Epoch: 66 [31232/50000]\tLoss: 4.8215\tLR: 6.562148\n",
      "Training Epoch: 66 [31360/50000]\tLoss: 4.6647\tLR: 6.562404\n",
      "Training Epoch: 66 [31488/50000]\tLoss: 4.8089\tLR: 6.562660\n",
      "Training Epoch: 66 [31616/50000]\tLoss: 4.7412\tLR: 6.562916\n",
      "Training Epoch: 66 [31744/50000]\tLoss: 4.7317\tLR: 6.563171\n",
      "Training Epoch: 66 [31872/50000]\tLoss: 4.7756\tLR: 6.563427\n",
      "Training Epoch: 66 [32000/50000]\tLoss: 4.7545\tLR: 6.563683\n",
      "Training Epoch: 66 [32128/50000]\tLoss: 4.6972\tLR: 6.563939\n",
      "Training Epoch: 66 [32256/50000]\tLoss: 4.6744\tLR: 6.564194\n",
      "Training Epoch: 66 [32384/50000]\tLoss: 4.6506\tLR: 6.564450\n",
      "Training Epoch: 66 [32512/50000]\tLoss: 4.6860\tLR: 6.564706\n",
      "Training Epoch: 66 [32640/50000]\tLoss: 4.6763\tLR: 6.564962\n",
      "Training Epoch: 66 [32768/50000]\tLoss: 4.7876\tLR: 6.565217\n",
      "Training Epoch: 66 [32896/50000]\tLoss: 4.7340\tLR: 6.565473\n",
      "Training Epoch: 66 [33024/50000]\tLoss: 4.7281\tLR: 6.565729\n",
      "Training Epoch: 66 [33152/50000]\tLoss: 4.8159\tLR: 6.565985\n",
      "Training Epoch: 66 [33280/50000]\tLoss: 4.8300\tLR: 6.566240\n",
      "Training Epoch: 66 [33408/50000]\tLoss: 4.6976\tLR: 6.566496\n",
      "Training Epoch: 66 [33536/50000]\tLoss: 4.6770\tLR: 6.566752\n",
      "Training Epoch: 66 [33664/50000]\tLoss: 4.7869\tLR: 6.567008\n",
      "Training Epoch: 66 [33792/50000]\tLoss: 4.7053\tLR: 6.567263\n",
      "Training Epoch: 66 [33920/50000]\tLoss: 4.6866\tLR: 6.567519\n",
      "Training Epoch: 66 [34048/50000]\tLoss: 4.8240\tLR: 6.567775\n",
      "Training Epoch: 66 [34176/50000]\tLoss: 4.7520\tLR: 6.568031\n",
      "Training Epoch: 66 [34304/50000]\tLoss: 4.7074\tLR: 6.568286\n",
      "Training Epoch: 66 [34432/50000]\tLoss: 4.6675\tLR: 6.568542\n",
      "Training Epoch: 66 [34560/50000]\tLoss: 4.7558\tLR: 6.568798\n",
      "Training Epoch: 66 [34688/50000]\tLoss: 4.8045\tLR: 6.569054\n",
      "Training Epoch: 66 [34816/50000]\tLoss: 4.7415\tLR: 6.569309\n",
      "Training Epoch: 66 [34944/50000]\tLoss: 4.7622\tLR: 6.569565\n",
      "Training Epoch: 66 [35072/50000]\tLoss: 4.6679\tLR: 6.569821\n",
      "Training Epoch: 66 [35200/50000]\tLoss: 4.7805\tLR: 6.570077\n",
      "Training Epoch: 66 [35328/50000]\tLoss: 4.7827\tLR: 6.570332\n",
      "Training Epoch: 66 [35456/50000]\tLoss: 4.7173\tLR: 6.570588\n",
      "Training Epoch: 66 [35584/50000]\tLoss: 4.7397\tLR: 6.570844\n",
      "Training Epoch: 66 [35712/50000]\tLoss: 4.7457\tLR: 6.571100\n",
      "Training Epoch: 66 [35840/50000]\tLoss: 4.7966\tLR: 6.571355\n",
      "Training Epoch: 66 [35968/50000]\tLoss: 4.7048\tLR: 6.571611\n",
      "Training Epoch: 66 [36096/50000]\tLoss: 4.7890\tLR: 6.571867\n",
      "Training Epoch: 66 [36224/50000]\tLoss: 4.7000\tLR: 6.572123\n",
      "Training Epoch: 66 [36352/50000]\tLoss: 4.6605\tLR: 6.572379\n",
      "Training Epoch: 66 [36480/50000]\tLoss: 4.7735\tLR: 6.572634\n",
      "Training Epoch: 66 [36608/50000]\tLoss: 4.6388\tLR: 6.572890\n",
      "Training Epoch: 66 [36736/50000]\tLoss: 4.6957\tLR: 6.573146\n",
      "Training Epoch: 66 [36864/50000]\tLoss: 4.7726\tLR: 6.573402\n",
      "Training Epoch: 66 [36992/50000]\tLoss: 4.7993\tLR: 6.573657\n",
      "Training Epoch: 66 [37120/50000]\tLoss: 4.7692\tLR: 6.573913\n",
      "Training Epoch: 66 [37248/50000]\tLoss: 4.6946\tLR: 6.574169\n",
      "Training Epoch: 66 [37376/50000]\tLoss: 4.7252\tLR: 6.574425\n",
      "Training Epoch: 66 [37504/50000]\tLoss: 4.6541\tLR: 6.574680\n",
      "Training Epoch: 66 [37632/50000]\tLoss: 4.7336\tLR: 6.574936\n",
      "Training Epoch: 66 [37760/50000]\tLoss: 4.7880\tLR: 6.575192\n",
      "Training Epoch: 66 [37888/50000]\tLoss: 4.7463\tLR: 6.575448\n",
      "Training Epoch: 66 [38016/50000]\tLoss: 4.7657\tLR: 6.575703\n",
      "Training Epoch: 66 [38144/50000]\tLoss: 4.7274\tLR: 6.575959\n",
      "Training Epoch: 66 [38272/50000]\tLoss: 4.7814\tLR: 6.576215\n",
      "Training Epoch: 66 [38400/50000]\tLoss: 4.7647\tLR: 6.576471\n",
      "Training Epoch: 66 [38528/50000]\tLoss: 4.7428\tLR: 6.576726\n",
      "Training Epoch: 66 [38656/50000]\tLoss: 4.7337\tLR: 6.576982\n",
      "Training Epoch: 66 [38784/50000]\tLoss: 4.7796\tLR: 6.577238\n",
      "Training Epoch: 66 [38912/50000]\tLoss: 4.7605\tLR: 6.577494\n",
      "Training Epoch: 66 [39040/50000]\tLoss: 4.6684\tLR: 6.577749\n",
      "Training Epoch: 66 [39168/50000]\tLoss: 4.6257\tLR: 6.578005\n",
      "Training Epoch: 66 [39296/50000]\tLoss: 4.7586\tLR: 6.578261\n",
      "Training Epoch: 66 [39424/50000]\tLoss: 4.7468\tLR: 6.578517\n",
      "Training Epoch: 66 [39552/50000]\tLoss: 4.6608\tLR: 6.578772\n",
      "Training Epoch: 66 [39680/50000]\tLoss: 4.7505\tLR: 6.579028\n",
      "Training Epoch: 66 [39808/50000]\tLoss: 4.7886\tLR: 6.579284\n",
      "Training Epoch: 66 [39936/50000]\tLoss: 4.7484\tLR: 6.579540\n",
      "Training Epoch: 66 [40064/50000]\tLoss: 4.7726\tLR: 6.579795\n",
      "Training Epoch: 66 [40192/50000]\tLoss: 4.7923\tLR: 6.580051\n",
      "Training Epoch: 66 [40320/50000]\tLoss: 4.7438\tLR: 6.580307\n",
      "Training Epoch: 66 [40448/50000]\tLoss: 4.7174\tLR: 6.580563\n",
      "Training Epoch: 66 [40576/50000]\tLoss: 4.7518\tLR: 6.580818\n",
      "Training Epoch: 66 [40704/50000]\tLoss: 4.7654\tLR: 6.581074\n",
      "Training Epoch: 66 [40832/50000]\tLoss: 4.7531\tLR: 6.581330\n",
      "Training Epoch: 66 [40960/50000]\tLoss: 4.7879\tLR: 6.581586\n",
      "Training Epoch: 66 [41088/50000]\tLoss: 4.7824\tLR: 6.581841\n",
      "Training Epoch: 66 [41216/50000]\tLoss: 4.7543\tLR: 6.582097\n",
      "Training Epoch: 66 [41344/50000]\tLoss: 4.7749\tLR: 6.582353\n",
      "Training Epoch: 66 [41472/50000]\tLoss: 4.7356\tLR: 6.582609\n",
      "Training Epoch: 66 [41600/50000]\tLoss: 4.7929\tLR: 6.582864\n",
      "Training Epoch: 66 [41728/50000]\tLoss: 4.7271\tLR: 6.583120\n",
      "Training Epoch: 66 [41856/50000]\tLoss: 4.6731\tLR: 6.583376\n",
      "Training Epoch: 66 [41984/50000]\tLoss: 4.7474\tLR: 6.583632\n",
      "Training Epoch: 66 [42112/50000]\tLoss: 4.7529\tLR: 6.583887\n",
      "Training Epoch: 66 [42240/50000]\tLoss: 4.7023\tLR: 6.584143\n",
      "Training Epoch: 66 [42368/50000]\tLoss: 4.7250\tLR: 6.584399\n",
      "Training Epoch: 66 [42496/50000]\tLoss: 4.7004\tLR: 6.584655\n",
      "Training Epoch: 66 [42624/50000]\tLoss: 4.6948\tLR: 6.584910\n",
      "Training Epoch: 66 [42752/50000]\tLoss: 4.6962\tLR: 6.585166\n",
      "Training Epoch: 66 [42880/50000]\tLoss: 4.7159\tLR: 6.585422\n",
      "Training Epoch: 66 [43008/50000]\tLoss: 4.6415\tLR: 6.585678\n",
      "Training Epoch: 66 [43136/50000]\tLoss: 4.6952\tLR: 6.585934\n",
      "Training Epoch: 66 [43264/50000]\tLoss: 4.7057\tLR: 6.586189\n",
      "Training Epoch: 66 [43392/50000]\tLoss: 4.7402\tLR: 6.586445\n",
      "Training Epoch: 66 [43520/50000]\tLoss: 4.8008\tLR: 6.586701\n",
      "Training Epoch: 66 [43648/50000]\tLoss: 4.7139\tLR: 6.586957\n",
      "Training Epoch: 66 [43776/50000]\tLoss: 4.7420\tLR: 6.587212\n",
      "Training Epoch: 66 [43904/50000]\tLoss: 4.7206\tLR: 6.587468\n",
      "Training Epoch: 66 [44032/50000]\tLoss: 4.7188\tLR: 6.587724\n",
      "Training Epoch: 66 [44160/50000]\tLoss: 4.6438\tLR: 6.587980\n",
      "Training Epoch: 66 [44288/50000]\tLoss: 4.6547\tLR: 6.588235\n",
      "Training Epoch: 66 [44416/50000]\tLoss: 4.6805\tLR: 6.588491\n",
      "Training Epoch: 66 [44544/50000]\tLoss: 4.6258\tLR: 6.588747\n",
      "Training Epoch: 66 [44672/50000]\tLoss: 4.7587\tLR: 6.589003\n",
      "Training Epoch: 66 [44800/50000]\tLoss: 4.7021\tLR: 6.589258\n",
      "Training Epoch: 66 [44928/50000]\tLoss: 4.6469\tLR: 6.589514\n",
      "Training Epoch: 66 [45056/50000]\tLoss: 4.6882\tLR: 6.589770\n",
      "Training Epoch: 66 [45184/50000]\tLoss: 4.7378\tLR: 6.590026\n",
      "Training Epoch: 66 [45312/50000]\tLoss: 4.7069\tLR: 6.590281\n",
      "Training Epoch: 66 [45440/50000]\tLoss: 4.7270\tLR: 6.590537\n",
      "Training Epoch: 66 [45568/50000]\tLoss: 4.7177\tLR: 6.590793\n",
      "Training Epoch: 66 [45696/50000]\tLoss: 4.6739\tLR: 6.591049\n",
      "Training Epoch: 66 [45824/50000]\tLoss: 4.7249\tLR: 6.591304\n",
      "Training Epoch: 66 [45952/50000]\tLoss: 4.7114\tLR: 6.591560\n",
      "Training Epoch: 66 [46080/50000]\tLoss: 4.7799\tLR: 6.591816\n",
      "Training Epoch: 66 [46208/50000]\tLoss: 4.7141\tLR: 6.592072\n",
      "Training Epoch: 66 [46336/50000]\tLoss: 4.7192\tLR: 6.592327\n",
      "Training Epoch: 66 [46464/50000]\tLoss: 4.6762\tLR: 6.592583\n",
      "Training Epoch: 66 [46592/50000]\tLoss: 4.7439\tLR: 6.592839\n",
      "Training Epoch: 66 [46720/50000]\tLoss: 4.7357\tLR: 6.593095\n",
      "Training Epoch: 66 [46848/50000]\tLoss: 4.7169\tLR: 6.593350\n",
      "Training Epoch: 66 [46976/50000]\tLoss: 4.7407\tLR: 6.593606\n",
      "Training Epoch: 66 [47104/50000]\tLoss: 4.6828\tLR: 6.593862\n",
      "Training Epoch: 66 [47232/50000]\tLoss: 4.8024\tLR: 6.594118\n",
      "Training Epoch: 66 [47360/50000]\tLoss: 4.7768\tLR: 6.594373\n",
      "Training Epoch: 66 [47488/50000]\tLoss: 4.6919\tLR: 6.594629\n",
      "Training Epoch: 66 [47616/50000]\tLoss: 4.6449\tLR: 6.594885\n",
      "Training Epoch: 66 [47744/50000]\tLoss: 4.7316\tLR: 6.595141\n",
      "Training Epoch: 66 [47872/50000]\tLoss: 4.6811\tLR: 6.595396\n",
      "Training Epoch: 66 [48000/50000]\tLoss: 4.7829\tLR: 6.595652\n",
      "Training Epoch: 66 [48128/50000]\tLoss: 4.7272\tLR: 6.595908\n",
      "Training Epoch: 66 [48256/50000]\tLoss: 4.7256\tLR: 6.596164\n",
      "Training Epoch: 66 [48384/50000]\tLoss: 4.7602\tLR: 6.596419\n",
      "Training Epoch: 66 [48512/50000]\tLoss: 4.8119\tLR: 6.596675\n",
      "Training Epoch: 66 [48640/50000]\tLoss: 4.7250\tLR: 6.596931\n",
      "Training Epoch: 66 [48768/50000]\tLoss: 4.8244\tLR: 6.597187\n",
      "Training Epoch: 66 [48896/50000]\tLoss: 4.6791\tLR: 6.597442\n",
      "Training Epoch: 66 [49024/50000]\tLoss: 4.6726\tLR: 6.597698\n",
      "Training Epoch: 66 [49152/50000]\tLoss: 4.6808\tLR: 6.597954\n",
      "Training Epoch: 66 [49280/50000]\tLoss: 4.7190\tLR: 6.598210\n",
      "Training Epoch: 66 [49408/50000]\tLoss: 4.7858\tLR: 6.598465\n",
      "Training Epoch: 66 [49536/50000]\tLoss: 4.6813\tLR: 6.598721\n",
      "Training Epoch: 66 [49664/50000]\tLoss: 4.6645\tLR: 6.598977\n",
      "Training Epoch: 66 [49792/50000]\tLoss: 4.7203\tLR: 6.599233\n",
      "Training Epoch: 66 [49920/50000]\tLoss: 4.7694\tLR: 6.599488\n",
      "Training Epoch: 66 [50000/50000]\tLoss: 4.6526\tLR: 6.599744\n",
      "epoch 66 training time consumed: 488.88s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   92528 GB |   92527 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   92243 GB |   92243 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     284 GB |     284 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   92528 GB |   92527 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   92243 GB |   92243 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     284 GB |     284 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   91226 GB |   91226 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   90942 GB |   90942 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     284 GB |     284 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    9811 K  |    9811 K  |\n",
      "|       from large pool |      24    |      65    |    4182 K  |    4182 K  |\n",
      "|       from small pool |     231    |     274    |    5628 K  |    5628 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    9811 K  |    9811 K  |\n",
      "|       from large pool |      24    |      65    |    4182 K  |    4182 K  |\n",
      "|       from small pool |     231    |     274    |    5628 K  |    5628 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      47    |    5686 K  |    5686 K  |\n",
      "|       from large pool |      10    |      23    |    2010 K  |    2010 K  |\n",
      "|       from small pool |      27    |      35    |    3676 K  |    3676 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 66, Average loss: 0.0376, Accuracy: 0.0100, Time consumed:31.19s\n",
      "\n",
      "Training Epoch: 67 [128/50000]\tLoss: 4.7319\tLR: 0.020000\n",
      "Training Epoch: 67 [256/50000]\tLoss: 4.7950\tLR: 6.600256\n",
      "Training Epoch: 67 [384/50000]\tLoss: 4.6436\tLR: 6.600512\n",
      "Training Epoch: 67 [512/50000]\tLoss: 4.7731\tLR: 6.600767\n",
      "Training Epoch: 67 [640/50000]\tLoss: 4.7440\tLR: 6.601023\n",
      "Training Epoch: 67 [768/50000]\tLoss: 4.7270\tLR: 6.601279\n",
      "Training Epoch: 67 [896/50000]\tLoss: 4.8113\tLR: 6.601535\n",
      "Training Epoch: 67 [1024/50000]\tLoss: 4.6739\tLR: 6.601790\n",
      "Training Epoch: 67 [1152/50000]\tLoss: 4.7924\tLR: 6.602046\n",
      "Training Epoch: 67 [1280/50000]\tLoss: 4.6455\tLR: 6.602302\n",
      "Training Epoch: 67 [1408/50000]\tLoss: 4.7636\tLR: 6.602558\n",
      "Training Epoch: 67 [1536/50000]\tLoss: 4.7605\tLR: 6.602813\n",
      "Training Epoch: 67 [1664/50000]\tLoss: 4.6581\tLR: 6.603069\n",
      "Training Epoch: 67 [1792/50000]\tLoss: 4.7453\tLR: 6.603325\n",
      "Training Epoch: 67 [1920/50000]\tLoss: 4.7537\tLR: 6.603581\n",
      "Training Epoch: 67 [2048/50000]\tLoss: 4.7573\tLR: 6.603836\n",
      "Training Epoch: 67 [2176/50000]\tLoss: 4.7805\tLR: 6.604092\n",
      "Training Epoch: 67 [2304/50000]\tLoss: 4.6789\tLR: 6.604348\n",
      "Training Epoch: 67 [2432/50000]\tLoss: 4.7748\tLR: 6.604604\n",
      "Training Epoch: 67 [2560/50000]\tLoss: 4.7111\tLR: 6.604859\n",
      "Training Epoch: 67 [2688/50000]\tLoss: 4.6891\tLR: 6.605115\n",
      "Training Epoch: 67 [2816/50000]\tLoss: 4.7760\tLR: 6.605371\n",
      "Training Epoch: 67 [2944/50000]\tLoss: 4.7150\tLR: 6.605627\n",
      "Training Epoch: 67 [3072/50000]\tLoss: 4.7410\tLR: 6.605882\n",
      "Training Epoch: 67 [3200/50000]\tLoss: 4.7129\tLR: 6.606138\n",
      "Training Epoch: 67 [3328/50000]\tLoss: 4.7344\tLR: 6.606394\n",
      "Training Epoch: 67 [3456/50000]\tLoss: 4.6733\tLR: 6.606650\n",
      "Training Epoch: 67 [3584/50000]\tLoss: 4.8576\tLR: 6.606905\n",
      "Training Epoch: 67 [3712/50000]\tLoss: 4.7033\tLR: 6.607161\n",
      "Training Epoch: 67 [3840/50000]\tLoss: 4.7012\tLR: 6.607417\n",
      "Training Epoch: 67 [3968/50000]\tLoss: 4.6647\tLR: 6.607673\n",
      "Training Epoch: 67 [4096/50000]\tLoss: 4.7314\tLR: 6.607928\n",
      "Training Epoch: 67 [4224/50000]\tLoss: 4.7820\tLR: 6.608184\n",
      "Training Epoch: 67 [4352/50000]\tLoss: 4.7384\tLR: 6.608440\n",
      "Training Epoch: 67 [4480/50000]\tLoss: 4.7922\tLR: 6.608696\n",
      "Training Epoch: 67 [4608/50000]\tLoss: 4.8381\tLR: 6.608951\n",
      "Training Epoch: 67 [4736/50000]\tLoss: 4.7044\tLR: 6.609207\n",
      "Training Epoch: 67 [4864/50000]\tLoss: 4.7509\tLR: 6.609463\n",
      "Training Epoch: 67 [4992/50000]\tLoss: 4.7725\tLR: 6.609719\n",
      "Training Epoch: 67 [5120/50000]\tLoss: 4.6619\tLR: 6.609974\n",
      "Training Epoch: 67 [5248/50000]\tLoss: 4.7483\tLR: 6.610230\n",
      "Training Epoch: 67 [5376/50000]\tLoss: 4.6903\tLR: 6.610486\n",
      "Training Epoch: 67 [5504/50000]\tLoss: 4.6769\tLR: 6.610742\n",
      "Training Epoch: 67 [5632/50000]\tLoss: 4.7540\tLR: 6.610997\n",
      "Training Epoch: 67 [5760/50000]\tLoss: 4.7180\tLR: 6.611253\n",
      "Training Epoch: 67 [5888/50000]\tLoss: 4.7363\tLR: 6.611509\n",
      "Training Epoch: 67 [6016/50000]\tLoss: 4.6749\tLR: 6.611765\n",
      "Training Epoch: 67 [6144/50000]\tLoss: 4.7104\tLR: 6.612020\n",
      "Training Epoch: 67 [6272/50000]\tLoss: 4.6863\tLR: 6.612276\n",
      "Training Epoch: 67 [6400/50000]\tLoss: 4.7204\tLR: 6.612532\n",
      "Training Epoch: 67 [6528/50000]\tLoss: 4.5984\tLR: 6.612788\n",
      "Training Epoch: 67 [6656/50000]\tLoss: 4.7392\tLR: 6.613043\n",
      "Training Epoch: 67 [6784/50000]\tLoss: 4.7037\tLR: 6.613299\n",
      "Training Epoch: 67 [6912/50000]\tLoss: 4.7190\tLR: 6.613555\n",
      "Training Epoch: 67 [7040/50000]\tLoss: 4.7354\tLR: 6.613811\n",
      "Training Epoch: 67 [7168/50000]\tLoss: 4.7576\tLR: 6.614066\n",
      "Training Epoch: 67 [7296/50000]\tLoss: 4.7613\tLR: 6.614322\n",
      "Training Epoch: 67 [7424/50000]\tLoss: 4.7458\tLR: 6.614578\n",
      "Training Epoch: 67 [7552/50000]\tLoss: 4.7484\tLR: 6.614834\n",
      "Training Epoch: 67 [7680/50000]\tLoss: 4.7308\tLR: 6.615090\n",
      "Training Epoch: 67 [7808/50000]\tLoss: 4.7421\tLR: 6.615345\n",
      "Training Epoch: 67 [7936/50000]\tLoss: 4.6684\tLR: 6.615601\n",
      "Training Epoch: 67 [8064/50000]\tLoss: 4.7136\tLR: 6.615857\n",
      "Training Epoch: 67 [8192/50000]\tLoss: 4.7063\tLR: 6.616113\n",
      "Training Epoch: 67 [8320/50000]\tLoss: 4.6372\tLR: 6.616368\n",
      "Training Epoch: 67 [8448/50000]\tLoss: 4.7453\tLR: 6.616624\n",
      "Training Epoch: 67 [8576/50000]\tLoss: 4.7371\tLR: 6.616880\n",
      "Training Epoch: 67 [8704/50000]\tLoss: 4.7711\tLR: 6.617136\n",
      "Training Epoch: 67 [8832/50000]\tLoss: 4.7968\tLR: 6.617391\n",
      "Training Epoch: 67 [8960/50000]\tLoss: 4.7690\tLR: 6.617647\n",
      "Training Epoch: 67 [9088/50000]\tLoss: 4.7041\tLR: 6.617903\n",
      "Training Epoch: 67 [9216/50000]\tLoss: 4.6001\tLR: 6.618159\n",
      "Training Epoch: 67 [9344/50000]\tLoss: 4.7335\tLR: 6.618414\n",
      "Training Epoch: 67 [9472/50000]\tLoss: 4.6646\tLR: 6.618670\n",
      "Training Epoch: 67 [9600/50000]\tLoss: 4.6686\tLR: 6.618926\n",
      "Training Epoch: 67 [9728/50000]\tLoss: 4.8237\tLR: 6.619182\n",
      "Training Epoch: 67 [9856/50000]\tLoss: 4.7078\tLR: 6.619437\n",
      "Training Epoch: 67 [9984/50000]\tLoss: 4.8241\tLR: 6.619693\n",
      "Training Epoch: 67 [10112/50000]\tLoss: 4.8508\tLR: 6.619949\n",
      "Training Epoch: 67 [10240/50000]\tLoss: 4.8072\tLR: 6.620205\n",
      "Training Epoch: 67 [10368/50000]\tLoss: 4.7350\tLR: 6.620460\n",
      "Training Epoch: 67 [10496/50000]\tLoss: 4.7763\tLR: 6.620716\n",
      "Training Epoch: 67 [10624/50000]\tLoss: 4.8110\tLR: 6.620972\n",
      "Training Epoch: 67 [10752/50000]\tLoss: 4.6778\tLR: 6.621228\n",
      "Training Epoch: 67 [10880/50000]\tLoss: 4.6948\tLR: 6.621483\n",
      "Training Epoch: 67 [11008/50000]\tLoss: 4.7574\tLR: 6.621739\n",
      "Training Epoch: 67 [11136/50000]\tLoss: 4.7618\tLR: 6.621995\n",
      "Training Epoch: 67 [11264/50000]\tLoss: 4.7157\tLR: 6.622251\n",
      "Training Epoch: 67 [11392/50000]\tLoss: 4.8093\tLR: 6.622506\n",
      "Training Epoch: 67 [11520/50000]\tLoss: 4.6951\tLR: 6.622762\n",
      "Training Epoch: 67 [11648/50000]\tLoss: 4.6344\tLR: 6.623018\n",
      "Training Epoch: 67 [11776/50000]\tLoss: 4.7296\tLR: 6.623274\n",
      "Training Epoch: 67 [11904/50000]\tLoss: 4.8009\tLR: 6.623529\n",
      "Training Epoch: 67 [12032/50000]\tLoss: 4.7775\tLR: 6.623785\n",
      "Training Epoch: 67 [12160/50000]\tLoss: 4.7838\tLR: 6.624041\n",
      "Training Epoch: 67 [12288/50000]\tLoss: 4.6519\tLR: 6.624297\n",
      "Training Epoch: 67 [12416/50000]\tLoss: 4.7709\tLR: 6.624552\n",
      "Training Epoch: 67 [12544/50000]\tLoss: 4.7074\tLR: 6.624808\n",
      "Training Epoch: 67 [12672/50000]\tLoss: 4.6815\tLR: 6.625064\n",
      "Training Epoch: 67 [12800/50000]\tLoss: 4.7547\tLR: 6.625320\n",
      "Training Epoch: 67 [12928/50000]\tLoss: 4.8037\tLR: 6.625575\n",
      "Training Epoch: 67 [13056/50000]\tLoss: 4.7161\tLR: 6.625831\n",
      "Training Epoch: 67 [13184/50000]\tLoss: 4.7308\tLR: 6.626087\n",
      "Training Epoch: 67 [13312/50000]\tLoss: 4.6932\tLR: 6.626343\n",
      "Training Epoch: 67 [13440/50000]\tLoss: 4.7438\tLR: 6.626598\n",
      "Training Epoch: 67 [13568/50000]\tLoss: 4.6867\tLR: 6.626854\n",
      "Training Epoch: 67 [13696/50000]\tLoss: 4.6941\tLR: 6.627110\n",
      "Training Epoch: 67 [13824/50000]\tLoss: 4.7067\tLR: 6.627366\n",
      "Training Epoch: 67 [13952/50000]\tLoss: 4.6988\tLR: 6.627621\n",
      "Training Epoch: 67 [14080/50000]\tLoss: 4.6844\tLR: 6.627877\n",
      "Training Epoch: 67 [14208/50000]\tLoss: 4.7315\tLR: 6.628133\n",
      "Training Epoch: 67 [14336/50000]\tLoss: 4.7313\tLR: 6.628389\n",
      "Training Epoch: 67 [14464/50000]\tLoss: 4.7078\tLR: 6.628645\n",
      "Training Epoch: 67 [14592/50000]\tLoss: 4.7689\tLR: 6.628900\n",
      "Training Epoch: 67 [14720/50000]\tLoss: 4.6770\tLR: 6.629156\n",
      "Training Epoch: 67 [14848/50000]\tLoss: 4.7250\tLR: 6.629412\n",
      "Training Epoch: 67 [14976/50000]\tLoss: 4.7009\tLR: 6.629668\n",
      "Training Epoch: 67 [15104/50000]\tLoss: 4.6723\tLR: 6.629923\n",
      "Training Epoch: 67 [15232/50000]\tLoss: 4.7830\tLR: 6.630179\n",
      "Training Epoch: 67 [15360/50000]\tLoss: 4.7071\tLR: 6.630435\n",
      "Training Epoch: 67 [15488/50000]\tLoss: 4.6884\tLR: 6.630691\n",
      "Training Epoch: 67 [15616/50000]\tLoss: 4.7333\tLR: 6.630946\n",
      "Training Epoch: 67 [15744/50000]\tLoss: 4.7153\tLR: 6.631202\n",
      "Training Epoch: 67 [15872/50000]\tLoss: 4.7648\tLR: 6.631458\n",
      "Training Epoch: 67 [16000/50000]\tLoss: 4.7465\tLR: 6.631714\n",
      "Training Epoch: 67 [16128/50000]\tLoss: 4.7797\tLR: 6.631969\n",
      "Training Epoch: 67 [16256/50000]\tLoss: 4.6374\tLR: 6.632225\n",
      "Training Epoch: 67 [16384/50000]\tLoss: 4.6869\tLR: 6.632481\n",
      "Training Epoch: 67 [16512/50000]\tLoss: 4.7634\tLR: 6.632737\n",
      "Training Epoch: 67 [16640/50000]\tLoss: 4.6943\tLR: 6.632992\n",
      "Training Epoch: 67 [16768/50000]\tLoss: 4.7643\tLR: 6.633248\n",
      "Training Epoch: 67 [16896/50000]\tLoss: 4.6697\tLR: 6.633504\n",
      "Training Epoch: 67 [17024/50000]\tLoss: 4.7797\tLR: 6.633760\n",
      "Training Epoch: 67 [17152/50000]\tLoss: 4.7281\tLR: 6.634015\n",
      "Training Epoch: 67 [17280/50000]\tLoss: 4.7609\tLR: 6.634271\n",
      "Training Epoch: 67 [17408/50000]\tLoss: 4.7751\tLR: 6.634527\n",
      "Training Epoch: 67 [17536/50000]\tLoss: 4.6363\tLR: 6.634783\n",
      "Training Epoch: 67 [17664/50000]\tLoss: 4.7245\tLR: 6.635038\n",
      "Training Epoch: 67 [17792/50000]\tLoss: 4.7259\tLR: 6.635294\n",
      "Training Epoch: 67 [17920/50000]\tLoss: 4.6439\tLR: 6.635550\n",
      "Training Epoch: 67 [18048/50000]\tLoss: 4.7048\tLR: 6.635806\n",
      "Training Epoch: 67 [18176/50000]\tLoss: 4.6758\tLR: 6.636061\n",
      "Training Epoch: 67 [18304/50000]\tLoss: 4.7514\tLR: 6.636317\n",
      "Training Epoch: 67 [18432/50000]\tLoss: 4.6842\tLR: 6.636573\n",
      "Training Epoch: 67 [18560/50000]\tLoss: 4.7705\tLR: 6.636829\n",
      "Training Epoch: 67 [18688/50000]\tLoss: 4.6915\tLR: 6.637084\n",
      "Training Epoch: 67 [18816/50000]\tLoss: 4.7357\tLR: 6.637340\n",
      "Training Epoch: 67 [18944/50000]\tLoss: 4.6946\tLR: 6.637596\n",
      "Training Epoch: 67 [19072/50000]\tLoss: 4.7712\tLR: 6.637852\n",
      "Training Epoch: 67 [19200/50000]\tLoss: 4.6414\tLR: 6.638107\n",
      "Training Epoch: 67 [19328/50000]\tLoss: 4.7849\tLR: 6.638363\n",
      "Training Epoch: 67 [19456/50000]\tLoss: 4.6764\tLR: 6.638619\n",
      "Training Epoch: 67 [19584/50000]\tLoss: 4.7143\tLR: 6.638875\n",
      "Training Epoch: 67 [19712/50000]\tLoss: 4.7735\tLR: 6.639130\n",
      "Training Epoch: 67 [19840/50000]\tLoss: 4.7976\tLR: 6.639386\n",
      "Training Epoch: 67 [19968/50000]\tLoss: 4.6965\tLR: 6.639642\n",
      "Training Epoch: 67 [20096/50000]\tLoss: 4.8018\tLR: 6.639898\n",
      "Training Epoch: 67 [20224/50000]\tLoss: 4.6980\tLR: 6.640153\n",
      "Training Epoch: 67 [20352/50000]\tLoss: 4.7458\tLR: 6.640409\n",
      "Training Epoch: 67 [20480/50000]\tLoss: 4.7537\tLR: 6.640665\n",
      "Training Epoch: 67 [20608/50000]\tLoss: 4.7941\tLR: 6.640921\n",
      "Training Epoch: 67 [20736/50000]\tLoss: 4.8033\tLR: 6.641176\n",
      "Training Epoch: 67 [20864/50000]\tLoss: 4.7251\tLR: 6.641432\n",
      "Training Epoch: 67 [20992/50000]\tLoss: 4.7714\tLR: 6.641688\n",
      "Training Epoch: 67 [21120/50000]\tLoss: 4.6995\tLR: 6.641944\n",
      "Training Epoch: 67 [21248/50000]\tLoss: 4.7665\tLR: 6.642199\n",
      "Training Epoch: 67 [21376/50000]\tLoss: 4.7904\tLR: 6.642455\n",
      "Training Epoch: 67 [21504/50000]\tLoss: 4.7273\tLR: 6.642711\n",
      "Training Epoch: 67 [21632/50000]\tLoss: 4.7135\tLR: 6.642967\n",
      "Training Epoch: 67 [21760/50000]\tLoss: 4.6616\tLR: 6.643223\n",
      "Training Epoch: 67 [21888/50000]\tLoss: 4.6799\tLR: 6.643478\n",
      "Training Epoch: 67 [22016/50000]\tLoss: 4.7584\tLR: 6.643734\n",
      "Training Epoch: 67 [22144/50000]\tLoss: 4.8448\tLR: 6.643990\n",
      "Training Epoch: 67 [22272/50000]\tLoss: 4.8142\tLR: 6.644246\n",
      "Training Epoch: 67 [22400/50000]\tLoss: 4.7878\tLR: 6.644501\n",
      "Training Epoch: 67 [22528/50000]\tLoss: 4.8258\tLR: 6.644757\n",
      "Training Epoch: 67 [22656/50000]\tLoss: 4.8080\tLR: 6.645013\n",
      "Training Epoch: 67 [22784/50000]\tLoss: 4.7259\tLR: 6.645269\n",
      "Training Epoch: 67 [22912/50000]\tLoss: 4.7890\tLR: 6.645524\n",
      "Training Epoch: 67 [23040/50000]\tLoss: 4.7412\tLR: 6.645780\n",
      "Training Epoch: 67 [23168/50000]\tLoss: 4.8025\tLR: 6.646036\n",
      "Training Epoch: 67 [23296/50000]\tLoss: 4.7299\tLR: 6.646292\n",
      "Training Epoch: 67 [23424/50000]\tLoss: 4.7870\tLR: 6.646547\n",
      "Training Epoch: 67 [23552/50000]\tLoss: 4.7600\tLR: 6.646803\n",
      "Training Epoch: 67 [23680/50000]\tLoss: 4.6469\tLR: 6.647059\n",
      "Training Epoch: 67 [23808/50000]\tLoss: 4.7207\tLR: 6.647315\n",
      "Training Epoch: 67 [23936/50000]\tLoss: 4.7323\tLR: 6.647570\n",
      "Training Epoch: 67 [24064/50000]\tLoss: 4.7972\tLR: 6.647826\n",
      "Training Epoch: 67 [24192/50000]\tLoss: 4.7245\tLR: 6.648082\n",
      "Training Epoch: 67 [24320/50000]\tLoss: 4.7299\tLR: 6.648338\n",
      "Training Epoch: 67 [24448/50000]\tLoss: 4.7377\tLR: 6.648593\n",
      "Training Epoch: 67 [24576/50000]\tLoss: 4.6850\tLR: 6.648849\n",
      "Training Epoch: 67 [24704/50000]\tLoss: 4.6776\tLR: 6.649105\n",
      "Training Epoch: 67 [24832/50000]\tLoss: 4.7492\tLR: 6.649361\n",
      "Training Epoch: 67 [24960/50000]\tLoss: 4.7464\tLR: 6.649616\n",
      "Training Epoch: 67 [25088/50000]\tLoss: 4.7414\tLR: 6.649872\n",
      "Training Epoch: 67 [25216/50000]\tLoss: 4.7648\tLR: 6.650128\n",
      "Training Epoch: 67 [25344/50000]\tLoss: 4.7876\tLR: 6.650384\n",
      "Training Epoch: 67 [25472/50000]\tLoss: 4.7315\tLR: 6.650639\n",
      "Training Epoch: 67 [25600/50000]\tLoss: 4.7440\tLR: 6.650895\n",
      "Training Epoch: 67 [25728/50000]\tLoss: 4.7174\tLR: 6.651151\n",
      "Training Epoch: 67 [25856/50000]\tLoss: 4.7902\tLR: 6.651407\n",
      "Training Epoch: 67 [25984/50000]\tLoss: 4.7543\tLR: 6.651662\n",
      "Training Epoch: 67 [26112/50000]\tLoss: 4.7823\tLR: 6.651918\n",
      "Training Epoch: 67 [26240/50000]\tLoss: 4.7355\tLR: 6.652174\n",
      "Training Epoch: 67 [26368/50000]\tLoss: 4.7341\tLR: 6.652430\n",
      "Training Epoch: 67 [26496/50000]\tLoss: 4.6801\tLR: 6.652685\n",
      "Training Epoch: 67 [26624/50000]\tLoss: 4.7075\tLR: 6.652941\n",
      "Training Epoch: 67 [26752/50000]\tLoss: 4.7891\tLR: 6.653197\n",
      "Training Epoch: 67 [26880/50000]\tLoss: 4.7544\tLR: 6.653453\n",
      "Training Epoch: 67 [27008/50000]\tLoss: 4.7782\tLR: 6.653708\n",
      "Training Epoch: 67 [27136/50000]\tLoss: 4.7565\tLR: 6.653964\n",
      "Training Epoch: 67 [27264/50000]\tLoss: 4.7187\tLR: 6.654220\n",
      "Training Epoch: 67 [27392/50000]\tLoss: 4.7510\tLR: 6.654476\n",
      "Training Epoch: 67 [27520/50000]\tLoss: 4.7786\tLR: 6.654731\n",
      "Training Epoch: 67 [27648/50000]\tLoss: 4.7715\tLR: 6.654987\n",
      "Training Epoch: 67 [27776/50000]\tLoss: 4.7759\tLR: 6.655243\n",
      "Training Epoch: 67 [27904/50000]\tLoss: 4.7767\tLR: 6.655499\n",
      "Training Epoch: 67 [28032/50000]\tLoss: 4.7193\tLR: 6.655754\n",
      "Training Epoch: 67 [28160/50000]\tLoss: 4.6508\tLR: 6.656010\n",
      "Training Epoch: 67 [28288/50000]\tLoss: 4.7357\tLR: 6.656266\n",
      "Training Epoch: 67 [28416/50000]\tLoss: 4.6697\tLR: 6.656522\n",
      "Training Epoch: 67 [28544/50000]\tLoss: 4.7052\tLR: 6.656777\n",
      "Training Epoch: 67 [28672/50000]\tLoss: 4.6849\tLR: 6.657033\n",
      "Training Epoch: 67 [28800/50000]\tLoss: 4.7032\tLR: 6.657289\n",
      "Training Epoch: 67 [28928/50000]\tLoss: 4.7987\tLR: 6.657545\n",
      "Training Epoch: 67 [29056/50000]\tLoss: 4.7922\tLR: 6.657801\n",
      "Training Epoch: 67 [29184/50000]\tLoss: 4.6755\tLR: 6.658056\n",
      "Training Epoch: 67 [29312/50000]\tLoss: 4.7134\tLR: 6.658312\n",
      "Training Epoch: 67 [29440/50000]\tLoss: 4.7180\tLR: 6.658568\n",
      "Training Epoch: 67 [29568/50000]\tLoss: 4.7722\tLR: 6.658824\n",
      "Training Epoch: 67 [29696/50000]\tLoss: 4.6805\tLR: 6.659079\n",
      "Training Epoch: 67 [29824/50000]\tLoss: 4.7584\tLR: 6.659335\n",
      "Training Epoch: 67 [29952/50000]\tLoss: 4.6956\tLR: 6.659591\n",
      "Training Epoch: 67 [30080/50000]\tLoss: 4.7430\tLR: 6.659847\n",
      "Training Epoch: 67 [30208/50000]\tLoss: 4.8113\tLR: 6.660102\n",
      "Training Epoch: 67 [30336/50000]\tLoss: 4.8106\tLR: 6.660358\n",
      "Training Epoch: 67 [30464/50000]\tLoss: 4.6844\tLR: 6.660614\n",
      "Training Epoch: 67 [30592/50000]\tLoss: 4.7630\tLR: 6.660870\n",
      "Training Epoch: 67 [30720/50000]\tLoss: 4.7509\tLR: 6.661125\n",
      "Training Epoch: 67 [30848/50000]\tLoss: 4.7391\tLR: 6.661381\n",
      "Training Epoch: 67 [30976/50000]\tLoss: 4.7440\tLR: 6.661637\n",
      "Training Epoch: 67 [31104/50000]\tLoss: 4.6971\tLR: 6.661893\n",
      "Training Epoch: 67 [31232/50000]\tLoss: 4.7058\tLR: 6.662148\n",
      "Training Epoch: 67 [31360/50000]\tLoss: 4.8077\tLR: 6.662404\n",
      "Training Epoch: 67 [31488/50000]\tLoss: 4.7739\tLR: 6.662660\n",
      "Training Epoch: 67 [31616/50000]\tLoss: 4.7251\tLR: 6.662916\n",
      "Training Epoch: 67 [31744/50000]\tLoss: 4.8247\tLR: 6.663171\n",
      "Training Epoch: 67 [31872/50000]\tLoss: 4.7215\tLR: 6.663427\n",
      "Training Epoch: 67 [32000/50000]\tLoss: 4.6911\tLR: 6.663683\n",
      "Training Epoch: 67 [32128/50000]\tLoss: 4.7345\tLR: 6.663939\n",
      "Training Epoch: 67 [32256/50000]\tLoss: 4.7479\tLR: 6.664194\n",
      "Training Epoch: 67 [32384/50000]\tLoss: 4.7086\tLR: 6.664450\n",
      "Training Epoch: 67 [32512/50000]\tLoss: 4.7396\tLR: 6.664706\n",
      "Training Epoch: 67 [32640/50000]\tLoss: 4.7176\tLR: 6.664962\n",
      "Training Epoch: 67 [32768/50000]\tLoss: 4.7332\tLR: 6.665217\n",
      "Training Epoch: 67 [32896/50000]\tLoss: 4.6358\tLR: 6.665473\n",
      "Training Epoch: 67 [33024/50000]\tLoss: 4.6392\tLR: 6.665729\n",
      "Training Epoch: 67 [33152/50000]\tLoss: 4.7048\tLR: 6.665985\n",
      "Training Epoch: 67 [33280/50000]\tLoss: 4.7957\tLR: 6.666240\n",
      "Training Epoch: 67 [33408/50000]\tLoss: 4.7678\tLR: 6.666496\n",
      "Training Epoch: 67 [33536/50000]\tLoss: 4.7382\tLR: 6.666752\n",
      "Training Epoch: 67 [33664/50000]\tLoss: 4.6873\tLR: 6.667008\n",
      "Training Epoch: 67 [33792/50000]\tLoss: 4.7408\tLR: 6.667263\n",
      "Training Epoch: 67 [33920/50000]\tLoss: 4.7050\tLR: 6.667519\n",
      "Training Epoch: 67 [34048/50000]\tLoss: 4.7548\tLR: 6.667775\n",
      "Training Epoch: 67 [34176/50000]\tLoss: 4.7774\tLR: 6.668031\n",
      "Training Epoch: 67 [34304/50000]\tLoss: 4.8038\tLR: 6.668286\n",
      "Training Epoch: 67 [34432/50000]\tLoss: 4.7782\tLR: 6.668542\n",
      "Training Epoch: 67 [34560/50000]\tLoss: 4.7323\tLR: 6.668798\n",
      "Training Epoch: 67 [34688/50000]\tLoss: 4.7997\tLR: 6.669054\n",
      "Training Epoch: 67 [34816/50000]\tLoss: 4.6848\tLR: 6.669309\n",
      "Training Epoch: 67 [34944/50000]\tLoss: 4.6783\tLR: 6.669565\n",
      "Training Epoch: 67 [35072/50000]\tLoss: 4.7070\tLR: 6.669821\n",
      "Training Epoch: 67 [35200/50000]\tLoss: 4.7160\tLR: 6.670077\n",
      "Training Epoch: 67 [35328/50000]\tLoss: 4.6749\tLR: 6.670332\n",
      "Training Epoch: 67 [35456/50000]\tLoss: 4.7885\tLR: 6.670588\n",
      "Training Epoch: 67 [35584/50000]\tLoss: 4.6730\tLR: 6.670844\n",
      "Training Epoch: 67 [35712/50000]\tLoss: 4.7460\tLR: 6.671100\n",
      "Training Epoch: 67 [35840/50000]\tLoss: 4.8059\tLR: 6.671355\n",
      "Training Epoch: 67 [35968/50000]\tLoss: 4.7287\tLR: 6.671611\n",
      "Training Epoch: 67 [36096/50000]\tLoss: 4.7414\tLR: 6.671867\n",
      "Training Epoch: 67 [36224/50000]\tLoss: 4.6991\tLR: 6.672123\n",
      "Training Epoch: 67 [36352/50000]\tLoss: 4.7570\tLR: 6.672379\n",
      "Training Epoch: 67 [36480/50000]\tLoss: 4.7095\tLR: 6.672634\n",
      "Training Epoch: 67 [36608/50000]\tLoss: 4.6769\tLR: 6.672890\n",
      "Training Epoch: 67 [36736/50000]\tLoss: 4.6557\tLR: 6.673146\n",
      "Training Epoch: 67 [36864/50000]\tLoss: 4.7510\tLR: 6.673402\n",
      "Training Epoch: 67 [36992/50000]\tLoss: 4.7246\tLR: 6.673657\n",
      "Training Epoch: 67 [37120/50000]\tLoss: 4.7729\tLR: 6.673913\n",
      "Training Epoch: 67 [37248/50000]\tLoss: 4.7364\tLR: 6.674169\n",
      "Training Epoch: 67 [37376/50000]\tLoss: 4.7012\tLR: 6.674425\n",
      "Training Epoch: 67 [37504/50000]\tLoss: 4.6892\tLR: 6.674680\n",
      "Training Epoch: 67 [37632/50000]\tLoss: 4.7940\tLR: 6.674936\n",
      "Training Epoch: 67 [37760/50000]\tLoss: 4.7405\tLR: 6.675192\n",
      "Training Epoch: 67 [37888/50000]\tLoss: 4.6714\tLR: 6.675448\n",
      "Training Epoch: 67 [38016/50000]\tLoss: 4.6903\tLR: 6.675703\n",
      "Training Epoch: 67 [38144/50000]\tLoss: 4.7649\tLR: 6.675959\n",
      "Training Epoch: 67 [38272/50000]\tLoss: 4.7561\tLR: 6.676215\n",
      "Training Epoch: 67 [38400/50000]\tLoss: 4.7811\tLR: 6.676471\n",
      "Training Epoch: 67 [38528/50000]\tLoss: 4.6955\tLR: 6.676726\n",
      "Training Epoch: 67 [38656/50000]\tLoss: 4.7013\tLR: 6.676982\n",
      "Training Epoch: 67 [38784/50000]\tLoss: 4.6997\tLR: 6.677238\n",
      "Training Epoch: 67 [38912/50000]\tLoss: 4.7194\tLR: 6.677494\n",
      "Training Epoch: 67 [39040/50000]\tLoss: 4.6622\tLR: 6.677749\n",
      "Training Epoch: 67 [39168/50000]\tLoss: 4.7315\tLR: 6.678005\n",
      "Training Epoch: 67 [39296/50000]\tLoss: 4.7698\tLR: 6.678261\n",
      "Training Epoch: 67 [39424/50000]\tLoss: 4.7816\tLR: 6.678517\n",
      "Training Epoch: 67 [39552/50000]\tLoss: 4.8082\tLR: 6.678772\n",
      "Training Epoch: 67 [39680/50000]\tLoss: 4.7303\tLR: 6.679028\n",
      "Training Epoch: 67 [39808/50000]\tLoss: 4.6541\tLR: 6.679284\n",
      "Training Epoch: 67 [39936/50000]\tLoss: 4.7415\tLR: 6.679540\n",
      "Training Epoch: 67 [40064/50000]\tLoss: 4.7239\tLR: 6.679795\n",
      "Training Epoch: 67 [40192/50000]\tLoss: 4.7095\tLR: 6.680051\n",
      "Training Epoch: 67 [40320/50000]\tLoss: 4.7432\tLR: 6.680307\n",
      "Training Epoch: 67 [40448/50000]\tLoss: 4.7172\tLR: 6.680563\n",
      "Training Epoch: 67 [40576/50000]\tLoss: 4.6438\tLR: 6.680818\n",
      "Training Epoch: 67 [40704/50000]\tLoss: 4.7988\tLR: 6.681074\n",
      "Training Epoch: 67 [40832/50000]\tLoss: 4.7400\tLR: 6.681330\n",
      "Training Epoch: 67 [40960/50000]\tLoss: 4.7376\tLR: 6.681586\n",
      "Training Epoch: 67 [41088/50000]\tLoss: 4.7610\tLR: 6.681841\n",
      "Training Epoch: 67 [41216/50000]\tLoss: 4.7737\tLR: 6.682097\n",
      "Training Epoch: 67 [41344/50000]\tLoss: 4.7445\tLR: 6.682353\n",
      "Training Epoch: 67 [41472/50000]\tLoss: 4.7257\tLR: 6.682609\n",
      "Training Epoch: 67 [41600/50000]\tLoss: 4.7184\tLR: 6.682864\n",
      "Training Epoch: 67 [41728/50000]\tLoss: 4.7764\tLR: 6.683120\n",
      "Training Epoch: 67 [41856/50000]\tLoss: 4.7906\tLR: 6.683376\n",
      "Training Epoch: 67 [41984/50000]\tLoss: 4.7851\tLR: 6.683632\n",
      "Training Epoch: 67 [42112/50000]\tLoss: 4.7157\tLR: 6.683887\n",
      "Training Epoch: 67 [42240/50000]\tLoss: 4.7344\tLR: 6.684143\n",
      "Training Epoch: 67 [42368/50000]\tLoss: 4.7788\tLR: 6.684399\n",
      "Training Epoch: 67 [42496/50000]\tLoss: 4.7118\tLR: 6.684655\n",
      "Training Epoch: 67 [42624/50000]\tLoss: 4.6809\tLR: 6.684910\n",
      "Training Epoch: 67 [42752/50000]\tLoss: 4.7150\tLR: 6.685166\n",
      "Training Epoch: 67 [42880/50000]\tLoss: 4.7717\tLR: 6.685422\n",
      "Training Epoch: 67 [43008/50000]\tLoss: 4.7434\tLR: 6.685678\n",
      "Training Epoch: 67 [43136/50000]\tLoss: 4.7476\tLR: 6.685934\n",
      "Training Epoch: 67 [43264/50000]\tLoss: 4.7704\tLR: 6.686189\n",
      "Training Epoch: 67 [43392/50000]\tLoss: 4.7471\tLR: 6.686445\n",
      "Training Epoch: 67 [43520/50000]\tLoss: 4.7436\tLR: 6.686701\n",
      "Training Epoch: 67 [43648/50000]\tLoss: 4.7445\tLR: 6.686957\n",
      "Training Epoch: 67 [43776/50000]\tLoss: 4.7052\tLR: 6.687212\n",
      "Training Epoch: 67 [43904/50000]\tLoss: 4.7303\tLR: 6.687468\n",
      "Training Epoch: 67 [44032/50000]\tLoss: 4.6794\tLR: 6.687724\n",
      "Training Epoch: 67 [44160/50000]\tLoss: 4.7378\tLR: 6.687980\n",
      "Training Epoch: 67 [44288/50000]\tLoss: 4.6776\tLR: 6.688235\n",
      "Training Epoch: 67 [44416/50000]\tLoss: 4.7935\tLR: 6.688491\n",
      "Training Epoch: 67 [44544/50000]\tLoss: 4.7206\tLR: 6.688747\n",
      "Training Epoch: 67 [44672/50000]\tLoss: 4.7230\tLR: 6.689003\n",
      "Training Epoch: 67 [44800/50000]\tLoss: 4.7449\tLR: 6.689258\n",
      "Training Epoch: 67 [44928/50000]\tLoss: 4.6996\tLR: 6.689514\n",
      "Training Epoch: 67 [45056/50000]\tLoss: 4.7207\tLR: 6.689770\n",
      "Training Epoch: 67 [45184/50000]\tLoss: 4.7119\tLR: 6.690026\n",
      "Training Epoch: 67 [45312/50000]\tLoss: 4.7264\tLR: 6.690281\n",
      "Training Epoch: 67 [45440/50000]\tLoss: 4.7941\tLR: 6.690537\n",
      "Training Epoch: 67 [45568/50000]\tLoss: 4.7938\tLR: 6.690793\n",
      "Training Epoch: 67 [45696/50000]\tLoss: 4.7322\tLR: 6.691049\n",
      "Training Epoch: 67 [45824/50000]\tLoss: 4.7415\tLR: 6.691304\n",
      "Training Epoch: 67 [45952/50000]\tLoss: 4.7619\tLR: 6.691560\n",
      "Training Epoch: 67 [46080/50000]\tLoss: 4.6794\tLR: 6.691816\n",
      "Training Epoch: 67 [46208/50000]\tLoss: 4.7757\tLR: 6.692072\n",
      "Training Epoch: 67 [46336/50000]\tLoss: 4.7632\tLR: 6.692327\n",
      "Training Epoch: 67 [46464/50000]\tLoss: 4.7357\tLR: 6.692583\n",
      "Training Epoch: 67 [46592/50000]\tLoss: 4.7993\tLR: 6.692839\n",
      "Training Epoch: 67 [46720/50000]\tLoss: 4.6990\tLR: 6.693095\n",
      "Training Epoch: 67 [46848/50000]\tLoss: 4.7295\tLR: 6.693350\n",
      "Training Epoch: 67 [46976/50000]\tLoss: 4.7665\tLR: 6.693606\n",
      "Training Epoch: 67 [47104/50000]\tLoss: 4.7072\tLR: 6.693862\n",
      "Training Epoch: 67 [47232/50000]\tLoss: 4.6846\tLR: 6.694118\n",
      "Training Epoch: 67 [47360/50000]\tLoss: 4.7448\tLR: 6.694373\n",
      "Training Epoch: 67 [47488/50000]\tLoss: 4.7573\tLR: 6.694629\n",
      "Training Epoch: 67 [47616/50000]\tLoss: 4.7744\tLR: 6.694885\n",
      "Training Epoch: 67 [47744/50000]\tLoss: 4.7490\tLR: 6.695141\n",
      "Training Epoch: 67 [47872/50000]\tLoss: 4.6658\tLR: 6.695396\n",
      "Training Epoch: 67 [48000/50000]\tLoss: 4.7115\tLR: 6.695652\n",
      "Training Epoch: 67 [48128/50000]\tLoss: 4.7680\tLR: 6.695908\n",
      "Training Epoch: 67 [48256/50000]\tLoss: 4.7145\tLR: 6.696164\n",
      "Training Epoch: 67 [48384/50000]\tLoss: 4.7006\tLR: 6.696419\n",
      "Training Epoch: 67 [48512/50000]\tLoss: 4.6882\tLR: 6.696675\n",
      "Training Epoch: 67 [48640/50000]\tLoss: 4.7283\tLR: 6.696931\n",
      "Training Epoch: 67 [48768/50000]\tLoss: 4.6707\tLR: 6.697187\n",
      "Training Epoch: 67 [48896/50000]\tLoss: 4.7462\tLR: 6.697442\n",
      "Training Epoch: 67 [49024/50000]\tLoss: 4.6714\tLR: 6.697698\n",
      "Training Epoch: 67 [49152/50000]\tLoss: 4.6670\tLR: 6.697954\n",
      "Training Epoch: 67 [49280/50000]\tLoss: 4.6813\tLR: 6.698210\n",
      "Training Epoch: 67 [49408/50000]\tLoss: 4.6992\tLR: 6.698465\n",
      "Training Epoch: 67 [49536/50000]\tLoss: 4.7209\tLR: 6.698721\n",
      "Training Epoch: 67 [49664/50000]\tLoss: 4.7338\tLR: 6.698977\n",
      "Training Epoch: 67 [49792/50000]\tLoss: 4.7231\tLR: 6.699233\n",
      "Training Epoch: 67 [49920/50000]\tLoss: 4.7748\tLR: 6.699488\n",
      "Training Epoch: 67 [50000/50000]\tLoss: 4.7392\tLR: 6.699744\n",
      "epoch 67 training time consumed: 488.98s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   93929 GB |   93929 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   93641 GB |   93641 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     288 GB |     288 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   93929 GB |   93929 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   93641 GB |   93641 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     288 GB |     288 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   92609 GB |   92609 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   92320 GB |   92320 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     288 GB |     288 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |    9959 K  |    9959 K  |\n",
      "|       from large pool |      24    |      65    |    4245 K  |    4245 K  |\n",
      "|       from small pool |     231    |     274    |    5714 K  |    5713 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |    9959 K  |    9959 K  |\n",
      "|       from large pool |      24    |      65    |    4245 K  |    4245 K  |\n",
      "|       from small pool |     231    |     274    |    5714 K  |    5713 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      47    |    5772 K  |    5772 K  |\n",
      "|       from large pool |      10    |      23    |    2040 K  |    2040 K  |\n",
      "|       from small pool |      26    |      35    |    3732 K  |    3732 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 67, Average loss: 0.0373, Accuracy: 0.0100, Time consumed:31.25s\n",
      "\n",
      "Training Epoch: 68 [128/50000]\tLoss: 4.7271\tLR: 0.020000\n",
      "Training Epoch: 68 [256/50000]\tLoss: 4.7823\tLR: 6.700256\n",
      "Training Epoch: 68 [384/50000]\tLoss: 4.6477\tLR: 6.700512\n",
      "Training Epoch: 68 [512/50000]\tLoss: 4.7166\tLR: 6.700767\n",
      "Training Epoch: 68 [640/50000]\tLoss: 4.6717\tLR: 6.701023\n",
      "Training Epoch: 68 [768/50000]\tLoss: 4.6933\tLR: 6.701279\n",
      "Training Epoch: 68 [896/50000]\tLoss: 4.7279\tLR: 6.701535\n",
      "Training Epoch: 68 [1024/50000]\tLoss: 4.6953\tLR: 6.701790\n",
      "Training Epoch: 68 [1152/50000]\tLoss: 4.7259\tLR: 6.702046\n",
      "Training Epoch: 68 [1280/50000]\tLoss: 4.8030\tLR: 6.702302\n",
      "Training Epoch: 68 [1408/50000]\tLoss: 4.7011\tLR: 6.702558\n",
      "Training Epoch: 68 [1536/50000]\tLoss: 4.7476\tLR: 6.702813\n",
      "Training Epoch: 68 [1664/50000]\tLoss: 4.7759\tLR: 6.703069\n",
      "Training Epoch: 68 [1792/50000]\tLoss: 4.7649\tLR: 6.703325\n",
      "Training Epoch: 68 [1920/50000]\tLoss: 4.7048\tLR: 6.703581\n",
      "Training Epoch: 68 [2048/50000]\tLoss: 4.8237\tLR: 6.703836\n",
      "Training Epoch: 68 [2176/50000]\tLoss: 4.7323\tLR: 6.704092\n",
      "Training Epoch: 68 [2304/50000]\tLoss: 4.8249\tLR: 6.704348\n",
      "Training Epoch: 68 [2432/50000]\tLoss: 4.6928\tLR: 6.704604\n",
      "Training Epoch: 68 [2560/50000]\tLoss: 4.7963\tLR: 6.704859\n",
      "Training Epoch: 68 [2688/50000]\tLoss: 4.8341\tLR: 6.705115\n",
      "Training Epoch: 68 [2816/50000]\tLoss: 4.6398\tLR: 6.705371\n",
      "Training Epoch: 68 [2944/50000]\tLoss: 4.7982\tLR: 6.705627\n",
      "Training Epoch: 68 [3072/50000]\tLoss: 4.8088\tLR: 6.705882\n",
      "Training Epoch: 68 [3200/50000]\tLoss: 4.7183\tLR: 6.706138\n",
      "Training Epoch: 68 [3328/50000]\tLoss: 4.8072\tLR: 6.706394\n",
      "Training Epoch: 68 [3456/50000]\tLoss: 4.7145\tLR: 6.706650\n",
      "Training Epoch: 68 [3584/50000]\tLoss: 4.7150\tLR: 6.706905\n",
      "Training Epoch: 68 [3712/50000]\tLoss: 4.6582\tLR: 6.707161\n",
      "Training Epoch: 68 [3840/50000]\tLoss: 4.7224\tLR: 6.707417\n",
      "Training Epoch: 68 [3968/50000]\tLoss: 4.7731\tLR: 6.707673\n",
      "Training Epoch: 68 [4096/50000]\tLoss: 4.7073\tLR: 6.707928\n",
      "Training Epoch: 68 [4224/50000]\tLoss: 4.7336\tLR: 6.708184\n",
      "Training Epoch: 68 [4352/50000]\tLoss: 4.6807\tLR: 6.708440\n",
      "Training Epoch: 68 [4480/50000]\tLoss: 4.7618\tLR: 6.708696\n",
      "Training Epoch: 68 [4608/50000]\tLoss: 4.8103\tLR: 6.708951\n",
      "Training Epoch: 68 [4736/50000]\tLoss: 4.7358\tLR: 6.709207\n",
      "Training Epoch: 68 [4864/50000]\tLoss: 4.7399\tLR: 6.709463\n",
      "Training Epoch: 68 [4992/50000]\tLoss: 4.7636\tLR: 6.709719\n",
      "Training Epoch: 68 [5120/50000]\tLoss: 4.7209\tLR: 6.709974\n",
      "Training Epoch: 68 [5248/50000]\tLoss: 4.6877\tLR: 6.710230\n",
      "Training Epoch: 68 [5376/50000]\tLoss: 4.7157\tLR: 6.710486\n",
      "Training Epoch: 68 [5504/50000]\tLoss: 4.8328\tLR: 6.710742\n",
      "Training Epoch: 68 [5632/50000]\tLoss: 4.7712\tLR: 6.710997\n",
      "Training Epoch: 68 [5760/50000]\tLoss: 4.7193\tLR: 6.711253\n",
      "Training Epoch: 68 [5888/50000]\tLoss: 4.7212\tLR: 6.711509\n",
      "Training Epoch: 68 [6016/50000]\tLoss: 4.6313\tLR: 6.711765\n",
      "Training Epoch: 68 [6144/50000]\tLoss: 4.7366\tLR: 6.712020\n",
      "Training Epoch: 68 [6272/50000]\tLoss: 4.8037\tLR: 6.712276\n",
      "Training Epoch: 68 [6400/50000]\tLoss: 4.7271\tLR: 6.712532\n",
      "Training Epoch: 68 [6528/50000]\tLoss: 4.7353\tLR: 6.712788\n",
      "Training Epoch: 68 [6656/50000]\tLoss: 4.7528\tLR: 6.713043\n",
      "Training Epoch: 68 [6784/50000]\tLoss: 4.7231\tLR: 6.713299\n",
      "Training Epoch: 68 [6912/50000]\tLoss: 4.6746\tLR: 6.713555\n",
      "Training Epoch: 68 [7040/50000]\tLoss: 4.7266\tLR: 6.713811\n",
      "Training Epoch: 68 [7168/50000]\tLoss: 4.6915\tLR: 6.714066\n",
      "Training Epoch: 68 [7296/50000]\tLoss: 4.6747\tLR: 6.714322\n",
      "Training Epoch: 68 [7424/50000]\tLoss: 4.7614\tLR: 6.714578\n",
      "Training Epoch: 68 [7552/50000]\tLoss: 4.7242\tLR: 6.714834\n",
      "Training Epoch: 68 [7680/50000]\tLoss: 4.7761\tLR: 6.715090\n",
      "Training Epoch: 68 [7808/50000]\tLoss: 4.6986\tLR: 6.715345\n",
      "Training Epoch: 68 [7936/50000]\tLoss: 4.7703\tLR: 6.715601\n",
      "Training Epoch: 68 [8064/50000]\tLoss: 4.7342\tLR: 6.715857\n",
      "Training Epoch: 68 [8192/50000]\tLoss: 4.6543\tLR: 6.716113\n",
      "Training Epoch: 68 [8320/50000]\tLoss: 4.7173\tLR: 6.716368\n",
      "Training Epoch: 68 [8448/50000]\tLoss: 4.7709\tLR: 6.716624\n",
      "Training Epoch: 68 [8576/50000]\tLoss: 4.6892\tLR: 6.716880\n",
      "Training Epoch: 68 [8704/50000]\tLoss: 4.6983\tLR: 6.717136\n",
      "Training Epoch: 68 [8832/50000]\tLoss: 4.7213\tLR: 6.717391\n",
      "Training Epoch: 68 [8960/50000]\tLoss: 4.7008\tLR: 6.717647\n",
      "Training Epoch: 68 [9088/50000]\tLoss: 4.7352\tLR: 6.717903\n",
      "Training Epoch: 68 [9216/50000]\tLoss: 4.7585\tLR: 6.718159\n",
      "Training Epoch: 68 [9344/50000]\tLoss: 4.7045\tLR: 6.718414\n",
      "Training Epoch: 68 [9472/50000]\tLoss: 4.7923\tLR: 6.718670\n",
      "Training Epoch: 68 [9600/50000]\tLoss: 4.7368\tLR: 6.718926\n",
      "Training Epoch: 68 [9728/50000]\tLoss: 4.6803\tLR: 6.719182\n",
      "Training Epoch: 68 [9856/50000]\tLoss: 4.7376\tLR: 6.719437\n",
      "Training Epoch: 68 [9984/50000]\tLoss: 4.8601\tLR: 6.719693\n",
      "Training Epoch: 68 [10112/50000]\tLoss: 4.7482\tLR: 6.719949\n",
      "Training Epoch: 68 [10240/50000]\tLoss: 4.7912\tLR: 6.720205\n",
      "Training Epoch: 68 [10368/50000]\tLoss: 4.7978\tLR: 6.720460\n",
      "Training Epoch: 68 [10496/50000]\tLoss: 4.7652\tLR: 6.720716\n",
      "Training Epoch: 68 [10624/50000]\tLoss: 4.8396\tLR: 6.720972\n",
      "Training Epoch: 68 [10752/50000]\tLoss: 4.7197\tLR: 6.721228\n",
      "Training Epoch: 68 [10880/50000]\tLoss: 4.7424\tLR: 6.721483\n",
      "Training Epoch: 68 [11008/50000]\tLoss: 4.6907\tLR: 6.721739\n",
      "Training Epoch: 68 [11136/50000]\tLoss: 4.7136\tLR: 6.721995\n",
      "Training Epoch: 68 [11264/50000]\tLoss: 4.7538\tLR: 6.722251\n",
      "Training Epoch: 68 [11392/50000]\tLoss: 4.7714\tLR: 6.722506\n",
      "Training Epoch: 68 [11520/50000]\tLoss: 4.8156\tLR: 6.722762\n",
      "Training Epoch: 68 [11648/50000]\tLoss: 4.7890\tLR: 6.723018\n",
      "Training Epoch: 68 [11776/50000]\tLoss: 4.7681\tLR: 6.723274\n",
      "Training Epoch: 68 [11904/50000]\tLoss: 4.7365\tLR: 6.723529\n",
      "Training Epoch: 68 [12032/50000]\tLoss: 4.8123\tLR: 6.723785\n",
      "Training Epoch: 68 [12160/50000]\tLoss: 4.7168\tLR: 6.724041\n",
      "Training Epoch: 68 [12288/50000]\tLoss: 4.7450\tLR: 6.724297\n",
      "Training Epoch: 68 [12416/50000]\tLoss: 4.6811\tLR: 6.724552\n",
      "Training Epoch: 68 [12544/50000]\tLoss: 4.7246\tLR: 6.724808\n",
      "Training Epoch: 68 [12672/50000]\tLoss: 4.7528\tLR: 6.725064\n",
      "Training Epoch: 68 [12800/50000]\tLoss: 4.7293\tLR: 6.725320\n",
      "Training Epoch: 68 [12928/50000]\tLoss: 4.7939\tLR: 6.725575\n",
      "Training Epoch: 68 [13056/50000]\tLoss: 4.7332\tLR: 6.725831\n",
      "Training Epoch: 68 [13184/50000]\tLoss: 4.7654\tLR: 6.726087\n",
      "Training Epoch: 68 [13312/50000]\tLoss: 4.7562\tLR: 6.726343\n",
      "Training Epoch: 68 [13440/50000]\tLoss: 4.7179\tLR: 6.726598\n",
      "Training Epoch: 68 [13568/50000]\tLoss: 4.7162\tLR: 6.726854\n",
      "Training Epoch: 68 [13696/50000]\tLoss: 4.7181\tLR: 6.727110\n",
      "Training Epoch: 68 [13824/50000]\tLoss: 4.6859\tLR: 6.727366\n",
      "Training Epoch: 68 [13952/50000]\tLoss: 4.7328\tLR: 6.727621\n",
      "Training Epoch: 68 [14080/50000]\tLoss: 4.7768\tLR: 6.727877\n",
      "Training Epoch: 68 [14208/50000]\tLoss: 4.7003\tLR: 6.728133\n",
      "Training Epoch: 68 [14336/50000]\tLoss: 4.6974\tLR: 6.728389\n",
      "Training Epoch: 68 [14464/50000]\tLoss: 4.7689\tLR: 6.728645\n",
      "Training Epoch: 68 [14592/50000]\tLoss: 4.7315\tLR: 6.728900\n",
      "Training Epoch: 68 [14720/50000]\tLoss: 4.7852\tLR: 6.729156\n",
      "Training Epoch: 68 [14848/50000]\tLoss: 4.6776\tLR: 6.729412\n",
      "Training Epoch: 68 [14976/50000]\tLoss: 4.7590\tLR: 6.729668\n",
      "Training Epoch: 68 [15104/50000]\tLoss: 4.6965\tLR: 6.729923\n",
      "Training Epoch: 68 [15232/50000]\tLoss: 4.8465\tLR: 6.730179\n",
      "Training Epoch: 68 [15360/50000]\tLoss: 4.6855\tLR: 6.730435\n",
      "Training Epoch: 68 [15488/50000]\tLoss: 4.7410\tLR: 6.730691\n",
      "Training Epoch: 68 [15616/50000]\tLoss: 4.7350\tLR: 6.730946\n",
      "Training Epoch: 68 [15744/50000]\tLoss: 4.6566\tLR: 6.731202\n",
      "Training Epoch: 68 [15872/50000]\tLoss: 4.6723\tLR: 6.731458\n",
      "Training Epoch: 68 [16000/50000]\tLoss: 4.7305\tLR: 6.731714\n",
      "Training Epoch: 68 [16128/50000]\tLoss: 4.8022\tLR: 6.731969\n",
      "Training Epoch: 68 [16256/50000]\tLoss: 4.6946\tLR: 6.732225\n",
      "Training Epoch: 68 [16384/50000]\tLoss: 4.7246\tLR: 6.732481\n",
      "Training Epoch: 68 [16512/50000]\tLoss: 4.7114\tLR: 6.732737\n",
      "Training Epoch: 68 [16640/50000]\tLoss: 4.7425\tLR: 6.732992\n",
      "Training Epoch: 68 [16768/50000]\tLoss: 4.7163\tLR: 6.733248\n",
      "Training Epoch: 68 [16896/50000]\tLoss: 4.7206\tLR: 6.733504\n",
      "Training Epoch: 68 [17024/50000]\tLoss: 4.6687\tLR: 6.733760\n",
      "Training Epoch: 68 [17152/50000]\tLoss: 4.6507\tLR: 6.734015\n",
      "Training Epoch: 68 [17280/50000]\tLoss: 4.7506\tLR: 6.734271\n",
      "Training Epoch: 68 [17408/50000]\tLoss: 4.7633\tLR: 6.734527\n",
      "Training Epoch: 68 [17536/50000]\tLoss: 4.7448\tLR: 6.734783\n",
      "Training Epoch: 68 [17664/50000]\tLoss: 4.7818\tLR: 6.735038\n",
      "Training Epoch: 68 [17792/50000]\tLoss: 4.6801\tLR: 6.735294\n",
      "Training Epoch: 68 [17920/50000]\tLoss: 4.6752\tLR: 6.735550\n",
      "Training Epoch: 68 [18048/50000]\tLoss: 4.7076\tLR: 6.735806\n",
      "Training Epoch: 68 [18176/50000]\tLoss: 4.7920\tLR: 6.736061\n",
      "Training Epoch: 68 [18304/50000]\tLoss: 4.7476\tLR: 6.736317\n",
      "Training Epoch: 68 [18432/50000]\tLoss: 4.7631\tLR: 6.736573\n",
      "Training Epoch: 68 [18560/50000]\tLoss: 4.7047\tLR: 6.736829\n",
      "Training Epoch: 68 [18688/50000]\tLoss: 4.7882\tLR: 6.737084\n",
      "Training Epoch: 68 [18816/50000]\tLoss: 4.7224\tLR: 6.737340\n",
      "Training Epoch: 68 [18944/50000]\tLoss: 4.6935\tLR: 6.737596\n",
      "Training Epoch: 68 [19072/50000]\tLoss: 4.7143\tLR: 6.737852\n",
      "Training Epoch: 68 [19200/50000]\tLoss: 4.6828\tLR: 6.738107\n",
      "Training Epoch: 68 [19328/50000]\tLoss: 4.6967\tLR: 6.738363\n",
      "Training Epoch: 68 [19456/50000]\tLoss: 4.7800\tLR: 6.738619\n",
      "Training Epoch: 68 [19584/50000]\tLoss: 4.6798\tLR: 6.738875\n",
      "Training Epoch: 68 [19712/50000]\tLoss: 4.6827\tLR: 6.739130\n",
      "Training Epoch: 68 [19840/50000]\tLoss: 4.7213\tLR: 6.739386\n",
      "Training Epoch: 68 [19968/50000]\tLoss: 4.6988\tLR: 6.739642\n",
      "Training Epoch: 68 [20096/50000]\tLoss: 4.7362\tLR: 6.739898\n",
      "Training Epoch: 68 [20224/50000]\tLoss: 4.6692\tLR: 6.740153\n",
      "Training Epoch: 68 [20352/50000]\tLoss: 4.7069\tLR: 6.740409\n",
      "Training Epoch: 68 [20480/50000]\tLoss: 4.7817\tLR: 6.740665\n",
      "Training Epoch: 68 [20608/50000]\tLoss: 4.7143\tLR: 6.740921\n",
      "Training Epoch: 68 [20736/50000]\tLoss: 4.7503\tLR: 6.741176\n",
      "Training Epoch: 68 [20864/50000]\tLoss: 4.7531\tLR: 6.741432\n",
      "Training Epoch: 68 [20992/50000]\tLoss: 4.7747\tLR: 6.741688\n",
      "Training Epoch: 68 [21120/50000]\tLoss: 4.7590\tLR: 6.741944\n",
      "Training Epoch: 68 [21248/50000]\tLoss: 4.7073\tLR: 6.742199\n",
      "Training Epoch: 68 [21376/50000]\tLoss: 4.8211\tLR: 6.742455\n",
      "Training Epoch: 68 [21504/50000]\tLoss: 4.6506\tLR: 6.742711\n",
      "Training Epoch: 68 [21632/50000]\tLoss: 4.7106\tLR: 6.742967\n",
      "Training Epoch: 68 [21760/50000]\tLoss: 4.7288\tLR: 6.743223\n",
      "Training Epoch: 68 [21888/50000]\tLoss: 4.6643\tLR: 6.743478\n",
      "Training Epoch: 68 [22016/50000]\tLoss: 4.7006\tLR: 6.743734\n",
      "Training Epoch: 68 [22144/50000]\tLoss: 4.8228\tLR: 6.743990\n",
      "Training Epoch: 68 [22272/50000]\tLoss: 4.6523\tLR: 6.744246\n",
      "Training Epoch: 68 [22400/50000]\tLoss: 4.6971\tLR: 6.744501\n",
      "Training Epoch: 68 [22528/50000]\tLoss: 4.6884\tLR: 6.744757\n",
      "Training Epoch: 68 [22656/50000]\tLoss: 4.7277\tLR: 6.745013\n",
      "Training Epoch: 68 [22784/50000]\tLoss: 4.7624\tLR: 6.745269\n",
      "Training Epoch: 68 [22912/50000]\tLoss: 4.8479\tLR: 6.745524\n",
      "Training Epoch: 68 [23040/50000]\tLoss: 4.7868\tLR: 6.745780\n",
      "Training Epoch: 68 [23168/50000]\tLoss: 4.7303\tLR: 6.746036\n",
      "Training Epoch: 68 [23296/50000]\tLoss: 4.6613\tLR: 6.746292\n",
      "Training Epoch: 68 [23424/50000]\tLoss: 4.7829\tLR: 6.746547\n",
      "Training Epoch: 68 [23552/50000]\tLoss: 4.7503\tLR: 6.746803\n",
      "Training Epoch: 68 [23680/50000]\tLoss: 4.7857\tLR: 6.747059\n",
      "Training Epoch: 68 [23808/50000]\tLoss: 4.7088\tLR: 6.747315\n",
      "Training Epoch: 68 [23936/50000]\tLoss: 4.8067\tLR: 6.747570\n",
      "Training Epoch: 68 [24064/50000]\tLoss: 4.7743\tLR: 6.747826\n",
      "Training Epoch: 68 [24192/50000]\tLoss: 4.7226\tLR: 6.748082\n",
      "Training Epoch: 68 [24320/50000]\tLoss: 4.7256\tLR: 6.748338\n",
      "Training Epoch: 68 [24448/50000]\tLoss: 4.7771\tLR: 6.748593\n",
      "Training Epoch: 68 [24576/50000]\tLoss: 4.8481\tLR: 6.748849\n",
      "Training Epoch: 68 [24704/50000]\tLoss: 4.7741\tLR: 6.749105\n",
      "Training Epoch: 68 [24832/50000]\tLoss: 4.7712\tLR: 6.749361\n",
      "Training Epoch: 68 [24960/50000]\tLoss: 4.7056\tLR: 6.749616\n",
      "Training Epoch: 68 [25088/50000]\tLoss: 4.7081\tLR: 6.749872\n",
      "Training Epoch: 68 [25216/50000]\tLoss: 4.7564\tLR: 6.750128\n",
      "Training Epoch: 68 [25344/50000]\tLoss: 4.7185\tLR: 6.750384\n",
      "Training Epoch: 68 [25472/50000]\tLoss: 4.7477\tLR: 6.750639\n",
      "Training Epoch: 68 [25600/50000]\tLoss: 4.6821\tLR: 6.750895\n",
      "Training Epoch: 68 [25728/50000]\tLoss: 4.7741\tLR: 6.751151\n",
      "Training Epoch: 68 [25856/50000]\tLoss: 4.7221\tLR: 6.751407\n",
      "Training Epoch: 68 [25984/50000]\tLoss: 4.8198\tLR: 6.751662\n",
      "Training Epoch: 68 [26112/50000]\tLoss: 4.7447\tLR: 6.751918\n",
      "Training Epoch: 68 [26240/50000]\tLoss: 4.7910\tLR: 6.752174\n",
      "Training Epoch: 68 [26368/50000]\tLoss: 4.7893\tLR: 6.752430\n",
      "Training Epoch: 68 [26496/50000]\tLoss: 4.7219\tLR: 6.752685\n",
      "Training Epoch: 68 [26624/50000]\tLoss: 4.7862\tLR: 6.752941\n",
      "Training Epoch: 68 [26752/50000]\tLoss: 4.7486\tLR: 6.753197\n",
      "Training Epoch: 68 [26880/50000]\tLoss: 4.7410\tLR: 6.753453\n",
      "Training Epoch: 68 [27008/50000]\tLoss: 4.7000\tLR: 6.753708\n",
      "Training Epoch: 68 [27136/50000]\tLoss: 4.8255\tLR: 6.753964\n",
      "Training Epoch: 68 [27264/50000]\tLoss: 4.6470\tLR: 6.754220\n",
      "Training Epoch: 68 [27392/50000]\tLoss: 4.7730\tLR: 6.754476\n",
      "Training Epoch: 68 [27520/50000]\tLoss: 4.7726\tLR: 6.754731\n",
      "Training Epoch: 68 [27648/50000]\tLoss: 4.7322\tLR: 6.754987\n",
      "Training Epoch: 68 [27776/50000]\tLoss: 4.8111\tLR: 6.755243\n",
      "Training Epoch: 68 [27904/50000]\tLoss: 4.7477\tLR: 6.755499\n",
      "Training Epoch: 68 [28032/50000]\tLoss: 4.7258\tLR: 6.755754\n",
      "Training Epoch: 68 [28160/50000]\tLoss: 4.7668\tLR: 6.756010\n",
      "Training Epoch: 68 [28288/50000]\tLoss: 4.7420\tLR: 6.756266\n",
      "Training Epoch: 68 [28416/50000]\tLoss: 4.6771\tLR: 6.756522\n",
      "Training Epoch: 68 [28544/50000]\tLoss: 4.7905\tLR: 6.756777\n",
      "Training Epoch: 68 [28672/50000]\tLoss: 4.7633\tLR: 6.757033\n",
      "Training Epoch: 68 [28800/50000]\tLoss: 4.6982\tLR: 6.757289\n",
      "Training Epoch: 68 [28928/50000]\tLoss: 4.7872\tLR: 6.757545\n",
      "Training Epoch: 68 [29056/50000]\tLoss: 4.7393\tLR: 6.757801\n",
      "Training Epoch: 68 [29184/50000]\tLoss: 4.7245\tLR: 6.758056\n",
      "Training Epoch: 68 [29312/50000]\tLoss: 4.7422\tLR: 6.758312\n",
      "Training Epoch: 68 [29440/50000]\tLoss: 4.7356\tLR: 6.758568\n",
      "Training Epoch: 68 [29568/50000]\tLoss: 4.6432\tLR: 6.758824\n",
      "Training Epoch: 68 [29696/50000]\tLoss: 4.7922\tLR: 6.759079\n",
      "Training Epoch: 68 [29824/50000]\tLoss: 4.7703\tLR: 6.759335\n",
      "Training Epoch: 68 [29952/50000]\tLoss: 4.7534\tLR: 6.759591\n",
      "Training Epoch: 68 [30080/50000]\tLoss: 4.7905\tLR: 6.759847\n",
      "Training Epoch: 68 [30208/50000]\tLoss: 4.7864\tLR: 6.760102\n",
      "Training Epoch: 68 [30336/50000]\tLoss: 4.8089\tLR: 6.760358\n",
      "Training Epoch: 68 [30464/50000]\tLoss: 4.6826\tLR: 6.760614\n",
      "Training Epoch: 68 [30592/50000]\tLoss: 4.7637\tLR: 6.760870\n",
      "Training Epoch: 68 [30720/50000]\tLoss: 4.6765\tLR: 6.761125\n",
      "Training Epoch: 68 [30848/50000]\tLoss: 4.7115\tLR: 6.761381\n",
      "Training Epoch: 68 [30976/50000]\tLoss: 4.7293\tLR: 6.761637\n",
      "Training Epoch: 68 [31104/50000]\tLoss: 4.7992\tLR: 6.761893\n",
      "Training Epoch: 68 [31232/50000]\tLoss: 4.8561\tLR: 6.762148\n",
      "Training Epoch: 68 [31360/50000]\tLoss: 4.6940\tLR: 6.762404\n",
      "Training Epoch: 68 [31488/50000]\tLoss: 4.8304\tLR: 6.762660\n",
      "Training Epoch: 68 [31616/50000]\tLoss: 4.8179\tLR: 6.762916\n",
      "Training Epoch: 68 [31744/50000]\tLoss: 4.8322\tLR: 6.763171\n",
      "Training Epoch: 68 [31872/50000]\tLoss: 4.7011\tLR: 6.763427\n",
      "Training Epoch: 68 [32000/50000]\tLoss: 4.6785\tLR: 6.763683\n",
      "Training Epoch: 68 [32128/50000]\tLoss: 4.7912\tLR: 6.763939\n",
      "Training Epoch: 68 [32256/50000]\tLoss: 4.6896\tLR: 6.764194\n",
      "Training Epoch: 68 [32384/50000]\tLoss: 4.7148\tLR: 6.764450\n",
      "Training Epoch: 68 [32512/50000]\tLoss: 4.7645\tLR: 6.764706\n",
      "Training Epoch: 68 [32640/50000]\tLoss: 4.6788\tLR: 6.764962\n",
      "Training Epoch: 68 [32768/50000]\tLoss: 4.8090\tLR: 6.765217\n",
      "Training Epoch: 68 [32896/50000]\tLoss: 4.7774\tLR: 6.765473\n",
      "Training Epoch: 68 [33024/50000]\tLoss: 4.6697\tLR: 6.765729\n",
      "Training Epoch: 68 [33152/50000]\tLoss: 4.8406\tLR: 6.765985\n",
      "Training Epoch: 68 [33280/50000]\tLoss: 4.7769\tLR: 6.766240\n",
      "Training Epoch: 68 [33408/50000]\tLoss: 4.6899\tLR: 6.766496\n",
      "Training Epoch: 68 [33536/50000]\tLoss: 4.7174\tLR: 6.766752\n",
      "Training Epoch: 68 [33664/50000]\tLoss: 4.7413\tLR: 6.767008\n",
      "Training Epoch: 68 [33792/50000]\tLoss: 4.6654\tLR: 6.767263\n",
      "Training Epoch: 68 [33920/50000]\tLoss: 4.7593\tLR: 6.767519\n",
      "Training Epoch: 68 [34048/50000]\tLoss: 4.7751\tLR: 6.767775\n",
      "Training Epoch: 68 [34176/50000]\tLoss: 4.7859\tLR: 6.768031\n",
      "Training Epoch: 68 [34304/50000]\tLoss: 4.7854\tLR: 6.768286\n",
      "Training Epoch: 68 [34432/50000]\tLoss: 4.7020\tLR: 6.768542\n",
      "Training Epoch: 68 [34560/50000]\tLoss: 4.6787\tLR: 6.768798\n",
      "Training Epoch: 68 [34688/50000]\tLoss: 4.7823\tLR: 6.769054\n",
      "Training Epoch: 68 [34816/50000]\tLoss: 4.7353\tLR: 6.769309\n",
      "Training Epoch: 68 [34944/50000]\tLoss: 4.7835\tLR: 6.769565\n",
      "Training Epoch: 68 [35072/50000]\tLoss: 4.7117\tLR: 6.769821\n",
      "Training Epoch: 68 [35200/50000]\tLoss: 4.7790\tLR: 6.770077\n",
      "Training Epoch: 68 [35328/50000]\tLoss: 4.6922\tLR: 6.770332\n",
      "Training Epoch: 68 [35456/50000]\tLoss: 4.8349\tLR: 6.770588\n",
      "Training Epoch: 68 [35584/50000]\tLoss: 4.7304\tLR: 6.770844\n",
      "Training Epoch: 68 [35712/50000]\tLoss: 4.6861\tLR: 6.771100\n",
      "Training Epoch: 68 [35840/50000]\tLoss: 4.7065\tLR: 6.771355\n",
      "Training Epoch: 68 [35968/50000]\tLoss: 4.7284\tLR: 6.771611\n",
      "Training Epoch: 68 [36096/50000]\tLoss: 4.6935\tLR: 6.771867\n",
      "Training Epoch: 68 [36224/50000]\tLoss: 4.7398\tLR: 6.772123\n",
      "Training Epoch: 68 [36352/50000]\tLoss: 4.7678\tLR: 6.772379\n",
      "Training Epoch: 68 [36480/50000]\tLoss: 4.7095\tLR: 6.772634\n",
      "Training Epoch: 68 [36608/50000]\tLoss: 4.7257\tLR: 6.772890\n",
      "Training Epoch: 68 [36736/50000]\tLoss: 4.7196\tLR: 6.773146\n",
      "Training Epoch: 68 [36864/50000]\tLoss: 4.7527\tLR: 6.773402\n",
      "Training Epoch: 68 [36992/50000]\tLoss: 4.7165\tLR: 6.773657\n",
      "Training Epoch: 68 [37120/50000]\tLoss: 4.6987\tLR: 6.773913\n",
      "Training Epoch: 68 [37248/50000]\tLoss: 4.7767\tLR: 6.774169\n",
      "Training Epoch: 68 [37376/50000]\tLoss: 4.7868\tLR: 6.774425\n",
      "Training Epoch: 68 [37504/50000]\tLoss: 4.8795\tLR: 6.774680\n",
      "Training Epoch: 68 [37632/50000]\tLoss: 4.7381\tLR: 6.774936\n",
      "Training Epoch: 68 [37760/50000]\tLoss: 4.7681\tLR: 6.775192\n",
      "Training Epoch: 68 [37888/50000]\tLoss: 4.7353\tLR: 6.775448\n",
      "Training Epoch: 68 [38016/50000]\tLoss: 4.7061\tLR: 6.775703\n",
      "Training Epoch: 68 [38144/50000]\tLoss: 4.7325\tLR: 6.775959\n",
      "Training Epoch: 68 [38272/50000]\tLoss: 4.7083\tLR: 6.776215\n",
      "Training Epoch: 68 [38400/50000]\tLoss: 4.7032\tLR: 6.776471\n",
      "Training Epoch: 68 [38528/50000]\tLoss: 4.7501\tLR: 6.776726\n",
      "Training Epoch: 68 [38656/50000]\tLoss: 4.6707\tLR: 6.776982\n",
      "Training Epoch: 68 [38784/50000]\tLoss: 4.7164\tLR: 6.777238\n",
      "Training Epoch: 68 [38912/50000]\tLoss: 4.8076\tLR: 6.777494\n",
      "Training Epoch: 68 [39040/50000]\tLoss: 4.7709\tLR: 6.777749\n",
      "Training Epoch: 68 [39168/50000]\tLoss: 4.7741\tLR: 6.778005\n",
      "Training Epoch: 68 [39296/50000]\tLoss: 4.6909\tLR: 6.778261\n",
      "Training Epoch: 68 [39424/50000]\tLoss: 4.6562\tLR: 6.778517\n",
      "Training Epoch: 68 [39552/50000]\tLoss: 4.7381\tLR: 6.778772\n",
      "Training Epoch: 68 [39680/50000]\tLoss: 4.7174\tLR: 6.779028\n",
      "Training Epoch: 68 [39808/50000]\tLoss: 4.6813\tLR: 6.779284\n",
      "Training Epoch: 68 [39936/50000]\tLoss: 4.7964\tLR: 6.779540\n",
      "Training Epoch: 68 [40064/50000]\tLoss: 4.7335\tLR: 6.779795\n",
      "Training Epoch: 68 [40192/50000]\tLoss: 4.7765\tLR: 6.780051\n",
      "Training Epoch: 68 [40320/50000]\tLoss: 4.7518\tLR: 6.780307\n",
      "Training Epoch: 68 [40448/50000]\tLoss: 4.7304\tLR: 6.780563\n",
      "Training Epoch: 68 [40576/50000]\tLoss: 4.7942\tLR: 6.780818\n",
      "Training Epoch: 68 [40704/50000]\tLoss: 4.7293\tLR: 6.781074\n",
      "Training Epoch: 68 [40832/50000]\tLoss: 4.8382\tLR: 6.781330\n",
      "Training Epoch: 68 [40960/50000]\tLoss: 4.7179\tLR: 6.781586\n",
      "Training Epoch: 68 [41088/50000]\tLoss: 4.7048\tLR: 6.781841\n",
      "Training Epoch: 68 [41216/50000]\tLoss: 4.7487\tLR: 6.782097\n",
      "Training Epoch: 68 [41344/50000]\tLoss: 4.7449\tLR: 6.782353\n",
      "Training Epoch: 68 [41472/50000]\tLoss: 4.6224\tLR: 6.782609\n",
      "Training Epoch: 68 [41600/50000]\tLoss: 4.6923\tLR: 6.782864\n",
      "Training Epoch: 68 [41728/50000]\tLoss: 4.7106\tLR: 6.783120\n",
      "Training Epoch: 68 [41856/50000]\tLoss: 4.8192\tLR: 6.783376\n",
      "Training Epoch: 68 [41984/50000]\tLoss: 4.8682\tLR: 6.783632\n",
      "Training Epoch: 68 [42112/50000]\tLoss: 4.8388\tLR: 6.783887\n",
      "Training Epoch: 68 [42240/50000]\tLoss: 4.7666\tLR: 6.784143\n",
      "Training Epoch: 68 [42368/50000]\tLoss: 4.8071\tLR: 6.784399\n",
      "Training Epoch: 68 [42496/50000]\tLoss: 4.7846\tLR: 6.784655\n",
      "Training Epoch: 68 [42624/50000]\tLoss: 4.7778\tLR: 6.784910\n",
      "Training Epoch: 68 [42752/50000]\tLoss: 4.6610\tLR: 6.785166\n",
      "Training Epoch: 68 [42880/50000]\tLoss: 4.7095\tLR: 6.785422\n",
      "Training Epoch: 68 [43008/50000]\tLoss: 4.7777\tLR: 6.785678\n",
      "Training Epoch: 68 [43136/50000]\tLoss: 4.7918\tLR: 6.785934\n",
      "Training Epoch: 68 [43264/50000]\tLoss: 4.8309\tLR: 6.786189\n",
      "Training Epoch: 68 [43392/50000]\tLoss: 4.7981\tLR: 6.786445\n",
      "Training Epoch: 68 [43520/50000]\tLoss: 4.8112\tLR: 6.786701\n",
      "Training Epoch: 68 [43648/50000]\tLoss: 4.9006\tLR: 6.786957\n",
      "Training Epoch: 68 [43776/50000]\tLoss: 4.7628\tLR: 6.787212\n",
      "Training Epoch: 68 [43904/50000]\tLoss: 4.7418\tLR: 6.787468\n",
      "Training Epoch: 68 [44032/50000]\tLoss: 4.7406\tLR: 6.787724\n",
      "Training Epoch: 68 [44160/50000]\tLoss: 4.7628\tLR: 6.787980\n",
      "Training Epoch: 68 [44288/50000]\tLoss: 4.6972\tLR: 6.788235\n",
      "Training Epoch: 68 [44416/50000]\tLoss: 4.6579\tLR: 6.788491\n",
      "Training Epoch: 68 [44544/50000]\tLoss: 4.8859\tLR: 6.788747\n",
      "Training Epoch: 68 [44672/50000]\tLoss: 4.7660\tLR: 6.789003\n",
      "Training Epoch: 68 [44800/50000]\tLoss: 4.8357\tLR: 6.789258\n",
      "Training Epoch: 68 [44928/50000]\tLoss: 4.7783\tLR: 6.789514\n",
      "Training Epoch: 68 [45056/50000]\tLoss: 4.7874\tLR: 6.789770\n",
      "Training Epoch: 68 [45184/50000]\tLoss: 4.6835\tLR: 6.790026\n",
      "Training Epoch: 68 [45312/50000]\tLoss: 4.6729\tLR: 6.790281\n",
      "Training Epoch: 68 [45440/50000]\tLoss: 4.6899\tLR: 6.790537\n",
      "Training Epoch: 68 [45568/50000]\tLoss: 4.8826\tLR: 6.790793\n",
      "Training Epoch: 68 [45696/50000]\tLoss: 4.6651\tLR: 6.791049\n",
      "Training Epoch: 68 [45824/50000]\tLoss: 4.8500\tLR: 6.791304\n",
      "Training Epoch: 68 [45952/50000]\tLoss: 4.7492\tLR: 6.791560\n",
      "Training Epoch: 68 [46080/50000]\tLoss: 4.7411\tLR: 6.791816\n",
      "Training Epoch: 68 [46208/50000]\tLoss: 4.7779\tLR: 6.792072\n",
      "Training Epoch: 68 [46336/50000]\tLoss: 4.7276\tLR: 6.792327\n",
      "Training Epoch: 68 [46464/50000]\tLoss: 4.7379\tLR: 6.792583\n",
      "Training Epoch: 68 [46592/50000]\tLoss: 4.6284\tLR: 6.792839\n",
      "Training Epoch: 68 [46720/50000]\tLoss: 4.7255\tLR: 6.793095\n",
      "Training Epoch: 68 [46848/50000]\tLoss: 4.7022\tLR: 6.793350\n",
      "Training Epoch: 68 [46976/50000]\tLoss: 4.8135\tLR: 6.793606\n",
      "Training Epoch: 68 [47104/50000]\tLoss: 4.7547\tLR: 6.793862\n",
      "Training Epoch: 68 [47232/50000]\tLoss: 4.7858\tLR: 6.794118\n",
      "Training Epoch: 68 [47360/50000]\tLoss: 4.7491\tLR: 6.794373\n",
      "Training Epoch: 68 [47488/50000]\tLoss: 4.7485\tLR: 6.794629\n",
      "Training Epoch: 68 [47616/50000]\tLoss: 4.7613\tLR: 6.794885\n",
      "Training Epoch: 68 [47744/50000]\tLoss: 4.7119\tLR: 6.795141\n",
      "Training Epoch: 68 [47872/50000]\tLoss: 4.7238\tLR: 6.795396\n",
      "Training Epoch: 68 [48000/50000]\tLoss: 4.6909\tLR: 6.795652\n",
      "Training Epoch: 68 [48128/50000]\tLoss: 4.7173\tLR: 6.795908\n",
      "Training Epoch: 68 [48256/50000]\tLoss: 4.7525\tLR: 6.796164\n",
      "Training Epoch: 68 [48384/50000]\tLoss: 4.7010\tLR: 6.796419\n",
      "Training Epoch: 68 [48512/50000]\tLoss: 4.6991\tLR: 6.796675\n",
      "Training Epoch: 68 [48640/50000]\tLoss: 4.7324\tLR: 6.796931\n",
      "Training Epoch: 68 [48768/50000]\tLoss: 4.7088\tLR: 6.797187\n",
      "Training Epoch: 68 [48896/50000]\tLoss: 4.7917\tLR: 6.797442\n",
      "Training Epoch: 68 [49024/50000]\tLoss: 4.7105\tLR: 6.797698\n",
      "Training Epoch: 68 [49152/50000]\tLoss: 4.7407\tLR: 6.797954\n",
      "Training Epoch: 68 [49280/50000]\tLoss: 4.7735\tLR: 6.798210\n",
      "Training Epoch: 68 [49408/50000]\tLoss: 4.6586\tLR: 6.798465\n",
      "Training Epoch: 68 [49536/50000]\tLoss: 4.6446\tLR: 6.798721\n",
      "Training Epoch: 68 [49664/50000]\tLoss: 4.7654\tLR: 6.798977\n",
      "Training Epoch: 68 [49792/50000]\tLoss: 4.7354\tLR: 6.799233\n",
      "Training Epoch: 68 [49920/50000]\tLoss: 4.7411\tLR: 6.799488\n",
      "Training Epoch: 68 [50000/50000]\tLoss: 4.7330\tLR: 6.799744\n",
      "epoch 68 training time consumed: 488.81s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   95331 GB |   95331 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   95039 GB |   95039 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     292 GB |     292 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   95331 GB |   95331 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   95039 GB |   95039 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     292 GB |     292 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   93991 GB |   93991 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   93698 GB |   93698 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     292 GB |     292 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   10108 K  |   10108 K  |\n",
      "|       from large pool |      24    |      65    |    4309 K  |    4309 K  |\n",
      "|       from small pool |     231    |     274    |    5799 K  |    5799 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   10108 K  |   10108 K  |\n",
      "|       from large pool |      24    |      65    |    4309 K  |    4309 K  |\n",
      "|       from small pool |     231    |     274    |    5799 K  |    5799 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      47    |    5859 K  |    5859 K  |\n",
      "|       from large pool |      10    |      23    |    2071 K  |    2071 K  |\n",
      "|       from small pool |      26    |      35    |    3788 K  |    3788 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 68, Average loss: 0.0375, Accuracy: 0.0100, Time consumed:31.17s\n",
      "\n",
      "Training Epoch: 69 [128/50000]\tLoss: 4.7479\tLR: 0.020000\n",
      "Training Epoch: 69 [256/50000]\tLoss: 4.7234\tLR: 6.800256\n",
      "Training Epoch: 69 [384/50000]\tLoss: 4.7229\tLR: 6.800512\n",
      "Training Epoch: 69 [512/50000]\tLoss: 4.7127\tLR: 6.800767\n",
      "Training Epoch: 69 [640/50000]\tLoss: 4.6955\tLR: 6.801023\n",
      "Training Epoch: 69 [768/50000]\tLoss: 4.6904\tLR: 6.801279\n",
      "Training Epoch: 69 [896/50000]\tLoss: 4.6972\tLR: 6.801535\n",
      "Training Epoch: 69 [1024/50000]\tLoss: 4.7256\tLR: 6.801790\n",
      "Training Epoch: 69 [1152/50000]\tLoss: 4.8140\tLR: 6.802046\n",
      "Training Epoch: 69 [1280/50000]\tLoss: 4.7852\tLR: 6.802302\n",
      "Training Epoch: 69 [1408/50000]\tLoss: 4.7811\tLR: 6.802558\n",
      "Training Epoch: 69 [1536/50000]\tLoss: 4.7387\tLR: 6.802813\n",
      "Training Epoch: 69 [1664/50000]\tLoss: 4.8223\tLR: 6.803069\n",
      "Training Epoch: 69 [1792/50000]\tLoss: 4.7506\tLR: 6.803325\n",
      "Training Epoch: 69 [1920/50000]\tLoss: 4.7490\tLR: 6.803581\n",
      "Training Epoch: 69 [2048/50000]\tLoss: 4.8211\tLR: 6.803836\n",
      "Training Epoch: 69 [2176/50000]\tLoss: 4.7286\tLR: 6.804092\n",
      "Training Epoch: 69 [2304/50000]\tLoss: 4.7604\tLR: 6.804348\n",
      "Training Epoch: 69 [2432/50000]\tLoss: 4.6916\tLR: 6.804604\n",
      "Training Epoch: 69 [2560/50000]\tLoss: 4.7338\tLR: 6.804859\n",
      "Training Epoch: 69 [2688/50000]\tLoss: 4.6726\tLR: 6.805115\n",
      "Training Epoch: 69 [2816/50000]\tLoss: 4.7959\tLR: 6.805371\n",
      "Training Epoch: 69 [2944/50000]\tLoss: 4.7272\tLR: 6.805627\n",
      "Training Epoch: 69 [3072/50000]\tLoss: 4.7387\tLR: 6.805882\n",
      "Training Epoch: 69 [3200/50000]\tLoss: 4.6863\tLR: 6.806138\n",
      "Training Epoch: 69 [3328/50000]\tLoss: 4.7061\tLR: 6.806394\n",
      "Training Epoch: 69 [3456/50000]\tLoss: 4.7936\tLR: 6.806650\n",
      "Training Epoch: 69 [3584/50000]\tLoss: 4.7741\tLR: 6.806905\n",
      "Training Epoch: 69 [3712/50000]\tLoss: 4.8243\tLR: 6.807161\n",
      "Training Epoch: 69 [3840/50000]\tLoss: 4.7677\tLR: 6.807417\n",
      "Training Epoch: 69 [3968/50000]\tLoss: 4.7578\tLR: 6.807673\n",
      "Training Epoch: 69 [4096/50000]\tLoss: 4.6208\tLR: 6.807928\n",
      "Training Epoch: 69 [4224/50000]\tLoss: 4.7737\tLR: 6.808184\n",
      "Training Epoch: 69 [4352/50000]\tLoss: 4.7446\tLR: 6.808440\n",
      "Training Epoch: 69 [4480/50000]\tLoss: 4.7298\tLR: 6.808696\n",
      "Training Epoch: 69 [4608/50000]\tLoss: 4.7179\tLR: 6.808951\n",
      "Training Epoch: 69 [4736/50000]\tLoss: 4.7336\tLR: 6.809207\n",
      "Training Epoch: 69 [4864/50000]\tLoss: 4.7700\tLR: 6.809463\n",
      "Training Epoch: 69 [4992/50000]\tLoss: 4.6859\tLR: 6.809719\n",
      "Training Epoch: 69 [5120/50000]\tLoss: 4.7156\tLR: 6.809974\n",
      "Training Epoch: 69 [5248/50000]\tLoss: 4.7008\tLR: 6.810230\n",
      "Training Epoch: 69 [5376/50000]\tLoss: 4.7413\tLR: 6.810486\n",
      "Training Epoch: 69 [5504/50000]\tLoss: 4.8209\tLR: 6.810742\n",
      "Training Epoch: 69 [5632/50000]\tLoss: 4.7076\tLR: 6.810997\n",
      "Training Epoch: 69 [5760/50000]\tLoss: 4.7213\tLR: 6.811253\n",
      "Training Epoch: 69 [5888/50000]\tLoss: 4.7032\tLR: 6.811509\n",
      "Training Epoch: 69 [6016/50000]\tLoss: 4.6739\tLR: 6.811765\n",
      "Training Epoch: 69 [6144/50000]\tLoss: 4.7075\tLR: 6.812020\n",
      "Training Epoch: 69 [6272/50000]\tLoss: 4.6944\tLR: 6.812276\n",
      "Training Epoch: 69 [6400/50000]\tLoss: 4.6625\tLR: 6.812532\n",
      "Training Epoch: 69 [6528/50000]\tLoss: 4.6707\tLR: 6.812788\n",
      "Training Epoch: 69 [6656/50000]\tLoss: 4.6830\tLR: 6.813043\n",
      "Training Epoch: 69 [6784/50000]\tLoss: 4.8216\tLR: 6.813299\n",
      "Training Epoch: 69 [6912/50000]\tLoss: 4.7398\tLR: 6.813555\n",
      "Training Epoch: 69 [7040/50000]\tLoss: 4.7371\tLR: 6.813811\n",
      "Training Epoch: 69 [7168/50000]\tLoss: 4.7846\tLR: 6.814066\n",
      "Training Epoch: 69 [7296/50000]\tLoss: 4.7010\tLR: 6.814322\n",
      "Training Epoch: 69 [7424/50000]\tLoss: 4.7061\tLR: 6.814578\n",
      "Training Epoch: 69 [7552/50000]\tLoss: 4.7035\tLR: 6.814834\n",
      "Training Epoch: 69 [7680/50000]\tLoss: 4.6329\tLR: 6.815090\n",
      "Training Epoch: 69 [7808/50000]\tLoss: 4.7038\tLR: 6.815345\n",
      "Training Epoch: 69 [7936/50000]\tLoss: 4.7261\tLR: 6.815601\n",
      "Training Epoch: 69 [8064/50000]\tLoss: 4.6813\tLR: 6.815857\n",
      "Training Epoch: 69 [8192/50000]\tLoss: 4.6803\tLR: 6.816113\n",
      "Training Epoch: 69 [8320/50000]\tLoss: 4.6618\tLR: 6.816368\n",
      "Training Epoch: 69 [8448/50000]\tLoss: 4.7093\tLR: 6.816624\n",
      "Training Epoch: 69 [8576/50000]\tLoss: 4.7103\tLR: 6.816880\n",
      "Training Epoch: 69 [8704/50000]\tLoss: 4.7026\tLR: 6.817136\n",
      "Training Epoch: 69 [8832/50000]\tLoss: 4.7001\tLR: 6.817391\n",
      "Training Epoch: 69 [8960/50000]\tLoss: 4.7120\tLR: 6.817647\n",
      "Training Epoch: 69 [9088/50000]\tLoss: 4.7510\tLR: 6.817903\n",
      "Training Epoch: 69 [9216/50000]\tLoss: 4.7979\tLR: 6.818159\n",
      "Training Epoch: 69 [9344/50000]\tLoss: 4.6954\tLR: 6.818414\n",
      "Training Epoch: 69 [9472/50000]\tLoss: 4.6821\tLR: 6.818670\n",
      "Training Epoch: 69 [9600/50000]\tLoss: 4.7438\tLR: 6.818926\n",
      "Training Epoch: 69 [9728/50000]\tLoss: 4.7275\tLR: 6.819182\n",
      "Training Epoch: 69 [9856/50000]\tLoss: 4.7648\tLR: 6.819437\n",
      "Training Epoch: 69 [9984/50000]\tLoss: 4.6776\tLR: 6.819693\n",
      "Training Epoch: 69 [10112/50000]\tLoss: 4.7452\tLR: 6.819949\n",
      "Training Epoch: 69 [10240/50000]\tLoss: 4.6857\tLR: 6.820205\n",
      "Training Epoch: 69 [10368/50000]\tLoss: 4.6584\tLR: 6.820460\n",
      "Training Epoch: 69 [10496/50000]\tLoss: 4.6661\tLR: 6.820716\n",
      "Training Epoch: 69 [10624/50000]\tLoss: 4.7073\tLR: 6.820972\n",
      "Training Epoch: 69 [10752/50000]\tLoss: 4.7829\tLR: 6.821228\n",
      "Training Epoch: 69 [10880/50000]\tLoss: 4.7182\tLR: 6.821483\n",
      "Training Epoch: 69 [11008/50000]\tLoss: 4.7732\tLR: 6.821739\n",
      "Training Epoch: 69 [11136/50000]\tLoss: 4.7874\tLR: 6.821995\n",
      "Training Epoch: 69 [11264/50000]\tLoss: 4.7403\tLR: 6.822251\n",
      "Training Epoch: 69 [11392/50000]\tLoss: 4.7717\tLR: 6.822506\n",
      "Training Epoch: 69 [11520/50000]\tLoss: 4.7168\tLR: 6.822762\n",
      "Training Epoch: 69 [11648/50000]\tLoss: 4.7367\tLR: 6.823018\n",
      "Training Epoch: 69 [11776/50000]\tLoss: 4.6764\tLR: 6.823274\n",
      "Training Epoch: 69 [11904/50000]\tLoss: 4.7827\tLR: 6.823529\n",
      "Training Epoch: 69 [12032/50000]\tLoss: 4.7419\tLR: 6.823785\n",
      "Training Epoch: 69 [12160/50000]\tLoss: 4.7056\tLR: 6.824041\n",
      "Training Epoch: 69 [12288/50000]\tLoss: 4.7734\tLR: 6.824297\n",
      "Training Epoch: 69 [12416/50000]\tLoss: 4.7080\tLR: 6.824552\n",
      "Training Epoch: 69 [12544/50000]\tLoss: 4.7170\tLR: 6.824808\n",
      "Training Epoch: 69 [12672/50000]\tLoss: 4.6892\tLR: 6.825064\n",
      "Training Epoch: 69 [12800/50000]\tLoss: 4.7931\tLR: 6.825320\n",
      "Training Epoch: 69 [12928/50000]\tLoss: 4.7393\tLR: 6.825575\n",
      "Training Epoch: 69 [13056/50000]\tLoss: 4.7505\tLR: 6.825831\n",
      "Training Epoch: 69 [13184/50000]\tLoss: 4.8236\tLR: 6.826087\n",
      "Training Epoch: 69 [13312/50000]\tLoss: 4.7441\tLR: 6.826343\n",
      "Training Epoch: 69 [13440/50000]\tLoss: 4.7212\tLR: 6.826598\n",
      "Training Epoch: 69 [13568/50000]\tLoss: 4.7523\tLR: 6.826854\n",
      "Training Epoch: 69 [13696/50000]\tLoss: 4.7154\tLR: 6.827110\n",
      "Training Epoch: 69 [13824/50000]\tLoss: 4.8064\tLR: 6.827366\n",
      "Training Epoch: 69 [13952/50000]\tLoss: 4.7076\tLR: 6.827621\n",
      "Training Epoch: 69 [14080/50000]\tLoss: 4.7986\tLR: 6.827877\n",
      "Training Epoch: 69 [14208/50000]\tLoss: 4.6858\tLR: 6.828133\n",
      "Training Epoch: 69 [14336/50000]\tLoss: 4.7752\tLR: 6.828389\n",
      "Training Epoch: 69 [14464/50000]\tLoss: 4.7694\tLR: 6.828645\n",
      "Training Epoch: 69 [14592/50000]\tLoss: 4.7619\tLR: 6.828900\n",
      "Training Epoch: 69 [14720/50000]\tLoss: 4.7685\tLR: 6.829156\n",
      "Training Epoch: 69 [14848/50000]\tLoss: 4.7405\tLR: 6.829412\n",
      "Training Epoch: 69 [14976/50000]\tLoss: 4.7697\tLR: 6.829668\n",
      "Training Epoch: 69 [15104/50000]\tLoss: 4.8287\tLR: 6.829923\n",
      "Training Epoch: 69 [15232/50000]\tLoss: 4.7346\tLR: 6.830179\n",
      "Training Epoch: 69 [15360/50000]\tLoss: 4.7396\tLR: 6.830435\n",
      "Training Epoch: 69 [15488/50000]\tLoss: 4.7765\tLR: 6.830691\n",
      "Training Epoch: 69 [15616/50000]\tLoss: 4.6791\tLR: 6.830946\n",
      "Training Epoch: 69 [15744/50000]\tLoss: 4.7114\tLR: 6.831202\n",
      "Training Epoch: 69 [15872/50000]\tLoss: 4.6323\tLR: 6.831458\n",
      "Training Epoch: 69 [16000/50000]\tLoss: 4.6982\tLR: 6.831714\n",
      "Training Epoch: 69 [16128/50000]\tLoss: 4.7384\tLR: 6.831969\n",
      "Training Epoch: 69 [16256/50000]\tLoss: 4.7748\tLR: 6.832225\n",
      "Training Epoch: 69 [16384/50000]\tLoss: 4.6659\tLR: 6.832481\n",
      "Training Epoch: 69 [16512/50000]\tLoss: 4.8405\tLR: 6.832737\n",
      "Training Epoch: 69 [16640/50000]\tLoss: 4.7474\tLR: 6.832992\n",
      "Training Epoch: 69 [16768/50000]\tLoss: 4.7179\tLR: 6.833248\n",
      "Training Epoch: 69 [16896/50000]\tLoss: 4.8486\tLR: 6.833504\n",
      "Training Epoch: 69 [17024/50000]\tLoss: 4.7196\tLR: 6.833760\n",
      "Training Epoch: 69 [17152/50000]\tLoss: 4.6955\tLR: 6.834015\n",
      "Training Epoch: 69 [17280/50000]\tLoss: 4.7631\tLR: 6.834271\n",
      "Training Epoch: 69 [17408/50000]\tLoss: 4.7767\tLR: 6.834527\n",
      "Training Epoch: 69 [17536/50000]\tLoss: 4.7262\tLR: 6.834783\n",
      "Training Epoch: 69 [17664/50000]\tLoss: 4.7092\tLR: 6.835038\n",
      "Training Epoch: 69 [17792/50000]\tLoss: 4.7363\tLR: 6.835294\n",
      "Training Epoch: 69 [17920/50000]\tLoss: 4.7700\tLR: 6.835550\n",
      "Training Epoch: 69 [18048/50000]\tLoss: 4.7039\tLR: 6.835806\n",
      "Training Epoch: 69 [18176/50000]\tLoss: 4.7291\tLR: 6.836061\n",
      "Training Epoch: 69 [18304/50000]\tLoss: 4.7323\tLR: 6.836317\n",
      "Training Epoch: 69 [18432/50000]\tLoss: 4.7461\tLR: 6.836573\n",
      "Training Epoch: 69 [18560/50000]\tLoss: 4.7518\tLR: 6.836829\n",
      "Training Epoch: 69 [18688/50000]\tLoss: 4.6902\tLR: 6.837084\n",
      "Training Epoch: 69 [18816/50000]\tLoss: 4.7694\tLR: 6.837340\n",
      "Training Epoch: 69 [18944/50000]\tLoss: 4.7163\tLR: 6.837596\n",
      "Training Epoch: 69 [19072/50000]\tLoss: 4.6121\tLR: 6.837852\n",
      "Training Epoch: 69 [19200/50000]\tLoss: 4.6546\tLR: 6.838107\n",
      "Training Epoch: 69 [19328/50000]\tLoss: 4.8237\tLR: 6.838363\n",
      "Training Epoch: 69 [19456/50000]\tLoss: 4.7016\tLR: 6.838619\n",
      "Training Epoch: 69 [19584/50000]\tLoss: 4.7592\tLR: 6.838875\n",
      "Training Epoch: 69 [19712/50000]\tLoss: 4.7497\tLR: 6.839130\n",
      "Training Epoch: 69 [19840/50000]\tLoss: 4.7394\tLR: 6.839386\n",
      "Training Epoch: 69 [19968/50000]\tLoss: 4.6976\tLR: 6.839642\n",
      "Training Epoch: 69 [20096/50000]\tLoss: 4.7150\tLR: 6.839898\n",
      "Training Epoch: 69 [20224/50000]\tLoss: 4.6123\tLR: 6.840153\n",
      "Training Epoch: 69 [20352/50000]\tLoss: 4.7647\tLR: 6.840409\n",
      "Training Epoch: 69 [20480/50000]\tLoss: 4.8314\tLR: 6.840665\n",
      "Training Epoch: 69 [20608/50000]\tLoss: 4.7965\tLR: 6.840921\n",
      "Training Epoch: 69 [20736/50000]\tLoss: 4.8075\tLR: 6.841176\n",
      "Training Epoch: 69 [20864/50000]\tLoss: 4.7349\tLR: 6.841432\n",
      "Training Epoch: 69 [20992/50000]\tLoss: 4.7343\tLR: 6.841688\n",
      "Training Epoch: 69 [21120/50000]\tLoss: 4.6904\tLR: 6.841944\n",
      "Training Epoch: 69 [21248/50000]\tLoss: 4.6784\tLR: 6.842199\n",
      "Training Epoch: 69 [21376/50000]\tLoss: 4.7258\tLR: 6.842455\n",
      "Training Epoch: 69 [21504/50000]\tLoss: 4.7067\tLR: 6.842711\n",
      "Training Epoch: 69 [21632/50000]\tLoss: 4.7530\tLR: 6.842967\n",
      "Training Epoch: 69 [21760/50000]\tLoss: 4.8175\tLR: 6.843223\n",
      "Training Epoch: 69 [21888/50000]\tLoss: 4.7134\tLR: 6.843478\n",
      "Training Epoch: 69 [22016/50000]\tLoss: 4.6957\tLR: 6.843734\n",
      "Training Epoch: 69 [22144/50000]\tLoss: 4.7049\tLR: 6.843990\n",
      "Training Epoch: 69 [22272/50000]\tLoss: 4.6896\tLR: 6.844246\n",
      "Training Epoch: 69 [22400/50000]\tLoss: 4.7866\tLR: 6.844501\n",
      "Training Epoch: 69 [22528/50000]\tLoss: 4.7518\tLR: 6.844757\n",
      "Training Epoch: 69 [22656/50000]\tLoss: 4.7607\tLR: 6.845013\n",
      "Training Epoch: 69 [22784/50000]\tLoss: 4.7457\tLR: 6.845269\n",
      "Training Epoch: 69 [22912/50000]\tLoss: 4.6420\tLR: 6.845524\n",
      "Training Epoch: 69 [23040/50000]\tLoss: 4.6824\tLR: 6.845780\n",
      "Training Epoch: 69 [23168/50000]\tLoss: 4.6943\tLR: 6.846036\n",
      "Training Epoch: 69 [23296/50000]\tLoss: 4.6835\tLR: 6.846292\n",
      "Training Epoch: 69 [23424/50000]\tLoss: 4.7682\tLR: 6.846547\n",
      "Training Epoch: 69 [23552/50000]\tLoss: 4.7098\tLR: 6.846803\n",
      "Training Epoch: 69 [23680/50000]\tLoss: 4.6756\tLR: 6.847059\n",
      "Training Epoch: 69 [23808/50000]\tLoss: 4.6596\tLR: 6.847315\n",
      "Training Epoch: 69 [23936/50000]\tLoss: 4.7438\tLR: 6.847570\n",
      "Training Epoch: 69 [24064/50000]\tLoss: 4.7263\tLR: 6.847826\n",
      "Training Epoch: 69 [24192/50000]\tLoss: 4.7221\tLR: 6.848082\n",
      "Training Epoch: 69 [24320/50000]\tLoss: 4.7997\tLR: 6.848338\n",
      "Training Epoch: 69 [24448/50000]\tLoss: 4.6442\tLR: 6.848593\n",
      "Training Epoch: 69 [24576/50000]\tLoss: 4.8037\tLR: 6.848849\n",
      "Training Epoch: 69 [24704/50000]\tLoss: 4.7699\tLR: 6.849105\n",
      "Training Epoch: 69 [24832/50000]\tLoss: 4.6897\tLR: 6.849361\n",
      "Training Epoch: 69 [24960/50000]\tLoss: 4.7135\tLR: 6.849616\n",
      "Training Epoch: 69 [25088/50000]\tLoss: 4.6648\tLR: 6.849872\n",
      "Training Epoch: 69 [25216/50000]\tLoss: 4.7375\tLR: 6.850128\n",
      "Training Epoch: 69 [25344/50000]\tLoss: 4.7859\tLR: 6.850384\n",
      "Training Epoch: 69 [25472/50000]\tLoss: 4.7513\tLR: 6.850639\n",
      "Training Epoch: 69 [25600/50000]\tLoss: 4.6726\tLR: 6.850895\n",
      "Training Epoch: 69 [25728/50000]\tLoss: 4.8223\tLR: 6.851151\n",
      "Training Epoch: 69 [25856/50000]\tLoss: 4.6655\tLR: 6.851407\n",
      "Training Epoch: 69 [25984/50000]\tLoss: 4.7538\tLR: 6.851662\n",
      "Training Epoch: 69 [26112/50000]\tLoss: 4.6733\tLR: 6.851918\n",
      "Training Epoch: 69 [26240/50000]\tLoss: 4.6748\tLR: 6.852174\n",
      "Training Epoch: 69 [26368/50000]\tLoss: 4.7292\tLR: 6.852430\n",
      "Training Epoch: 69 [26496/50000]\tLoss: 4.6473\tLR: 6.852685\n",
      "Training Epoch: 69 [26624/50000]\tLoss: 4.6595\tLR: 6.852941\n",
      "Training Epoch: 69 [26752/50000]\tLoss: 4.8021\tLR: 6.853197\n",
      "Training Epoch: 69 [26880/50000]\tLoss: 4.7255\tLR: 6.853453\n",
      "Training Epoch: 69 [27008/50000]\tLoss: 4.7536\tLR: 6.853708\n",
      "Training Epoch: 69 [27136/50000]\tLoss: 4.6915\tLR: 6.853964\n",
      "Training Epoch: 69 [27264/50000]\tLoss: 4.6862\tLR: 6.854220\n",
      "Training Epoch: 69 [27392/50000]\tLoss: 4.8079\tLR: 6.854476\n",
      "Training Epoch: 69 [27520/50000]\tLoss: 4.7664\tLR: 6.854731\n",
      "Training Epoch: 69 [27648/50000]\tLoss: 4.7331\tLR: 6.854987\n",
      "Training Epoch: 69 [27776/50000]\tLoss: 4.7004\tLR: 6.855243\n",
      "Training Epoch: 69 [27904/50000]\tLoss: 4.7115\tLR: 6.855499\n",
      "Training Epoch: 69 [28032/50000]\tLoss: 4.6252\tLR: 6.855754\n",
      "Training Epoch: 69 [28160/50000]\tLoss: 4.6729\tLR: 6.856010\n",
      "Training Epoch: 69 [28288/50000]\tLoss: 4.7417\tLR: 6.856266\n",
      "Training Epoch: 69 [28416/50000]\tLoss: 4.7476\tLR: 6.856522\n",
      "Training Epoch: 69 [28544/50000]\tLoss: 4.7065\tLR: 6.856777\n",
      "Training Epoch: 69 [28672/50000]\tLoss: 4.7174\tLR: 6.857033\n",
      "Training Epoch: 69 [28800/50000]\tLoss: 4.8043\tLR: 6.857289\n",
      "Training Epoch: 69 [28928/50000]\tLoss: 4.7519\tLR: 6.857545\n",
      "Training Epoch: 69 [29056/50000]\tLoss: 4.7305\tLR: 6.857801\n",
      "Training Epoch: 69 [29184/50000]\tLoss: 4.7593\tLR: 6.858056\n",
      "Training Epoch: 69 [29312/50000]\tLoss: 4.7459\tLR: 6.858312\n",
      "Training Epoch: 69 [29440/50000]\tLoss: 4.6629\tLR: 6.858568\n",
      "Training Epoch: 69 [29568/50000]\tLoss: 4.7096\tLR: 6.858824\n",
      "Training Epoch: 69 [29696/50000]\tLoss: 4.7014\tLR: 6.859079\n",
      "Training Epoch: 69 [29824/50000]\tLoss: 4.7545\tLR: 6.859335\n",
      "Training Epoch: 69 [29952/50000]\tLoss: 4.7417\tLR: 6.859591\n",
      "Training Epoch: 69 [30080/50000]\tLoss: 4.7474\tLR: 6.859847\n",
      "Training Epoch: 69 [30208/50000]\tLoss: 4.7202\tLR: 6.860102\n",
      "Training Epoch: 69 [30336/50000]\tLoss: 4.6900\tLR: 6.860358\n",
      "Training Epoch: 69 [30464/50000]\tLoss: 4.7736\tLR: 6.860614\n",
      "Training Epoch: 69 [30592/50000]\tLoss: 4.7250\tLR: 6.860870\n",
      "Training Epoch: 69 [30720/50000]\tLoss: 4.7642\tLR: 6.861125\n",
      "Training Epoch: 69 [30848/50000]\tLoss: 4.7275\tLR: 6.861381\n",
      "Training Epoch: 69 [30976/50000]\tLoss: 4.6413\tLR: 6.861637\n",
      "Training Epoch: 69 [31104/50000]\tLoss: 4.7973\tLR: 6.861893\n",
      "Training Epoch: 69 [31232/50000]\tLoss: 4.7121\tLR: 6.862148\n",
      "Training Epoch: 69 [31360/50000]\tLoss: 4.7039\tLR: 6.862404\n",
      "Training Epoch: 69 [31488/50000]\tLoss: 4.6564\tLR: 6.862660\n",
      "Training Epoch: 69 [31616/50000]\tLoss: 4.7882\tLR: 6.862916\n",
      "Training Epoch: 69 [31744/50000]\tLoss: 4.7371\tLR: 6.863171\n",
      "Training Epoch: 69 [31872/50000]\tLoss: 4.6711\tLR: 6.863427\n",
      "Training Epoch: 69 [32000/50000]\tLoss: 4.8033\tLR: 6.863683\n",
      "Training Epoch: 69 [32128/50000]\tLoss: 4.7794\tLR: 6.863939\n",
      "Training Epoch: 69 [32256/50000]\tLoss: 4.8149\tLR: 6.864194\n",
      "Training Epoch: 69 [32384/50000]\tLoss: 4.7374\tLR: 6.864450\n",
      "Training Epoch: 69 [32512/50000]\tLoss: 4.6772\tLR: 6.864706\n",
      "Training Epoch: 69 [32640/50000]\tLoss: 4.7339\tLR: 6.864962\n",
      "Training Epoch: 69 [32768/50000]\tLoss: 4.7554\tLR: 6.865217\n",
      "Training Epoch: 69 [32896/50000]\tLoss: 4.7555\tLR: 6.865473\n",
      "Training Epoch: 69 [33024/50000]\tLoss: 4.7621\tLR: 6.865729\n",
      "Training Epoch: 69 [33152/50000]\tLoss: 4.7061\tLR: 6.865985\n",
      "Training Epoch: 69 [33280/50000]\tLoss: 4.7303\tLR: 6.866240\n",
      "Training Epoch: 69 [33408/50000]\tLoss: 4.7209\tLR: 6.866496\n",
      "Training Epoch: 69 [33536/50000]\tLoss: 4.6928\tLR: 6.866752\n",
      "Training Epoch: 69 [33664/50000]\tLoss: 4.7946\tLR: 6.867008\n",
      "Training Epoch: 69 [33792/50000]\tLoss: 4.7074\tLR: 6.867263\n",
      "Training Epoch: 69 [33920/50000]\tLoss: 4.6838\tLR: 6.867519\n",
      "Training Epoch: 69 [34048/50000]\tLoss: 4.6964\tLR: 6.867775\n",
      "Training Epoch: 69 [34176/50000]\tLoss: 4.6831\tLR: 6.868031\n",
      "Training Epoch: 69 [34304/50000]\tLoss: 4.7509\tLR: 6.868286\n",
      "Training Epoch: 69 [34432/50000]\tLoss: 4.8060\tLR: 6.868542\n",
      "Training Epoch: 69 [34560/50000]\tLoss: 4.7299\tLR: 6.868798\n",
      "Training Epoch: 69 [34688/50000]\tLoss: 4.7225\tLR: 6.869054\n",
      "Training Epoch: 69 [34816/50000]\tLoss: 4.7913\tLR: 6.869309\n",
      "Training Epoch: 69 [34944/50000]\tLoss: 4.7742\tLR: 6.869565\n",
      "Training Epoch: 69 [35072/50000]\tLoss: 4.7995\tLR: 6.869821\n",
      "Training Epoch: 69 [35200/50000]\tLoss: 4.7033\tLR: 6.870077\n",
      "Training Epoch: 69 [35328/50000]\tLoss: 4.7268\tLR: 6.870332\n",
      "Training Epoch: 69 [35456/50000]\tLoss: 4.7633\tLR: 6.870588\n",
      "Training Epoch: 69 [35584/50000]\tLoss: 4.7351\tLR: 6.870844\n",
      "Training Epoch: 69 [35712/50000]\tLoss: 4.7350\tLR: 6.871100\n",
      "Training Epoch: 69 [35840/50000]\tLoss: 4.7248\tLR: 6.871355\n",
      "Training Epoch: 69 [35968/50000]\tLoss: 4.7692\tLR: 6.871611\n",
      "Training Epoch: 69 [36096/50000]\tLoss: 4.8298\tLR: 6.871867\n",
      "Training Epoch: 69 [36224/50000]\tLoss: 4.7635\tLR: 6.872123\n",
      "Training Epoch: 69 [36352/50000]\tLoss: 4.7058\tLR: 6.872379\n",
      "Training Epoch: 69 [36480/50000]\tLoss: 4.7547\tLR: 6.872634\n",
      "Training Epoch: 69 [36608/50000]\tLoss: 4.8493\tLR: 6.872890\n",
      "Training Epoch: 69 [36736/50000]\tLoss: 4.7264\tLR: 6.873146\n",
      "Training Epoch: 69 [36864/50000]\tLoss: 4.7142\tLR: 6.873402\n",
      "Training Epoch: 69 [36992/50000]\tLoss: 4.6587\tLR: 6.873657\n",
      "Training Epoch: 69 [37120/50000]\tLoss: 4.7543\tLR: 6.873913\n",
      "Training Epoch: 69 [37248/50000]\tLoss: 4.7293\tLR: 6.874169\n",
      "Training Epoch: 69 [37376/50000]\tLoss: 4.8102\tLR: 6.874425\n",
      "Training Epoch: 69 [37504/50000]\tLoss: 4.8285\tLR: 6.874680\n",
      "Training Epoch: 69 [37632/50000]\tLoss: 4.6823\tLR: 6.874936\n",
      "Training Epoch: 69 [37760/50000]\tLoss: 4.7073\tLR: 6.875192\n",
      "Training Epoch: 69 [37888/50000]\tLoss: 4.7400\tLR: 6.875448\n",
      "Training Epoch: 69 [38016/50000]\tLoss: 4.6995\tLR: 6.875703\n",
      "Training Epoch: 69 [38144/50000]\tLoss: 4.7326\tLR: 6.875959\n",
      "Training Epoch: 69 [38272/50000]\tLoss: 4.7785\tLR: 6.876215\n",
      "Training Epoch: 69 [38400/50000]\tLoss: 4.7218\tLR: 6.876471\n",
      "Training Epoch: 69 [38528/50000]\tLoss: 4.6865\tLR: 6.876726\n",
      "Training Epoch: 69 [38656/50000]\tLoss: 4.7137\tLR: 6.876982\n",
      "Training Epoch: 69 [38784/50000]\tLoss: 4.8053\tLR: 6.877238\n",
      "Training Epoch: 69 [38912/50000]\tLoss: 4.6534\tLR: 6.877494\n",
      "Training Epoch: 69 [39040/50000]\tLoss: 4.7108\tLR: 6.877749\n",
      "Training Epoch: 69 [39168/50000]\tLoss: 4.6983\tLR: 6.878005\n",
      "Training Epoch: 69 [39296/50000]\tLoss: 4.7461\tLR: 6.878261\n",
      "Training Epoch: 69 [39424/50000]\tLoss: 4.6974\tLR: 6.878517\n",
      "Training Epoch: 69 [39552/50000]\tLoss: 4.7782\tLR: 6.878772\n",
      "Training Epoch: 69 [39680/50000]\tLoss: 4.7992\tLR: 6.879028\n",
      "Training Epoch: 69 [39808/50000]\tLoss: 4.7323\tLR: 6.879284\n",
      "Training Epoch: 69 [39936/50000]\tLoss: 4.6783\tLR: 6.879540\n",
      "Training Epoch: 69 [40064/50000]\tLoss: 4.7490\tLR: 6.879795\n",
      "Training Epoch: 69 [40192/50000]\tLoss: 4.6805\tLR: 6.880051\n",
      "Training Epoch: 69 [40320/50000]\tLoss: 4.7461\tLR: 6.880307\n",
      "Training Epoch: 69 [40448/50000]\tLoss: 4.7370\tLR: 6.880563\n",
      "Training Epoch: 69 [40576/50000]\tLoss: 4.7149\tLR: 6.880818\n",
      "Training Epoch: 69 [40704/50000]\tLoss: 4.7540\tLR: 6.881074\n",
      "Training Epoch: 69 [40832/50000]\tLoss: 4.7343\tLR: 6.881330\n",
      "Training Epoch: 69 [40960/50000]\tLoss: 4.6873\tLR: 6.881586\n",
      "Training Epoch: 69 [41088/50000]\tLoss: 4.7012\tLR: 6.881841\n",
      "Training Epoch: 69 [41216/50000]\tLoss: 4.6650\tLR: 6.882097\n",
      "Training Epoch: 69 [41344/50000]\tLoss: 4.8148\tLR: 6.882353\n",
      "Training Epoch: 69 [41472/50000]\tLoss: 4.7312\tLR: 6.882609\n",
      "Training Epoch: 69 [41600/50000]\tLoss: 4.7395\tLR: 6.882864\n",
      "Training Epoch: 69 [41728/50000]\tLoss: 4.7434\tLR: 6.883120\n",
      "Training Epoch: 69 [41856/50000]\tLoss: 4.7024\tLR: 6.883376\n",
      "Training Epoch: 69 [41984/50000]\tLoss: 4.7305\tLR: 6.883632\n",
      "Training Epoch: 69 [42112/50000]\tLoss: 4.7072\tLR: 6.883887\n",
      "Training Epoch: 69 [42240/50000]\tLoss: 4.7900\tLR: 6.884143\n",
      "Training Epoch: 69 [42368/50000]\tLoss: 4.7154\tLR: 6.884399\n",
      "Training Epoch: 69 [42496/50000]\tLoss: 4.7896\tLR: 6.884655\n",
      "Training Epoch: 69 [42624/50000]\tLoss: 4.7630\tLR: 6.884910\n",
      "Training Epoch: 69 [42752/50000]\tLoss: 4.8030\tLR: 6.885166\n",
      "Training Epoch: 69 [42880/50000]\tLoss: 4.6986\tLR: 6.885422\n",
      "Training Epoch: 69 [43008/50000]\tLoss: 4.6781\tLR: 6.885678\n",
      "Training Epoch: 69 [43136/50000]\tLoss: 4.7624\tLR: 6.885934\n",
      "Training Epoch: 69 [43264/50000]\tLoss: 4.7827\tLR: 6.886189\n",
      "Training Epoch: 69 [43392/50000]\tLoss: 4.7791\tLR: 6.886445\n",
      "Training Epoch: 69 [43520/50000]\tLoss: 4.7581\tLR: 6.886701\n",
      "Training Epoch: 69 [43648/50000]\tLoss: 4.7493\tLR: 6.886957\n",
      "Training Epoch: 69 [43776/50000]\tLoss: 4.7487\tLR: 6.887212\n",
      "Training Epoch: 69 [43904/50000]\tLoss: 4.7281\tLR: 6.887468\n",
      "Training Epoch: 69 [44032/50000]\tLoss: 4.7214\tLR: 6.887724\n",
      "Training Epoch: 69 [44160/50000]\tLoss: 4.6932\tLR: 6.887980\n",
      "Training Epoch: 69 [44288/50000]\tLoss: 4.6983\tLR: 6.888235\n",
      "Training Epoch: 69 [44416/50000]\tLoss: 4.7167\tLR: 6.888491\n",
      "Training Epoch: 69 [44544/50000]\tLoss: 4.7237\tLR: 6.888747\n",
      "Training Epoch: 69 [44672/50000]\tLoss: 4.8246\tLR: 6.889003\n",
      "Training Epoch: 69 [44800/50000]\tLoss: 4.6910\tLR: 6.889258\n",
      "Training Epoch: 69 [44928/50000]\tLoss: 4.6300\tLR: 6.889514\n",
      "Training Epoch: 69 [45056/50000]\tLoss: 4.7227\tLR: 6.889770\n",
      "Training Epoch: 69 [45184/50000]\tLoss: 4.6823\tLR: 6.890026\n",
      "Training Epoch: 69 [45312/50000]\tLoss: 4.7690\tLR: 6.890281\n",
      "Training Epoch: 69 [45440/50000]\tLoss: 4.8231\tLR: 6.890537\n",
      "Training Epoch: 69 [45568/50000]\tLoss: 4.6825\tLR: 6.890793\n",
      "Training Epoch: 69 [45696/50000]\tLoss: 4.7503\tLR: 6.891049\n",
      "Training Epoch: 69 [45824/50000]\tLoss: 4.7789\tLR: 6.891304\n",
      "Training Epoch: 69 [45952/50000]\tLoss: 4.7610\tLR: 6.891560\n",
      "Training Epoch: 69 [46080/50000]\tLoss: 4.6397\tLR: 6.891816\n",
      "Training Epoch: 69 [46208/50000]\tLoss: 4.6966\tLR: 6.892072\n",
      "Training Epoch: 69 [46336/50000]\tLoss: 4.6966\tLR: 6.892327\n",
      "Training Epoch: 69 [46464/50000]\tLoss: 4.7609\tLR: 6.892583\n",
      "Training Epoch: 69 [46592/50000]\tLoss: 4.7569\tLR: 6.892839\n",
      "Training Epoch: 69 [46720/50000]\tLoss: 4.7321\tLR: 6.893095\n",
      "Training Epoch: 69 [46848/50000]\tLoss: 4.7549\tLR: 6.893350\n",
      "Training Epoch: 69 [46976/50000]\tLoss: 4.7321\tLR: 6.893606\n",
      "Training Epoch: 69 [47104/50000]\tLoss: 4.8496\tLR: 6.893862\n",
      "Training Epoch: 69 [47232/50000]\tLoss: 4.7128\tLR: 6.894118\n",
      "Training Epoch: 69 [47360/50000]\tLoss: 4.7608\tLR: 6.894373\n",
      "Training Epoch: 69 [47488/50000]\tLoss: 4.7820\tLR: 6.894629\n",
      "Training Epoch: 69 [47616/50000]\tLoss: 4.6893\tLR: 6.894885\n",
      "Training Epoch: 69 [47744/50000]\tLoss: 4.7155\tLR: 6.895141\n",
      "Training Epoch: 69 [47872/50000]\tLoss: 4.7603\tLR: 6.895396\n",
      "Training Epoch: 69 [48000/50000]\tLoss: 4.8008\tLR: 6.895652\n",
      "Training Epoch: 69 [48128/50000]\tLoss: 4.8412\tLR: 6.895908\n",
      "Training Epoch: 69 [48256/50000]\tLoss: 4.7986\tLR: 6.896164\n",
      "Training Epoch: 69 [48384/50000]\tLoss: 4.8329\tLR: 6.896419\n",
      "Training Epoch: 69 [48512/50000]\tLoss: 4.7370\tLR: 6.896675\n",
      "Training Epoch: 69 [48640/50000]\tLoss: 4.6979\tLR: 6.896931\n",
      "Training Epoch: 69 [48768/50000]\tLoss: 4.6534\tLR: 6.897187\n",
      "Training Epoch: 69 [48896/50000]\tLoss: 4.7196\tLR: 6.897442\n",
      "Training Epoch: 69 [49024/50000]\tLoss: 4.7371\tLR: 6.897698\n",
      "Training Epoch: 69 [49152/50000]\tLoss: 4.7390\tLR: 6.897954\n",
      "Training Epoch: 69 [49280/50000]\tLoss: 4.7802\tLR: 6.898210\n",
      "Training Epoch: 69 [49408/50000]\tLoss: 4.7972\tLR: 6.898465\n",
      "Training Epoch: 69 [49536/50000]\tLoss: 4.7161\tLR: 6.898721\n",
      "Training Epoch: 69 [49664/50000]\tLoss: 4.6545\tLR: 6.898977\n",
      "Training Epoch: 69 [49792/50000]\tLoss: 4.6263\tLR: 6.899233\n",
      "Training Epoch: 69 [49920/50000]\tLoss: 4.6926\tLR: 6.899488\n",
      "Training Epoch: 69 [50000/50000]\tLoss: 4.7020\tLR: 6.899744\n",
      "epoch 69 training time consumed: 489.03s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   96733 GB |   96733 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   96436 GB |   96436 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     296 GB |     296 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   96733 GB |   96733 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   96436 GB |   96436 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     296 GB |     296 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   95373 GB |   95373 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   95076 GB |   95076 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     297 GB |     297 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   10257 K  |   10256 K  |\n",
      "|       from large pool |      24    |      65    |    4372 K  |    4372 K  |\n",
      "|       from small pool |     231    |     274    |    5884 K  |    5884 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   10257 K  |   10256 K  |\n",
      "|       from large pool |      24    |      65    |    4372 K  |    4372 K  |\n",
      "|       from small pool |     231    |     274    |    5884 K  |    5884 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      47    |    5945 K  |    5945 K  |\n",
      "|       from large pool |      10    |      23    |    2101 K  |    2101 K  |\n",
      "|       from small pool |      25    |      35    |    3843 K  |    3843 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 69, Average loss: 0.0374, Accuracy: 0.0100, Time consumed:31.18s\n",
      "\n",
      "Training Epoch: 70 [128/50000]\tLoss: 4.6814\tLR: 0.020000\n",
      "Training Epoch: 70 [256/50000]\tLoss: 4.7561\tLR: 6.900256\n",
      "Training Epoch: 70 [384/50000]\tLoss: 4.7420\tLR: 6.900512\n",
      "Training Epoch: 70 [512/50000]\tLoss: 4.7002\tLR: 6.900767\n",
      "Training Epoch: 70 [640/50000]\tLoss: 4.6964\tLR: 6.901023\n",
      "Training Epoch: 70 [768/50000]\tLoss: 4.7095\tLR: 6.901279\n",
      "Training Epoch: 70 [896/50000]\tLoss: 4.7317\tLR: 6.901535\n",
      "Training Epoch: 70 [1024/50000]\tLoss: 4.7443\tLR: 6.901790\n",
      "Training Epoch: 70 [1152/50000]\tLoss: 4.7030\tLR: 6.902046\n",
      "Training Epoch: 70 [1280/50000]\tLoss: 4.7464\tLR: 6.902302\n",
      "Training Epoch: 70 [1408/50000]\tLoss: 4.6515\tLR: 6.902558\n",
      "Training Epoch: 70 [1536/50000]\tLoss: 4.7068\tLR: 6.902813\n",
      "Training Epoch: 70 [1664/50000]\tLoss: 4.7329\tLR: 6.903069\n",
      "Training Epoch: 70 [1792/50000]\tLoss: 4.7553\tLR: 6.903325\n",
      "Training Epoch: 70 [1920/50000]\tLoss: 4.8138\tLR: 6.903581\n",
      "Training Epoch: 70 [2048/50000]\tLoss: 4.7698\tLR: 6.903836\n",
      "Training Epoch: 70 [2176/50000]\tLoss: 4.7833\tLR: 6.904092\n",
      "Training Epoch: 70 [2304/50000]\tLoss: 4.6542\tLR: 6.904348\n",
      "Training Epoch: 70 [2432/50000]\tLoss: 4.7377\tLR: 6.904604\n",
      "Training Epoch: 70 [2560/50000]\tLoss: 4.6294\tLR: 6.904859\n",
      "Training Epoch: 70 [2688/50000]\tLoss: 4.7017\tLR: 6.905115\n",
      "Training Epoch: 70 [2816/50000]\tLoss: 4.7511\tLR: 6.905371\n",
      "Training Epoch: 70 [2944/50000]\tLoss: 4.6756\tLR: 6.905627\n",
      "Training Epoch: 70 [3072/50000]\tLoss: 4.7051\tLR: 6.905882\n",
      "Training Epoch: 70 [3200/50000]\tLoss: 4.7318\tLR: 6.906138\n",
      "Training Epoch: 70 [3328/50000]\tLoss: 4.8152\tLR: 6.906394\n",
      "Training Epoch: 70 [3456/50000]\tLoss: 4.7421\tLR: 6.906650\n",
      "Training Epoch: 70 [3584/50000]\tLoss: 4.7259\tLR: 6.906905\n",
      "Training Epoch: 70 [3712/50000]\tLoss: 4.6921\tLR: 6.907161\n",
      "Training Epoch: 70 [3840/50000]\tLoss: 4.7517\tLR: 6.907417\n",
      "Training Epoch: 70 [3968/50000]\tLoss: 4.7190\tLR: 6.907673\n",
      "Training Epoch: 70 [4096/50000]\tLoss: 4.6420\tLR: 6.907928\n",
      "Training Epoch: 70 [4224/50000]\tLoss: 4.6766\tLR: 6.908184\n",
      "Training Epoch: 70 [4352/50000]\tLoss: 4.8298\tLR: 6.908440\n",
      "Training Epoch: 70 [4480/50000]\tLoss: 4.8692\tLR: 6.908696\n",
      "Training Epoch: 70 [4608/50000]\tLoss: 4.7527\tLR: 6.908951\n",
      "Training Epoch: 70 [4736/50000]\tLoss: 4.7977\tLR: 6.909207\n",
      "Training Epoch: 70 [4864/50000]\tLoss: 4.7704\tLR: 6.909463\n",
      "Training Epoch: 70 [4992/50000]\tLoss: 4.7147\tLR: 6.909719\n",
      "Training Epoch: 70 [5120/50000]\tLoss: 4.8261\tLR: 6.909974\n",
      "Training Epoch: 70 [5248/50000]\tLoss: 4.6282\tLR: 6.910230\n",
      "Training Epoch: 70 [5376/50000]\tLoss: 4.7160\tLR: 6.910486\n",
      "Training Epoch: 70 [5504/50000]\tLoss: 4.7266\tLR: 6.910742\n",
      "Training Epoch: 70 [5632/50000]\tLoss: 4.7619\tLR: 6.910997\n",
      "Training Epoch: 70 [5760/50000]\tLoss: 4.7615\tLR: 6.911253\n",
      "Training Epoch: 70 [5888/50000]\tLoss: 4.6561\tLR: 6.911509\n",
      "Training Epoch: 70 [6016/50000]\tLoss: 4.8005\tLR: 6.911765\n",
      "Training Epoch: 70 [6144/50000]\tLoss: 4.8075\tLR: 6.912020\n",
      "Training Epoch: 70 [6272/50000]\tLoss: 4.9843\tLR: 6.912276\n",
      "Training Epoch: 70 [6400/50000]\tLoss: 4.8347\tLR: 6.912532\n",
      "Training Epoch: 70 [6528/50000]\tLoss: 4.7226\tLR: 6.912788\n",
      "Training Epoch: 70 [6656/50000]\tLoss: 4.7604\tLR: 6.913043\n",
      "Training Epoch: 70 [6784/50000]\tLoss: 4.6520\tLR: 6.913299\n",
      "Training Epoch: 70 [6912/50000]\tLoss: 4.7360\tLR: 6.913555\n",
      "Training Epoch: 70 [7040/50000]\tLoss: 4.6440\tLR: 6.913811\n",
      "Training Epoch: 70 [7168/50000]\tLoss: 4.7565\tLR: 6.914066\n",
      "Training Epoch: 70 [7296/50000]\tLoss: 4.7283\tLR: 6.914322\n",
      "Training Epoch: 70 [7424/50000]\tLoss: 4.7264\tLR: 6.914578\n",
      "Training Epoch: 70 [7552/50000]\tLoss: 4.7317\tLR: 6.914834\n",
      "Training Epoch: 70 [7680/50000]\tLoss: 4.7818\tLR: 6.915090\n",
      "Training Epoch: 70 [7808/50000]\tLoss: 4.7858\tLR: 6.915345\n",
      "Training Epoch: 70 [7936/50000]\tLoss: 4.8128\tLR: 6.915601\n",
      "Training Epoch: 70 [8064/50000]\tLoss: 4.7658\tLR: 6.915857\n",
      "Training Epoch: 70 [8192/50000]\tLoss: 4.7899\tLR: 6.916113\n",
      "Training Epoch: 70 [8320/50000]\tLoss: 4.7899\tLR: 6.916368\n",
      "Training Epoch: 70 [8448/50000]\tLoss: 4.6281\tLR: 6.916624\n",
      "Training Epoch: 70 [8576/50000]\tLoss: 4.6984\tLR: 6.916880\n",
      "Training Epoch: 70 [8704/50000]\tLoss: 4.7525\tLR: 6.917136\n",
      "Training Epoch: 70 [8832/50000]\tLoss: 4.6675\tLR: 6.917391\n",
      "Training Epoch: 70 [8960/50000]\tLoss: 4.7759\tLR: 6.917647\n",
      "Training Epoch: 70 [9088/50000]\tLoss: 4.7939\tLR: 6.917903\n",
      "Training Epoch: 70 [9216/50000]\tLoss: 4.7466\tLR: 6.918159\n",
      "Training Epoch: 70 [9344/50000]\tLoss: 4.7760\tLR: 6.918414\n",
      "Training Epoch: 70 [9472/50000]\tLoss: 4.7725\tLR: 6.918670\n",
      "Training Epoch: 70 [9600/50000]\tLoss: 4.7318\tLR: 6.918926\n",
      "Training Epoch: 70 [9728/50000]\tLoss: 4.7664\tLR: 6.919182\n",
      "Training Epoch: 70 [9856/50000]\tLoss: 4.7757\tLR: 6.919437\n",
      "Training Epoch: 70 [9984/50000]\tLoss: 4.8118\tLR: 6.919693\n",
      "Training Epoch: 70 [10112/50000]\tLoss: 4.7657\tLR: 6.919949\n",
      "Training Epoch: 70 [10240/50000]\tLoss: 4.7882\tLR: 6.920205\n",
      "Training Epoch: 70 [10368/50000]\tLoss: 4.6965\tLR: 6.920460\n",
      "Training Epoch: 70 [10496/50000]\tLoss: 4.7280\tLR: 6.920716\n",
      "Training Epoch: 70 [10624/50000]\tLoss: 4.7202\tLR: 6.920972\n",
      "Training Epoch: 70 [10752/50000]\tLoss: 4.7366\tLR: 6.921228\n",
      "Training Epoch: 70 [10880/50000]\tLoss: 4.7446\tLR: 6.921483\n",
      "Training Epoch: 70 [11008/50000]\tLoss: 4.6594\tLR: 6.921739\n",
      "Training Epoch: 70 [11136/50000]\tLoss: 4.7444\tLR: 6.921995\n",
      "Training Epoch: 70 [11264/50000]\tLoss: 4.6852\tLR: 6.922251\n",
      "Training Epoch: 70 [11392/50000]\tLoss: 4.7287\tLR: 6.922506\n",
      "Training Epoch: 70 [11520/50000]\tLoss: 4.8019\tLR: 6.922762\n",
      "Training Epoch: 70 [11648/50000]\tLoss: 4.6653\tLR: 6.923018\n",
      "Training Epoch: 70 [11776/50000]\tLoss: 4.7189\tLR: 6.923274\n",
      "Training Epoch: 70 [11904/50000]\tLoss: 4.6541\tLR: 6.923529\n",
      "Training Epoch: 70 [12032/50000]\tLoss: 4.7202\tLR: 6.923785\n",
      "Training Epoch: 70 [12160/50000]\tLoss: 4.6759\tLR: 6.924041\n",
      "Training Epoch: 70 [12288/50000]\tLoss: 4.6809\tLR: 6.924297\n",
      "Training Epoch: 70 [12416/50000]\tLoss: 4.8414\tLR: 6.924552\n",
      "Training Epoch: 70 [12544/50000]\tLoss: 4.7544\tLR: 6.924808\n",
      "Training Epoch: 70 [12672/50000]\tLoss: 4.7452\tLR: 6.925064\n",
      "Training Epoch: 70 [12800/50000]\tLoss: 4.6537\tLR: 6.925320\n",
      "Training Epoch: 70 [12928/50000]\tLoss: 4.7454\tLR: 6.925575\n",
      "Training Epoch: 70 [13056/50000]\tLoss: 4.7194\tLR: 6.925831\n",
      "Training Epoch: 70 [13184/50000]\tLoss: 4.7923\tLR: 6.926087\n",
      "Training Epoch: 70 [13312/50000]\tLoss: 4.7394\tLR: 6.926343\n",
      "Training Epoch: 70 [13440/50000]\tLoss: 4.7385\tLR: 6.926598\n",
      "Training Epoch: 70 [13568/50000]\tLoss: 4.7445\tLR: 6.926854\n",
      "Training Epoch: 70 [13696/50000]\tLoss: 4.7609\tLR: 6.927110\n",
      "Training Epoch: 70 [13824/50000]\tLoss: 4.7275\tLR: 6.927366\n",
      "Training Epoch: 70 [13952/50000]\tLoss: 4.8075\tLR: 6.927621\n",
      "Training Epoch: 70 [14080/50000]\tLoss: 4.7603\tLR: 6.927877\n",
      "Training Epoch: 70 [14208/50000]\tLoss: 4.6575\tLR: 6.928133\n",
      "Training Epoch: 70 [14336/50000]\tLoss: 4.7187\tLR: 6.928389\n",
      "Training Epoch: 70 [14464/50000]\tLoss: 4.6946\tLR: 6.928645\n",
      "Training Epoch: 70 [14592/50000]\tLoss: 4.8207\tLR: 6.928900\n",
      "Training Epoch: 70 [14720/50000]\tLoss: 4.8232\tLR: 6.929156\n",
      "Training Epoch: 70 [14848/50000]\tLoss: 4.8446\tLR: 6.929412\n",
      "Training Epoch: 70 [14976/50000]\tLoss: 4.7293\tLR: 6.929668\n",
      "Training Epoch: 70 [15104/50000]\tLoss: 4.7444\tLR: 6.929923\n",
      "Training Epoch: 70 [15232/50000]\tLoss: 4.7584\tLR: 6.930179\n",
      "Training Epoch: 70 [15360/50000]\tLoss: 4.7912\tLR: 6.930435\n",
      "Training Epoch: 70 [15488/50000]\tLoss: 4.7934\tLR: 6.930691\n",
      "Training Epoch: 70 [15616/50000]\tLoss: 4.7684\tLR: 6.930946\n",
      "Training Epoch: 70 [15744/50000]\tLoss: 4.7514\tLR: 6.931202\n",
      "Training Epoch: 70 [15872/50000]\tLoss: 4.7306\tLR: 6.931458\n",
      "Training Epoch: 70 [16000/50000]\tLoss: 4.7486\tLR: 6.931714\n",
      "Training Epoch: 70 [16128/50000]\tLoss: 4.6795\tLR: 6.931969\n",
      "Training Epoch: 70 [16256/50000]\tLoss: 4.7289\tLR: 6.932225\n",
      "Training Epoch: 70 [16384/50000]\tLoss: 4.8024\tLR: 6.932481\n",
      "Training Epoch: 70 [16512/50000]\tLoss: 4.7688\tLR: 6.932737\n",
      "Training Epoch: 70 [16640/50000]\tLoss: 4.7660\tLR: 6.932992\n",
      "Training Epoch: 70 [16768/50000]\tLoss: 4.8621\tLR: 6.933248\n",
      "Training Epoch: 70 [16896/50000]\tLoss: 4.7784\tLR: 6.933504\n",
      "Training Epoch: 70 [17024/50000]\tLoss: 4.6996\tLR: 6.933760\n",
      "Training Epoch: 70 [17152/50000]\tLoss: 4.7581\tLR: 6.934015\n",
      "Training Epoch: 70 [17280/50000]\tLoss: 4.8581\tLR: 6.934271\n",
      "Training Epoch: 70 [17408/50000]\tLoss: 4.6945\tLR: 6.934527\n",
      "Training Epoch: 70 [17536/50000]\tLoss: 4.7092\tLR: 6.934783\n",
      "Training Epoch: 70 [17664/50000]\tLoss: 4.6367\tLR: 6.935038\n",
      "Training Epoch: 70 [17792/50000]\tLoss: 4.7974\tLR: 6.935294\n",
      "Training Epoch: 70 [17920/50000]\tLoss: 4.7341\tLR: 6.935550\n",
      "Training Epoch: 70 [18048/50000]\tLoss: 4.7993\tLR: 6.935806\n",
      "Training Epoch: 70 [18176/50000]\tLoss: 4.7626\tLR: 6.936061\n",
      "Training Epoch: 70 [18304/50000]\tLoss: 4.7593\tLR: 6.936317\n",
      "Training Epoch: 70 [18432/50000]\tLoss: 4.7065\tLR: 6.936573\n",
      "Training Epoch: 70 [18560/50000]\tLoss: 4.7410\tLR: 6.936829\n",
      "Training Epoch: 70 [18688/50000]\tLoss: 4.7138\tLR: 6.937084\n",
      "Training Epoch: 70 [18816/50000]\tLoss: 4.7325\tLR: 6.937340\n",
      "Training Epoch: 70 [18944/50000]\tLoss: 4.7448\tLR: 6.937596\n",
      "Training Epoch: 70 [19072/50000]\tLoss: 4.6934\tLR: 6.937852\n",
      "Training Epoch: 70 [19200/50000]\tLoss: 4.7300\tLR: 6.938107\n",
      "Training Epoch: 70 [19328/50000]\tLoss: 4.8003\tLR: 6.938363\n",
      "Training Epoch: 70 [19456/50000]\tLoss: 4.7849\tLR: 6.938619\n",
      "Training Epoch: 70 [19584/50000]\tLoss: 4.6700\tLR: 6.938875\n",
      "Training Epoch: 70 [19712/50000]\tLoss: 4.7546\tLR: 6.939130\n",
      "Training Epoch: 70 [19840/50000]\tLoss: 4.7650\tLR: 6.939386\n",
      "Training Epoch: 70 [19968/50000]\tLoss: 4.8345\tLR: 6.939642\n",
      "Training Epoch: 70 [20096/50000]\tLoss: 4.7062\tLR: 6.939898\n",
      "Training Epoch: 70 [20224/50000]\tLoss: 4.7666\tLR: 6.940153\n",
      "Training Epoch: 70 [20352/50000]\tLoss: 4.8001\tLR: 6.940409\n",
      "Training Epoch: 70 [20480/50000]\tLoss: 4.8037\tLR: 6.940665\n",
      "Training Epoch: 70 [20608/50000]\tLoss: 4.7676\tLR: 6.940921\n",
      "Training Epoch: 70 [20736/50000]\tLoss: 4.7183\tLR: 6.941176\n",
      "Training Epoch: 70 [20864/50000]\tLoss: 4.6284\tLR: 6.941432\n",
      "Training Epoch: 70 [20992/50000]\tLoss: 4.6870\tLR: 6.941688\n",
      "Training Epoch: 70 [21120/50000]\tLoss: 4.7490\tLR: 6.941944\n",
      "Training Epoch: 70 [21248/50000]\tLoss: 4.7571\tLR: 6.942199\n",
      "Training Epoch: 70 [21376/50000]\tLoss: 4.7240\tLR: 6.942455\n",
      "Training Epoch: 70 [21504/50000]\tLoss: 4.7652\tLR: 6.942711\n",
      "Training Epoch: 70 [21632/50000]\tLoss: 4.7076\tLR: 6.942967\n",
      "Training Epoch: 70 [21760/50000]\tLoss: 4.6871\tLR: 6.943223\n",
      "Training Epoch: 70 [21888/50000]\tLoss: 4.7208\tLR: 6.943478\n",
      "Training Epoch: 70 [22016/50000]\tLoss: 4.7590\tLR: 6.943734\n",
      "Training Epoch: 70 [22144/50000]\tLoss: 4.7217\tLR: 6.943990\n",
      "Training Epoch: 70 [22272/50000]\tLoss: 4.7138\tLR: 6.944246\n",
      "Training Epoch: 70 [22400/50000]\tLoss: 4.8054\tLR: 6.944501\n",
      "Training Epoch: 70 [22528/50000]\tLoss: 4.8166\tLR: 6.944757\n",
      "Training Epoch: 70 [22656/50000]\tLoss: 4.8314\tLR: 6.945013\n",
      "Training Epoch: 70 [22784/50000]\tLoss: 4.8469\tLR: 6.945269\n",
      "Training Epoch: 70 [22912/50000]\tLoss: 4.7332\tLR: 6.945524\n",
      "Training Epoch: 70 [23040/50000]\tLoss: 4.7213\tLR: 6.945780\n",
      "Training Epoch: 70 [23168/50000]\tLoss: 4.7302\tLR: 6.946036\n",
      "Training Epoch: 70 [23296/50000]\tLoss: 4.6261\tLR: 6.946292\n",
      "Training Epoch: 70 [23424/50000]\tLoss: 4.7582\tLR: 6.946547\n",
      "Training Epoch: 70 [23552/50000]\tLoss: 4.9304\tLR: 6.946803\n",
      "Training Epoch: 70 [23680/50000]\tLoss: 4.7034\tLR: 6.947059\n",
      "Training Epoch: 70 [23808/50000]\tLoss: 4.7376\tLR: 6.947315\n",
      "Training Epoch: 70 [23936/50000]\tLoss: 4.7560\tLR: 6.947570\n",
      "Training Epoch: 70 [24064/50000]\tLoss: 4.7679\tLR: 6.947826\n",
      "Training Epoch: 70 [24192/50000]\tLoss: 4.7023\tLR: 6.948082\n",
      "Training Epoch: 70 [24320/50000]\tLoss: 4.8043\tLR: 6.948338\n",
      "Training Epoch: 70 [24448/50000]\tLoss: 4.7100\tLR: 6.948593\n",
      "Training Epoch: 70 [24576/50000]\tLoss: 4.7342\tLR: 6.948849\n",
      "Training Epoch: 70 [24704/50000]\tLoss: 4.7522\tLR: 6.949105\n",
      "Training Epoch: 70 [24832/50000]\tLoss: 4.6313\tLR: 6.949361\n",
      "Training Epoch: 70 [24960/50000]\tLoss: 4.7098\tLR: 6.949616\n",
      "Training Epoch: 70 [25088/50000]\tLoss: 4.6866\tLR: 6.949872\n",
      "Training Epoch: 70 [25216/50000]\tLoss: 4.7154\tLR: 6.950128\n",
      "Training Epoch: 70 [25344/50000]\tLoss: 4.7873\tLR: 6.950384\n",
      "Training Epoch: 70 [25472/50000]\tLoss: 4.9065\tLR: 6.950639\n",
      "Training Epoch: 70 [25600/50000]\tLoss: 4.7879\tLR: 6.950895\n",
      "Training Epoch: 70 [25728/50000]\tLoss: 4.7919\tLR: 6.951151\n",
      "Training Epoch: 70 [25856/50000]\tLoss: 4.7769\tLR: 6.951407\n",
      "Training Epoch: 70 [25984/50000]\tLoss: 4.7194\tLR: 6.951662\n",
      "Training Epoch: 70 [26112/50000]\tLoss: 4.7296\tLR: 6.951918\n",
      "Training Epoch: 70 [26240/50000]\tLoss: 4.7365\tLR: 6.952174\n",
      "Training Epoch: 70 [26368/50000]\tLoss: 4.8042\tLR: 6.952430\n",
      "Training Epoch: 70 [26496/50000]\tLoss: 4.6891\tLR: 6.952685\n",
      "Training Epoch: 70 [26624/50000]\tLoss: 4.7443\tLR: 6.952941\n",
      "Training Epoch: 70 [26752/50000]\tLoss: 4.7434\tLR: 6.953197\n",
      "Training Epoch: 70 [26880/50000]\tLoss: 4.7437\tLR: 6.953453\n",
      "Training Epoch: 70 [27008/50000]\tLoss: 4.6548\tLR: 6.953708\n",
      "Training Epoch: 70 [27136/50000]\tLoss: 4.8203\tLR: 6.953964\n",
      "Training Epoch: 70 [27264/50000]\tLoss: 4.7436\tLR: 6.954220\n",
      "Training Epoch: 70 [27392/50000]\tLoss: 4.6374\tLR: 6.954476\n",
      "Training Epoch: 70 [27520/50000]\tLoss: 4.6592\tLR: 6.954731\n",
      "Training Epoch: 70 [27648/50000]\tLoss: 4.6618\tLR: 6.954987\n",
      "Training Epoch: 70 [27776/50000]\tLoss: 4.6685\tLR: 6.955243\n",
      "Training Epoch: 70 [27904/50000]\tLoss: 4.7780\tLR: 6.955499\n",
      "Training Epoch: 70 [28032/50000]\tLoss: 4.6665\tLR: 6.955754\n",
      "Training Epoch: 70 [28160/50000]\tLoss: 4.7259\tLR: 6.956010\n",
      "Training Epoch: 70 [28288/50000]\tLoss: 4.7053\tLR: 6.956266\n",
      "Training Epoch: 70 [28416/50000]\tLoss: 4.7400\tLR: 6.956522\n",
      "Training Epoch: 70 [28544/50000]\tLoss: 4.6221\tLR: 6.956777\n",
      "Training Epoch: 70 [28672/50000]\tLoss: 4.7927\tLR: 6.957033\n",
      "Training Epoch: 70 [28800/50000]\tLoss: 4.7068\tLR: 6.957289\n",
      "Training Epoch: 70 [28928/50000]\tLoss: 4.6699\tLR: 6.957545\n",
      "Training Epoch: 70 [29056/50000]\tLoss: 4.7809\tLR: 6.957801\n",
      "Training Epoch: 70 [29184/50000]\tLoss: 4.6981\tLR: 6.958056\n",
      "Training Epoch: 70 [29312/50000]\tLoss: 4.7239\tLR: 6.958312\n",
      "Training Epoch: 70 [29440/50000]\tLoss: 4.7459\tLR: 6.958568\n",
      "Training Epoch: 70 [29568/50000]\tLoss: 4.8541\tLR: 6.958824\n",
      "Training Epoch: 70 [29696/50000]\tLoss: 4.7079\tLR: 6.959079\n",
      "Training Epoch: 70 [29824/50000]\tLoss: 4.7467\tLR: 6.959335\n",
      "Training Epoch: 70 [29952/50000]\tLoss: 4.6917\tLR: 6.959591\n",
      "Training Epoch: 70 [30080/50000]\tLoss: 4.7968\tLR: 6.959847\n",
      "Training Epoch: 70 [30208/50000]\tLoss: 4.7912\tLR: 6.960102\n",
      "Training Epoch: 70 [30336/50000]\tLoss: 4.7000\tLR: 6.960358\n",
      "Training Epoch: 70 [30464/50000]\tLoss: 4.7106\tLR: 6.960614\n",
      "Training Epoch: 70 [30592/50000]\tLoss: 4.6338\tLR: 6.960870\n",
      "Training Epoch: 70 [30720/50000]\tLoss: 4.7280\tLR: 6.961125\n",
      "Training Epoch: 70 [30848/50000]\tLoss: 4.7672\tLR: 6.961381\n",
      "Training Epoch: 70 [30976/50000]\tLoss: 4.8480\tLR: 6.961637\n",
      "Training Epoch: 70 [31104/50000]\tLoss: 4.7124\tLR: 6.961893\n",
      "Training Epoch: 70 [31232/50000]\tLoss: 4.7163\tLR: 6.962148\n",
      "Training Epoch: 70 [31360/50000]\tLoss: 4.6836\tLR: 6.962404\n",
      "Training Epoch: 70 [31488/50000]\tLoss: 4.7714\tLR: 6.962660\n",
      "Training Epoch: 70 [31616/50000]\tLoss: 4.7700\tLR: 6.962916\n",
      "Training Epoch: 70 [31744/50000]\tLoss: 4.7261\tLR: 6.963171\n",
      "Training Epoch: 70 [31872/50000]\tLoss: 4.8000\tLR: 6.963427\n",
      "Training Epoch: 70 [32000/50000]\tLoss: 4.7123\tLR: 6.963683\n",
      "Training Epoch: 70 [32128/50000]\tLoss: 4.6940\tLR: 6.963939\n",
      "Training Epoch: 70 [32256/50000]\tLoss: 4.7316\tLR: 6.964194\n",
      "Training Epoch: 70 [32384/50000]\tLoss: 4.6465\tLR: 6.964450\n",
      "Training Epoch: 70 [32512/50000]\tLoss: 4.7147\tLR: 6.964706\n",
      "Training Epoch: 70 [32640/50000]\tLoss: 4.6784\tLR: 6.964962\n",
      "Training Epoch: 70 [32768/50000]\tLoss: 4.7455\tLR: 6.965217\n",
      "Training Epoch: 70 [32896/50000]\tLoss: 4.7247\tLR: 6.965473\n",
      "Training Epoch: 70 [33024/50000]\tLoss: 4.7097\tLR: 6.965729\n",
      "Training Epoch: 70 [33152/50000]\tLoss: 4.7428\tLR: 6.965985\n",
      "Training Epoch: 70 [33280/50000]\tLoss: 4.7507\tLR: 6.966240\n",
      "Training Epoch: 70 [33408/50000]\tLoss: 4.6458\tLR: 6.966496\n",
      "Training Epoch: 70 [33536/50000]\tLoss: 4.7451\tLR: 6.966752\n",
      "Training Epoch: 70 [33664/50000]\tLoss: 4.6774\tLR: 6.967008\n",
      "Training Epoch: 70 [33792/50000]\tLoss: 4.7085\tLR: 6.967263\n",
      "Training Epoch: 70 [33920/50000]\tLoss: 4.6995\tLR: 6.967519\n",
      "Training Epoch: 70 [34048/50000]\tLoss: 4.7149\tLR: 6.967775\n",
      "Training Epoch: 70 [34176/50000]\tLoss: 4.6736\tLR: 6.968031\n",
      "Training Epoch: 70 [34304/50000]\tLoss: 4.7290\tLR: 6.968286\n",
      "Training Epoch: 70 [34432/50000]\tLoss: 4.7630\tLR: 6.968542\n",
      "Training Epoch: 70 [34560/50000]\tLoss: 4.7127\tLR: 6.968798\n",
      "Training Epoch: 70 [34688/50000]\tLoss: 4.8568\tLR: 6.969054\n",
      "Training Epoch: 70 [34816/50000]\tLoss: 4.8620\tLR: 6.969309\n",
      "Training Epoch: 70 [34944/50000]\tLoss: 4.7549\tLR: 6.969565\n",
      "Training Epoch: 70 [35072/50000]\tLoss: 4.7164\tLR: 6.969821\n",
      "Training Epoch: 70 [35200/50000]\tLoss: 4.6744\tLR: 6.970077\n",
      "Training Epoch: 70 [35328/50000]\tLoss: 4.7533\tLR: 6.970332\n",
      "Training Epoch: 70 [35456/50000]\tLoss: 4.7462\tLR: 6.970588\n",
      "Training Epoch: 70 [35584/50000]\tLoss: 4.7335\tLR: 6.970844\n",
      "Training Epoch: 70 [35712/50000]\tLoss: 4.6534\tLR: 6.971100\n",
      "Training Epoch: 70 [35840/50000]\tLoss: 4.7787\tLR: 6.971355\n",
      "Training Epoch: 70 [35968/50000]\tLoss: 4.8357\tLR: 6.971611\n",
      "Training Epoch: 70 [36096/50000]\tLoss: 4.7047\tLR: 6.971867\n",
      "Training Epoch: 70 [36224/50000]\tLoss: 4.7716\tLR: 6.972123\n",
      "Training Epoch: 70 [36352/50000]\tLoss: 4.7528\tLR: 6.972379\n",
      "Training Epoch: 70 [36480/50000]\tLoss: 4.7187\tLR: 6.972634\n",
      "Training Epoch: 70 [36608/50000]\tLoss: 4.8072\tLR: 6.972890\n",
      "Training Epoch: 70 [36736/50000]\tLoss: 4.7358\tLR: 6.973146\n",
      "Training Epoch: 70 [36864/50000]\tLoss: 4.8472\tLR: 6.973402\n",
      "Training Epoch: 70 [36992/50000]\tLoss: 4.7627\tLR: 6.973657\n",
      "Training Epoch: 70 [37120/50000]\tLoss: 4.7279\tLR: 6.973913\n",
      "Training Epoch: 70 [37248/50000]\tLoss: 4.7880\tLR: 6.974169\n",
      "Training Epoch: 70 [37376/50000]\tLoss: 4.7830\tLR: 6.974425\n",
      "Training Epoch: 70 [37504/50000]\tLoss: 4.7748\tLR: 6.974680\n",
      "Training Epoch: 70 [37632/50000]\tLoss: 4.7672\tLR: 6.974936\n",
      "Training Epoch: 70 [37760/50000]\tLoss: 4.6832\tLR: 6.975192\n",
      "Training Epoch: 70 [37888/50000]\tLoss: 4.7179\tLR: 6.975448\n",
      "Training Epoch: 70 [38016/50000]\tLoss: 4.6914\tLR: 6.975703\n",
      "Training Epoch: 70 [38144/50000]\tLoss: 4.7249\tLR: 6.975959\n",
      "Training Epoch: 70 [38272/50000]\tLoss: 4.6291\tLR: 6.976215\n",
      "Training Epoch: 70 [38400/50000]\tLoss: 4.8231\tLR: 6.976471\n",
      "Training Epoch: 70 [38528/50000]\tLoss: 4.6844\tLR: 6.976726\n",
      "Training Epoch: 70 [38656/50000]\tLoss: 4.6277\tLR: 6.976982\n",
      "Training Epoch: 70 [38784/50000]\tLoss: 4.7257\tLR: 6.977238\n",
      "Training Epoch: 70 [38912/50000]\tLoss: 4.7016\tLR: 6.977494\n",
      "Training Epoch: 70 [39040/50000]\tLoss: 4.7533\tLR: 6.977749\n",
      "Training Epoch: 70 [39168/50000]\tLoss: 4.7186\tLR: 6.978005\n",
      "Training Epoch: 70 [39296/50000]\tLoss: 4.7874\tLR: 6.978261\n",
      "Training Epoch: 70 [39424/50000]\tLoss: 4.6578\tLR: 6.978517\n",
      "Training Epoch: 70 [39552/50000]\tLoss: 4.7381\tLR: 6.978772\n",
      "Training Epoch: 70 [39680/50000]\tLoss: 4.7382\tLR: 6.979028\n",
      "Training Epoch: 70 [39808/50000]\tLoss: 4.7547\tLR: 6.979284\n",
      "Training Epoch: 70 [39936/50000]\tLoss: 4.6841\tLR: 6.979540\n",
      "Training Epoch: 70 [40064/50000]\tLoss: 4.8184\tLR: 6.979795\n",
      "Training Epoch: 70 [40192/50000]\tLoss: 4.7727\tLR: 6.980051\n",
      "Training Epoch: 70 [40320/50000]\tLoss: 4.6982\tLR: 6.980307\n",
      "Training Epoch: 70 [40448/50000]\tLoss: 4.7708\tLR: 6.980563\n",
      "Training Epoch: 70 [40576/50000]\tLoss: 4.7712\tLR: 6.980818\n",
      "Training Epoch: 70 [40704/50000]\tLoss: 4.8281\tLR: 6.981074\n",
      "Training Epoch: 70 [40832/50000]\tLoss: 4.7003\tLR: 6.981330\n",
      "Training Epoch: 70 [40960/50000]\tLoss: 4.6186\tLR: 6.981586\n",
      "Training Epoch: 70 [41088/50000]\tLoss: 4.7385\tLR: 6.981841\n",
      "Training Epoch: 70 [41216/50000]\tLoss: 4.7089\tLR: 6.982097\n",
      "Training Epoch: 70 [41344/50000]\tLoss: 4.8303\tLR: 6.982353\n",
      "Training Epoch: 70 [41472/50000]\tLoss: 4.7339\tLR: 6.982609\n",
      "Training Epoch: 70 [41600/50000]\tLoss: 4.7941\tLR: 6.982864\n",
      "Training Epoch: 70 [41728/50000]\tLoss: 4.7276\tLR: 6.983120\n",
      "Training Epoch: 70 [41856/50000]\tLoss: 4.6537\tLR: 6.983376\n",
      "Training Epoch: 70 [41984/50000]\tLoss: 4.7017\tLR: 6.983632\n",
      "Training Epoch: 70 [42112/50000]\tLoss: 4.6179\tLR: 6.983887\n",
      "Training Epoch: 70 [42240/50000]\tLoss: 4.7894\tLR: 6.984143\n",
      "Training Epoch: 70 [42368/50000]\tLoss: 4.6914\tLR: 6.984399\n",
      "Training Epoch: 70 [42496/50000]\tLoss: 4.6903\tLR: 6.984655\n",
      "Training Epoch: 70 [42624/50000]\tLoss: 4.7725\tLR: 6.984910\n",
      "Training Epoch: 70 [42752/50000]\tLoss: 4.7235\tLR: 6.985166\n",
      "Training Epoch: 70 [42880/50000]\tLoss: 4.6860\tLR: 6.985422\n",
      "Training Epoch: 70 [43008/50000]\tLoss: 4.7134\tLR: 6.985678\n",
      "Training Epoch: 70 [43136/50000]\tLoss: 4.7399\tLR: 6.985934\n",
      "Training Epoch: 70 [43264/50000]\tLoss: 4.8040\tLR: 6.986189\n",
      "Training Epoch: 70 [43392/50000]\tLoss: 4.6950\tLR: 6.986445\n",
      "Training Epoch: 70 [43520/50000]\tLoss: 4.7503\tLR: 6.986701\n",
      "Training Epoch: 70 [43648/50000]\tLoss: 4.6569\tLR: 6.986957\n",
      "Training Epoch: 70 [43776/50000]\tLoss: 4.7537\tLR: 6.987212\n",
      "Training Epoch: 70 [43904/50000]\tLoss: 4.7436\tLR: 6.987468\n",
      "Training Epoch: 70 [44032/50000]\tLoss: 4.7067\tLR: 6.987724\n",
      "Training Epoch: 70 [44160/50000]\tLoss: 4.7716\tLR: 6.987980\n",
      "Training Epoch: 70 [44288/50000]\tLoss: 4.7988\tLR: 6.988235\n",
      "Training Epoch: 70 [44416/50000]\tLoss: 4.8126\tLR: 6.988491\n",
      "Training Epoch: 70 [44544/50000]\tLoss: 4.7292\tLR: 6.988747\n",
      "Training Epoch: 70 [44672/50000]\tLoss: 4.7718\tLR: 6.989003\n",
      "Training Epoch: 70 [44800/50000]\tLoss: 4.7890\tLR: 6.989258\n",
      "Training Epoch: 70 [44928/50000]\tLoss: 4.7381\tLR: 6.989514\n",
      "Training Epoch: 70 [45056/50000]\tLoss: 4.7625\tLR: 6.989770\n",
      "Training Epoch: 70 [45184/50000]\tLoss: 4.6754\tLR: 6.990026\n",
      "Training Epoch: 70 [45312/50000]\tLoss: 4.7174\tLR: 6.990281\n",
      "Training Epoch: 70 [45440/50000]\tLoss: 4.7543\tLR: 6.990537\n",
      "Training Epoch: 70 [45568/50000]\tLoss: 4.7561\tLR: 6.990793\n",
      "Training Epoch: 70 [45696/50000]\tLoss: 4.7377\tLR: 6.991049\n",
      "Training Epoch: 70 [45824/50000]\tLoss: 4.8235\tLR: 6.991304\n",
      "Training Epoch: 70 [45952/50000]\tLoss: 4.8574\tLR: 6.991560\n",
      "Training Epoch: 70 [46080/50000]\tLoss: 4.6872\tLR: 6.991816\n",
      "Training Epoch: 70 [46208/50000]\tLoss: 4.7201\tLR: 6.992072\n",
      "Training Epoch: 70 [46336/50000]\tLoss: 4.7002\tLR: 6.992327\n",
      "Training Epoch: 70 [46464/50000]\tLoss: 4.7188\tLR: 6.992583\n",
      "Training Epoch: 70 [46592/50000]\tLoss: 4.8027\tLR: 6.992839\n",
      "Training Epoch: 70 [46720/50000]\tLoss: 4.7829\tLR: 6.993095\n",
      "Training Epoch: 70 [46848/50000]\tLoss: 4.7415\tLR: 6.993350\n",
      "Training Epoch: 70 [46976/50000]\tLoss: 4.8050\tLR: 6.993606\n",
      "Training Epoch: 70 [47104/50000]\tLoss: 4.8006\tLR: 6.993862\n",
      "Training Epoch: 70 [47232/50000]\tLoss: 4.7826\tLR: 6.994118\n",
      "Training Epoch: 70 [47360/50000]\tLoss: 4.6816\tLR: 6.994373\n",
      "Training Epoch: 70 [47488/50000]\tLoss: 4.6907\tLR: 6.994629\n",
      "Training Epoch: 70 [47616/50000]\tLoss: 4.6623\tLR: 6.994885\n",
      "Training Epoch: 70 [47744/50000]\tLoss: 4.7661\tLR: 6.995141\n",
      "Training Epoch: 70 [47872/50000]\tLoss: 4.7598\tLR: 6.995396\n",
      "Training Epoch: 70 [48000/50000]\tLoss: 4.7537\tLR: 6.995652\n",
      "Training Epoch: 70 [48128/50000]\tLoss: 4.7297\tLR: 6.995908\n",
      "Training Epoch: 70 [48256/50000]\tLoss: 4.6736\tLR: 6.996164\n",
      "Training Epoch: 70 [48384/50000]\tLoss: 4.7060\tLR: 6.996419\n",
      "Training Epoch: 70 [48512/50000]\tLoss: 4.7015\tLR: 6.996675\n",
      "Training Epoch: 70 [48640/50000]\tLoss: 4.7829\tLR: 6.996931\n",
      "Training Epoch: 70 [48768/50000]\tLoss: 4.6422\tLR: 6.997187\n",
      "Training Epoch: 70 [48896/50000]\tLoss: 4.7862\tLR: 6.997442\n",
      "Training Epoch: 70 [49024/50000]\tLoss: 4.7487\tLR: 6.997698\n",
      "Training Epoch: 70 [49152/50000]\tLoss: 4.7162\tLR: 6.997954\n",
      "Training Epoch: 70 [49280/50000]\tLoss: 4.7537\tLR: 6.998210\n",
      "Training Epoch: 70 [49408/50000]\tLoss: 4.7298\tLR: 6.998465\n",
      "Training Epoch: 70 [49536/50000]\tLoss: 4.6857\tLR: 6.998721\n",
      "Training Epoch: 70 [49664/50000]\tLoss: 4.8124\tLR: 6.998977\n",
      "Training Epoch: 70 [49792/50000]\tLoss: 4.7663\tLR: 6.999233\n",
      "Training Epoch: 70 [49920/50000]\tLoss: 4.7590\tLR: 6.999488\n",
      "Training Epoch: 70 [50000/50000]\tLoss: 4.7496\tLR: 6.999744\n",
      "epoch 70 training time consumed: 489.10s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   98135 GB |   98135 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   97834 GB |   97834 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     301 GB |     301 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   98135 GB |   98135 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   97834 GB |   97834 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     301 GB |     301 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   96755 GB |   96755 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   96454 GB |   96454 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     301 GB |     301 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   10405 K  |   10405 K  |\n",
      "|       from large pool |      24    |      65    |    4435 K  |    4435 K  |\n",
      "|       from small pool |     231    |     274    |    5969 K  |    5969 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   10405 K  |   10405 K  |\n",
      "|       from large pool |      24    |      65    |    4435 K  |    4435 K  |\n",
      "|       from small pool |     231    |     274    |    5969 K  |    5969 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      47    |    6031 K  |    6031 K  |\n",
      "|       from large pool |      10    |      23    |    2132 K  |    2132 K  |\n",
      "|       from small pool |      25    |      35    |    3899 K  |    3899 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 70, Average loss: 0.0373, Accuracy: 0.0100, Time consumed:31.17s\n",
      "\n",
      "saving weights file to checkpoint\\resnet18\\Monday_25_July_2022_16h_56m_47s\\resnet18-70-regular.pth\n",
      "Training Epoch: 71 [128/50000]\tLoss: 4.7240\tLR: 0.020000\n",
      "Training Epoch: 71 [256/50000]\tLoss: 4.7585\tLR: 7.000256\n",
      "Training Epoch: 71 [384/50000]\tLoss: 4.6465\tLR: 7.000512\n",
      "Training Epoch: 71 [512/50000]\tLoss: 4.7031\tLR: 7.000767\n",
      "Training Epoch: 71 [640/50000]\tLoss: 4.6847\tLR: 7.001023\n",
      "Training Epoch: 71 [768/50000]\tLoss: 4.7324\tLR: 7.001279\n",
      "Training Epoch: 71 [896/50000]\tLoss: 4.8396\tLR: 7.001535\n",
      "Training Epoch: 71 [1024/50000]\tLoss: 4.7616\tLR: 7.001790\n",
      "Training Epoch: 71 [1152/50000]\tLoss: 4.7467\tLR: 7.002046\n",
      "Training Epoch: 71 [1280/50000]\tLoss: 4.7472\tLR: 7.002302\n",
      "Training Epoch: 71 [1408/50000]\tLoss: 4.7640\tLR: 7.002558\n",
      "Training Epoch: 71 [1536/50000]\tLoss: 4.8116\tLR: 7.002813\n",
      "Training Epoch: 71 [1664/50000]\tLoss: 4.7524\tLR: 7.003069\n",
      "Training Epoch: 71 [1792/50000]\tLoss: 4.7735\tLR: 7.003325\n",
      "Training Epoch: 71 [1920/50000]\tLoss: 4.7187\tLR: 7.003581\n",
      "Training Epoch: 71 [2048/50000]\tLoss: 4.7381\tLR: 7.003836\n",
      "Training Epoch: 71 [2176/50000]\tLoss: 4.6996\tLR: 7.004092\n",
      "Training Epoch: 71 [2304/50000]\tLoss: 4.7180\tLR: 7.004348\n",
      "Training Epoch: 71 [2432/50000]\tLoss: 4.7773\tLR: 7.004604\n",
      "Training Epoch: 71 [2560/50000]\tLoss: 4.7235\tLR: 7.004859\n",
      "Training Epoch: 71 [2688/50000]\tLoss: 4.7220\tLR: 7.005115\n",
      "Training Epoch: 71 [2816/50000]\tLoss: 4.7187\tLR: 7.005371\n",
      "Training Epoch: 71 [2944/50000]\tLoss: 4.7352\tLR: 7.005627\n",
      "Training Epoch: 71 [3072/50000]\tLoss: 4.7995\tLR: 7.005882\n",
      "Training Epoch: 71 [3200/50000]\tLoss: 4.7237\tLR: 7.006138\n",
      "Training Epoch: 71 [3328/50000]\tLoss: 4.7737\tLR: 7.006394\n",
      "Training Epoch: 71 [3456/50000]\tLoss: 4.7968\tLR: 7.006650\n",
      "Training Epoch: 71 [3584/50000]\tLoss: 4.7682\tLR: 7.006905\n",
      "Training Epoch: 71 [3712/50000]\tLoss: 4.7152\tLR: 7.007161\n",
      "Training Epoch: 71 [3840/50000]\tLoss: 4.7590\tLR: 7.007417\n",
      "Training Epoch: 71 [3968/50000]\tLoss: 4.7880\tLR: 7.007673\n",
      "Training Epoch: 71 [4096/50000]\tLoss: 4.7418\tLR: 7.007928\n",
      "Training Epoch: 71 [4224/50000]\tLoss: 4.7421\tLR: 7.008184\n",
      "Training Epoch: 71 [4352/50000]\tLoss: 4.8056\tLR: 7.008440\n",
      "Training Epoch: 71 [4480/50000]\tLoss: 4.6300\tLR: 7.008696\n",
      "Training Epoch: 71 [4608/50000]\tLoss: 4.7767\tLR: 7.008951\n",
      "Training Epoch: 71 [4736/50000]\tLoss: 4.8294\tLR: 7.009207\n",
      "Training Epoch: 71 [4864/50000]\tLoss: 4.6312\tLR: 7.009463\n",
      "Training Epoch: 71 [4992/50000]\tLoss: 4.7386\tLR: 7.009719\n",
      "Training Epoch: 71 [5120/50000]\tLoss: 4.7569\tLR: 7.009974\n",
      "Training Epoch: 71 [5248/50000]\tLoss: 4.7465\tLR: 7.010230\n",
      "Training Epoch: 71 [5376/50000]\tLoss: 4.7813\tLR: 7.010486\n",
      "Training Epoch: 71 [5504/50000]\tLoss: 4.7494\tLR: 7.010742\n",
      "Training Epoch: 71 [5632/50000]\tLoss: 4.7769\tLR: 7.010997\n",
      "Training Epoch: 71 [5760/50000]\tLoss: 4.7568\tLR: 7.011253\n",
      "Training Epoch: 71 [5888/50000]\tLoss: 4.7124\tLR: 7.011509\n",
      "Training Epoch: 71 [6016/50000]\tLoss: 4.7628\tLR: 7.011765\n",
      "Training Epoch: 71 [6144/50000]\tLoss: 4.8166\tLR: 7.012020\n",
      "Training Epoch: 71 [6272/50000]\tLoss: 4.8316\tLR: 7.012276\n",
      "Training Epoch: 71 [6400/50000]\tLoss: 4.7312\tLR: 7.012532\n",
      "Training Epoch: 71 [6528/50000]\tLoss: 4.8312\tLR: 7.012788\n",
      "Training Epoch: 71 [6656/50000]\tLoss: 4.7414\tLR: 7.013043\n",
      "Training Epoch: 71 [6784/50000]\tLoss: 4.8615\tLR: 7.013299\n",
      "Training Epoch: 71 [6912/50000]\tLoss: 4.6904\tLR: 7.013555\n",
      "Training Epoch: 71 [7040/50000]\tLoss: 4.6257\tLR: 7.013811\n",
      "Training Epoch: 71 [7168/50000]\tLoss: 4.8005\tLR: 7.014066\n",
      "Training Epoch: 71 [7296/50000]\tLoss: 4.7532\tLR: 7.014322\n",
      "Training Epoch: 71 [7424/50000]\tLoss: 4.7867\tLR: 7.014578\n",
      "Training Epoch: 71 [7552/50000]\tLoss: 4.7985\tLR: 7.014834\n",
      "Training Epoch: 71 [7680/50000]\tLoss: 4.6642\tLR: 7.015090\n",
      "Training Epoch: 71 [7808/50000]\tLoss: 4.7334\tLR: 7.015345\n",
      "Training Epoch: 71 [7936/50000]\tLoss: 4.7534\tLR: 7.015601\n",
      "Training Epoch: 71 [8064/50000]\tLoss: 4.6875\tLR: 7.015857\n",
      "Training Epoch: 71 [8192/50000]\tLoss: 4.7503\tLR: 7.016113\n",
      "Training Epoch: 71 [8320/50000]\tLoss: 4.7890\tLR: 7.016368\n",
      "Training Epoch: 71 [8448/50000]\tLoss: 4.7335\tLR: 7.016624\n",
      "Training Epoch: 71 [8576/50000]\tLoss: 4.7015\tLR: 7.016880\n",
      "Training Epoch: 71 [8704/50000]\tLoss: 4.7410\tLR: 7.017136\n",
      "Training Epoch: 71 [8832/50000]\tLoss: 4.7875\tLR: 7.017391\n",
      "Training Epoch: 71 [8960/50000]\tLoss: 4.7269\tLR: 7.017647\n",
      "Training Epoch: 71 [9088/50000]\tLoss: 4.6195\tLR: 7.017903\n",
      "Training Epoch: 71 [9216/50000]\tLoss: 4.7272\tLR: 7.018159\n",
      "Training Epoch: 71 [9344/50000]\tLoss: 4.7631\tLR: 7.018414\n",
      "Training Epoch: 71 [9472/50000]\tLoss: 4.7445\tLR: 7.018670\n",
      "Training Epoch: 71 [9600/50000]\tLoss: 4.7778\tLR: 7.018926\n",
      "Training Epoch: 71 [9728/50000]\tLoss: 4.7404\tLR: 7.019182\n",
      "Training Epoch: 71 [9856/50000]\tLoss: 4.6989\tLR: 7.019437\n",
      "Training Epoch: 71 [9984/50000]\tLoss: 4.7590\tLR: 7.019693\n",
      "Training Epoch: 71 [10112/50000]\tLoss: 4.7849\tLR: 7.019949\n",
      "Training Epoch: 71 [10240/50000]\tLoss: 4.8148\tLR: 7.020205\n",
      "Training Epoch: 71 [10368/50000]\tLoss: 4.7962\tLR: 7.020460\n",
      "Training Epoch: 71 [10496/50000]\tLoss: 4.7162\tLR: 7.020716\n",
      "Training Epoch: 71 [10624/50000]\tLoss: 4.8241\tLR: 7.020972\n",
      "Training Epoch: 71 [10752/50000]\tLoss: 4.7320\tLR: 7.021228\n",
      "Training Epoch: 71 [10880/50000]\tLoss: 4.6721\tLR: 7.021483\n",
      "Training Epoch: 71 [11008/50000]\tLoss: 4.8080\tLR: 7.021739\n",
      "Training Epoch: 71 [11136/50000]\tLoss: 4.6685\tLR: 7.021995\n",
      "Training Epoch: 71 [11264/50000]\tLoss: 4.7284\tLR: 7.022251\n",
      "Training Epoch: 71 [11392/50000]\tLoss: 4.7394\tLR: 7.022506\n",
      "Training Epoch: 71 [11520/50000]\tLoss: 4.7482\tLR: 7.022762\n",
      "Training Epoch: 71 [11648/50000]\tLoss: 4.6346\tLR: 7.023018\n",
      "Training Epoch: 71 [11776/50000]\tLoss: 4.7431\tLR: 7.023274\n",
      "Training Epoch: 71 [11904/50000]\tLoss: 4.7689\tLR: 7.023529\n",
      "Training Epoch: 71 [12032/50000]\tLoss: 4.7731\tLR: 7.023785\n",
      "Training Epoch: 71 [12160/50000]\tLoss: 4.6760\tLR: 7.024041\n",
      "Training Epoch: 71 [12288/50000]\tLoss: 4.7309\tLR: 7.024297\n",
      "Training Epoch: 71 [12416/50000]\tLoss: 4.7084\tLR: 7.024552\n",
      "Training Epoch: 71 [12544/50000]\tLoss: 4.7430\tLR: 7.024808\n",
      "Training Epoch: 71 [12672/50000]\tLoss: 4.7165\tLR: 7.025064\n",
      "Training Epoch: 71 [12800/50000]\tLoss: 4.7508\tLR: 7.025320\n",
      "Training Epoch: 71 [12928/50000]\tLoss: 4.7409\tLR: 7.025575\n",
      "Training Epoch: 71 [13056/50000]\tLoss: 4.7230\tLR: 7.025831\n",
      "Training Epoch: 71 [13184/50000]\tLoss: 4.6496\tLR: 7.026087\n",
      "Training Epoch: 71 [13312/50000]\tLoss: 4.7061\tLR: 7.026343\n",
      "Training Epoch: 71 [13440/50000]\tLoss: 4.7296\tLR: 7.026598\n",
      "Training Epoch: 71 [13568/50000]\tLoss: 4.7588\tLR: 7.026854\n",
      "Training Epoch: 71 [13696/50000]\tLoss: 4.6963\tLR: 7.027110\n",
      "Training Epoch: 71 [13824/50000]\tLoss: 4.6976\tLR: 7.027366\n",
      "Training Epoch: 71 [13952/50000]\tLoss: 4.7409\tLR: 7.027621\n",
      "Training Epoch: 71 [14080/50000]\tLoss: 4.7492\tLR: 7.027877\n",
      "Training Epoch: 71 [14208/50000]\tLoss: 4.7981\tLR: 7.028133\n",
      "Training Epoch: 71 [14336/50000]\tLoss: 4.7661\tLR: 7.028389\n",
      "Training Epoch: 71 [14464/50000]\tLoss: 4.7442\tLR: 7.028645\n",
      "Training Epoch: 71 [14592/50000]\tLoss: 4.6938\tLR: 7.028900\n",
      "Training Epoch: 71 [14720/50000]\tLoss: 4.7582\tLR: 7.029156\n",
      "Training Epoch: 71 [14848/50000]\tLoss: 4.7183\tLR: 7.029412\n",
      "Training Epoch: 71 [14976/50000]\tLoss: 4.7842\tLR: 7.029668\n",
      "Training Epoch: 71 [15104/50000]\tLoss: 4.7093\tLR: 7.029923\n",
      "Training Epoch: 71 [15232/50000]\tLoss: 4.6855\tLR: 7.030179\n",
      "Training Epoch: 71 [15360/50000]\tLoss: 4.6883\tLR: 7.030435\n",
      "Training Epoch: 71 [15488/50000]\tLoss: 4.8243\tLR: 7.030691\n",
      "Training Epoch: 71 [15616/50000]\tLoss: 4.7098\tLR: 7.030946\n",
      "Training Epoch: 71 [15744/50000]\tLoss: 4.6496\tLR: 7.031202\n",
      "Training Epoch: 71 [15872/50000]\tLoss: 4.7548\tLR: 7.031458\n",
      "Training Epoch: 71 [16000/50000]\tLoss: 4.8101\tLR: 7.031714\n",
      "Training Epoch: 71 [16128/50000]\tLoss: 4.7011\tLR: 7.031969\n",
      "Training Epoch: 71 [16256/50000]\tLoss: 4.7341\tLR: 7.032225\n",
      "Training Epoch: 71 [16384/50000]\tLoss: 4.7731\tLR: 7.032481\n",
      "Training Epoch: 71 [16512/50000]\tLoss: 4.7148\tLR: 7.032737\n",
      "Training Epoch: 71 [16640/50000]\tLoss: 4.6287\tLR: 7.032992\n",
      "Training Epoch: 71 [16768/50000]\tLoss: 4.7366\tLR: 7.033248\n",
      "Training Epoch: 71 [16896/50000]\tLoss: 4.7804\tLR: 7.033504\n",
      "Training Epoch: 71 [17024/50000]\tLoss: 4.7177\tLR: 7.033760\n",
      "Training Epoch: 71 [17152/50000]\tLoss: 4.8906\tLR: 7.034015\n",
      "Training Epoch: 71 [17280/50000]\tLoss: 4.7790\tLR: 7.034271\n",
      "Training Epoch: 71 [17408/50000]\tLoss: 4.7736\tLR: 7.034527\n",
      "Training Epoch: 71 [17536/50000]\tLoss: 4.7459\tLR: 7.034783\n",
      "Training Epoch: 71 [17664/50000]\tLoss: 4.7764\tLR: 7.035038\n",
      "Training Epoch: 71 [17792/50000]\tLoss: 4.6713\tLR: 7.035294\n",
      "Training Epoch: 71 [17920/50000]\tLoss: 4.8400\tLR: 7.035550\n",
      "Training Epoch: 71 [18048/50000]\tLoss: 4.7965\tLR: 7.035806\n",
      "Training Epoch: 71 [18176/50000]\tLoss: 4.7982\tLR: 7.036061\n",
      "Training Epoch: 71 [18304/50000]\tLoss: 4.7451\tLR: 7.036317\n",
      "Training Epoch: 71 [18432/50000]\tLoss: 4.7839\tLR: 7.036573\n",
      "Training Epoch: 71 [18560/50000]\tLoss: 4.8266\tLR: 7.036829\n",
      "Training Epoch: 71 [18688/50000]\tLoss: 4.7817\tLR: 7.037084\n",
      "Training Epoch: 71 [18816/50000]\tLoss: 4.7054\tLR: 7.037340\n",
      "Training Epoch: 71 [18944/50000]\tLoss: 4.7702\tLR: 7.037596\n",
      "Training Epoch: 71 [19072/50000]\tLoss: 4.7705\tLR: 7.037852\n",
      "Training Epoch: 71 [19200/50000]\tLoss: 4.7787\tLR: 7.038107\n",
      "Training Epoch: 71 [19328/50000]\tLoss: 4.7477\tLR: 7.038363\n",
      "Training Epoch: 71 [19456/50000]\tLoss: 4.7803\tLR: 7.038619\n",
      "Training Epoch: 71 [19584/50000]\tLoss: 4.6783\tLR: 7.038875\n",
      "Training Epoch: 71 [19712/50000]\tLoss: 4.7255\tLR: 7.039130\n",
      "Training Epoch: 71 [19840/50000]\tLoss: 4.6852\tLR: 7.039386\n",
      "Training Epoch: 71 [19968/50000]\tLoss: 4.7559\tLR: 7.039642\n",
      "Training Epoch: 71 [20096/50000]\tLoss: 4.7959\tLR: 7.039898\n",
      "Training Epoch: 71 [20224/50000]\tLoss: 4.6973\tLR: 7.040153\n",
      "Training Epoch: 71 [20352/50000]\tLoss: 4.8027\tLR: 7.040409\n",
      "Training Epoch: 71 [20480/50000]\tLoss: 4.7329\tLR: 7.040665\n",
      "Training Epoch: 71 [20608/50000]\tLoss: 4.6935\tLR: 7.040921\n",
      "Training Epoch: 71 [20736/50000]\tLoss: 4.7685\tLR: 7.041176\n",
      "Training Epoch: 71 [20864/50000]\tLoss: 4.6325\tLR: 7.041432\n",
      "Training Epoch: 71 [20992/50000]\tLoss: 4.7135\tLR: 7.041688\n",
      "Training Epoch: 71 [21120/50000]\tLoss: 4.7433\tLR: 7.041944\n",
      "Training Epoch: 71 [21248/50000]\tLoss: 4.7473\tLR: 7.042199\n",
      "Training Epoch: 71 [21376/50000]\tLoss: 4.8231\tLR: 7.042455\n",
      "Training Epoch: 71 [21504/50000]\tLoss: 4.7862\tLR: 7.042711\n",
      "Training Epoch: 71 [21632/50000]\tLoss: 4.7754\tLR: 7.042967\n",
      "Training Epoch: 71 [21760/50000]\tLoss: 4.7686\tLR: 7.043223\n",
      "Training Epoch: 71 [21888/50000]\tLoss: 4.7683\tLR: 7.043478\n",
      "Training Epoch: 71 [22016/50000]\tLoss: 4.7504\tLR: 7.043734\n",
      "Training Epoch: 71 [22144/50000]\tLoss: 4.6342\tLR: 7.043990\n",
      "Training Epoch: 71 [22272/50000]\tLoss: 4.7661\tLR: 7.044246\n",
      "Training Epoch: 71 [22400/50000]\tLoss: 4.7210\tLR: 7.044501\n",
      "Training Epoch: 71 [22528/50000]\tLoss: 4.7180\tLR: 7.044757\n",
      "Training Epoch: 71 [22656/50000]\tLoss: 4.7165\tLR: 7.045013\n",
      "Training Epoch: 71 [22784/50000]\tLoss: 4.7327\tLR: 7.045269\n",
      "Training Epoch: 71 [22912/50000]\tLoss: 4.7583\tLR: 7.045524\n",
      "Training Epoch: 71 [23040/50000]\tLoss: 4.6975\tLR: 7.045780\n",
      "Training Epoch: 71 [23168/50000]\tLoss: 4.6437\tLR: 7.046036\n",
      "Training Epoch: 71 [23296/50000]\tLoss: 4.8000\tLR: 7.046292\n",
      "Training Epoch: 71 [23424/50000]\tLoss: 4.7105\tLR: 7.046547\n",
      "Training Epoch: 71 [23552/50000]\tLoss: 4.7597\tLR: 7.046803\n",
      "Training Epoch: 71 [23680/50000]\tLoss: 4.8151\tLR: 7.047059\n",
      "Training Epoch: 71 [23808/50000]\tLoss: 4.7355\tLR: 7.047315\n",
      "Training Epoch: 71 [23936/50000]\tLoss: 4.7425\tLR: 7.047570\n",
      "Training Epoch: 71 [24064/50000]\tLoss: 4.7199\tLR: 7.047826\n",
      "Training Epoch: 71 [24192/50000]\tLoss: 4.7106\tLR: 7.048082\n",
      "Training Epoch: 71 [24320/50000]\tLoss: 4.6901\tLR: 7.048338\n",
      "Training Epoch: 71 [24448/50000]\tLoss: 4.7668\tLR: 7.048593\n",
      "Training Epoch: 71 [24576/50000]\tLoss: 4.7559\tLR: 7.048849\n",
      "Training Epoch: 71 [24704/50000]\tLoss: 4.7645\tLR: 7.049105\n",
      "Training Epoch: 71 [24832/50000]\tLoss: 4.8409\tLR: 7.049361\n",
      "Training Epoch: 71 [24960/50000]\tLoss: 4.7730\tLR: 7.049616\n",
      "Training Epoch: 71 [25088/50000]\tLoss: 4.8302\tLR: 7.049872\n",
      "Training Epoch: 71 [25216/50000]\tLoss: 4.6701\tLR: 7.050128\n",
      "Training Epoch: 71 [25344/50000]\tLoss: 4.7289\tLR: 7.050384\n",
      "Training Epoch: 71 [25472/50000]\tLoss: 4.7267\tLR: 7.050639\n",
      "Training Epoch: 71 [25600/50000]\tLoss: 4.7680\tLR: 7.050895\n",
      "Training Epoch: 71 [25728/50000]\tLoss: 4.7548\tLR: 7.051151\n",
      "Training Epoch: 71 [25856/50000]\tLoss: 4.6898\tLR: 7.051407\n",
      "Training Epoch: 71 [25984/50000]\tLoss: 4.6394\tLR: 7.051662\n",
      "Training Epoch: 71 [26112/50000]\tLoss: 4.6792\tLR: 7.051918\n",
      "Training Epoch: 71 [26240/50000]\tLoss: 4.7584\tLR: 7.052174\n",
      "Training Epoch: 71 [26368/50000]\tLoss: 4.7005\tLR: 7.052430\n",
      "Training Epoch: 71 [26496/50000]\tLoss: 4.7088\tLR: 7.052685\n",
      "Training Epoch: 71 [26624/50000]\tLoss: 4.6881\tLR: 7.052941\n",
      "Training Epoch: 71 [26752/50000]\tLoss: 4.7120\tLR: 7.053197\n",
      "Training Epoch: 71 [26880/50000]\tLoss: 4.7574\tLR: 7.053453\n",
      "Training Epoch: 71 [27008/50000]\tLoss: 4.8188\tLR: 7.053708\n",
      "Training Epoch: 71 [27136/50000]\tLoss: 4.6882\tLR: 7.053964\n",
      "Training Epoch: 71 [27264/50000]\tLoss: 4.7521\tLR: 7.054220\n",
      "Training Epoch: 71 [27392/50000]\tLoss: 4.7510\tLR: 7.054476\n",
      "Training Epoch: 71 [27520/50000]\tLoss: 4.7630\tLR: 7.054731\n",
      "Training Epoch: 71 [27648/50000]\tLoss: 4.8216\tLR: 7.054987\n",
      "Training Epoch: 71 [27776/50000]\tLoss: 4.7021\tLR: 7.055243\n",
      "Training Epoch: 71 [27904/50000]\tLoss: 4.7280\tLR: 7.055499\n",
      "Training Epoch: 71 [28032/50000]\tLoss: 4.7428\tLR: 7.055754\n",
      "Training Epoch: 71 [28160/50000]\tLoss: 4.7603\tLR: 7.056010\n",
      "Training Epoch: 71 [28288/50000]\tLoss: 4.8468\tLR: 7.056266\n",
      "Training Epoch: 71 [28416/50000]\tLoss: 4.6290\tLR: 7.056522\n",
      "Training Epoch: 71 [28544/50000]\tLoss: 4.7368\tLR: 7.056777\n",
      "Training Epoch: 71 [28672/50000]\tLoss: 4.6880\tLR: 7.057033\n",
      "Training Epoch: 71 [28800/50000]\tLoss: 4.7243\tLR: 7.057289\n",
      "Training Epoch: 71 [28928/50000]\tLoss: 4.8486\tLR: 7.057545\n",
      "Training Epoch: 71 [29056/50000]\tLoss: 4.7884\tLR: 7.057801\n",
      "Training Epoch: 71 [29184/50000]\tLoss: 4.8570\tLR: 7.058056\n",
      "Training Epoch: 71 [29312/50000]\tLoss: 4.7661\tLR: 7.058312\n",
      "Training Epoch: 71 [29440/50000]\tLoss: 4.7082\tLR: 7.058568\n",
      "Training Epoch: 71 [29568/50000]\tLoss: 4.7289\tLR: 7.058824\n",
      "Training Epoch: 71 [29696/50000]\tLoss: 4.7165\tLR: 7.059079\n",
      "Training Epoch: 71 [29824/50000]\tLoss: 4.8472\tLR: 7.059335\n",
      "Training Epoch: 71 [29952/50000]\tLoss: 4.7829\tLR: 7.059591\n",
      "Training Epoch: 71 [30080/50000]\tLoss: 4.7552\tLR: 7.059847\n",
      "Training Epoch: 71 [30208/50000]\tLoss: 4.6798\tLR: 7.060102\n",
      "Training Epoch: 71 [30336/50000]\tLoss: 4.6953\tLR: 7.060358\n",
      "Training Epoch: 71 [30464/50000]\tLoss: 4.7309\tLR: 7.060614\n",
      "Training Epoch: 71 [30592/50000]\tLoss: 4.7694\tLR: 7.060870\n",
      "Training Epoch: 71 [30720/50000]\tLoss: 4.6797\tLR: 7.061125\n",
      "Training Epoch: 71 [30848/50000]\tLoss: 4.7927\tLR: 7.061381\n",
      "Training Epoch: 71 [30976/50000]\tLoss: 4.7835\tLR: 7.061637\n",
      "Training Epoch: 71 [31104/50000]\tLoss: 4.8298\tLR: 7.061893\n",
      "Training Epoch: 71 [31232/50000]\tLoss: 4.7822\tLR: 7.062148\n",
      "Training Epoch: 71 [31360/50000]\tLoss: 4.7255\tLR: 7.062404\n",
      "Training Epoch: 71 [31488/50000]\tLoss: 4.7114\tLR: 7.062660\n",
      "Training Epoch: 71 [31616/50000]\tLoss: 4.7638\tLR: 7.062916\n",
      "Training Epoch: 71 [31744/50000]\tLoss: 4.6615\tLR: 7.063171\n",
      "Training Epoch: 71 [31872/50000]\tLoss: 4.6910\tLR: 7.063427\n",
      "Training Epoch: 71 [32000/50000]\tLoss: 4.7702\tLR: 7.063683\n",
      "Training Epoch: 71 [32128/50000]\tLoss: 4.7072\tLR: 7.063939\n",
      "Training Epoch: 71 [32256/50000]\tLoss: 4.7722\tLR: 7.064194\n",
      "Training Epoch: 71 [32384/50000]\tLoss: 4.8495\tLR: 7.064450\n",
      "Training Epoch: 71 [32512/50000]\tLoss: 4.8200\tLR: 7.064706\n",
      "Training Epoch: 71 [32640/50000]\tLoss: 4.7697\tLR: 7.064962\n",
      "Training Epoch: 71 [32768/50000]\tLoss: 4.7769\tLR: 7.065217\n",
      "Training Epoch: 71 [32896/50000]\tLoss: 4.7338\tLR: 7.065473\n",
      "Training Epoch: 71 [33024/50000]\tLoss: 4.7195\tLR: 7.065729\n",
      "Training Epoch: 71 [33152/50000]\tLoss: 4.7990\tLR: 7.065985\n",
      "Training Epoch: 71 [33280/50000]\tLoss: 4.7050\tLR: 7.066240\n",
      "Training Epoch: 71 [33408/50000]\tLoss: 4.7438\tLR: 7.066496\n",
      "Training Epoch: 71 [33536/50000]\tLoss: 4.6719\tLR: 7.066752\n",
      "Training Epoch: 71 [33664/50000]\tLoss: 4.8307\tLR: 7.067008\n",
      "Training Epoch: 71 [33792/50000]\tLoss: 4.7838\tLR: 7.067263\n",
      "Training Epoch: 71 [33920/50000]\tLoss: 4.7312\tLR: 7.067519\n",
      "Training Epoch: 71 [34048/50000]\tLoss: 4.7109\tLR: 7.067775\n",
      "Training Epoch: 71 [34176/50000]\tLoss: 4.7948\tLR: 7.068031\n",
      "Training Epoch: 71 [34304/50000]\tLoss: 4.7086\tLR: 7.068286\n",
      "Training Epoch: 71 [34432/50000]\tLoss: 4.7443\tLR: 7.068542\n",
      "Training Epoch: 71 [34560/50000]\tLoss: 4.6898\tLR: 7.068798\n",
      "Training Epoch: 71 [34688/50000]\tLoss: 4.6980\tLR: 7.069054\n",
      "Training Epoch: 71 [34816/50000]\tLoss: 4.7506\tLR: 7.069309\n",
      "Training Epoch: 71 [34944/50000]\tLoss: 4.7127\tLR: 7.069565\n",
      "Training Epoch: 71 [35072/50000]\tLoss: 4.6769\tLR: 7.069821\n",
      "Training Epoch: 71 [35200/50000]\tLoss: 4.7772\tLR: 7.070077\n",
      "Training Epoch: 71 [35328/50000]\tLoss: 4.8234\tLR: 7.070332\n",
      "Training Epoch: 71 [35456/50000]\tLoss: 4.7601\tLR: 7.070588\n",
      "Training Epoch: 71 [35584/50000]\tLoss: 4.7428\tLR: 7.070844\n",
      "Training Epoch: 71 [35712/50000]\tLoss: 4.5908\tLR: 7.071100\n",
      "Training Epoch: 71 [35840/50000]\tLoss: 4.7095\tLR: 7.071355\n",
      "Training Epoch: 71 [35968/50000]\tLoss: 4.6978\tLR: 7.071611\n",
      "Training Epoch: 71 [36096/50000]\tLoss: 4.7041\tLR: 7.071867\n",
      "Training Epoch: 71 [36224/50000]\tLoss: 4.7736\tLR: 7.072123\n",
      "Training Epoch: 71 [36352/50000]\tLoss: 4.7660\tLR: 7.072379\n",
      "Training Epoch: 71 [36480/50000]\tLoss: 4.7538\tLR: 7.072634\n",
      "Training Epoch: 71 [36608/50000]\tLoss: 4.6590\tLR: 7.072890\n",
      "Training Epoch: 71 [36736/50000]\tLoss: 4.7572\tLR: 7.073146\n",
      "Training Epoch: 71 [36864/50000]\tLoss: 4.6532\tLR: 7.073402\n",
      "Training Epoch: 71 [36992/50000]\tLoss: 4.7368\tLR: 7.073657\n",
      "Training Epoch: 71 [37120/50000]\tLoss: 4.7406\tLR: 7.073913\n",
      "Training Epoch: 71 [37248/50000]\tLoss: 4.7042\tLR: 7.074169\n",
      "Training Epoch: 71 [37376/50000]\tLoss: 4.7792\tLR: 7.074425\n",
      "Training Epoch: 71 [37504/50000]\tLoss: 4.7219\tLR: 7.074680\n",
      "Training Epoch: 71 [37632/50000]\tLoss: 4.7472\tLR: 7.074936\n",
      "Training Epoch: 71 [37760/50000]\tLoss: 4.7592\tLR: 7.075192\n",
      "Training Epoch: 71 [37888/50000]\tLoss: 4.7313\tLR: 7.075448\n",
      "Training Epoch: 71 [38016/50000]\tLoss: 4.8194\tLR: 7.075703\n",
      "Training Epoch: 71 [38144/50000]\tLoss: 4.6971\tLR: 7.075959\n",
      "Training Epoch: 71 [38272/50000]\tLoss: 4.6947\tLR: 7.076215\n",
      "Training Epoch: 71 [38400/50000]\tLoss: 4.7596\tLR: 7.076471\n",
      "Training Epoch: 71 [38528/50000]\tLoss: 4.7595\tLR: 7.076726\n",
      "Training Epoch: 71 [38656/50000]\tLoss: 4.7393\tLR: 7.076982\n",
      "Training Epoch: 71 [38784/50000]\tLoss: 4.7711\tLR: 7.077238\n",
      "Training Epoch: 71 [38912/50000]\tLoss: 4.7373\tLR: 7.077494\n",
      "Training Epoch: 71 [39040/50000]\tLoss: 4.6687\tLR: 7.077749\n",
      "Training Epoch: 71 [39168/50000]\tLoss: 4.6864\tLR: 7.078005\n",
      "Training Epoch: 71 [39296/50000]\tLoss: 4.7659\tLR: 7.078261\n",
      "Training Epoch: 71 [39424/50000]\tLoss: 4.8041\tLR: 7.078517\n",
      "Training Epoch: 71 [39552/50000]\tLoss: 4.7250\tLR: 7.078772\n",
      "Training Epoch: 71 [39680/50000]\tLoss: 4.7672\tLR: 7.079028\n",
      "Training Epoch: 71 [39808/50000]\tLoss: 4.6295\tLR: 7.079284\n",
      "Training Epoch: 71 [39936/50000]\tLoss: 4.7592\tLR: 7.079540\n",
      "Training Epoch: 71 [40064/50000]\tLoss: 4.7961\tLR: 7.079795\n",
      "Training Epoch: 71 [40192/50000]\tLoss: 4.6836\tLR: 7.080051\n",
      "Training Epoch: 71 [40320/50000]\tLoss: 4.7519\tLR: 7.080307\n",
      "Training Epoch: 71 [40448/50000]\tLoss: 4.7520\tLR: 7.080563\n",
      "Training Epoch: 71 [40576/50000]\tLoss: 4.6788\tLR: 7.080818\n",
      "Training Epoch: 71 [40704/50000]\tLoss: 4.7245\tLR: 7.081074\n",
      "Training Epoch: 71 [40832/50000]\tLoss: 4.6950\tLR: 7.081330\n",
      "Training Epoch: 71 [40960/50000]\tLoss: 4.7394\tLR: 7.081586\n",
      "Training Epoch: 71 [41088/50000]\tLoss: 4.7303\tLR: 7.081841\n",
      "Training Epoch: 71 [41216/50000]\tLoss: 4.6895\tLR: 7.082097\n",
      "Training Epoch: 71 [41344/50000]\tLoss: 4.7784\tLR: 7.082353\n",
      "Training Epoch: 71 [41472/50000]\tLoss: 4.7529\tLR: 7.082609\n",
      "Training Epoch: 71 [41600/50000]\tLoss: 4.6386\tLR: 7.082864\n",
      "Training Epoch: 71 [41728/50000]\tLoss: 4.6791\tLR: 7.083120\n",
      "Training Epoch: 71 [41856/50000]\tLoss: 4.7251\tLR: 7.083376\n",
      "Training Epoch: 71 [41984/50000]\tLoss: 4.7239\tLR: 7.083632\n",
      "Training Epoch: 71 [42112/50000]\tLoss: 4.7905\tLR: 7.083887\n",
      "Training Epoch: 71 [42240/50000]\tLoss: 4.7916\tLR: 7.084143\n",
      "Training Epoch: 71 [42368/50000]\tLoss: 4.8391\tLR: 7.084399\n",
      "Training Epoch: 71 [42496/50000]\tLoss: 4.6791\tLR: 7.084655\n",
      "Training Epoch: 71 [42624/50000]\tLoss: 4.7659\tLR: 7.084910\n",
      "Training Epoch: 71 [42752/50000]\tLoss: 4.6655\tLR: 7.085166\n",
      "Training Epoch: 71 [42880/50000]\tLoss: 4.6856\tLR: 7.085422\n",
      "Training Epoch: 71 [43008/50000]\tLoss: 4.8028\tLR: 7.085678\n",
      "Training Epoch: 71 [43136/50000]\tLoss: 4.7995\tLR: 7.085934\n",
      "Training Epoch: 71 [43264/50000]\tLoss: 4.7593\tLR: 7.086189\n",
      "Training Epoch: 71 [43392/50000]\tLoss: 4.7329\tLR: 7.086445\n",
      "Training Epoch: 71 [43520/50000]\tLoss: 4.7208\tLR: 7.086701\n",
      "Training Epoch: 71 [43648/50000]\tLoss: 4.8489\tLR: 7.086957\n",
      "Training Epoch: 71 [43776/50000]\tLoss: 4.7423\tLR: 7.087212\n",
      "Training Epoch: 71 [43904/50000]\tLoss: 4.7312\tLR: 7.087468\n",
      "Training Epoch: 71 [44032/50000]\tLoss: 4.7576\tLR: 7.087724\n",
      "Training Epoch: 71 [44160/50000]\tLoss: 4.7870\tLR: 7.087980\n",
      "Training Epoch: 71 [44288/50000]\tLoss: 4.7769\tLR: 7.088235\n",
      "Training Epoch: 71 [44416/50000]\tLoss: 4.7851\tLR: 7.088491\n",
      "Training Epoch: 71 [44544/50000]\tLoss: 4.6814\tLR: 7.088747\n",
      "Training Epoch: 71 [44672/50000]\tLoss: 4.7300\tLR: 7.089003\n",
      "Training Epoch: 71 [44800/50000]\tLoss: 4.7259\tLR: 7.089258\n",
      "Training Epoch: 71 [44928/50000]\tLoss: 4.7606\tLR: 7.089514\n",
      "Training Epoch: 71 [45056/50000]\tLoss: 4.6476\tLR: 7.089770\n",
      "Training Epoch: 71 [45184/50000]\tLoss: 4.8367\tLR: 7.090026\n",
      "Training Epoch: 71 [45312/50000]\tLoss: 4.6709\tLR: 7.090281\n",
      "Training Epoch: 71 [45440/50000]\tLoss: 4.7921\tLR: 7.090537\n",
      "Training Epoch: 71 [45568/50000]\tLoss: 4.8113\tLR: 7.090793\n",
      "Training Epoch: 71 [45696/50000]\tLoss: 4.8030\tLR: 7.091049\n",
      "Training Epoch: 71 [45824/50000]\tLoss: 4.7655\tLR: 7.091304\n",
      "Training Epoch: 71 [45952/50000]\tLoss: 4.7350\tLR: 7.091560\n",
      "Training Epoch: 71 [46080/50000]\tLoss: 4.7660\tLR: 7.091816\n",
      "Training Epoch: 71 [46208/50000]\tLoss: 4.7652\tLR: 7.092072\n",
      "Training Epoch: 71 [46336/50000]\tLoss: 4.7451\tLR: 7.092327\n",
      "Training Epoch: 71 [46464/50000]\tLoss: 4.7444\tLR: 7.092583\n",
      "Training Epoch: 71 [46592/50000]\tLoss: 4.6925\tLR: 7.092839\n",
      "Training Epoch: 71 [46720/50000]\tLoss: 4.6734\tLR: 7.093095\n",
      "Training Epoch: 71 [46848/50000]\tLoss: 4.7114\tLR: 7.093350\n",
      "Training Epoch: 71 [46976/50000]\tLoss: 4.7923\tLR: 7.093606\n",
      "Training Epoch: 71 [47104/50000]\tLoss: 4.7955\tLR: 7.093862\n",
      "Training Epoch: 71 [47232/50000]\tLoss: 4.7078\tLR: 7.094118\n",
      "Training Epoch: 71 [47360/50000]\tLoss: 4.7903\tLR: 7.094373\n",
      "Training Epoch: 71 [47488/50000]\tLoss: 4.8148\tLR: 7.094629\n",
      "Training Epoch: 71 [47616/50000]\tLoss: 4.7909\tLR: 7.094885\n",
      "Training Epoch: 71 [47744/50000]\tLoss: 4.7200\tLR: 7.095141\n",
      "Training Epoch: 71 [47872/50000]\tLoss: 4.7215\tLR: 7.095396\n",
      "Training Epoch: 71 [48000/50000]\tLoss: 4.7205\tLR: 7.095652\n",
      "Training Epoch: 71 [48128/50000]\tLoss: 4.8586\tLR: 7.095908\n",
      "Training Epoch: 71 [48256/50000]\tLoss: 4.8065\tLR: 7.096164\n",
      "Training Epoch: 71 [48384/50000]\tLoss: 4.6925\tLR: 7.096419\n",
      "Training Epoch: 71 [48512/50000]\tLoss: 4.8273\tLR: 7.096675\n",
      "Training Epoch: 71 [48640/50000]\tLoss: 4.8237\tLR: 7.096931\n",
      "Training Epoch: 71 [48768/50000]\tLoss: 4.7092\tLR: 7.097187\n",
      "Training Epoch: 71 [48896/50000]\tLoss: 4.9452\tLR: 7.097442\n",
      "Training Epoch: 71 [49024/50000]\tLoss: 4.8021\tLR: 7.097698\n",
      "Training Epoch: 71 [49152/50000]\tLoss: 4.7528\tLR: 7.097954\n",
      "Training Epoch: 71 [49280/50000]\tLoss: 4.7353\tLR: 7.098210\n",
      "Training Epoch: 71 [49408/50000]\tLoss: 4.8335\tLR: 7.098465\n",
      "Training Epoch: 71 [49536/50000]\tLoss: 4.8422\tLR: 7.098721\n",
      "Training Epoch: 71 [49664/50000]\tLoss: 4.7474\tLR: 7.098977\n",
      "Training Epoch: 71 [49792/50000]\tLoss: 4.7643\tLR: 7.099233\n",
      "Training Epoch: 71 [49920/50000]\tLoss: 4.7434\tLR: 7.099488\n",
      "Training Epoch: 71 [50000/50000]\tLoss: 4.7259\tLR: 7.099744\n",
      "epoch 71 training time consumed: 488.93s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |   99537 GB |   99537 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   99232 GB |   99231 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     305 GB |     305 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |   99537 GB |   99537 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |   99232 GB |   99231 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     305 GB |     305 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   98138 GB |   98138 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   97832 GB |   97832 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     305 GB |     305 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   10554 K  |   10554 K  |\n",
      "|       from large pool |      24    |      65    |    4499 K  |    4499 K  |\n",
      "|       from small pool |     231    |     274    |    6055 K  |    6055 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   10554 K  |   10554 K  |\n",
      "|       from large pool |      24    |      65    |    4499 K  |    4499 K  |\n",
      "|       from small pool |     231    |     274    |    6055 K  |    6055 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      47    |    6117 K  |    6117 K  |\n",
      "|       from large pool |      10    |      23    |    2162 K  |    2162 K  |\n",
      "|       from small pool |      27    |      35    |    3954 K  |    3954 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 71, Average loss: 0.0374, Accuracy: 0.0100, Time consumed:31.21s\n",
      "\n",
      "Training Epoch: 72 [128/50000]\tLoss: 4.7345\tLR: 0.020000\n",
      "Training Epoch: 72 [256/50000]\tLoss: 4.7191\tLR: 7.100256\n",
      "Training Epoch: 72 [384/50000]\tLoss: 4.6626\tLR: 7.100512\n",
      "Training Epoch: 72 [512/50000]\tLoss: 4.7218\tLR: 7.100767\n",
      "Training Epoch: 72 [640/50000]\tLoss: 4.7066\tLR: 7.101023\n",
      "Training Epoch: 72 [768/50000]\tLoss: 4.8186\tLR: 7.101279\n",
      "Training Epoch: 72 [896/50000]\tLoss: 4.7797\tLR: 7.101535\n",
      "Training Epoch: 72 [1024/50000]\tLoss: 4.7603\tLR: 7.101790\n",
      "Training Epoch: 72 [1152/50000]\tLoss: 4.8112\tLR: 7.102046\n",
      "Training Epoch: 72 [1280/50000]\tLoss: 4.7215\tLR: 7.102302\n",
      "Training Epoch: 72 [1408/50000]\tLoss: 4.6130\tLR: 7.102558\n",
      "Training Epoch: 72 [1536/50000]\tLoss: 4.8367\tLR: 7.102813\n",
      "Training Epoch: 72 [1664/50000]\tLoss: 4.7771\tLR: 7.103069\n",
      "Training Epoch: 72 [1792/50000]\tLoss: 4.7795\tLR: 7.103325\n",
      "Training Epoch: 72 [1920/50000]\tLoss: 4.8051\tLR: 7.103581\n",
      "Training Epoch: 72 [2048/50000]\tLoss: 4.6791\tLR: 7.103836\n",
      "Training Epoch: 72 [2176/50000]\tLoss: 4.7492\tLR: 7.104092\n",
      "Training Epoch: 72 [2304/50000]\tLoss: 4.6688\tLR: 7.104348\n",
      "Training Epoch: 72 [2432/50000]\tLoss: 4.8002\tLR: 7.104604\n",
      "Training Epoch: 72 [2560/50000]\tLoss: 4.7319\tLR: 7.104859\n",
      "Training Epoch: 72 [2688/50000]\tLoss: 4.8068\tLR: 7.105115\n",
      "Training Epoch: 72 [2816/50000]\tLoss: 4.7135\tLR: 7.105371\n",
      "Training Epoch: 72 [2944/50000]\tLoss: 4.7476\tLR: 7.105627\n",
      "Training Epoch: 72 [3072/50000]\tLoss: 4.7064\tLR: 7.105882\n",
      "Training Epoch: 72 [3200/50000]\tLoss: 4.8158\tLR: 7.106138\n",
      "Training Epoch: 72 [3328/50000]\tLoss: 4.7630\tLR: 7.106394\n",
      "Training Epoch: 72 [3456/50000]\tLoss: 4.7942\tLR: 7.106650\n",
      "Training Epoch: 72 [3584/50000]\tLoss: 4.7771\tLR: 7.106905\n",
      "Training Epoch: 72 [3712/50000]\tLoss: 4.7721\tLR: 7.107161\n",
      "Training Epoch: 72 [3840/50000]\tLoss: 4.7242\tLR: 7.107417\n",
      "Training Epoch: 72 [3968/50000]\tLoss: 4.6786\tLR: 7.107673\n",
      "Training Epoch: 72 [4096/50000]\tLoss: 4.6532\tLR: 7.107928\n",
      "Training Epoch: 72 [4224/50000]\tLoss: 4.7192\tLR: 7.108184\n",
      "Training Epoch: 72 [4352/50000]\tLoss: 4.6719\tLR: 7.108440\n",
      "Training Epoch: 72 [4480/50000]\tLoss: 4.6723\tLR: 7.108696\n",
      "Training Epoch: 72 [4608/50000]\tLoss: 4.7527\tLR: 7.108951\n",
      "Training Epoch: 72 [4736/50000]\tLoss: 4.8239\tLR: 7.109207\n",
      "Training Epoch: 72 [4864/50000]\tLoss: 4.7747\tLR: 7.109463\n",
      "Training Epoch: 72 [4992/50000]\tLoss: 4.6725\tLR: 7.109719\n",
      "Training Epoch: 72 [5120/50000]\tLoss: 4.6995\tLR: 7.109974\n",
      "Training Epoch: 72 [5248/50000]\tLoss: 4.7230\tLR: 7.110230\n",
      "Training Epoch: 72 [5376/50000]\tLoss: 4.7572\tLR: 7.110486\n",
      "Training Epoch: 72 [5504/50000]\tLoss: 4.6891\tLR: 7.110742\n",
      "Training Epoch: 72 [5632/50000]\tLoss: 4.8038\tLR: 7.110997\n",
      "Training Epoch: 72 [5760/50000]\tLoss: 4.8807\tLR: 7.111253\n",
      "Training Epoch: 72 [5888/50000]\tLoss: 4.7449\tLR: 7.111509\n",
      "Training Epoch: 72 [6016/50000]\tLoss: 4.7942\tLR: 7.111765\n",
      "Training Epoch: 72 [6144/50000]\tLoss: 4.7085\tLR: 7.112020\n",
      "Training Epoch: 72 [6272/50000]\tLoss: 4.7691\tLR: 7.112276\n",
      "Training Epoch: 72 [6400/50000]\tLoss: 4.7623\tLR: 7.112532\n",
      "Training Epoch: 72 [6528/50000]\tLoss: 4.7099\tLR: 7.112788\n",
      "Training Epoch: 72 [6656/50000]\tLoss: 4.7118\tLR: 7.113043\n",
      "Training Epoch: 72 [6784/50000]\tLoss: 4.6994\tLR: 7.113299\n",
      "Training Epoch: 72 [6912/50000]\tLoss: 4.6938\tLR: 7.113555\n",
      "Training Epoch: 72 [7040/50000]\tLoss: 4.7923\tLR: 7.113811\n",
      "Training Epoch: 72 [7168/50000]\tLoss: 4.7902\tLR: 7.114066\n",
      "Training Epoch: 72 [7296/50000]\tLoss: 4.8340\tLR: 7.114322\n",
      "Training Epoch: 72 [7424/50000]\tLoss: 4.8667\tLR: 7.114578\n",
      "Training Epoch: 72 [7552/50000]\tLoss: 4.7208\tLR: 7.114834\n",
      "Training Epoch: 72 [7680/50000]\tLoss: 4.7673\tLR: 7.115090\n",
      "Training Epoch: 72 [7808/50000]\tLoss: 4.7996\tLR: 7.115345\n",
      "Training Epoch: 72 [7936/50000]\tLoss: 4.7342\tLR: 7.115601\n",
      "Training Epoch: 72 [8064/50000]\tLoss: 4.6441\tLR: 7.115857\n",
      "Training Epoch: 72 [8192/50000]\tLoss: 4.7565\tLR: 7.116113\n",
      "Training Epoch: 72 [8320/50000]\tLoss: 4.7190\tLR: 7.116368\n",
      "Training Epoch: 72 [8448/50000]\tLoss: 4.7595\tLR: 7.116624\n",
      "Training Epoch: 72 [8576/50000]\tLoss: 4.6680\tLR: 7.116880\n",
      "Training Epoch: 72 [8704/50000]\tLoss: 4.6707\tLR: 7.117136\n",
      "Training Epoch: 72 [8832/50000]\tLoss: 4.6720\tLR: 7.117391\n",
      "Training Epoch: 72 [8960/50000]\tLoss: 4.8650\tLR: 7.117647\n",
      "Training Epoch: 72 [9088/50000]\tLoss: 4.8250\tLR: 7.117903\n",
      "Training Epoch: 72 [9216/50000]\tLoss: 4.6893\tLR: 7.118159\n",
      "Training Epoch: 72 [9344/50000]\tLoss: 4.7766\tLR: 7.118414\n",
      "Training Epoch: 72 [9472/50000]\tLoss: 4.7104\tLR: 7.118670\n",
      "Training Epoch: 72 [9600/50000]\tLoss: 4.6961\tLR: 7.118926\n",
      "Training Epoch: 72 [9728/50000]\tLoss: 4.6985\tLR: 7.119182\n",
      "Training Epoch: 72 [9856/50000]\tLoss: 4.7092\tLR: 7.119437\n",
      "Training Epoch: 72 [9984/50000]\tLoss: 4.7859\tLR: 7.119693\n",
      "Training Epoch: 72 [10112/50000]\tLoss: 4.7522\tLR: 7.119949\n",
      "Training Epoch: 72 [10240/50000]\tLoss: 4.7758\tLR: 7.120205\n",
      "Training Epoch: 72 [10368/50000]\tLoss: 4.7699\tLR: 7.120460\n",
      "Training Epoch: 72 [10496/50000]\tLoss: 4.6977\tLR: 7.120716\n",
      "Training Epoch: 72 [10624/50000]\tLoss: 4.7264\tLR: 7.120972\n",
      "Training Epoch: 72 [10752/50000]\tLoss: 4.7662\tLR: 7.121228\n",
      "Training Epoch: 72 [10880/50000]\tLoss: 4.7961\tLR: 7.121483\n",
      "Training Epoch: 72 [11008/50000]\tLoss: 4.7180\tLR: 7.121739\n",
      "Training Epoch: 72 [11136/50000]\tLoss: 4.7545\tLR: 7.121995\n",
      "Training Epoch: 72 [11264/50000]\tLoss: 4.7019\tLR: 7.122251\n",
      "Training Epoch: 72 [11392/50000]\tLoss: 4.7639\tLR: 7.122506\n",
      "Training Epoch: 72 [11520/50000]\tLoss: 4.7583\tLR: 7.122762\n",
      "Training Epoch: 72 [11648/50000]\tLoss: 4.7223\tLR: 7.123018\n",
      "Training Epoch: 72 [11776/50000]\tLoss: 4.7715\tLR: 7.123274\n",
      "Training Epoch: 72 [11904/50000]\tLoss: 4.7500\tLR: 7.123529\n",
      "Training Epoch: 72 [12032/50000]\tLoss: 4.7702\tLR: 7.123785\n",
      "Training Epoch: 72 [12160/50000]\tLoss: 4.7576\tLR: 7.124041\n",
      "Training Epoch: 72 [12288/50000]\tLoss: 4.7307\tLR: 7.124297\n",
      "Training Epoch: 72 [12416/50000]\tLoss: 4.6875\tLR: 7.124552\n",
      "Training Epoch: 72 [12544/50000]\tLoss: 4.7297\tLR: 7.124808\n",
      "Training Epoch: 72 [12672/50000]\tLoss: 4.7127\tLR: 7.125064\n",
      "Training Epoch: 72 [12800/50000]\tLoss: 4.7523\tLR: 7.125320\n",
      "Training Epoch: 72 [12928/50000]\tLoss: 4.7507\tLR: 7.125575\n",
      "Training Epoch: 72 [13056/50000]\tLoss: 4.7349\tLR: 7.125831\n",
      "Training Epoch: 72 [13184/50000]\tLoss: 4.6892\tLR: 7.126087\n",
      "Training Epoch: 72 [13312/50000]\tLoss: 4.7217\tLR: 7.126343\n",
      "Training Epoch: 72 [13440/50000]\tLoss: 4.8014\tLR: 7.126598\n",
      "Training Epoch: 72 [13568/50000]\tLoss: 4.7188\tLR: 7.126854\n",
      "Training Epoch: 72 [13696/50000]\tLoss: 4.6856\tLR: 7.127110\n",
      "Training Epoch: 72 [13824/50000]\tLoss: 4.6940\tLR: 7.127366\n",
      "Training Epoch: 72 [13952/50000]\tLoss: 4.7040\tLR: 7.127621\n",
      "Training Epoch: 72 [14080/50000]\tLoss: 4.7549\tLR: 7.127877\n",
      "Training Epoch: 72 [14208/50000]\tLoss: 4.7281\tLR: 7.128133\n",
      "Training Epoch: 72 [14336/50000]\tLoss: 4.7175\tLR: 7.128389\n",
      "Training Epoch: 72 [14464/50000]\tLoss: 4.7590\tLR: 7.128645\n",
      "Training Epoch: 72 [14592/50000]\tLoss: 4.7964\tLR: 7.128900\n",
      "Training Epoch: 72 [14720/50000]\tLoss: 4.7905\tLR: 7.129156\n",
      "Training Epoch: 72 [14848/50000]\tLoss: 4.7010\tLR: 7.129412\n",
      "Training Epoch: 72 [14976/50000]\tLoss: 4.7482\tLR: 7.129668\n",
      "Training Epoch: 72 [15104/50000]\tLoss: 4.6199\tLR: 7.129923\n",
      "Training Epoch: 72 [15232/50000]\tLoss: 4.7447\tLR: 7.130179\n",
      "Training Epoch: 72 [15360/50000]\tLoss: 4.7443\tLR: 7.130435\n",
      "Training Epoch: 72 [15488/50000]\tLoss: 4.6367\tLR: 7.130691\n",
      "Training Epoch: 72 [15616/50000]\tLoss: 4.7563\tLR: 7.130946\n",
      "Training Epoch: 72 [15744/50000]\tLoss: 4.7432\tLR: 7.131202\n",
      "Training Epoch: 72 [15872/50000]\tLoss: 4.6772\tLR: 7.131458\n",
      "Training Epoch: 72 [16000/50000]\tLoss: 4.8311\tLR: 7.131714\n",
      "Training Epoch: 72 [16128/50000]\tLoss: 4.7058\tLR: 7.131969\n",
      "Training Epoch: 72 [16256/50000]\tLoss: 4.7384\tLR: 7.132225\n",
      "Training Epoch: 72 [16384/50000]\tLoss: 4.7045\tLR: 7.132481\n",
      "Training Epoch: 72 [16512/50000]\tLoss: 4.6771\tLR: 7.132737\n",
      "Training Epoch: 72 [16640/50000]\tLoss: 4.7650\tLR: 7.132992\n",
      "Training Epoch: 72 [16768/50000]\tLoss: 4.7264\tLR: 7.133248\n",
      "Training Epoch: 72 [16896/50000]\tLoss: 4.7105\tLR: 7.133504\n",
      "Training Epoch: 72 [17024/50000]\tLoss: 4.7539\tLR: 7.133760\n",
      "Training Epoch: 72 [17152/50000]\tLoss: 4.6887\tLR: 7.134015\n",
      "Training Epoch: 72 [17280/50000]\tLoss: 4.7805\tLR: 7.134271\n",
      "Training Epoch: 72 [17408/50000]\tLoss: 4.7564\tLR: 7.134527\n",
      "Training Epoch: 72 [17536/50000]\tLoss: 4.7319\tLR: 7.134783\n",
      "Training Epoch: 72 [17664/50000]\tLoss: 4.8448\tLR: 7.135038\n",
      "Training Epoch: 72 [17792/50000]\tLoss: 4.7356\tLR: 7.135294\n",
      "Training Epoch: 72 [17920/50000]\tLoss: 4.8060\tLR: 7.135550\n",
      "Training Epoch: 72 [18048/50000]\tLoss: 4.8103\tLR: 7.135806\n",
      "Training Epoch: 72 [18176/50000]\tLoss: 4.7163\tLR: 7.136061\n",
      "Training Epoch: 72 [18304/50000]\tLoss: 4.7866\tLR: 7.136317\n",
      "Training Epoch: 72 [18432/50000]\tLoss: 4.6740\tLR: 7.136573\n",
      "Training Epoch: 72 [18560/50000]\tLoss: 4.6702\tLR: 7.136829\n",
      "Training Epoch: 72 [18688/50000]\tLoss: 4.7227\tLR: 7.137084\n",
      "Training Epoch: 72 [18816/50000]\tLoss: 4.6343\tLR: 7.137340\n",
      "Training Epoch: 72 [18944/50000]\tLoss: 4.8290\tLR: 7.137596\n",
      "Training Epoch: 72 [19072/50000]\tLoss: 4.7583\tLR: 7.137852\n",
      "Training Epoch: 72 [19200/50000]\tLoss: 4.7593\tLR: 7.138107\n",
      "Training Epoch: 72 [19328/50000]\tLoss: 4.7622\tLR: 7.138363\n",
      "Training Epoch: 72 [19456/50000]\tLoss: 4.6854\tLR: 7.138619\n",
      "Training Epoch: 72 [19584/50000]\tLoss: 4.7759\tLR: 7.138875\n",
      "Training Epoch: 72 [19712/50000]\tLoss: 4.6829\tLR: 7.139130\n",
      "Training Epoch: 72 [19840/50000]\tLoss: 4.7013\tLR: 7.139386\n",
      "Training Epoch: 72 [19968/50000]\tLoss: 4.7152\tLR: 7.139642\n",
      "Training Epoch: 72 [20096/50000]\tLoss: 4.7452\tLR: 7.139898\n",
      "Training Epoch: 72 [20224/50000]\tLoss: 4.6821\tLR: 7.140153\n",
      "Training Epoch: 72 [20352/50000]\tLoss: 4.7227\tLR: 7.140409\n",
      "Training Epoch: 72 [20480/50000]\tLoss: 4.7533\tLR: 7.140665\n",
      "Training Epoch: 72 [20608/50000]\tLoss: 4.7387\tLR: 7.140921\n",
      "Training Epoch: 72 [20736/50000]\tLoss: 4.7070\tLR: 7.141176\n",
      "Training Epoch: 72 [20864/50000]\tLoss: 4.7339\tLR: 7.141432\n",
      "Training Epoch: 72 [20992/50000]\tLoss: 4.6807\tLR: 7.141688\n",
      "Training Epoch: 72 [21120/50000]\tLoss: 4.6970\tLR: 7.141944\n",
      "Training Epoch: 72 [21248/50000]\tLoss: 4.7097\tLR: 7.142199\n",
      "Training Epoch: 72 [21376/50000]\tLoss: 4.7448\tLR: 7.142455\n",
      "Training Epoch: 72 [21504/50000]\tLoss: 4.6398\tLR: 7.142711\n",
      "Training Epoch: 72 [21632/50000]\tLoss: 4.6696\tLR: 7.142967\n",
      "Training Epoch: 72 [21760/50000]\tLoss: 4.8092\tLR: 7.143223\n",
      "Training Epoch: 72 [21888/50000]\tLoss: 4.7242\tLR: 7.143478\n",
      "Training Epoch: 72 [22016/50000]\tLoss: 4.7390\tLR: 7.143734\n",
      "Training Epoch: 72 [22144/50000]\tLoss: 4.6987\tLR: 7.143990\n",
      "Training Epoch: 72 [22272/50000]\tLoss: 4.7391\tLR: 7.144246\n",
      "Training Epoch: 72 [22400/50000]\tLoss: 4.7019\tLR: 7.144501\n",
      "Training Epoch: 72 [22528/50000]\tLoss: 4.7727\tLR: 7.144757\n",
      "Training Epoch: 72 [22656/50000]\tLoss: 4.7051\tLR: 7.145013\n",
      "Training Epoch: 72 [22784/50000]\tLoss: 4.6774\tLR: 7.145269\n",
      "Training Epoch: 72 [22912/50000]\tLoss: 4.6975\tLR: 7.145524\n",
      "Training Epoch: 72 [23040/50000]\tLoss: 4.7013\tLR: 7.145780\n",
      "Training Epoch: 72 [23168/50000]\tLoss: 4.6526\tLR: 7.146036\n",
      "Training Epoch: 72 [23296/50000]\tLoss: 4.7278\tLR: 7.146292\n",
      "Training Epoch: 72 [23424/50000]\tLoss: 4.7539\tLR: 7.146547\n",
      "Training Epoch: 72 [23552/50000]\tLoss: 4.7689\tLR: 7.146803\n",
      "Training Epoch: 72 [23680/50000]\tLoss: 4.7489\tLR: 7.147059\n",
      "Training Epoch: 72 [23808/50000]\tLoss: 4.7870\tLR: 7.147315\n",
      "Training Epoch: 72 [23936/50000]\tLoss: 4.7439\tLR: 7.147570\n",
      "Training Epoch: 72 [24064/50000]\tLoss: 4.7433\tLR: 7.147826\n",
      "Training Epoch: 72 [24192/50000]\tLoss: 4.6224\tLR: 7.148082\n",
      "Training Epoch: 72 [24320/50000]\tLoss: 4.7785\tLR: 7.148338\n",
      "Training Epoch: 72 [24448/50000]\tLoss: 4.6803\tLR: 7.148593\n",
      "Training Epoch: 72 [24576/50000]\tLoss: 4.7147\tLR: 7.148849\n",
      "Training Epoch: 72 [24704/50000]\tLoss: 4.7793\tLR: 7.149105\n",
      "Training Epoch: 72 [24832/50000]\tLoss: 4.7299\tLR: 7.149361\n",
      "Training Epoch: 72 [24960/50000]\tLoss: 4.6367\tLR: 7.149616\n",
      "Training Epoch: 72 [25088/50000]\tLoss: 4.7438\tLR: 7.149872\n",
      "Training Epoch: 72 [25216/50000]\tLoss: 4.7138\tLR: 7.150128\n",
      "Training Epoch: 72 [25344/50000]\tLoss: 4.6962\tLR: 7.150384\n",
      "Training Epoch: 72 [25472/50000]\tLoss: 4.6936\tLR: 7.150639\n",
      "Training Epoch: 72 [25600/50000]\tLoss: 4.7233\tLR: 7.150895\n",
      "Training Epoch: 72 [25728/50000]\tLoss: 4.8042\tLR: 7.151151\n",
      "Training Epoch: 72 [25856/50000]\tLoss: 4.7245\tLR: 7.151407\n",
      "Training Epoch: 72 [25984/50000]\tLoss: 4.7148\tLR: 7.151662\n",
      "Training Epoch: 72 [26112/50000]\tLoss: 4.7210\tLR: 7.151918\n",
      "Training Epoch: 72 [26240/50000]\tLoss: 4.7215\tLR: 7.152174\n",
      "Training Epoch: 72 [26368/50000]\tLoss: 4.6693\tLR: 7.152430\n",
      "Training Epoch: 72 [26496/50000]\tLoss: 4.7441\tLR: 7.152685\n",
      "Training Epoch: 72 [26624/50000]\tLoss: 4.7604\tLR: 7.152941\n",
      "Training Epoch: 72 [26752/50000]\tLoss: 4.7599\tLR: 7.153197\n",
      "Training Epoch: 72 [26880/50000]\tLoss: 4.7559\tLR: 7.153453\n",
      "Training Epoch: 72 [27008/50000]\tLoss: 4.7512\tLR: 7.153708\n",
      "Training Epoch: 72 [27136/50000]\tLoss: 4.7672\tLR: 7.153964\n",
      "Training Epoch: 72 [27264/50000]\tLoss: 4.6882\tLR: 7.154220\n",
      "Training Epoch: 72 [27392/50000]\tLoss: 4.7408\tLR: 7.154476\n",
      "Training Epoch: 72 [27520/50000]\tLoss: 4.7105\tLR: 7.154731\n",
      "Training Epoch: 72 [27648/50000]\tLoss: 4.7674\tLR: 7.154987\n",
      "Training Epoch: 72 [27776/50000]\tLoss: 4.7565\tLR: 7.155243\n",
      "Training Epoch: 72 [27904/50000]\tLoss: 4.6988\tLR: 7.155499\n",
      "Training Epoch: 72 [28032/50000]\tLoss: 4.7630\tLR: 7.155754\n",
      "Training Epoch: 72 [28160/50000]\tLoss: 4.7376\tLR: 7.156010\n",
      "Training Epoch: 72 [28288/50000]\tLoss: 4.7481\tLR: 7.156266\n",
      "Training Epoch: 72 [28416/50000]\tLoss: 4.8156\tLR: 7.156522\n",
      "Training Epoch: 72 [28544/50000]\tLoss: 4.6147\tLR: 7.156777\n",
      "Training Epoch: 72 [28672/50000]\tLoss: 4.7546\tLR: 7.157033\n",
      "Training Epoch: 72 [28800/50000]\tLoss: 4.7957\tLR: 7.157289\n",
      "Training Epoch: 72 [28928/50000]\tLoss: 4.7624\tLR: 7.157545\n",
      "Training Epoch: 72 [29056/50000]\tLoss: 4.7710\tLR: 7.157801\n",
      "Training Epoch: 72 [29184/50000]\tLoss: 4.7046\tLR: 7.158056\n",
      "Training Epoch: 72 [29312/50000]\tLoss: 4.7367\tLR: 7.158312\n",
      "Training Epoch: 72 [29440/50000]\tLoss: 4.7543\tLR: 7.158568\n",
      "Training Epoch: 72 [29568/50000]\tLoss: 4.7360\tLR: 7.158824\n",
      "Training Epoch: 72 [29696/50000]\tLoss: 4.7479\tLR: 7.159079\n",
      "Training Epoch: 72 [29824/50000]\tLoss: 4.8139\tLR: 7.159335\n",
      "Training Epoch: 72 [29952/50000]\tLoss: 4.7749\tLR: 7.159591\n",
      "Training Epoch: 72 [30080/50000]\tLoss: 4.7325\tLR: 7.159847\n",
      "Training Epoch: 72 [30208/50000]\tLoss: 4.7260\tLR: 7.160102\n",
      "Training Epoch: 72 [30336/50000]\tLoss: 4.7035\tLR: 7.160358\n",
      "Training Epoch: 72 [30464/50000]\tLoss: 4.7172\tLR: 7.160614\n",
      "Training Epoch: 72 [30592/50000]\tLoss: 4.8426\tLR: 7.160870\n",
      "Training Epoch: 72 [30720/50000]\tLoss: 4.6888\tLR: 7.161125\n",
      "Training Epoch: 72 [30848/50000]\tLoss: 4.7684\tLR: 7.161381\n",
      "Training Epoch: 72 [30976/50000]\tLoss: 4.8418\tLR: 7.161637\n",
      "Training Epoch: 72 [31104/50000]\tLoss: 4.7227\tLR: 7.161893\n",
      "Training Epoch: 72 [31232/50000]\tLoss: 4.7315\tLR: 7.162148\n",
      "Training Epoch: 72 [31360/50000]\tLoss: 4.7697\tLR: 7.162404\n",
      "Training Epoch: 72 [31488/50000]\tLoss: 4.7612\tLR: 7.162660\n",
      "Training Epoch: 72 [31616/50000]\tLoss: 4.7489\tLR: 7.162916\n",
      "Training Epoch: 72 [31744/50000]\tLoss: 4.8418\tLR: 7.163171\n",
      "Training Epoch: 72 [31872/50000]\tLoss: 4.7175\tLR: 7.163427\n",
      "Training Epoch: 72 [32000/50000]\tLoss: 4.7167\tLR: 7.163683\n",
      "Training Epoch: 72 [32128/50000]\tLoss: 4.7629\tLR: 7.163939\n",
      "Training Epoch: 72 [32256/50000]\tLoss: 4.6506\tLR: 7.164194\n",
      "Training Epoch: 72 [32384/50000]\tLoss: 4.7537\tLR: 7.164450\n",
      "Training Epoch: 72 [32512/50000]\tLoss: 4.6151\tLR: 7.164706\n",
      "Training Epoch: 72 [32640/50000]\tLoss: 4.7539\tLR: 7.164962\n",
      "Training Epoch: 72 [32768/50000]\tLoss: 4.7586\tLR: 7.165217\n",
      "Training Epoch: 72 [32896/50000]\tLoss: 4.8036\tLR: 7.165473\n",
      "Training Epoch: 72 [33024/50000]\tLoss: 4.7715\tLR: 7.165729\n",
      "Training Epoch: 72 [33152/50000]\tLoss: 4.7866\tLR: 7.165985\n",
      "Training Epoch: 72 [33280/50000]\tLoss: 4.6683\tLR: 7.166240\n",
      "Training Epoch: 72 [33408/50000]\tLoss: 4.8379\tLR: 7.166496\n",
      "Training Epoch: 72 [33536/50000]\tLoss: 4.7336\tLR: 7.166752\n",
      "Training Epoch: 72 [33664/50000]\tLoss: 4.7930\tLR: 7.167008\n",
      "Training Epoch: 72 [33792/50000]\tLoss: 4.6987\tLR: 7.167263\n",
      "Training Epoch: 72 [33920/50000]\tLoss: 4.6924\tLR: 7.167519\n",
      "Training Epoch: 72 [34048/50000]\tLoss: 4.6601\tLR: 7.167775\n",
      "Training Epoch: 72 [34176/50000]\tLoss: 4.7291\tLR: 7.168031\n",
      "Training Epoch: 72 [34304/50000]\tLoss: 4.7543\tLR: 7.168286\n",
      "Training Epoch: 72 [34432/50000]\tLoss: 4.7804\tLR: 7.168542\n",
      "Training Epoch: 72 [34560/50000]\tLoss: 4.7612\tLR: 7.168798\n",
      "Training Epoch: 72 [34688/50000]\tLoss: 4.8169\tLR: 7.169054\n",
      "Training Epoch: 72 [34816/50000]\tLoss: 4.7707\tLR: 7.169309\n",
      "Training Epoch: 72 [34944/50000]\tLoss: 4.6376\tLR: 7.169565\n",
      "Training Epoch: 72 [35072/50000]\tLoss: 4.7498\tLR: 7.169821\n",
      "Training Epoch: 72 [35200/50000]\tLoss: 4.6833\tLR: 7.170077\n",
      "Training Epoch: 72 [35328/50000]\tLoss: 4.7601\tLR: 7.170332\n",
      "Training Epoch: 72 [35456/50000]\tLoss: 4.7997\tLR: 7.170588\n",
      "Training Epoch: 72 [35584/50000]\tLoss: 4.7911\tLR: 7.170844\n",
      "Training Epoch: 72 [35712/50000]\tLoss: 4.6990\tLR: 7.171100\n",
      "Training Epoch: 72 [35840/50000]\tLoss: 4.7752\tLR: 7.171355\n",
      "Training Epoch: 72 [35968/50000]\tLoss: 4.7646\tLR: 7.171611\n",
      "Training Epoch: 72 [36096/50000]\tLoss: 4.7093\tLR: 7.171867\n",
      "Training Epoch: 72 [36224/50000]\tLoss: 4.7647\tLR: 7.172123\n",
      "Training Epoch: 72 [36352/50000]\tLoss: 4.7557\tLR: 7.172379\n",
      "Training Epoch: 72 [36480/50000]\tLoss: 4.7017\tLR: 7.172634\n",
      "Training Epoch: 72 [36608/50000]\tLoss: 4.7426\tLR: 7.172890\n",
      "Training Epoch: 72 [36736/50000]\tLoss: 4.7250\tLR: 7.173146\n",
      "Training Epoch: 72 [36864/50000]\tLoss: 4.7824\tLR: 7.173402\n",
      "Training Epoch: 72 [36992/50000]\tLoss: 4.6770\tLR: 7.173657\n",
      "Training Epoch: 72 [37120/50000]\tLoss: 4.7414\tLR: 7.173913\n",
      "Training Epoch: 72 [37248/50000]\tLoss: 4.7704\tLR: 7.174169\n",
      "Training Epoch: 72 [37376/50000]\tLoss: 4.8144\tLR: 7.174425\n",
      "Training Epoch: 72 [37504/50000]\tLoss: 4.7383\tLR: 7.174680\n",
      "Training Epoch: 72 [37632/50000]\tLoss: 4.7097\tLR: 7.174936\n",
      "Training Epoch: 72 [37760/50000]\tLoss: 4.8005\tLR: 7.175192\n",
      "Training Epoch: 72 [37888/50000]\tLoss: 4.7099\tLR: 7.175448\n",
      "Training Epoch: 72 [38016/50000]\tLoss: 4.7981\tLR: 7.175703\n",
      "Training Epoch: 72 [38144/50000]\tLoss: 4.7710\tLR: 7.175959\n",
      "Training Epoch: 72 [38272/50000]\tLoss: 4.8358\tLR: 7.176215\n",
      "Training Epoch: 72 [38400/50000]\tLoss: 4.6263\tLR: 7.176471\n",
      "Training Epoch: 72 [38528/50000]\tLoss: 4.7845\tLR: 7.176726\n",
      "Training Epoch: 72 [38656/50000]\tLoss: 4.7620\tLR: 7.176982\n",
      "Training Epoch: 72 [38784/50000]\tLoss: 4.8153\tLR: 7.177238\n",
      "Training Epoch: 72 [38912/50000]\tLoss: 4.8248\tLR: 7.177494\n",
      "Training Epoch: 72 [39040/50000]\tLoss: 4.7067\tLR: 7.177749\n",
      "Training Epoch: 72 [39168/50000]\tLoss: 4.7500\tLR: 7.178005\n",
      "Training Epoch: 72 [39296/50000]\tLoss: 4.7768\tLR: 7.178261\n",
      "Training Epoch: 72 [39424/50000]\tLoss: 4.7327\tLR: 7.178517\n",
      "Training Epoch: 72 [39552/50000]\tLoss: 4.6600\tLR: 7.178772\n",
      "Training Epoch: 72 [39680/50000]\tLoss: 4.8156\tLR: 7.179028\n",
      "Training Epoch: 72 [39808/50000]\tLoss: 4.7639\tLR: 7.179284\n",
      "Training Epoch: 72 [39936/50000]\tLoss: 4.7532\tLR: 7.179540\n",
      "Training Epoch: 72 [40064/50000]\tLoss: 4.7686\tLR: 7.179795\n",
      "Training Epoch: 72 [40192/50000]\tLoss: 4.7110\tLR: 7.180051\n",
      "Training Epoch: 72 [40320/50000]\tLoss: 4.7534\tLR: 7.180307\n",
      "Training Epoch: 72 [40448/50000]\tLoss: 4.8312\tLR: 7.180563\n",
      "Training Epoch: 72 [40576/50000]\tLoss: 4.7593\tLR: 7.180818\n",
      "Training Epoch: 72 [40704/50000]\tLoss: 4.7024\tLR: 7.181074\n",
      "Training Epoch: 72 [40832/50000]\tLoss: 4.7769\tLR: 7.181330\n",
      "Training Epoch: 72 [40960/50000]\tLoss: 4.7583\tLR: 7.181586\n",
      "Training Epoch: 72 [41088/50000]\tLoss: 4.7942\tLR: 7.181841\n",
      "Training Epoch: 72 [41216/50000]\tLoss: 4.7346\tLR: 7.182097\n",
      "Training Epoch: 72 [41344/50000]\tLoss: 4.6736\tLR: 7.182353\n",
      "Training Epoch: 72 [41472/50000]\tLoss: 4.7158\tLR: 7.182609\n",
      "Training Epoch: 72 [41600/50000]\tLoss: 4.7402\tLR: 7.182864\n",
      "Training Epoch: 72 [41728/50000]\tLoss: 4.6659\tLR: 7.183120\n",
      "Training Epoch: 72 [41856/50000]\tLoss: 4.8056\tLR: 7.183376\n",
      "Training Epoch: 72 [41984/50000]\tLoss: 4.7383\tLR: 7.183632\n",
      "Training Epoch: 72 [42112/50000]\tLoss: 4.7775\tLR: 7.183887\n",
      "Training Epoch: 72 [42240/50000]\tLoss: 4.7699\tLR: 7.184143\n",
      "Training Epoch: 72 [42368/50000]\tLoss: 4.7813\tLR: 7.184399\n",
      "Training Epoch: 72 [42496/50000]\tLoss: 4.7633\tLR: 7.184655\n",
      "Training Epoch: 72 [42624/50000]\tLoss: 4.7758\tLR: 7.184910\n",
      "Training Epoch: 72 [42752/50000]\tLoss: 4.7177\tLR: 7.185166\n",
      "Training Epoch: 72 [42880/50000]\tLoss: 4.6813\tLR: 7.185422\n",
      "Training Epoch: 72 [43008/50000]\tLoss: 4.7664\tLR: 7.185678\n",
      "Training Epoch: 72 [43136/50000]\tLoss: 4.7539\tLR: 7.185934\n",
      "Training Epoch: 72 [43264/50000]\tLoss: 4.7516\tLR: 7.186189\n",
      "Training Epoch: 72 [43392/50000]\tLoss: 4.7900\tLR: 7.186445\n",
      "Training Epoch: 72 [43520/50000]\tLoss: 4.7125\tLR: 7.186701\n",
      "Training Epoch: 72 [43648/50000]\tLoss: 4.6864\tLR: 7.186957\n",
      "Training Epoch: 72 [43776/50000]\tLoss: 4.6807\tLR: 7.187212\n",
      "Training Epoch: 72 [43904/50000]\tLoss: 4.8170\tLR: 7.187468\n",
      "Training Epoch: 72 [44032/50000]\tLoss: 4.7006\tLR: 7.187724\n",
      "Training Epoch: 72 [44160/50000]\tLoss: 4.7734\tLR: 7.187980\n",
      "Training Epoch: 72 [44288/50000]\tLoss: 4.6686\tLR: 7.188235\n",
      "Training Epoch: 72 [44416/50000]\tLoss: 4.7531\tLR: 7.188491\n",
      "Training Epoch: 72 [44544/50000]\tLoss: 4.7873\tLR: 7.188747\n",
      "Training Epoch: 72 [44672/50000]\tLoss: 4.7905\tLR: 7.189003\n",
      "Training Epoch: 72 [44800/50000]\tLoss: 4.7387\tLR: 7.189258\n",
      "Training Epoch: 72 [44928/50000]\tLoss: 4.7653\tLR: 7.189514\n",
      "Training Epoch: 72 [45056/50000]\tLoss: 4.6710\tLR: 7.189770\n",
      "Training Epoch: 72 [45184/50000]\tLoss: 4.7216\tLR: 7.190026\n",
      "Training Epoch: 72 [45312/50000]\tLoss: 4.6821\tLR: 7.190281\n",
      "Training Epoch: 72 [45440/50000]\tLoss: 4.7595\tLR: 7.190537\n",
      "Training Epoch: 72 [45568/50000]\tLoss: 4.7984\tLR: 7.190793\n",
      "Training Epoch: 72 [45696/50000]\tLoss: 4.7155\tLR: 7.191049\n",
      "Training Epoch: 72 [45824/50000]\tLoss: 4.6407\tLR: 7.191304\n",
      "Training Epoch: 72 [45952/50000]\tLoss: 4.8213\tLR: 7.191560\n",
      "Training Epoch: 72 [46080/50000]\tLoss: 4.7234\tLR: 7.191816\n",
      "Training Epoch: 72 [46208/50000]\tLoss: 4.6771\tLR: 7.192072\n",
      "Training Epoch: 72 [46336/50000]\tLoss: 4.7594\tLR: 7.192327\n",
      "Training Epoch: 72 [46464/50000]\tLoss: 4.7489\tLR: 7.192583\n",
      "Training Epoch: 72 [46592/50000]\tLoss: 4.8147\tLR: 7.192839\n",
      "Training Epoch: 72 [46720/50000]\tLoss: 4.7806\tLR: 7.193095\n",
      "Training Epoch: 72 [46848/50000]\tLoss: 4.7039\tLR: 7.193350\n",
      "Training Epoch: 72 [46976/50000]\tLoss: 4.8094\tLR: 7.193606\n",
      "Training Epoch: 72 [47104/50000]\tLoss: 4.7334\tLR: 7.193862\n",
      "Training Epoch: 72 [47232/50000]\tLoss: 4.6785\tLR: 7.194118\n",
      "Training Epoch: 72 [47360/50000]\tLoss: 4.7928\tLR: 7.194373\n",
      "Training Epoch: 72 [47488/50000]\tLoss: 4.6411\tLR: 7.194629\n",
      "Training Epoch: 72 [47616/50000]\tLoss: 4.7921\tLR: 7.194885\n",
      "Training Epoch: 72 [47744/50000]\tLoss: 4.6737\tLR: 7.195141\n",
      "Training Epoch: 72 [47872/50000]\tLoss: 4.6636\tLR: 7.195396\n",
      "Training Epoch: 72 [48000/50000]\tLoss: 4.7480\tLR: 7.195652\n",
      "Training Epoch: 72 [48128/50000]\tLoss: 4.7353\tLR: 7.195908\n",
      "Training Epoch: 72 [48256/50000]\tLoss: 4.8483\tLR: 7.196164\n",
      "Training Epoch: 72 [48384/50000]\tLoss: 4.7461\tLR: 7.196419\n",
      "Training Epoch: 72 [48512/50000]\tLoss: 4.7461\tLR: 7.196675\n",
      "Training Epoch: 72 [48640/50000]\tLoss: 4.7404\tLR: 7.196931\n",
      "Training Epoch: 72 [48768/50000]\tLoss: 4.7521\tLR: 7.197187\n",
      "Training Epoch: 72 [48896/50000]\tLoss: 4.6601\tLR: 7.197442\n",
      "Training Epoch: 72 [49024/50000]\tLoss: 4.7165\tLR: 7.197698\n",
      "Training Epoch: 72 [49152/50000]\tLoss: 4.7380\tLR: 7.197954\n",
      "Training Epoch: 72 [49280/50000]\tLoss: 4.7331\tLR: 7.198210\n",
      "Training Epoch: 72 [49408/50000]\tLoss: 4.7573\tLR: 7.198465\n",
      "Training Epoch: 72 [49536/50000]\tLoss: 4.7811\tLR: 7.198721\n",
      "Training Epoch: 72 [49664/50000]\tLoss: 4.6518\tLR: 7.198977\n",
      "Training Epoch: 72 [49792/50000]\tLoss: 4.7348\tLR: 7.199233\n",
      "Training Epoch: 72 [49920/50000]\tLoss: 4.7384\tLR: 7.199488\n",
      "Training Epoch: 72 [50000/50000]\tLoss: 4.7165\tLR: 7.199744\n",
      "epoch 72 training time consumed: 488.91s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  100939 GB |  100939 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  100629 GB |  100629 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     309 GB |     309 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  100939 GB |  100939 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  100629 GB |  100629 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     309 GB |     309 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |   99520 GB |   99520 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |   99210 GB |   99210 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     309 GB |     309 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   10703 K  |   10702 K  |\n",
      "|       from large pool |      24    |      65    |    4562 K  |    4562 K  |\n",
      "|       from small pool |     231    |     274    |    6140 K  |    6140 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   10703 K  |   10702 K  |\n",
      "|       from large pool |      24    |      65    |    4562 K  |    4562 K  |\n",
      "|       from small pool |     231    |     274    |    6140 K  |    6140 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      34    |      47    |    6203 K  |    6203 K  |\n",
      "|       from large pool |      10    |      23    |    2193 K  |    2193 K  |\n",
      "|       from small pool |      24    |      35    |    4010 K  |    4010 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 72, Average loss: 0.0374, Accuracy: 0.0100, Time consumed:31.29s\n",
      "\n",
      "Training Epoch: 73 [128/50000]\tLoss: 4.7262\tLR: 0.020000\n",
      "Training Epoch: 73 [256/50000]\tLoss: 4.7891\tLR: 7.200256\n",
      "Training Epoch: 73 [384/50000]\tLoss: 4.7905\tLR: 7.200512\n",
      "Training Epoch: 73 [512/50000]\tLoss: 4.6094\tLR: 7.200767\n",
      "Training Epoch: 73 [640/50000]\tLoss: 4.6749\tLR: 7.201023\n",
      "Training Epoch: 73 [768/50000]\tLoss: 4.6749\tLR: 7.201279\n",
      "Training Epoch: 73 [896/50000]\tLoss: 4.7575\tLR: 7.201535\n",
      "Training Epoch: 73 [1024/50000]\tLoss: 4.8045\tLR: 7.201790\n",
      "Training Epoch: 73 [1152/50000]\tLoss: 4.7389\tLR: 7.202046\n",
      "Training Epoch: 73 [1280/50000]\tLoss: 4.7894\tLR: 7.202302\n",
      "Training Epoch: 73 [1408/50000]\tLoss: 4.7550\tLR: 7.202558\n",
      "Training Epoch: 73 [1536/50000]\tLoss: 4.7169\tLR: 7.202813\n",
      "Training Epoch: 73 [1664/50000]\tLoss: 4.7085\tLR: 7.203069\n",
      "Training Epoch: 73 [1792/50000]\tLoss: 4.7118\tLR: 7.203325\n",
      "Training Epoch: 73 [1920/50000]\tLoss: 4.6555\tLR: 7.203581\n",
      "Training Epoch: 73 [2048/50000]\tLoss: 4.6984\tLR: 7.203836\n",
      "Training Epoch: 73 [2176/50000]\tLoss: 4.6888\tLR: 7.204092\n",
      "Training Epoch: 73 [2304/50000]\tLoss: 4.6709\tLR: 7.204348\n",
      "Training Epoch: 73 [2432/50000]\tLoss: 4.7633\tLR: 7.204604\n",
      "Training Epoch: 73 [2560/50000]\tLoss: 4.7171\tLR: 7.204859\n",
      "Training Epoch: 73 [2688/50000]\tLoss: 4.7371\tLR: 7.205115\n",
      "Training Epoch: 73 [2816/50000]\tLoss: 4.7589\tLR: 7.205371\n",
      "Training Epoch: 73 [2944/50000]\tLoss: 4.7418\tLR: 7.205627\n",
      "Training Epoch: 73 [3072/50000]\tLoss: 4.7674\tLR: 7.205882\n",
      "Training Epoch: 73 [3200/50000]\tLoss: 4.6861\tLR: 7.206138\n",
      "Training Epoch: 73 [3328/50000]\tLoss: 4.6338\tLR: 7.206394\n",
      "Training Epoch: 73 [3456/50000]\tLoss: 4.8108\tLR: 7.206650\n",
      "Training Epoch: 73 [3584/50000]\tLoss: 4.6757\tLR: 7.206905\n",
      "Training Epoch: 73 [3712/50000]\tLoss: 4.7814\tLR: 7.207161\n",
      "Training Epoch: 73 [3840/50000]\tLoss: 4.6820\tLR: 7.207417\n",
      "Training Epoch: 73 [3968/50000]\tLoss: 4.7083\tLR: 7.207673\n",
      "Training Epoch: 73 [4096/50000]\tLoss: 4.7489\tLR: 7.207928\n",
      "Training Epoch: 73 [4224/50000]\tLoss: 4.7912\tLR: 7.208184\n",
      "Training Epoch: 73 [4352/50000]\tLoss: 4.7884\tLR: 7.208440\n",
      "Training Epoch: 73 [4480/50000]\tLoss: 4.8666\tLR: 7.208696\n",
      "Training Epoch: 73 [4608/50000]\tLoss: 4.7776\tLR: 7.208951\n",
      "Training Epoch: 73 [4736/50000]\tLoss: 4.6965\tLR: 7.209207\n",
      "Training Epoch: 73 [4864/50000]\tLoss: 4.6809\tLR: 7.209463\n",
      "Training Epoch: 73 [4992/50000]\tLoss: 4.7004\tLR: 7.209719\n",
      "Training Epoch: 73 [5120/50000]\tLoss: 4.7412\tLR: 7.209974\n",
      "Training Epoch: 73 [5248/50000]\tLoss: 4.7042\tLR: 7.210230\n",
      "Training Epoch: 73 [5376/50000]\tLoss: 4.6697\tLR: 7.210486\n",
      "Training Epoch: 73 [5504/50000]\tLoss: 4.8120\tLR: 7.210742\n",
      "Training Epoch: 73 [5632/50000]\tLoss: 4.8266\tLR: 7.210997\n",
      "Training Epoch: 73 [5760/50000]\tLoss: 4.8078\tLR: 7.211253\n",
      "Training Epoch: 73 [5888/50000]\tLoss: 4.7473\tLR: 7.211509\n",
      "Training Epoch: 73 [6016/50000]\tLoss: 4.7834\tLR: 7.211765\n",
      "Training Epoch: 73 [6144/50000]\tLoss: 4.7774\tLR: 7.212020\n",
      "Training Epoch: 73 [6272/50000]\tLoss: 4.7082\tLR: 7.212276\n",
      "Training Epoch: 73 [6400/50000]\tLoss: 4.7147\tLR: 7.212532\n",
      "Training Epoch: 73 [6528/50000]\tLoss: 4.8012\tLR: 7.212788\n",
      "Training Epoch: 73 [6656/50000]\tLoss: 4.6325\tLR: 7.213043\n",
      "Training Epoch: 73 [6784/50000]\tLoss: 4.7314\tLR: 7.213299\n",
      "Training Epoch: 73 [6912/50000]\tLoss: 4.7347\tLR: 7.213555\n",
      "Training Epoch: 73 [7040/50000]\tLoss: 4.7622\tLR: 7.213811\n",
      "Training Epoch: 73 [7168/50000]\tLoss: 4.8179\tLR: 7.214066\n",
      "Training Epoch: 73 [7296/50000]\tLoss: 4.7169\tLR: 7.214322\n",
      "Training Epoch: 73 [7424/50000]\tLoss: 4.7742\tLR: 7.214578\n",
      "Training Epoch: 73 [7552/50000]\tLoss: 4.7777\tLR: 7.214834\n",
      "Training Epoch: 73 [7680/50000]\tLoss: 4.7903\tLR: 7.215090\n",
      "Training Epoch: 73 [7808/50000]\tLoss: 4.7605\tLR: 7.215345\n",
      "Training Epoch: 73 [7936/50000]\tLoss: 4.6250\tLR: 7.215601\n",
      "Training Epoch: 73 [8064/50000]\tLoss: 4.7856\tLR: 7.215857\n",
      "Training Epoch: 73 [8192/50000]\tLoss: 4.8679\tLR: 7.216113\n",
      "Training Epoch: 73 [8320/50000]\tLoss: 4.7302\tLR: 7.216368\n",
      "Training Epoch: 73 [8448/50000]\tLoss: 4.6976\tLR: 7.216624\n",
      "Training Epoch: 73 [8576/50000]\tLoss: 4.7074\tLR: 7.216880\n",
      "Training Epoch: 73 [8704/50000]\tLoss: 4.7180\tLR: 7.217136\n",
      "Training Epoch: 73 [8832/50000]\tLoss: 4.7981\tLR: 7.217391\n",
      "Training Epoch: 73 [8960/50000]\tLoss: 4.7940\tLR: 7.217647\n",
      "Training Epoch: 73 [9088/50000]\tLoss: 4.7861\tLR: 7.217903\n",
      "Training Epoch: 73 [9216/50000]\tLoss: 4.8011\tLR: 7.218159\n",
      "Training Epoch: 73 [9344/50000]\tLoss: 4.7274\tLR: 7.218414\n",
      "Training Epoch: 73 [9472/50000]\tLoss: 4.7447\tLR: 7.218670\n",
      "Training Epoch: 73 [9600/50000]\tLoss: 4.6719\tLR: 7.218926\n",
      "Training Epoch: 73 [9728/50000]\tLoss: 4.8232\tLR: 7.219182\n",
      "Training Epoch: 73 [9856/50000]\tLoss: 4.7628\tLR: 7.219437\n",
      "Training Epoch: 73 [9984/50000]\tLoss: 4.7551\tLR: 7.219693\n",
      "Training Epoch: 73 [10112/50000]\tLoss: 4.7659\tLR: 7.219949\n",
      "Training Epoch: 73 [10240/50000]\tLoss: 4.7503\tLR: 7.220205\n",
      "Training Epoch: 73 [10368/50000]\tLoss: 4.7978\tLR: 7.220460\n",
      "Training Epoch: 73 [10496/50000]\tLoss: 4.7518\tLR: 7.220716\n",
      "Training Epoch: 73 [10624/50000]\tLoss: 4.7329\tLR: 7.220972\n",
      "Training Epoch: 73 [10752/50000]\tLoss: 4.6691\tLR: 7.221228\n",
      "Training Epoch: 73 [10880/50000]\tLoss: 4.7536\tLR: 7.221483\n",
      "Training Epoch: 73 [11008/50000]\tLoss: 4.8593\tLR: 7.221739\n",
      "Training Epoch: 73 [11136/50000]\tLoss: 4.7599\tLR: 7.221995\n",
      "Training Epoch: 73 [11264/50000]\tLoss: 4.7843\tLR: 7.222251\n",
      "Training Epoch: 73 [11392/50000]\tLoss: 4.6730\tLR: 7.222506\n",
      "Training Epoch: 73 [11520/50000]\tLoss: 4.6396\tLR: 7.222762\n",
      "Training Epoch: 73 [11648/50000]\tLoss: 4.7593\tLR: 7.223018\n",
      "Training Epoch: 73 [11776/50000]\tLoss: 4.7253\tLR: 7.223274\n",
      "Training Epoch: 73 [11904/50000]\tLoss: 4.6320\tLR: 7.223529\n",
      "Training Epoch: 73 [12032/50000]\tLoss: 4.6767\tLR: 7.223785\n",
      "Training Epoch: 73 [12160/50000]\tLoss: 4.7581\tLR: 7.224041\n",
      "Training Epoch: 73 [12288/50000]\tLoss: 4.8687\tLR: 7.224297\n",
      "Training Epoch: 73 [12416/50000]\tLoss: 4.7115\tLR: 7.224552\n",
      "Training Epoch: 73 [12544/50000]\tLoss: 4.9177\tLR: 7.224808\n",
      "Training Epoch: 73 [12672/50000]\tLoss: 4.7730\tLR: 7.225064\n",
      "Training Epoch: 73 [12800/50000]\tLoss: 4.7480\tLR: 7.225320\n",
      "Training Epoch: 73 [12928/50000]\tLoss: 4.8195\tLR: 7.225575\n",
      "Training Epoch: 73 [13056/50000]\tLoss: 4.7212\tLR: 7.225831\n",
      "Training Epoch: 73 [13184/50000]\tLoss: 4.7365\tLR: 7.226087\n",
      "Training Epoch: 73 [13312/50000]\tLoss: 4.8140\tLR: 7.226343\n",
      "Training Epoch: 73 [13440/50000]\tLoss: 4.7887\tLR: 7.226598\n",
      "Training Epoch: 73 [13568/50000]\tLoss: 4.8596\tLR: 7.226854\n",
      "Training Epoch: 73 [13696/50000]\tLoss: 4.7683\tLR: 7.227110\n",
      "Training Epoch: 73 [13824/50000]\tLoss: 4.7276\tLR: 7.227366\n",
      "Training Epoch: 73 [13952/50000]\tLoss: 4.7170\tLR: 7.227621\n",
      "Training Epoch: 73 [14080/50000]\tLoss: 4.7314\tLR: 7.227877\n",
      "Training Epoch: 73 [14208/50000]\tLoss: 4.6152\tLR: 7.228133\n",
      "Training Epoch: 73 [14336/50000]\tLoss: 4.7596\tLR: 7.228389\n",
      "Training Epoch: 73 [14464/50000]\tLoss: 4.7206\tLR: 7.228645\n",
      "Training Epoch: 73 [14592/50000]\tLoss: 4.7105\tLR: 7.228900\n",
      "Training Epoch: 73 [14720/50000]\tLoss: 4.7434\tLR: 7.229156\n",
      "Training Epoch: 73 [14848/50000]\tLoss: 4.7257\tLR: 7.229412\n",
      "Training Epoch: 73 [14976/50000]\tLoss: 4.7209\tLR: 7.229668\n",
      "Training Epoch: 73 [15104/50000]\tLoss: 4.7703\tLR: 7.229923\n",
      "Training Epoch: 73 [15232/50000]\tLoss: 4.8126\tLR: 7.230179\n",
      "Training Epoch: 73 [15360/50000]\tLoss: 4.7570\tLR: 7.230435\n",
      "Training Epoch: 73 [15488/50000]\tLoss: 4.7084\tLR: 7.230691\n",
      "Training Epoch: 73 [15616/50000]\tLoss: 4.6499\tLR: 7.230946\n",
      "Training Epoch: 73 [15744/50000]\tLoss: 4.6824\tLR: 7.231202\n",
      "Training Epoch: 73 [15872/50000]\tLoss: 4.7857\tLR: 7.231458\n",
      "Training Epoch: 73 [16000/50000]\tLoss: 4.7150\tLR: 7.231714\n",
      "Training Epoch: 73 [16128/50000]\tLoss: 4.7447\tLR: 7.231969\n",
      "Training Epoch: 73 [16256/50000]\tLoss: 4.7910\tLR: 7.232225\n",
      "Training Epoch: 73 [16384/50000]\tLoss: 4.7975\tLR: 7.232481\n",
      "Training Epoch: 73 [16512/50000]\tLoss: 4.7314\tLR: 7.232737\n",
      "Training Epoch: 73 [16640/50000]\tLoss: 4.8152\tLR: 7.232992\n",
      "Training Epoch: 73 [16768/50000]\tLoss: 4.6818\tLR: 7.233248\n",
      "Training Epoch: 73 [16896/50000]\tLoss: 4.7112\tLR: 7.233504\n",
      "Training Epoch: 73 [17024/50000]\tLoss: 4.7071\tLR: 7.233760\n",
      "Training Epoch: 73 [17152/50000]\tLoss: 4.6559\tLR: 7.234015\n",
      "Training Epoch: 73 [17280/50000]\tLoss: 4.6996\tLR: 7.234271\n",
      "Training Epoch: 73 [17408/50000]\tLoss: 4.7066\tLR: 7.234527\n",
      "Training Epoch: 73 [17536/50000]\tLoss: 4.6719\tLR: 7.234783\n",
      "Training Epoch: 73 [17664/50000]\tLoss: 4.7003\tLR: 7.235038\n",
      "Training Epoch: 73 [17792/50000]\tLoss: 4.7596\tLR: 7.235294\n",
      "Training Epoch: 73 [17920/50000]\tLoss: 4.7059\tLR: 7.235550\n",
      "Training Epoch: 73 [18048/50000]\tLoss: 4.7733\tLR: 7.235806\n",
      "Training Epoch: 73 [18176/50000]\tLoss: 4.7121\tLR: 7.236061\n",
      "Training Epoch: 73 [18304/50000]\tLoss: 4.8227\tLR: 7.236317\n",
      "Training Epoch: 73 [18432/50000]\tLoss: 4.7833\tLR: 7.236573\n",
      "Training Epoch: 73 [18560/50000]\tLoss: 4.8187\tLR: 7.236829\n",
      "Training Epoch: 73 [18688/50000]\tLoss: 4.7164\tLR: 7.237084\n",
      "Training Epoch: 73 [18816/50000]\tLoss: 4.7673\tLR: 7.237340\n",
      "Training Epoch: 73 [18944/50000]\tLoss: 4.7263\tLR: 7.237596\n",
      "Training Epoch: 73 [19072/50000]\tLoss: 4.7845\tLR: 7.237852\n",
      "Training Epoch: 73 [19200/50000]\tLoss: 4.7764\tLR: 7.238107\n",
      "Training Epoch: 73 [19328/50000]\tLoss: 4.7642\tLR: 7.238363\n",
      "Training Epoch: 73 [19456/50000]\tLoss: 4.7943\tLR: 7.238619\n",
      "Training Epoch: 73 [19584/50000]\tLoss: 4.8182\tLR: 7.238875\n",
      "Training Epoch: 73 [19712/50000]\tLoss: 4.7375\tLR: 7.239130\n",
      "Training Epoch: 73 [19840/50000]\tLoss: 4.8024\tLR: 7.239386\n",
      "Training Epoch: 73 [19968/50000]\tLoss: 4.7138\tLR: 7.239642\n",
      "Training Epoch: 73 [20096/50000]\tLoss: 4.7030\tLR: 7.239898\n",
      "Training Epoch: 73 [20224/50000]\tLoss: 4.6937\tLR: 7.240153\n",
      "Training Epoch: 73 [20352/50000]\tLoss: 4.7684\tLR: 7.240409\n",
      "Training Epoch: 73 [20480/50000]\tLoss: 4.8179\tLR: 7.240665\n",
      "Training Epoch: 73 [20608/50000]\tLoss: 4.7541\tLR: 7.240921\n",
      "Training Epoch: 73 [20736/50000]\tLoss: 4.8417\tLR: 7.241176\n",
      "Training Epoch: 73 [20864/50000]\tLoss: 4.7430\tLR: 7.241432\n",
      "Training Epoch: 73 [20992/50000]\tLoss: 4.6837\tLR: 7.241688\n",
      "Training Epoch: 73 [21120/50000]\tLoss: 4.7073\tLR: 7.241944\n",
      "Training Epoch: 73 [21248/50000]\tLoss: 4.7170\tLR: 7.242199\n",
      "Training Epoch: 73 [21376/50000]\tLoss: 4.7224\tLR: 7.242455\n",
      "Training Epoch: 73 [21504/50000]\tLoss: 4.7721\tLR: 7.242711\n",
      "Training Epoch: 73 [21632/50000]\tLoss: 4.7329\tLR: 7.242967\n",
      "Training Epoch: 73 [21760/50000]\tLoss: 4.7679\tLR: 7.243223\n",
      "Training Epoch: 73 [21888/50000]\tLoss: 4.7479\tLR: 7.243478\n",
      "Training Epoch: 73 [22016/50000]\tLoss: 4.7974\tLR: 7.243734\n",
      "Training Epoch: 73 [22144/50000]\tLoss: 4.6908\tLR: 7.243990\n",
      "Training Epoch: 73 [22272/50000]\tLoss: 4.7436\tLR: 7.244246\n",
      "Training Epoch: 73 [22400/50000]\tLoss: 4.7997\tLR: 7.244501\n",
      "Training Epoch: 73 [22528/50000]\tLoss: 4.8311\tLR: 7.244757\n",
      "Training Epoch: 73 [22656/50000]\tLoss: 4.7486\tLR: 7.245013\n",
      "Training Epoch: 73 [22784/50000]\tLoss: 4.6848\tLR: 7.245269\n",
      "Training Epoch: 73 [22912/50000]\tLoss: 4.6438\tLR: 7.245524\n",
      "Training Epoch: 73 [23040/50000]\tLoss: 4.7365\tLR: 7.245780\n",
      "Training Epoch: 73 [23168/50000]\tLoss: 4.7546\tLR: 7.246036\n",
      "Training Epoch: 73 [23296/50000]\tLoss: 4.6982\tLR: 7.246292\n",
      "Training Epoch: 73 [23424/50000]\tLoss: 4.6986\tLR: 7.246547\n",
      "Training Epoch: 73 [23552/50000]\tLoss: 4.7878\tLR: 7.246803\n",
      "Training Epoch: 73 [23680/50000]\tLoss: 4.6925\tLR: 7.247059\n",
      "Training Epoch: 73 [23808/50000]\tLoss: 4.7893\tLR: 7.247315\n",
      "Training Epoch: 73 [23936/50000]\tLoss: 4.7560\tLR: 7.247570\n",
      "Training Epoch: 73 [24064/50000]\tLoss: 4.7000\tLR: 7.247826\n",
      "Training Epoch: 73 [24192/50000]\tLoss: 4.7500\tLR: 7.248082\n",
      "Training Epoch: 73 [24320/50000]\tLoss: 4.6773\tLR: 7.248338\n",
      "Training Epoch: 73 [24448/50000]\tLoss: 4.7701\tLR: 7.248593\n",
      "Training Epoch: 73 [24576/50000]\tLoss: 4.7292\tLR: 7.248849\n",
      "Training Epoch: 73 [24704/50000]\tLoss: 4.7665\tLR: 7.249105\n",
      "Training Epoch: 73 [24832/50000]\tLoss: 4.7698\tLR: 7.249361\n",
      "Training Epoch: 73 [24960/50000]\tLoss: 4.7771\tLR: 7.249616\n",
      "Training Epoch: 73 [25088/50000]\tLoss: 4.7013\tLR: 7.249872\n",
      "Training Epoch: 73 [25216/50000]\tLoss: 4.6747\tLR: 7.250128\n",
      "Training Epoch: 73 [25344/50000]\tLoss: 4.8300\tLR: 7.250384\n",
      "Training Epoch: 73 [25472/50000]\tLoss: 4.7204\tLR: 7.250639\n",
      "Training Epoch: 73 [25600/50000]\tLoss: 4.6696\tLR: 7.250895\n",
      "Training Epoch: 73 [25728/50000]\tLoss: 4.7426\tLR: 7.251151\n",
      "Training Epoch: 73 [25856/50000]\tLoss: 4.7977\tLR: 7.251407\n",
      "Training Epoch: 73 [25984/50000]\tLoss: 4.8002\tLR: 7.251662\n",
      "Training Epoch: 73 [26112/50000]\tLoss: 4.8126\tLR: 7.251918\n",
      "Training Epoch: 73 [26240/50000]\tLoss: 4.7567\tLR: 7.252174\n",
      "Training Epoch: 73 [26368/50000]\tLoss: 4.7439\tLR: 7.252430\n",
      "Training Epoch: 73 [26496/50000]\tLoss: 4.7313\tLR: 7.252685\n",
      "Training Epoch: 73 [26624/50000]\tLoss: 4.8017\tLR: 7.252941\n",
      "Training Epoch: 73 [26752/50000]\tLoss: 4.7246\tLR: 7.253197\n",
      "Training Epoch: 73 [26880/50000]\tLoss: 4.7282\tLR: 7.253453\n",
      "Training Epoch: 73 [27008/50000]\tLoss: 4.7431\tLR: 7.253708\n",
      "Training Epoch: 73 [27136/50000]\tLoss: 4.7828\tLR: 7.253964\n",
      "Training Epoch: 73 [27264/50000]\tLoss: 4.7205\tLR: 7.254220\n",
      "Training Epoch: 73 [27392/50000]\tLoss: 4.7457\tLR: 7.254476\n",
      "Training Epoch: 73 [27520/50000]\tLoss: 4.7503\tLR: 7.254731\n",
      "Training Epoch: 73 [27648/50000]\tLoss: 4.7700\tLR: 7.254987\n",
      "Training Epoch: 73 [27776/50000]\tLoss: 4.7783\tLR: 7.255243\n",
      "Training Epoch: 73 [27904/50000]\tLoss: 4.8043\tLR: 7.255499\n",
      "Training Epoch: 73 [28032/50000]\tLoss: 4.7330\tLR: 7.255754\n",
      "Training Epoch: 73 [28160/50000]\tLoss: 4.7333\tLR: 7.256010\n",
      "Training Epoch: 73 [28288/50000]\tLoss: 4.6985\tLR: 7.256266\n",
      "Training Epoch: 73 [28416/50000]\tLoss: 4.7472\tLR: 7.256522\n",
      "Training Epoch: 73 [28544/50000]\tLoss: 4.8264\tLR: 7.256777\n",
      "Training Epoch: 73 [28672/50000]\tLoss: 4.6988\tLR: 7.257033\n",
      "Training Epoch: 73 [28800/50000]\tLoss: 4.8270\tLR: 7.257289\n",
      "Training Epoch: 73 [28928/50000]\tLoss: 4.8217\tLR: 7.257545\n",
      "Training Epoch: 73 [29056/50000]\tLoss: 4.7994\tLR: 7.257801\n",
      "Training Epoch: 73 [29184/50000]\tLoss: 4.7991\tLR: 7.258056\n",
      "Training Epoch: 73 [29312/50000]\tLoss: 4.7103\tLR: 7.258312\n",
      "Training Epoch: 73 [29440/50000]\tLoss: 4.7431\tLR: 7.258568\n",
      "Training Epoch: 73 [29568/50000]\tLoss: 4.6828\tLR: 7.258824\n",
      "Training Epoch: 73 [29696/50000]\tLoss: 4.7337\tLR: 7.259079\n",
      "Training Epoch: 73 [29824/50000]\tLoss: 4.6993\tLR: 7.259335\n",
      "Training Epoch: 73 [29952/50000]\tLoss: 4.7042\tLR: 7.259591\n",
      "Training Epoch: 73 [30080/50000]\tLoss: 4.7940\tLR: 7.259847\n",
      "Training Epoch: 73 [30208/50000]\tLoss: 4.7279\tLR: 7.260102\n",
      "Training Epoch: 73 [30336/50000]\tLoss: 4.8338\tLR: 7.260358\n",
      "Training Epoch: 73 [30464/50000]\tLoss: 4.8010\tLR: 7.260614\n",
      "Training Epoch: 73 [30592/50000]\tLoss: 4.7231\tLR: 7.260870\n",
      "Training Epoch: 73 [30720/50000]\tLoss: 4.6943\tLR: 7.261125\n",
      "Training Epoch: 73 [30848/50000]\tLoss: 4.7259\tLR: 7.261381\n",
      "Training Epoch: 73 [30976/50000]\tLoss: 4.7008\tLR: 7.261637\n",
      "Training Epoch: 73 [31104/50000]\tLoss: 4.7190\tLR: 7.261893\n",
      "Training Epoch: 73 [31232/50000]\tLoss: 4.7220\tLR: 7.262148\n",
      "Training Epoch: 73 [31360/50000]\tLoss: 4.7618\tLR: 7.262404\n",
      "Training Epoch: 73 [31488/50000]\tLoss: 4.8183\tLR: 7.262660\n",
      "Training Epoch: 73 [31616/50000]\tLoss: 4.8144\tLR: 7.262916\n",
      "Training Epoch: 73 [31744/50000]\tLoss: 4.6737\tLR: 7.263171\n",
      "Training Epoch: 73 [31872/50000]\tLoss: 4.8603\tLR: 7.263427\n",
      "Training Epoch: 73 [32000/50000]\tLoss: 4.8322\tLR: 7.263683\n",
      "Training Epoch: 73 [32128/50000]\tLoss: 4.7654\tLR: 7.263939\n",
      "Training Epoch: 73 [32256/50000]\tLoss: 4.6803\tLR: 7.264194\n",
      "Training Epoch: 73 [32384/50000]\tLoss: 4.8340\tLR: 7.264450\n",
      "Training Epoch: 73 [32512/50000]\tLoss: 4.6883\tLR: 7.264706\n",
      "Training Epoch: 73 [32640/50000]\tLoss: 4.7383\tLR: 7.264962\n",
      "Training Epoch: 73 [32768/50000]\tLoss: 4.7362\tLR: 7.265217\n",
      "Training Epoch: 73 [32896/50000]\tLoss: 4.7561\tLR: 7.265473\n",
      "Training Epoch: 73 [33024/50000]\tLoss: 4.7086\tLR: 7.265729\n",
      "Training Epoch: 73 [33152/50000]\tLoss: 4.7474\tLR: 7.265985\n",
      "Training Epoch: 73 [33280/50000]\tLoss: 4.7492\tLR: 7.266240\n",
      "Training Epoch: 73 [33408/50000]\tLoss: 4.6379\tLR: 7.266496\n",
      "Training Epoch: 73 [33536/50000]\tLoss: 4.7558\tLR: 7.266752\n",
      "Training Epoch: 73 [33664/50000]\tLoss: 4.7374\tLR: 7.267008\n",
      "Training Epoch: 73 [33792/50000]\tLoss: 4.7070\tLR: 7.267263\n",
      "Training Epoch: 73 [33920/50000]\tLoss: 4.7587\tLR: 7.267519\n",
      "Training Epoch: 73 [34048/50000]\tLoss: 4.7053\tLR: 7.267775\n",
      "Training Epoch: 73 [34176/50000]\tLoss: 4.7372\tLR: 7.268031\n",
      "Training Epoch: 73 [34304/50000]\tLoss: 4.7378\tLR: 7.268286\n",
      "Training Epoch: 73 [34432/50000]\tLoss: 4.6578\tLR: 7.268542\n",
      "Training Epoch: 73 [34560/50000]\tLoss: 4.8324\tLR: 7.268798\n",
      "Training Epoch: 73 [34688/50000]\tLoss: 4.7944\tLR: 7.269054\n",
      "Training Epoch: 73 [34816/50000]\tLoss: 4.7132\tLR: 7.269309\n",
      "Training Epoch: 73 [34944/50000]\tLoss: 4.8026\tLR: 7.269565\n",
      "Training Epoch: 73 [35072/50000]\tLoss: 4.7430\tLR: 7.269821\n",
      "Training Epoch: 73 [35200/50000]\tLoss: 4.6506\tLR: 7.270077\n",
      "Training Epoch: 73 [35328/50000]\tLoss: 4.6626\tLR: 7.270332\n",
      "Training Epoch: 73 [35456/50000]\tLoss: 4.6667\tLR: 7.270588\n",
      "Training Epoch: 73 [35584/50000]\tLoss: 4.7582\tLR: 7.270844\n",
      "Training Epoch: 73 [35712/50000]\tLoss: 4.7853\tLR: 7.271100\n",
      "Training Epoch: 73 [35840/50000]\tLoss: 4.6719\tLR: 7.271355\n",
      "Training Epoch: 73 [35968/50000]\tLoss: 4.6388\tLR: 7.271611\n",
      "Training Epoch: 73 [36096/50000]\tLoss: 4.7617\tLR: 7.271867\n",
      "Training Epoch: 73 [36224/50000]\tLoss: 4.8297\tLR: 7.272123\n",
      "Training Epoch: 73 [36352/50000]\tLoss: 4.6653\tLR: 7.272379\n",
      "Training Epoch: 73 [36480/50000]\tLoss: 4.7984\tLR: 7.272634\n",
      "Training Epoch: 73 [36608/50000]\tLoss: 4.7173\tLR: 7.272890\n",
      "Training Epoch: 73 [36736/50000]\tLoss: 4.6490\tLR: 7.273146\n",
      "Training Epoch: 73 [36864/50000]\tLoss: 4.6573\tLR: 7.273402\n",
      "Training Epoch: 73 [36992/50000]\tLoss: 4.8414\tLR: 7.273657\n",
      "Training Epoch: 73 [37120/50000]\tLoss: 4.7735\tLR: 7.273913\n",
      "Training Epoch: 73 [37248/50000]\tLoss: 4.7954\tLR: 7.274169\n",
      "Training Epoch: 73 [37376/50000]\tLoss: 4.6602\tLR: 7.274425\n",
      "Training Epoch: 73 [37504/50000]\tLoss: 4.7594\tLR: 7.274680\n",
      "Training Epoch: 73 [37632/50000]\tLoss: 4.8086\tLR: 7.274936\n",
      "Training Epoch: 73 [37760/50000]\tLoss: 4.7127\tLR: 7.275192\n",
      "Training Epoch: 73 [37888/50000]\tLoss: 4.7238\tLR: 7.275448\n",
      "Training Epoch: 73 [38016/50000]\tLoss: 4.7300\tLR: 7.275703\n",
      "Training Epoch: 73 [38144/50000]\tLoss: 4.8676\tLR: 7.275959\n",
      "Training Epoch: 73 [38272/50000]\tLoss: 4.6600\tLR: 7.276215\n",
      "Training Epoch: 73 [38400/50000]\tLoss: 4.7340\tLR: 7.276471\n",
      "Training Epoch: 73 [38528/50000]\tLoss: 4.7305\tLR: 7.276726\n",
      "Training Epoch: 73 [38656/50000]\tLoss: 4.7198\tLR: 7.276982\n",
      "Training Epoch: 73 [38784/50000]\tLoss: 4.7452\tLR: 7.277238\n",
      "Training Epoch: 73 [38912/50000]\tLoss: 4.7654\tLR: 7.277494\n",
      "Training Epoch: 73 [39040/50000]\tLoss: 4.7598\tLR: 7.277749\n",
      "Training Epoch: 73 [39168/50000]\tLoss: 4.7708\tLR: 7.278005\n",
      "Training Epoch: 73 [39296/50000]\tLoss: 4.7677\tLR: 7.278261\n",
      "Training Epoch: 73 [39424/50000]\tLoss: 4.7832\tLR: 7.278517\n",
      "Training Epoch: 73 [39552/50000]\tLoss: 4.7536\tLR: 7.278772\n",
      "Training Epoch: 73 [39680/50000]\tLoss: 4.9325\tLR: 7.279028\n",
      "Training Epoch: 73 [39808/50000]\tLoss: 4.7791\tLR: 7.279284\n",
      "Training Epoch: 73 [39936/50000]\tLoss: 4.7749\tLR: 7.279540\n",
      "Training Epoch: 73 [40064/50000]\tLoss: 4.8485\tLR: 7.279795\n",
      "Training Epoch: 73 [40192/50000]\tLoss: 4.6703\tLR: 7.280051\n",
      "Training Epoch: 73 [40320/50000]\tLoss: 4.6354\tLR: 7.280307\n",
      "Training Epoch: 73 [40448/50000]\tLoss: 4.7622\tLR: 7.280563\n",
      "Training Epoch: 73 [40576/50000]\tLoss: 4.7832\tLR: 7.280818\n",
      "Training Epoch: 73 [40704/50000]\tLoss: 4.7032\tLR: 7.281074\n",
      "Training Epoch: 73 [40832/50000]\tLoss: 4.8280\tLR: 7.281330\n",
      "Training Epoch: 73 [40960/50000]\tLoss: 4.7608\tLR: 7.281586\n",
      "Training Epoch: 73 [41088/50000]\tLoss: 4.6382\tLR: 7.281841\n",
      "Training Epoch: 73 [41216/50000]\tLoss: 4.7709\tLR: 7.282097\n",
      "Training Epoch: 73 [41344/50000]\tLoss: 4.6923\tLR: 7.282353\n",
      "Training Epoch: 73 [41472/50000]\tLoss: 4.7899\tLR: 7.282609\n",
      "Training Epoch: 73 [41600/50000]\tLoss: 4.6565\tLR: 7.282864\n",
      "Training Epoch: 73 [41728/50000]\tLoss: 4.7263\tLR: 7.283120\n",
      "Training Epoch: 73 [41856/50000]\tLoss: 4.7417\tLR: 7.283376\n",
      "Training Epoch: 73 [41984/50000]\tLoss: 4.7059\tLR: 7.283632\n",
      "Training Epoch: 73 [42112/50000]\tLoss: 4.6405\tLR: 7.283887\n",
      "Training Epoch: 73 [42240/50000]\tLoss: 4.7551\tLR: 7.284143\n",
      "Training Epoch: 73 [42368/50000]\tLoss: 4.8949\tLR: 7.284399\n",
      "Training Epoch: 73 [42496/50000]\tLoss: 4.8425\tLR: 7.284655\n",
      "Training Epoch: 73 [42624/50000]\tLoss: 4.6951\tLR: 7.284910\n",
      "Training Epoch: 73 [42752/50000]\tLoss: 4.8621\tLR: 7.285166\n",
      "Training Epoch: 73 [42880/50000]\tLoss: 4.6544\tLR: 7.285422\n",
      "Training Epoch: 73 [43008/50000]\tLoss: 4.7018\tLR: 7.285678\n",
      "Training Epoch: 73 [43136/50000]\tLoss: 4.7116\tLR: 7.285934\n",
      "Training Epoch: 73 [43264/50000]\tLoss: 4.8051\tLR: 7.286189\n",
      "Training Epoch: 73 [43392/50000]\tLoss: 4.7296\tLR: 7.286445\n",
      "Training Epoch: 73 [43520/50000]\tLoss: 4.7900\tLR: 7.286701\n",
      "Training Epoch: 73 [43648/50000]\tLoss: 4.6572\tLR: 7.286957\n",
      "Training Epoch: 73 [43776/50000]\tLoss: 4.6818\tLR: 7.287212\n",
      "Training Epoch: 73 [43904/50000]\tLoss: 4.6689\tLR: 7.287468\n",
      "Training Epoch: 73 [44032/50000]\tLoss: 4.6826\tLR: 7.287724\n",
      "Training Epoch: 73 [44160/50000]\tLoss: 4.7658\tLR: 7.287980\n",
      "Training Epoch: 73 [44288/50000]\tLoss: 4.7298\tLR: 7.288235\n",
      "Training Epoch: 73 [44416/50000]\tLoss: 4.7668\tLR: 7.288491\n",
      "Training Epoch: 73 [44544/50000]\tLoss: 4.7842\tLR: 7.288747\n",
      "Training Epoch: 73 [44672/50000]\tLoss: 4.7253\tLR: 7.289003\n",
      "Training Epoch: 73 [44800/50000]\tLoss: 4.6481\tLR: 7.289258\n",
      "Training Epoch: 73 [44928/50000]\tLoss: 4.6528\tLR: 7.289514\n",
      "Training Epoch: 73 [45056/50000]\tLoss: 4.6978\tLR: 7.289770\n",
      "Training Epoch: 73 [45184/50000]\tLoss: 4.7513\tLR: 7.290026\n",
      "Training Epoch: 73 [45312/50000]\tLoss: 4.7571\tLR: 7.290281\n",
      "Training Epoch: 73 [45440/50000]\tLoss: 4.7970\tLR: 7.290537\n",
      "Training Epoch: 73 [45568/50000]\tLoss: 4.6805\tLR: 7.290793\n",
      "Training Epoch: 73 [45696/50000]\tLoss: 4.7118\tLR: 7.291049\n",
      "Training Epoch: 73 [45824/50000]\tLoss: 4.7637\tLR: 7.291304\n",
      "Training Epoch: 73 [45952/50000]\tLoss: 4.7916\tLR: 7.291560\n",
      "Training Epoch: 73 [46080/50000]\tLoss: 4.6486\tLR: 7.291816\n",
      "Training Epoch: 73 [46208/50000]\tLoss: 4.7503\tLR: 7.292072\n",
      "Training Epoch: 73 [46336/50000]\tLoss: 4.7297\tLR: 7.292327\n",
      "Training Epoch: 73 [46464/50000]\tLoss: 4.8140\tLR: 7.292583\n",
      "Training Epoch: 73 [46592/50000]\tLoss: 4.8145\tLR: 7.292839\n",
      "Training Epoch: 73 [46720/50000]\tLoss: 4.6999\tLR: 7.293095\n",
      "Training Epoch: 73 [46848/50000]\tLoss: 4.8492\tLR: 7.293350\n",
      "Training Epoch: 73 [46976/50000]\tLoss: 4.7158\tLR: 7.293606\n",
      "Training Epoch: 73 [47104/50000]\tLoss: 4.7997\tLR: 7.293862\n",
      "Training Epoch: 73 [47232/50000]\tLoss: 4.7759\tLR: 7.294118\n",
      "Training Epoch: 73 [47360/50000]\tLoss: 4.7435\tLR: 7.294373\n",
      "Training Epoch: 73 [47488/50000]\tLoss: 4.7480\tLR: 7.294629\n",
      "Training Epoch: 73 [47616/50000]\tLoss: 4.6901\tLR: 7.294885\n",
      "Training Epoch: 73 [47744/50000]\tLoss: 4.7261\tLR: 7.295141\n",
      "Training Epoch: 73 [47872/50000]\tLoss: 4.7196\tLR: 7.295396\n",
      "Training Epoch: 73 [48000/50000]\tLoss: 4.8642\tLR: 7.295652\n",
      "Training Epoch: 73 [48128/50000]\tLoss: 4.7966\tLR: 7.295908\n",
      "Training Epoch: 73 [48256/50000]\tLoss: 4.7108\tLR: 7.296164\n",
      "Training Epoch: 73 [48384/50000]\tLoss: 4.7637\tLR: 7.296419\n",
      "Training Epoch: 73 [48512/50000]\tLoss: 4.7312\tLR: 7.296675\n",
      "Training Epoch: 73 [48640/50000]\tLoss: 4.8069\tLR: 7.296931\n",
      "Training Epoch: 73 [48768/50000]\tLoss: 4.7210\tLR: 7.297187\n",
      "Training Epoch: 73 [48896/50000]\tLoss: 4.8596\tLR: 7.297442\n",
      "Training Epoch: 73 [49024/50000]\tLoss: 4.7800\tLR: 7.297698\n",
      "Training Epoch: 73 [49152/50000]\tLoss: 4.7097\tLR: 7.297954\n",
      "Training Epoch: 73 [49280/50000]\tLoss: 4.6434\tLR: 7.298210\n",
      "Training Epoch: 73 [49408/50000]\tLoss: 4.6677\tLR: 7.298465\n",
      "Training Epoch: 73 [49536/50000]\tLoss: 4.7577\tLR: 7.298721\n",
      "Training Epoch: 73 [49664/50000]\tLoss: 4.6577\tLR: 7.298977\n",
      "Training Epoch: 73 [49792/50000]\tLoss: 4.7615\tLR: 7.299233\n",
      "Training Epoch: 73 [49920/50000]\tLoss: 4.6922\tLR: 7.299488\n",
      "Training Epoch: 73 [50000/50000]\tLoss: 4.8734\tLR: 7.299744\n",
      "epoch 73 training time consumed: 489.08s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  102341 GB |  102341 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  102027 GB |  102027 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     314 GB |     314 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  102341 GB |  102341 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  102027 GB |  102027 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     314 GB |     314 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  100902 GB |  100902 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  100588 GB |  100588 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     314 GB |     314 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   10851 K  |   10851 K  |\n",
      "|       from large pool |      24    |      65    |    4626 K  |    4625 K  |\n",
      "|       from small pool |     231    |     274    |    6225 K  |    6225 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   10851 K  |   10851 K  |\n",
      "|       from large pool |      24    |      65    |    4626 K  |    4625 K  |\n",
      "|       from small pool |     231    |     274    |    6225 K  |    6225 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      47    |    6290 K  |    6290 K  |\n",
      "|       from large pool |      10    |      23    |    2223 K  |    2223 K  |\n",
      "|       from small pool |      25    |      35    |    4066 K  |    4066 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 73, Average loss: 0.0377, Accuracy: 0.0100, Time consumed:31.23s\n",
      "\n",
      "Training Epoch: 74 [128/50000]\tLoss: 4.8470\tLR: 0.020000\n",
      "Training Epoch: 74 [256/50000]\tLoss: 4.6879\tLR: 7.300256\n",
      "Training Epoch: 74 [384/50000]\tLoss: 4.7107\tLR: 7.300512\n",
      "Training Epoch: 74 [512/50000]\tLoss: 4.7149\tLR: 7.300767\n",
      "Training Epoch: 74 [640/50000]\tLoss: 4.7376\tLR: 7.301023\n",
      "Training Epoch: 74 [768/50000]\tLoss: 4.8077\tLR: 7.301279\n",
      "Training Epoch: 74 [896/50000]\tLoss: 4.7308\tLR: 7.301535\n",
      "Training Epoch: 74 [1024/50000]\tLoss: 4.7919\tLR: 7.301790\n",
      "Training Epoch: 74 [1152/50000]\tLoss: 4.7682\tLR: 7.302046\n",
      "Training Epoch: 74 [1280/50000]\tLoss: 4.8339\tLR: 7.302302\n",
      "Training Epoch: 74 [1408/50000]\tLoss: 4.7995\tLR: 7.302558\n",
      "Training Epoch: 74 [1536/50000]\tLoss: 4.7078\tLR: 7.302813\n",
      "Training Epoch: 74 [1664/50000]\tLoss: 4.7570\tLR: 7.303069\n",
      "Training Epoch: 74 [1792/50000]\tLoss: 4.7755\tLR: 7.303325\n",
      "Training Epoch: 74 [1920/50000]\tLoss: 4.7627\tLR: 7.303581\n",
      "Training Epoch: 74 [2048/50000]\tLoss: 4.7538\tLR: 7.303836\n",
      "Training Epoch: 74 [2176/50000]\tLoss: 4.7696\tLR: 7.304092\n",
      "Training Epoch: 74 [2304/50000]\tLoss: 4.7946\tLR: 7.304348\n",
      "Training Epoch: 74 [2432/50000]\tLoss: 4.8408\tLR: 7.304604\n",
      "Training Epoch: 74 [2560/50000]\tLoss: 4.6898\tLR: 7.304859\n",
      "Training Epoch: 74 [2688/50000]\tLoss: 4.7562\tLR: 7.305115\n",
      "Training Epoch: 74 [2816/50000]\tLoss: 4.7111\tLR: 7.305371\n",
      "Training Epoch: 74 [2944/50000]\tLoss: 4.7566\tLR: 7.305627\n",
      "Training Epoch: 74 [3072/50000]\tLoss: 4.7106\tLR: 7.305882\n",
      "Training Epoch: 74 [3200/50000]\tLoss: 4.7157\tLR: 7.306138\n",
      "Training Epoch: 74 [3328/50000]\tLoss: 4.7325\tLR: 7.306394\n",
      "Training Epoch: 74 [3456/50000]\tLoss: 4.6622\tLR: 7.306650\n",
      "Training Epoch: 74 [3584/50000]\tLoss: 4.7744\tLR: 7.306905\n",
      "Training Epoch: 74 [3712/50000]\tLoss: 4.6918\tLR: 7.307161\n",
      "Training Epoch: 74 [3840/50000]\tLoss: 4.7575\tLR: 7.307417\n",
      "Training Epoch: 74 [3968/50000]\tLoss: 4.7406\tLR: 7.307673\n",
      "Training Epoch: 74 [4096/50000]\tLoss: 4.7360\tLR: 7.307928\n",
      "Training Epoch: 74 [4224/50000]\tLoss: 4.8932\tLR: 7.308184\n",
      "Training Epoch: 74 [4352/50000]\tLoss: 4.7850\tLR: 7.308440\n",
      "Training Epoch: 74 [4480/50000]\tLoss: 4.6794\tLR: 7.308696\n",
      "Training Epoch: 74 [4608/50000]\tLoss: 4.6979\tLR: 7.308951\n",
      "Training Epoch: 74 [4736/50000]\tLoss: 4.7883\tLR: 7.309207\n",
      "Training Epoch: 74 [4864/50000]\tLoss: 4.8367\tLR: 7.309463\n",
      "Training Epoch: 74 [4992/50000]\tLoss: 4.7686\tLR: 7.309719\n",
      "Training Epoch: 74 [5120/50000]\tLoss: 4.7694\tLR: 7.309974\n",
      "Training Epoch: 74 [5248/50000]\tLoss: 4.7464\tLR: 7.310230\n",
      "Training Epoch: 74 [5376/50000]\tLoss: 4.8002\tLR: 7.310486\n",
      "Training Epoch: 74 [5504/50000]\tLoss: 4.8002\tLR: 7.310742\n",
      "Training Epoch: 74 [5632/50000]\tLoss: 4.7601\tLR: 7.310997\n",
      "Training Epoch: 74 [5760/50000]\tLoss: 4.8556\tLR: 7.311253\n",
      "Training Epoch: 74 [5888/50000]\tLoss: 4.6749\tLR: 7.311509\n",
      "Training Epoch: 74 [6016/50000]\tLoss: 4.7033\tLR: 7.311765\n",
      "Training Epoch: 74 [6144/50000]\tLoss: 4.7064\tLR: 7.312020\n",
      "Training Epoch: 74 [6272/50000]\tLoss: 4.8008\tLR: 7.312276\n",
      "Training Epoch: 74 [6400/50000]\tLoss: 4.7066\tLR: 7.312532\n",
      "Training Epoch: 74 [6528/50000]\tLoss: 4.7296\tLR: 7.312788\n",
      "Training Epoch: 74 [6656/50000]\tLoss: 4.8179\tLR: 7.313043\n",
      "Training Epoch: 74 [6784/50000]\tLoss: 4.7055\tLR: 7.313299\n",
      "Training Epoch: 74 [6912/50000]\tLoss: 4.6805\tLR: 7.313555\n",
      "Training Epoch: 74 [7040/50000]\tLoss: 4.7417\tLR: 7.313811\n",
      "Training Epoch: 74 [7168/50000]\tLoss: 4.7735\tLR: 7.314066\n",
      "Training Epoch: 74 [7296/50000]\tLoss: 4.7477\tLR: 7.314322\n",
      "Training Epoch: 74 [7424/50000]\tLoss: 4.6799\tLR: 7.314578\n",
      "Training Epoch: 74 [7552/50000]\tLoss: 4.7094\tLR: 7.314834\n",
      "Training Epoch: 74 [7680/50000]\tLoss: 4.7881\tLR: 7.315090\n",
      "Training Epoch: 74 [7808/50000]\tLoss: 4.7782\tLR: 7.315345\n",
      "Training Epoch: 74 [7936/50000]\tLoss: 4.6890\tLR: 7.315601\n",
      "Training Epoch: 74 [8064/50000]\tLoss: 4.7484\tLR: 7.315857\n",
      "Training Epoch: 74 [8192/50000]\tLoss: 4.6877\tLR: 7.316113\n",
      "Training Epoch: 74 [8320/50000]\tLoss: 4.6827\tLR: 7.316368\n",
      "Training Epoch: 74 [8448/50000]\tLoss: 4.7324\tLR: 7.316624\n",
      "Training Epoch: 74 [8576/50000]\tLoss: 4.8095\tLR: 7.316880\n",
      "Training Epoch: 74 [8704/50000]\tLoss: 4.7546\tLR: 7.317136\n",
      "Training Epoch: 74 [8832/50000]\tLoss: 4.7539\tLR: 7.317391\n",
      "Training Epoch: 74 [8960/50000]\tLoss: 4.7952\tLR: 7.317647\n",
      "Training Epoch: 74 [9088/50000]\tLoss: 4.7294\tLR: 7.317903\n",
      "Training Epoch: 74 [9216/50000]\tLoss: 4.7295\tLR: 7.318159\n",
      "Training Epoch: 74 [9344/50000]\tLoss: 4.7642\tLR: 7.318414\n",
      "Training Epoch: 74 [9472/50000]\tLoss: 4.7730\tLR: 7.318670\n",
      "Training Epoch: 74 [9600/50000]\tLoss: 4.8095\tLR: 7.318926\n",
      "Training Epoch: 74 [9728/50000]\tLoss: 4.7508\tLR: 7.319182\n",
      "Training Epoch: 74 [9856/50000]\tLoss: 4.7317\tLR: 7.319437\n",
      "Training Epoch: 74 [9984/50000]\tLoss: 4.7505\tLR: 7.319693\n",
      "Training Epoch: 74 [10112/50000]\tLoss: 4.8007\tLR: 7.319949\n",
      "Training Epoch: 74 [10240/50000]\tLoss: 4.7396\tLR: 7.320205\n",
      "Training Epoch: 74 [10368/50000]\tLoss: 4.7580\tLR: 7.320460\n",
      "Training Epoch: 74 [10496/50000]\tLoss: 4.7201\tLR: 7.320716\n",
      "Training Epoch: 74 [10624/50000]\tLoss: 4.7845\tLR: 7.320972\n",
      "Training Epoch: 74 [10752/50000]\tLoss: 4.7233\tLR: 7.321228\n",
      "Training Epoch: 74 [10880/50000]\tLoss: 4.8233\tLR: 7.321483\n",
      "Training Epoch: 74 [11008/50000]\tLoss: 4.8119\tLR: 7.321739\n",
      "Training Epoch: 74 [11136/50000]\tLoss: 4.7578\tLR: 7.321995\n",
      "Training Epoch: 74 [11264/50000]\tLoss: 4.7392\tLR: 7.322251\n",
      "Training Epoch: 74 [11392/50000]\tLoss: 4.7596\tLR: 7.322506\n",
      "Training Epoch: 74 [11520/50000]\tLoss: 4.7564\tLR: 7.322762\n",
      "Training Epoch: 74 [11648/50000]\tLoss: 4.7141\tLR: 7.323018\n",
      "Training Epoch: 74 [11776/50000]\tLoss: 4.7108\tLR: 7.323274\n",
      "Training Epoch: 74 [11904/50000]\tLoss: 4.7206\tLR: 7.323529\n",
      "Training Epoch: 74 [12032/50000]\tLoss: 4.7850\tLR: 7.323785\n",
      "Training Epoch: 74 [12160/50000]\tLoss: 4.7745\tLR: 7.324041\n",
      "Training Epoch: 74 [12288/50000]\tLoss: 4.8017\tLR: 7.324297\n",
      "Training Epoch: 74 [12416/50000]\tLoss: 4.7617\tLR: 7.324552\n",
      "Training Epoch: 74 [12544/50000]\tLoss: 4.7520\tLR: 7.324808\n",
      "Training Epoch: 74 [12672/50000]\tLoss: 4.7684\tLR: 7.325064\n",
      "Training Epoch: 74 [12800/50000]\tLoss: 4.7475\tLR: 7.325320\n",
      "Training Epoch: 74 [12928/50000]\tLoss: 4.7156\tLR: 7.325575\n",
      "Training Epoch: 74 [13056/50000]\tLoss: 4.7958\tLR: 7.325831\n",
      "Training Epoch: 74 [13184/50000]\tLoss: 4.7557\tLR: 7.326087\n",
      "Training Epoch: 74 [13312/50000]\tLoss: 4.7552\tLR: 7.326343\n",
      "Training Epoch: 74 [13440/50000]\tLoss: 4.8308\tLR: 7.326598\n",
      "Training Epoch: 74 [13568/50000]\tLoss: 4.6600\tLR: 7.326854\n",
      "Training Epoch: 74 [13696/50000]\tLoss: 4.7010\tLR: 7.327110\n",
      "Training Epoch: 74 [13824/50000]\tLoss: 4.7293\tLR: 7.327366\n",
      "Training Epoch: 74 [13952/50000]\tLoss: 4.7850\tLR: 7.327621\n",
      "Training Epoch: 74 [14080/50000]\tLoss: 4.8329\tLR: 7.327877\n",
      "Training Epoch: 74 [14208/50000]\tLoss: 4.7538\tLR: 7.328133\n",
      "Training Epoch: 74 [14336/50000]\tLoss: 4.6437\tLR: 7.328389\n",
      "Training Epoch: 74 [14464/50000]\tLoss: 4.8073\tLR: 7.328645\n",
      "Training Epoch: 74 [14592/50000]\tLoss: 4.7694\tLR: 7.328900\n",
      "Training Epoch: 74 [14720/50000]\tLoss: 4.7034\tLR: 7.329156\n",
      "Training Epoch: 74 [14848/50000]\tLoss: 4.6887\tLR: 7.329412\n",
      "Training Epoch: 74 [14976/50000]\tLoss: 4.7487\tLR: 7.329668\n",
      "Training Epoch: 74 [15104/50000]\tLoss: 4.7813\tLR: 7.329923\n",
      "Training Epoch: 74 [15232/50000]\tLoss: 4.7928\tLR: 7.330179\n",
      "Training Epoch: 74 [15360/50000]\tLoss: 4.6856\tLR: 7.330435\n",
      "Training Epoch: 74 [15488/50000]\tLoss: 4.7929\tLR: 7.330691\n",
      "Training Epoch: 74 [15616/50000]\tLoss: 4.8453\tLR: 7.330946\n",
      "Training Epoch: 74 [15744/50000]\tLoss: 4.7067\tLR: 7.331202\n",
      "Training Epoch: 74 [15872/50000]\tLoss: 4.7605\tLR: 7.331458\n",
      "Training Epoch: 74 [16000/50000]\tLoss: 4.7259\tLR: 7.331714\n",
      "Training Epoch: 74 [16128/50000]\tLoss: 4.7566\tLR: 7.331969\n",
      "Training Epoch: 74 [16256/50000]\tLoss: 4.7655\tLR: 7.332225\n",
      "Training Epoch: 74 [16384/50000]\tLoss: 4.7564\tLR: 7.332481\n",
      "Training Epoch: 74 [16512/50000]\tLoss: 4.7665\tLR: 7.332737\n",
      "Training Epoch: 74 [16640/50000]\tLoss: 4.7893\tLR: 7.332992\n",
      "Training Epoch: 74 [16768/50000]\tLoss: 4.7981\tLR: 7.333248\n",
      "Training Epoch: 74 [16896/50000]\tLoss: 4.7710\tLR: 7.333504\n",
      "Training Epoch: 74 [17024/50000]\tLoss: 4.7778\tLR: 7.333760\n",
      "Training Epoch: 74 [17152/50000]\tLoss: 4.7056\tLR: 7.334015\n",
      "Training Epoch: 74 [17280/50000]\tLoss: 4.6722\tLR: 7.334271\n",
      "Training Epoch: 74 [17408/50000]\tLoss: 4.8182\tLR: 7.334527\n",
      "Training Epoch: 74 [17536/50000]\tLoss: 4.7216\tLR: 7.334783\n",
      "Training Epoch: 74 [17664/50000]\tLoss: 4.7128\tLR: 7.335038\n",
      "Training Epoch: 74 [17792/50000]\tLoss: 4.7852\tLR: 7.335294\n",
      "Training Epoch: 74 [17920/50000]\tLoss: 4.7726\tLR: 7.335550\n",
      "Training Epoch: 74 [18048/50000]\tLoss: 4.7749\tLR: 7.335806\n",
      "Training Epoch: 74 [18176/50000]\tLoss: 4.7333\tLR: 7.336061\n",
      "Training Epoch: 74 [18304/50000]\tLoss: 4.7263\tLR: 7.336317\n",
      "Training Epoch: 74 [18432/50000]\tLoss: 4.6640\tLR: 7.336573\n",
      "Training Epoch: 74 [18560/50000]\tLoss: 4.7662\tLR: 7.336829\n",
      "Training Epoch: 74 [18688/50000]\tLoss: 4.7991\tLR: 7.337084\n",
      "Training Epoch: 74 [18816/50000]\tLoss: 4.8032\tLR: 7.337340\n",
      "Training Epoch: 74 [18944/50000]\tLoss: 4.6362\tLR: 7.337596\n",
      "Training Epoch: 74 [19072/50000]\tLoss: 4.7689\tLR: 7.337852\n",
      "Training Epoch: 74 [19200/50000]\tLoss: 4.6428\tLR: 7.338107\n",
      "Training Epoch: 74 [19328/50000]\tLoss: 4.7096\tLR: 7.338363\n",
      "Training Epoch: 74 [19456/50000]\tLoss: 4.6815\tLR: 7.338619\n",
      "Training Epoch: 74 [19584/50000]\tLoss: 4.7056\tLR: 7.338875\n",
      "Training Epoch: 74 [19712/50000]\tLoss: 4.7572\tLR: 7.339130\n",
      "Training Epoch: 74 [19840/50000]\tLoss: 4.7463\tLR: 7.339386\n",
      "Training Epoch: 74 [19968/50000]\tLoss: 4.8106\tLR: 7.339642\n",
      "Training Epoch: 74 [20096/50000]\tLoss: 4.7271\tLR: 7.339898\n",
      "Training Epoch: 74 [20224/50000]\tLoss: 4.8041\tLR: 7.340153\n",
      "Training Epoch: 74 [20352/50000]\tLoss: 4.8065\tLR: 7.340409\n",
      "Training Epoch: 74 [20480/50000]\tLoss: 4.6975\tLR: 7.340665\n",
      "Training Epoch: 74 [20608/50000]\tLoss: 4.7577\tLR: 7.340921\n",
      "Training Epoch: 74 [20736/50000]\tLoss: 4.6613\tLR: 7.341176\n",
      "Training Epoch: 74 [20864/50000]\tLoss: 4.7324\tLR: 7.341432\n",
      "Training Epoch: 74 [20992/50000]\tLoss: 4.7671\tLR: 7.341688\n",
      "Training Epoch: 74 [21120/50000]\tLoss: 4.8781\tLR: 7.341944\n",
      "Training Epoch: 74 [21248/50000]\tLoss: 4.7856\tLR: 7.342199\n",
      "Training Epoch: 74 [21376/50000]\tLoss: 4.9140\tLR: 7.342455\n",
      "Training Epoch: 74 [21504/50000]\tLoss: 4.7997\tLR: 7.342711\n",
      "Training Epoch: 74 [21632/50000]\tLoss: 4.6193\tLR: 7.342967\n",
      "Training Epoch: 74 [21760/50000]\tLoss: 4.7405\tLR: 7.343223\n",
      "Training Epoch: 74 [21888/50000]\tLoss: 4.6951\tLR: 7.343478\n",
      "Training Epoch: 74 [22016/50000]\tLoss: 4.8716\tLR: 7.343734\n",
      "Training Epoch: 74 [22144/50000]\tLoss: 4.7571\tLR: 7.343990\n",
      "Training Epoch: 74 [22272/50000]\tLoss: 4.7671\tLR: 7.344246\n",
      "Training Epoch: 74 [22400/50000]\tLoss: 4.7408\tLR: 7.344501\n",
      "Training Epoch: 74 [22528/50000]\tLoss: 4.7187\tLR: 7.344757\n",
      "Training Epoch: 74 [22656/50000]\tLoss: 4.7923\tLR: 7.345013\n",
      "Training Epoch: 74 [22784/50000]\tLoss: 4.7867\tLR: 7.345269\n",
      "Training Epoch: 74 [22912/50000]\tLoss: 4.7390\tLR: 7.345524\n",
      "Training Epoch: 74 [23040/50000]\tLoss: 4.7767\tLR: 7.345780\n",
      "Training Epoch: 74 [23168/50000]\tLoss: 4.6824\tLR: 7.346036\n",
      "Training Epoch: 74 [23296/50000]\tLoss: 4.7037\tLR: 7.346292\n",
      "Training Epoch: 74 [23424/50000]\tLoss: 4.7034\tLR: 7.346547\n",
      "Training Epoch: 74 [23552/50000]\tLoss: 4.7724\tLR: 7.346803\n",
      "Training Epoch: 74 [23680/50000]\tLoss: 4.7791\tLR: 7.347059\n",
      "Training Epoch: 74 [23808/50000]\tLoss: 4.7117\tLR: 7.347315\n",
      "Training Epoch: 74 [23936/50000]\tLoss: 4.6507\tLR: 7.347570\n",
      "Training Epoch: 74 [24064/50000]\tLoss: 4.7767\tLR: 7.347826\n",
      "Training Epoch: 74 [24192/50000]\tLoss: 4.7751\tLR: 7.348082\n",
      "Training Epoch: 74 [24320/50000]\tLoss: 4.7786\tLR: 7.348338\n",
      "Training Epoch: 74 [24448/50000]\tLoss: 4.7738\tLR: 7.348593\n",
      "Training Epoch: 74 [24576/50000]\tLoss: 4.7832\tLR: 7.348849\n",
      "Training Epoch: 74 [24704/50000]\tLoss: 4.7569\tLR: 7.349105\n",
      "Training Epoch: 74 [24832/50000]\tLoss: 4.7817\tLR: 7.349361\n",
      "Training Epoch: 74 [24960/50000]\tLoss: 4.6956\tLR: 7.349616\n",
      "Training Epoch: 74 [25088/50000]\tLoss: 4.7701\tLR: 7.349872\n",
      "Training Epoch: 74 [25216/50000]\tLoss: 4.7076\tLR: 7.350128\n",
      "Training Epoch: 74 [25344/50000]\tLoss: 4.6240\tLR: 7.350384\n",
      "Training Epoch: 74 [25472/50000]\tLoss: 4.7259\tLR: 7.350639\n",
      "Training Epoch: 74 [25600/50000]\tLoss: 4.7231\tLR: 7.350895\n",
      "Training Epoch: 74 [25728/50000]\tLoss: 4.7039\tLR: 7.351151\n",
      "Training Epoch: 74 [25856/50000]\tLoss: 4.8293\tLR: 7.351407\n",
      "Training Epoch: 74 [25984/50000]\tLoss: 4.7630\tLR: 7.351662\n",
      "Training Epoch: 74 [26112/50000]\tLoss: 4.7088\tLR: 7.351918\n",
      "Training Epoch: 74 [26240/50000]\tLoss: 4.7062\tLR: 7.352174\n",
      "Training Epoch: 74 [26368/50000]\tLoss: 4.7208\tLR: 7.352430\n",
      "Training Epoch: 74 [26496/50000]\tLoss: 4.8676\tLR: 7.352685\n",
      "Training Epoch: 74 [26624/50000]\tLoss: 4.7636\tLR: 7.352941\n",
      "Training Epoch: 74 [26752/50000]\tLoss: 4.7669\tLR: 7.353197\n",
      "Training Epoch: 74 [26880/50000]\tLoss: 4.7526\tLR: 7.353453\n",
      "Training Epoch: 74 [27008/50000]\tLoss: 4.6407\tLR: 7.353708\n",
      "Training Epoch: 74 [27136/50000]\tLoss: 4.7139\tLR: 7.353964\n",
      "Training Epoch: 74 [27264/50000]\tLoss: 4.7475\tLR: 7.354220\n",
      "Training Epoch: 74 [27392/50000]\tLoss: 4.7530\tLR: 7.354476\n",
      "Training Epoch: 74 [27520/50000]\tLoss: 4.7902\tLR: 7.354731\n",
      "Training Epoch: 74 [27648/50000]\tLoss: 4.7893\tLR: 7.354987\n",
      "Training Epoch: 74 [27776/50000]\tLoss: 4.6975\tLR: 7.355243\n",
      "Training Epoch: 74 [27904/50000]\tLoss: 4.7332\tLR: 7.355499\n",
      "Training Epoch: 74 [28032/50000]\tLoss: 4.6812\tLR: 7.355754\n",
      "Training Epoch: 74 [28160/50000]\tLoss: 4.6922\tLR: 7.356010\n",
      "Training Epoch: 74 [28288/50000]\tLoss: 4.7291\tLR: 7.356266\n",
      "Training Epoch: 74 [28416/50000]\tLoss: 4.8145\tLR: 7.356522\n",
      "Training Epoch: 74 [28544/50000]\tLoss: 4.6729\tLR: 7.356777\n",
      "Training Epoch: 74 [28672/50000]\tLoss: 4.7102\tLR: 7.357033\n",
      "Training Epoch: 74 [28800/50000]\tLoss: 4.7360\tLR: 7.357289\n",
      "Training Epoch: 74 [28928/50000]\tLoss: 4.7267\tLR: 7.357545\n",
      "Training Epoch: 74 [29056/50000]\tLoss: 4.6978\tLR: 7.357801\n",
      "Training Epoch: 74 [29184/50000]\tLoss: 4.8101\tLR: 7.358056\n",
      "Training Epoch: 74 [29312/50000]\tLoss: 4.7073\tLR: 7.358312\n",
      "Training Epoch: 74 [29440/50000]\tLoss: 4.6327\tLR: 7.358568\n",
      "Training Epoch: 74 [29568/50000]\tLoss: 4.7744\tLR: 7.358824\n",
      "Training Epoch: 74 [29696/50000]\tLoss: 4.7535\tLR: 7.359079\n",
      "Training Epoch: 74 [29824/50000]\tLoss: 4.7432\tLR: 7.359335\n",
      "Training Epoch: 74 [29952/50000]\tLoss: 4.6553\tLR: 7.359591\n",
      "Training Epoch: 74 [30080/50000]\tLoss: 4.7676\tLR: 7.359847\n",
      "Training Epoch: 74 [30208/50000]\tLoss: 4.7398\tLR: 7.360102\n",
      "Training Epoch: 74 [30336/50000]\tLoss: 4.7801\tLR: 7.360358\n",
      "Training Epoch: 74 [30464/50000]\tLoss: 4.7459\tLR: 7.360614\n",
      "Training Epoch: 74 [30592/50000]\tLoss: 4.7471\tLR: 7.360870\n",
      "Training Epoch: 74 [30720/50000]\tLoss: 4.6961\tLR: 7.361125\n",
      "Training Epoch: 74 [30848/50000]\tLoss: 4.7272\tLR: 7.361381\n",
      "Training Epoch: 74 [30976/50000]\tLoss: 4.7815\tLR: 7.361637\n",
      "Training Epoch: 74 [31104/50000]\tLoss: 4.7940\tLR: 7.361893\n",
      "Training Epoch: 74 [31232/50000]\tLoss: 4.7945\tLR: 7.362148\n",
      "Training Epoch: 74 [31360/50000]\tLoss: 4.7456\tLR: 7.362404\n",
      "Training Epoch: 74 [31488/50000]\tLoss: 4.7866\tLR: 7.362660\n",
      "Training Epoch: 74 [31616/50000]\tLoss: 4.8109\tLR: 7.362916\n",
      "Training Epoch: 74 [31744/50000]\tLoss: 4.8123\tLR: 7.363171\n",
      "Training Epoch: 74 [31872/50000]\tLoss: 4.7465\tLR: 7.363427\n",
      "Training Epoch: 74 [32000/50000]\tLoss: 4.7897\tLR: 7.363683\n",
      "Training Epoch: 74 [32128/50000]\tLoss: 4.6738\tLR: 7.363939\n",
      "Training Epoch: 74 [32256/50000]\tLoss: 4.7627\tLR: 7.364194\n",
      "Training Epoch: 74 [32384/50000]\tLoss: 4.7463\tLR: 7.364450\n",
      "Training Epoch: 74 [32512/50000]\tLoss: 4.7851\tLR: 7.364706\n",
      "Training Epoch: 74 [32640/50000]\tLoss: 4.7336\tLR: 7.364962\n",
      "Training Epoch: 74 [32768/50000]\tLoss: 4.7380\tLR: 7.365217\n",
      "Training Epoch: 74 [32896/50000]\tLoss: 4.7655\tLR: 7.365473\n",
      "Training Epoch: 74 [33024/50000]\tLoss: 4.7773\tLR: 7.365729\n",
      "Training Epoch: 74 [33152/50000]\tLoss: 4.7249\tLR: 7.365985\n",
      "Training Epoch: 74 [33280/50000]\tLoss: 4.7322\tLR: 7.366240\n",
      "Training Epoch: 74 [33408/50000]\tLoss: 4.6897\tLR: 7.366496\n",
      "Training Epoch: 74 [33536/50000]\tLoss: 4.7339\tLR: 7.366752\n",
      "Training Epoch: 74 [33664/50000]\tLoss: 4.8326\tLR: 7.367008\n",
      "Training Epoch: 74 [33792/50000]\tLoss: 4.7632\tLR: 7.367263\n",
      "Training Epoch: 74 [33920/50000]\tLoss: 4.8141\tLR: 7.367519\n",
      "Training Epoch: 74 [34048/50000]\tLoss: 4.7339\tLR: 7.367775\n",
      "Training Epoch: 74 [34176/50000]\tLoss: 4.6996\tLR: 7.368031\n",
      "Training Epoch: 74 [34304/50000]\tLoss: 4.8093\tLR: 7.368286\n",
      "Training Epoch: 74 [34432/50000]\tLoss: 4.7862\tLR: 7.368542\n",
      "Training Epoch: 74 [34560/50000]\tLoss: 4.7302\tLR: 7.368798\n",
      "Training Epoch: 74 [34688/50000]\tLoss: 4.7481\tLR: 7.369054\n",
      "Training Epoch: 74 [34816/50000]\tLoss: 4.7360\tLR: 7.369309\n",
      "Training Epoch: 74 [34944/50000]\tLoss: 4.8765\tLR: 7.369565\n",
      "Training Epoch: 74 [35072/50000]\tLoss: 4.7364\tLR: 7.369821\n",
      "Training Epoch: 74 [35200/50000]\tLoss: 4.7406\tLR: 7.370077\n",
      "Training Epoch: 74 [35328/50000]\tLoss: 4.6256\tLR: 7.370332\n",
      "Training Epoch: 74 [35456/50000]\tLoss: 4.8311\tLR: 7.370588\n",
      "Training Epoch: 74 [35584/50000]\tLoss: 4.7579\tLR: 7.370844\n",
      "Training Epoch: 74 [35712/50000]\tLoss: 4.7611\tLR: 7.371100\n",
      "Training Epoch: 74 [35840/50000]\tLoss: 4.7500\tLR: 7.371355\n",
      "Training Epoch: 74 [35968/50000]\tLoss: 4.7258\tLR: 7.371611\n",
      "Training Epoch: 74 [36096/50000]\tLoss: 4.7581\tLR: 7.371867\n",
      "Training Epoch: 74 [36224/50000]\tLoss: 4.7667\tLR: 7.372123\n",
      "Training Epoch: 74 [36352/50000]\tLoss: 4.6772\tLR: 7.372379\n",
      "Training Epoch: 74 [36480/50000]\tLoss: 4.7036\tLR: 7.372634\n",
      "Training Epoch: 74 [36608/50000]\tLoss: 4.7567\tLR: 7.372890\n",
      "Training Epoch: 74 [36736/50000]\tLoss: 4.8204\tLR: 7.373146\n",
      "Training Epoch: 74 [36864/50000]\tLoss: 4.7210\tLR: 7.373402\n",
      "Training Epoch: 74 [36992/50000]\tLoss: 4.7642\tLR: 7.373657\n",
      "Training Epoch: 74 [37120/50000]\tLoss: 4.7832\tLR: 7.373913\n",
      "Training Epoch: 74 [37248/50000]\tLoss: 4.6888\tLR: 7.374169\n",
      "Training Epoch: 74 [37376/50000]\tLoss: 4.7361\tLR: 7.374425\n",
      "Training Epoch: 74 [37504/50000]\tLoss: 4.7530\tLR: 7.374680\n",
      "Training Epoch: 74 [37632/50000]\tLoss: 4.7159\tLR: 7.374936\n",
      "Training Epoch: 74 [37760/50000]\tLoss: 4.6737\tLR: 7.375192\n",
      "Training Epoch: 74 [37888/50000]\tLoss: 4.8026\tLR: 7.375448\n",
      "Training Epoch: 74 [38016/50000]\tLoss: 4.7733\tLR: 7.375703\n",
      "Training Epoch: 74 [38144/50000]\tLoss: 4.7958\tLR: 7.375959\n",
      "Training Epoch: 74 [38272/50000]\tLoss: 4.7905\tLR: 7.376215\n",
      "Training Epoch: 74 [38400/50000]\tLoss: 4.7178\tLR: 7.376471\n",
      "Training Epoch: 74 [38528/50000]\tLoss: 4.7270\tLR: 7.376726\n",
      "Training Epoch: 74 [38656/50000]\tLoss: 4.6613\tLR: 7.376982\n",
      "Training Epoch: 74 [38784/50000]\tLoss: 4.7583\tLR: 7.377238\n",
      "Training Epoch: 74 [38912/50000]\tLoss: 4.7533\tLR: 7.377494\n",
      "Training Epoch: 74 [39040/50000]\tLoss: 4.7082\tLR: 7.377749\n",
      "Training Epoch: 74 [39168/50000]\tLoss: 4.7870\tLR: 7.378005\n",
      "Training Epoch: 74 [39296/50000]\tLoss: 4.8116\tLR: 7.378261\n",
      "Training Epoch: 74 [39424/50000]\tLoss: 4.8659\tLR: 7.378517\n",
      "Training Epoch: 74 [39552/50000]\tLoss: 4.6969\tLR: 7.378772\n",
      "Training Epoch: 74 [39680/50000]\tLoss: 4.8393\tLR: 7.379028\n",
      "Training Epoch: 74 [39808/50000]\tLoss: 4.8063\tLR: 7.379284\n",
      "Training Epoch: 74 [39936/50000]\tLoss: 4.6978\tLR: 7.379540\n",
      "Training Epoch: 74 [40064/50000]\tLoss: 4.8063\tLR: 7.379795\n",
      "Training Epoch: 74 [40192/50000]\tLoss: 4.7968\tLR: 7.380051\n",
      "Training Epoch: 74 [40320/50000]\tLoss: 4.7816\tLR: 7.380307\n",
      "Training Epoch: 74 [40448/50000]\tLoss: 4.7692\tLR: 7.380563\n",
      "Training Epoch: 74 [40576/50000]\tLoss: 4.7710\tLR: 7.380818\n",
      "Training Epoch: 74 [40704/50000]\tLoss: 4.6660\tLR: 7.381074\n",
      "Training Epoch: 74 [40832/50000]\tLoss: 4.6787\tLR: 7.381330\n",
      "Training Epoch: 74 [40960/50000]\tLoss: 4.6749\tLR: 7.381586\n",
      "Training Epoch: 74 [41088/50000]\tLoss: 4.8079\tLR: 7.381841\n",
      "Training Epoch: 74 [41216/50000]\tLoss: 4.6078\tLR: 7.382097\n",
      "Training Epoch: 74 [41344/50000]\tLoss: 4.7462\tLR: 7.382353\n",
      "Training Epoch: 74 [41472/50000]\tLoss: 4.8307\tLR: 7.382609\n",
      "Training Epoch: 74 [41600/50000]\tLoss: 4.7764\tLR: 7.382864\n",
      "Training Epoch: 74 [41728/50000]\tLoss: 4.7000\tLR: 7.383120\n",
      "Training Epoch: 74 [41856/50000]\tLoss: 4.7716\tLR: 7.383376\n",
      "Training Epoch: 74 [41984/50000]\tLoss: 4.7426\tLR: 7.383632\n",
      "Training Epoch: 74 [42112/50000]\tLoss: 4.7468\tLR: 7.383887\n",
      "Training Epoch: 74 [42240/50000]\tLoss: 4.7044\tLR: 7.384143\n",
      "Training Epoch: 74 [42368/50000]\tLoss: 4.6775\tLR: 7.384399\n",
      "Training Epoch: 74 [42496/50000]\tLoss: 4.7803\tLR: 7.384655\n",
      "Training Epoch: 74 [42624/50000]\tLoss: 4.7043\tLR: 7.384910\n",
      "Training Epoch: 74 [42752/50000]\tLoss: 4.7776\tLR: 7.385166\n",
      "Training Epoch: 74 [42880/50000]\tLoss: 4.6915\tLR: 7.385422\n",
      "Training Epoch: 74 [43008/50000]\tLoss: 4.7882\tLR: 7.385678\n",
      "Training Epoch: 74 [43136/50000]\tLoss: 4.7158\tLR: 7.385934\n",
      "Training Epoch: 74 [43264/50000]\tLoss: 4.6764\tLR: 7.386189\n",
      "Training Epoch: 74 [43392/50000]\tLoss: 4.7374\tLR: 7.386445\n",
      "Training Epoch: 74 [43520/50000]\tLoss: 4.7751\tLR: 7.386701\n",
      "Training Epoch: 74 [43648/50000]\tLoss: 4.7734\tLR: 7.386957\n",
      "Training Epoch: 74 [43776/50000]\tLoss: 4.7214\tLR: 7.387212\n",
      "Training Epoch: 74 [43904/50000]\tLoss: 4.7179\tLR: 7.387468\n",
      "Training Epoch: 74 [44032/50000]\tLoss: 4.6616\tLR: 7.387724\n",
      "Training Epoch: 74 [44160/50000]\tLoss: 4.6554\tLR: 7.387980\n",
      "Training Epoch: 74 [44288/50000]\tLoss: 4.7406\tLR: 7.388235\n",
      "Training Epoch: 74 [44416/50000]\tLoss: 4.7253\tLR: 7.388491\n",
      "Training Epoch: 74 [44544/50000]\tLoss: 4.7292\tLR: 7.388747\n",
      "Training Epoch: 74 [44672/50000]\tLoss: 4.7374\tLR: 7.389003\n",
      "Training Epoch: 74 [44800/50000]\tLoss: 4.8311\tLR: 7.389258\n",
      "Training Epoch: 74 [44928/50000]\tLoss: 4.8071\tLR: 7.389514\n",
      "Training Epoch: 74 [45056/50000]\tLoss: 4.7704\tLR: 7.389770\n",
      "Training Epoch: 74 [45184/50000]\tLoss: 4.6687\tLR: 7.390026\n",
      "Training Epoch: 74 [45312/50000]\tLoss: 4.7539\tLR: 7.390281\n",
      "Training Epoch: 74 [45440/50000]\tLoss: 4.7165\tLR: 7.390537\n",
      "Training Epoch: 74 [45568/50000]\tLoss: 4.6869\tLR: 7.390793\n",
      "Training Epoch: 74 [45696/50000]\tLoss: 4.7463\tLR: 7.391049\n",
      "Training Epoch: 74 [45824/50000]\tLoss: 4.7450\tLR: 7.391304\n",
      "Training Epoch: 74 [45952/50000]\tLoss: 4.7010\tLR: 7.391560\n",
      "Training Epoch: 74 [46080/50000]\tLoss: 4.7371\tLR: 7.391816\n",
      "Training Epoch: 74 [46208/50000]\tLoss: 4.6867\tLR: 7.392072\n",
      "Training Epoch: 74 [46336/50000]\tLoss: 4.7541\tLR: 7.392327\n",
      "Training Epoch: 74 [46464/50000]\tLoss: 4.6980\tLR: 7.392583\n",
      "Training Epoch: 74 [46592/50000]\tLoss: 4.8292\tLR: 7.392839\n",
      "Training Epoch: 74 [46720/50000]\tLoss: 4.6971\tLR: 7.393095\n",
      "Training Epoch: 74 [46848/50000]\tLoss: 4.7809\tLR: 7.393350\n",
      "Training Epoch: 74 [46976/50000]\tLoss: 4.7087\tLR: 7.393606\n",
      "Training Epoch: 74 [47104/50000]\tLoss: 4.7630\tLR: 7.393862\n",
      "Training Epoch: 74 [47232/50000]\tLoss: 4.7518\tLR: 7.394118\n",
      "Training Epoch: 74 [47360/50000]\tLoss: 4.7527\tLR: 7.394373\n",
      "Training Epoch: 74 [47488/50000]\tLoss: 4.7474\tLR: 7.394629\n",
      "Training Epoch: 74 [47616/50000]\tLoss: 4.7865\tLR: 7.394885\n",
      "Training Epoch: 74 [47744/50000]\tLoss: 4.7695\tLR: 7.395141\n",
      "Training Epoch: 74 [47872/50000]\tLoss: 4.7981\tLR: 7.395396\n",
      "Training Epoch: 74 [48000/50000]\tLoss: 4.6968\tLR: 7.395652\n",
      "Training Epoch: 74 [48128/50000]\tLoss: 4.8133\tLR: 7.395908\n",
      "Training Epoch: 74 [48256/50000]\tLoss: 4.7732\tLR: 7.396164\n",
      "Training Epoch: 74 [48384/50000]\tLoss: 4.7276\tLR: 7.396419\n",
      "Training Epoch: 74 [48512/50000]\tLoss: 4.8307\tLR: 7.396675\n",
      "Training Epoch: 74 [48640/50000]\tLoss: 4.6695\tLR: 7.396931\n",
      "Training Epoch: 74 [48768/50000]\tLoss: 4.7138\tLR: 7.397187\n",
      "Training Epoch: 74 [48896/50000]\tLoss: 4.6971\tLR: 7.397442\n",
      "Training Epoch: 74 [49024/50000]\tLoss: 4.7196\tLR: 7.397698\n",
      "Training Epoch: 74 [49152/50000]\tLoss: 4.7127\tLR: 7.397954\n",
      "Training Epoch: 74 [49280/50000]\tLoss: 4.7404\tLR: 7.398210\n",
      "Training Epoch: 74 [49408/50000]\tLoss: 4.7292\tLR: 7.398465\n",
      "Training Epoch: 74 [49536/50000]\tLoss: 4.7045\tLR: 7.398721\n",
      "Training Epoch: 74 [49664/50000]\tLoss: 4.6757\tLR: 7.398977\n",
      "Training Epoch: 74 [49792/50000]\tLoss: 4.7638\tLR: 7.399233\n",
      "Training Epoch: 74 [49920/50000]\tLoss: 4.7362\tLR: 7.399488\n",
      "Training Epoch: 74 [50000/50000]\tLoss: 4.7058\tLR: 7.399744\n",
      "epoch 74 training time consumed: 488.89s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  103743 GB |  103743 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  103424 GB |  103424 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     318 GB |     318 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  103743 GB |  103743 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  103424 GB |  103424 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     318 GB |     318 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  102284 GB |  102284 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  101966 GB |  101966 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     318 GB |     318 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   11000 K  |   11000 K  |\n",
      "|       from large pool |      24    |      65    |    4689 K  |    4689 K  |\n",
      "|       from small pool |     231    |     274    |    6311 K  |    6310 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   11000 K  |   11000 K  |\n",
      "|       from large pool |      24    |      65    |    4689 K  |    4689 K  |\n",
      "|       from small pool |     231    |     274    |    6311 K  |    6310 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      47    |    6376 K  |    6376 K  |\n",
      "|       from large pool |      10    |      23    |    2254 K  |    2254 K  |\n",
      "|       from small pool |      26    |      35    |    4122 K  |    4122 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 74, Average loss: 0.0376, Accuracy: 0.0100, Time consumed:31.19s\n",
      "\n",
      "Training Epoch: 75 [128/50000]\tLoss: 4.6293\tLR: 0.020000\n",
      "Training Epoch: 75 [256/50000]\tLoss: 4.7609\tLR: 7.400256\n",
      "Training Epoch: 75 [384/50000]\tLoss: 4.6851\tLR: 7.400512\n",
      "Training Epoch: 75 [512/50000]\tLoss: 4.7611\tLR: 7.400767\n",
      "Training Epoch: 75 [640/50000]\tLoss: 4.8462\tLR: 7.401023\n",
      "Training Epoch: 75 [768/50000]\tLoss: 4.7285\tLR: 7.401279\n",
      "Training Epoch: 75 [896/50000]\tLoss: 4.7747\tLR: 7.401535\n",
      "Training Epoch: 75 [1024/50000]\tLoss: 4.7246\tLR: 7.401790\n",
      "Training Epoch: 75 [1152/50000]\tLoss: 4.7324\tLR: 7.402046\n",
      "Training Epoch: 75 [1280/50000]\tLoss: 4.7533\tLR: 7.402302\n",
      "Training Epoch: 75 [1408/50000]\tLoss: 4.8096\tLR: 7.402558\n",
      "Training Epoch: 75 [1536/50000]\tLoss: 4.7809\tLR: 7.402813\n",
      "Training Epoch: 75 [1664/50000]\tLoss: 4.7206\tLR: 7.403069\n",
      "Training Epoch: 75 [1792/50000]\tLoss: 4.6624\tLR: 7.403325\n",
      "Training Epoch: 75 [1920/50000]\tLoss: 4.7299\tLR: 7.403581\n",
      "Training Epoch: 75 [2048/50000]\tLoss: 4.7189\tLR: 7.403836\n",
      "Training Epoch: 75 [2176/50000]\tLoss: 4.7388\tLR: 7.404092\n",
      "Training Epoch: 75 [2304/50000]\tLoss: 4.7737\tLR: 7.404348\n",
      "Training Epoch: 75 [2432/50000]\tLoss: 4.8079\tLR: 7.404604\n",
      "Training Epoch: 75 [2560/50000]\tLoss: 4.6519\tLR: 7.404859\n",
      "Training Epoch: 75 [2688/50000]\tLoss: 4.7284\tLR: 7.405115\n",
      "Training Epoch: 75 [2816/50000]\tLoss: 4.7421\tLR: 7.405371\n",
      "Training Epoch: 75 [2944/50000]\tLoss: 4.7257\tLR: 7.405627\n",
      "Training Epoch: 75 [3072/50000]\tLoss: 4.7645\tLR: 7.405882\n",
      "Training Epoch: 75 [3200/50000]\tLoss: 4.6638\tLR: 7.406138\n",
      "Training Epoch: 75 [3328/50000]\tLoss: 4.7969\tLR: 7.406394\n",
      "Training Epoch: 75 [3456/50000]\tLoss: 4.7632\tLR: 7.406650\n",
      "Training Epoch: 75 [3584/50000]\tLoss: 4.7315\tLR: 7.406905\n",
      "Training Epoch: 75 [3712/50000]\tLoss: 4.7755\tLR: 7.407161\n",
      "Training Epoch: 75 [3840/50000]\tLoss: 4.6622\tLR: 7.407417\n",
      "Training Epoch: 75 [3968/50000]\tLoss: 4.8176\tLR: 7.407673\n",
      "Training Epoch: 75 [4096/50000]\tLoss: 4.6999\tLR: 7.407928\n",
      "Training Epoch: 75 [4224/50000]\tLoss: 4.7730\tLR: 7.408184\n",
      "Training Epoch: 75 [4352/50000]\tLoss: 4.7681\tLR: 7.408440\n",
      "Training Epoch: 75 [4480/50000]\tLoss: 4.8328\tLR: 7.408696\n",
      "Training Epoch: 75 [4608/50000]\tLoss: 4.6885\tLR: 7.408951\n",
      "Training Epoch: 75 [4736/50000]\tLoss: 4.7510\tLR: 7.409207\n",
      "Training Epoch: 75 [4864/50000]\tLoss: 4.7846\tLR: 7.409463\n",
      "Training Epoch: 75 [4992/50000]\tLoss: 4.6950\tLR: 7.409719\n",
      "Training Epoch: 75 [5120/50000]\tLoss: 4.7923\tLR: 7.409974\n",
      "Training Epoch: 75 [5248/50000]\tLoss: 4.7993\tLR: 7.410230\n",
      "Training Epoch: 75 [5376/50000]\tLoss: 4.7558\tLR: 7.410486\n",
      "Training Epoch: 75 [5504/50000]\tLoss: 4.7545\tLR: 7.410742\n",
      "Training Epoch: 75 [5632/50000]\tLoss: 4.7715\tLR: 7.410997\n",
      "Training Epoch: 75 [5760/50000]\tLoss: 4.8079\tLR: 7.411253\n",
      "Training Epoch: 75 [5888/50000]\tLoss: 4.7380\tLR: 7.411509\n",
      "Training Epoch: 75 [6016/50000]\tLoss: 4.7860\tLR: 7.411765\n",
      "Training Epoch: 75 [6144/50000]\tLoss: 4.7221\tLR: 7.412020\n",
      "Training Epoch: 75 [6272/50000]\tLoss: 4.7240\tLR: 7.412276\n",
      "Training Epoch: 75 [6400/50000]\tLoss: 4.7813\tLR: 7.412532\n",
      "Training Epoch: 75 [6528/50000]\tLoss: 4.7222\tLR: 7.412788\n",
      "Training Epoch: 75 [6656/50000]\tLoss: 4.7183\tLR: 7.413043\n",
      "Training Epoch: 75 [6784/50000]\tLoss: 4.7418\tLR: 7.413299\n",
      "Training Epoch: 75 [6912/50000]\tLoss: 4.8195\tLR: 7.413555\n",
      "Training Epoch: 75 [7040/50000]\tLoss: 4.7881\tLR: 7.413811\n",
      "Training Epoch: 75 [7168/50000]\tLoss: 4.7293\tLR: 7.414066\n",
      "Training Epoch: 75 [7296/50000]\tLoss: 4.6778\tLR: 7.414322\n",
      "Training Epoch: 75 [7424/50000]\tLoss: 4.7600\tLR: 7.414578\n",
      "Training Epoch: 75 [7552/50000]\tLoss: 4.7020\tLR: 7.414834\n",
      "Training Epoch: 75 [7680/50000]\tLoss: 4.8959\tLR: 7.415090\n",
      "Training Epoch: 75 [7808/50000]\tLoss: 4.7425\tLR: 7.415345\n",
      "Training Epoch: 75 [7936/50000]\tLoss: 4.6249\tLR: 7.415601\n",
      "Training Epoch: 75 [8064/50000]\tLoss: 4.8126\tLR: 7.415857\n",
      "Training Epoch: 75 [8192/50000]\tLoss: 4.7313\tLR: 7.416113\n",
      "Training Epoch: 75 [8320/50000]\tLoss: 4.7176\tLR: 7.416368\n",
      "Training Epoch: 75 [8448/50000]\tLoss: 4.7683\tLR: 7.416624\n",
      "Training Epoch: 75 [8576/50000]\tLoss: 4.6369\tLR: 7.416880\n",
      "Training Epoch: 75 [8704/50000]\tLoss: 4.7311\tLR: 7.417136\n",
      "Training Epoch: 75 [8832/50000]\tLoss: 4.7409\tLR: 7.417391\n",
      "Training Epoch: 75 [8960/50000]\tLoss: 4.7064\tLR: 7.417647\n",
      "Training Epoch: 75 [9088/50000]\tLoss: 4.7088\tLR: 7.417903\n",
      "Training Epoch: 75 [9216/50000]\tLoss: 4.7662\tLR: 7.418159\n",
      "Training Epoch: 75 [9344/50000]\tLoss: 4.6728\tLR: 7.418414\n",
      "Training Epoch: 75 [9472/50000]\tLoss: 4.7291\tLR: 7.418670\n",
      "Training Epoch: 75 [9600/50000]\tLoss: 4.8021\tLR: 7.418926\n",
      "Training Epoch: 75 [9728/50000]\tLoss: 4.7253\tLR: 7.419182\n",
      "Training Epoch: 75 [9856/50000]\tLoss: 4.7770\tLR: 7.419437\n",
      "Training Epoch: 75 [9984/50000]\tLoss: 4.7198\tLR: 7.419693\n",
      "Training Epoch: 75 [10112/50000]\tLoss: 4.7927\tLR: 7.419949\n",
      "Training Epoch: 75 [10240/50000]\tLoss: 4.7331\tLR: 7.420205\n",
      "Training Epoch: 75 [10368/50000]\tLoss: 4.7201\tLR: 7.420460\n",
      "Training Epoch: 75 [10496/50000]\tLoss: 4.7285\tLR: 7.420716\n",
      "Training Epoch: 75 [10624/50000]\tLoss: 4.7596\tLR: 7.420972\n",
      "Training Epoch: 75 [10752/50000]\tLoss: 4.7453\tLR: 7.421228\n",
      "Training Epoch: 75 [10880/50000]\tLoss: 4.6512\tLR: 7.421483\n",
      "Training Epoch: 75 [11008/50000]\tLoss: 4.8369\tLR: 7.421739\n",
      "Training Epoch: 75 [11136/50000]\tLoss: 4.8177\tLR: 7.421995\n",
      "Training Epoch: 75 [11264/50000]\tLoss: 4.7395\tLR: 7.422251\n",
      "Training Epoch: 75 [11392/50000]\tLoss: 4.7058\tLR: 7.422506\n",
      "Training Epoch: 75 [11520/50000]\tLoss: 4.7283\tLR: 7.422762\n",
      "Training Epoch: 75 [11648/50000]\tLoss: 4.7915\tLR: 7.423018\n",
      "Training Epoch: 75 [11776/50000]\tLoss: 4.7314\tLR: 7.423274\n",
      "Training Epoch: 75 [11904/50000]\tLoss: 4.8098\tLR: 7.423529\n",
      "Training Epoch: 75 [12032/50000]\tLoss: 4.6706\tLR: 7.423785\n",
      "Training Epoch: 75 [12160/50000]\tLoss: 4.6957\tLR: 7.424041\n",
      "Training Epoch: 75 [12288/50000]\tLoss: 4.7387\tLR: 7.424297\n",
      "Training Epoch: 75 [12416/50000]\tLoss: 4.8635\tLR: 7.424552\n",
      "Training Epoch: 75 [12544/50000]\tLoss: 4.7533\tLR: 7.424808\n",
      "Training Epoch: 75 [12672/50000]\tLoss: 4.7502\tLR: 7.425064\n",
      "Training Epoch: 75 [12800/50000]\tLoss: 4.7508\tLR: 7.425320\n",
      "Training Epoch: 75 [12928/50000]\tLoss: 4.7693\tLR: 7.425575\n",
      "Training Epoch: 75 [13056/50000]\tLoss: 4.6988\tLR: 7.425831\n",
      "Training Epoch: 75 [13184/50000]\tLoss: 4.7183\tLR: 7.426087\n",
      "Training Epoch: 75 [13312/50000]\tLoss: 4.7818\tLR: 7.426343\n",
      "Training Epoch: 75 [13440/50000]\tLoss: 4.8502\tLR: 7.426598\n",
      "Training Epoch: 75 [13568/50000]\tLoss: 4.7629\tLR: 7.426854\n",
      "Training Epoch: 75 [13696/50000]\tLoss: 4.7916\tLR: 7.427110\n",
      "Training Epoch: 75 [13824/50000]\tLoss: 4.7964\tLR: 7.427366\n",
      "Training Epoch: 75 [13952/50000]\tLoss: 4.8159\tLR: 7.427621\n",
      "Training Epoch: 75 [14080/50000]\tLoss: 4.7465\tLR: 7.427877\n",
      "Training Epoch: 75 [14208/50000]\tLoss: 4.8191\tLR: 7.428133\n",
      "Training Epoch: 75 [14336/50000]\tLoss: 4.7460\tLR: 7.428389\n",
      "Training Epoch: 75 [14464/50000]\tLoss: 4.7269\tLR: 7.428645\n",
      "Training Epoch: 75 [14592/50000]\tLoss: 4.7568\tLR: 7.428900\n",
      "Training Epoch: 75 [14720/50000]\tLoss: 4.7638\tLR: 7.429156\n",
      "Training Epoch: 75 [14848/50000]\tLoss: 4.7108\tLR: 7.429412\n",
      "Training Epoch: 75 [14976/50000]\tLoss: 4.7117\tLR: 7.429668\n",
      "Training Epoch: 75 [15104/50000]\tLoss: 4.7241\tLR: 7.429923\n",
      "Training Epoch: 75 [15232/50000]\tLoss: 4.7202\tLR: 7.430179\n",
      "Training Epoch: 75 [15360/50000]\tLoss: 4.7511\tLR: 7.430435\n",
      "Training Epoch: 75 [15488/50000]\tLoss: 4.8082\tLR: 7.430691\n",
      "Training Epoch: 75 [15616/50000]\tLoss: 4.7550\tLR: 7.430946\n",
      "Training Epoch: 75 [15744/50000]\tLoss: 4.7051\tLR: 7.431202\n",
      "Training Epoch: 75 [15872/50000]\tLoss: 4.7582\tLR: 7.431458\n",
      "Training Epoch: 75 [16000/50000]\tLoss: 4.7272\tLR: 7.431714\n",
      "Training Epoch: 75 [16128/50000]\tLoss: 4.7562\tLR: 7.431969\n",
      "Training Epoch: 75 [16256/50000]\tLoss: 4.7512\tLR: 7.432225\n",
      "Training Epoch: 75 [16384/50000]\tLoss: 4.7304\tLR: 7.432481\n",
      "Training Epoch: 75 [16512/50000]\tLoss: 4.8452\tLR: 7.432737\n",
      "Training Epoch: 75 [16640/50000]\tLoss: 4.7541\tLR: 7.432992\n",
      "Training Epoch: 75 [16768/50000]\tLoss: 4.7034\tLR: 7.433248\n",
      "Training Epoch: 75 [16896/50000]\tLoss: 4.7730\tLR: 7.433504\n",
      "Training Epoch: 75 [17024/50000]\tLoss: 4.7083\tLR: 7.433760\n",
      "Training Epoch: 75 [17152/50000]\tLoss: 4.7140\tLR: 7.434015\n",
      "Training Epoch: 75 [17280/50000]\tLoss: 4.7383\tLR: 7.434271\n",
      "Training Epoch: 75 [17408/50000]\tLoss: 4.7332\tLR: 7.434527\n",
      "Training Epoch: 75 [17536/50000]\tLoss: 4.8143\tLR: 7.434783\n",
      "Training Epoch: 75 [17664/50000]\tLoss: 4.8564\tLR: 7.435038\n",
      "Training Epoch: 75 [17792/50000]\tLoss: 4.8125\tLR: 7.435294\n",
      "Training Epoch: 75 [17920/50000]\tLoss: 4.7364\tLR: 7.435550\n",
      "Training Epoch: 75 [18048/50000]\tLoss: 4.7348\tLR: 7.435806\n",
      "Training Epoch: 75 [18176/50000]\tLoss: 4.7853\tLR: 7.436061\n",
      "Training Epoch: 75 [18304/50000]\tLoss: 4.7376\tLR: 7.436317\n",
      "Training Epoch: 75 [18432/50000]\tLoss: 4.7771\tLR: 7.436573\n",
      "Training Epoch: 75 [18560/50000]\tLoss: 4.6782\tLR: 7.436829\n",
      "Training Epoch: 75 [18688/50000]\tLoss: 4.8079\tLR: 7.437084\n",
      "Training Epoch: 75 [18816/50000]\tLoss: 4.8097\tLR: 7.437340\n",
      "Training Epoch: 75 [18944/50000]\tLoss: 4.6929\tLR: 7.437596\n",
      "Training Epoch: 75 [19072/50000]\tLoss: 4.7656\tLR: 7.437852\n",
      "Training Epoch: 75 [19200/50000]\tLoss: 4.7919\tLR: 7.438107\n",
      "Training Epoch: 75 [19328/50000]\tLoss: 4.8184\tLR: 7.438363\n",
      "Training Epoch: 75 [19456/50000]\tLoss: 4.7511\tLR: 7.438619\n",
      "Training Epoch: 75 [19584/50000]\tLoss: 4.8133\tLR: 7.438875\n",
      "Training Epoch: 75 [19712/50000]\tLoss: 4.8307\tLR: 7.439130\n",
      "Training Epoch: 75 [19840/50000]\tLoss: 4.8393\tLR: 7.439386\n",
      "Training Epoch: 75 [19968/50000]\tLoss: 4.7826\tLR: 7.439642\n",
      "Training Epoch: 75 [20096/50000]\tLoss: 4.7735\tLR: 7.439898\n",
      "Training Epoch: 75 [20224/50000]\tLoss: 4.7707\tLR: 7.440153\n",
      "Training Epoch: 75 [20352/50000]\tLoss: 4.7845\tLR: 7.440409\n",
      "Training Epoch: 75 [20480/50000]\tLoss: 4.7595\tLR: 7.440665\n",
      "Training Epoch: 75 [20608/50000]\tLoss: 4.7384\tLR: 7.440921\n",
      "Training Epoch: 75 [20736/50000]\tLoss: 4.7471\tLR: 7.441176\n",
      "Training Epoch: 75 [20864/50000]\tLoss: 4.8365\tLR: 7.441432\n",
      "Training Epoch: 75 [20992/50000]\tLoss: 4.7923\tLR: 7.441688\n",
      "Training Epoch: 75 [21120/50000]\tLoss: 4.7459\tLR: 7.441944\n",
      "Training Epoch: 75 [21248/50000]\tLoss: 4.7344\tLR: 7.442199\n",
      "Training Epoch: 75 [21376/50000]\tLoss: 4.8521\tLR: 7.442455\n",
      "Training Epoch: 75 [21504/50000]\tLoss: 4.8546\tLR: 7.442711\n",
      "Training Epoch: 75 [21632/50000]\tLoss: 4.7984\tLR: 7.442967\n",
      "Training Epoch: 75 [21760/50000]\tLoss: 4.8312\tLR: 7.443223\n",
      "Training Epoch: 75 [21888/50000]\tLoss: 4.7671\tLR: 7.443478\n",
      "Training Epoch: 75 [22016/50000]\tLoss: 4.7516\tLR: 7.443734\n",
      "Training Epoch: 75 [22144/50000]\tLoss: 4.7437\tLR: 7.443990\n",
      "Training Epoch: 75 [22272/50000]\tLoss: 4.8073\tLR: 7.444246\n",
      "Training Epoch: 75 [22400/50000]\tLoss: 4.7754\tLR: 7.444501\n",
      "Training Epoch: 75 [22528/50000]\tLoss: 4.7180\tLR: 7.444757\n",
      "Training Epoch: 75 [22656/50000]\tLoss: 4.7319\tLR: 7.445013\n",
      "Training Epoch: 75 [22784/50000]\tLoss: 4.6587\tLR: 7.445269\n",
      "Training Epoch: 75 [22912/50000]\tLoss: 4.7526\tLR: 7.445524\n",
      "Training Epoch: 75 [23040/50000]\tLoss: 4.7898\tLR: 7.445780\n",
      "Training Epoch: 75 [23168/50000]\tLoss: 4.7541\tLR: 7.446036\n",
      "Training Epoch: 75 [23296/50000]\tLoss: 4.8383\tLR: 7.446292\n",
      "Training Epoch: 75 [23424/50000]\tLoss: 4.7416\tLR: 7.446547\n",
      "Training Epoch: 75 [23552/50000]\tLoss: 4.8274\tLR: 7.446803\n",
      "Training Epoch: 75 [23680/50000]\tLoss: 4.6823\tLR: 7.447059\n",
      "Training Epoch: 75 [23808/50000]\tLoss: 4.7713\tLR: 7.447315\n",
      "Training Epoch: 75 [23936/50000]\tLoss: 4.7392\tLR: 7.447570\n",
      "Training Epoch: 75 [24064/50000]\tLoss: 4.7940\tLR: 7.447826\n",
      "Training Epoch: 75 [24192/50000]\tLoss: 4.7912\tLR: 7.448082\n",
      "Training Epoch: 75 [24320/50000]\tLoss: 4.8190\tLR: 7.448338\n",
      "Training Epoch: 75 [24448/50000]\tLoss: 4.7895\tLR: 7.448593\n",
      "Training Epoch: 75 [24576/50000]\tLoss: 4.7806\tLR: 7.448849\n",
      "Training Epoch: 75 [24704/50000]\tLoss: 4.7887\tLR: 7.449105\n",
      "Training Epoch: 75 [24832/50000]\tLoss: 4.7177\tLR: 7.449361\n",
      "Training Epoch: 75 [24960/50000]\tLoss: 4.7563\tLR: 7.449616\n",
      "Training Epoch: 75 [25088/50000]\tLoss: 4.7492\tLR: 7.449872\n",
      "Training Epoch: 75 [25216/50000]\tLoss: 4.7834\tLR: 7.450128\n",
      "Training Epoch: 75 [25344/50000]\tLoss: 4.7251\tLR: 7.450384\n",
      "Training Epoch: 75 [25472/50000]\tLoss: 4.8091\tLR: 7.450639\n",
      "Training Epoch: 75 [25600/50000]\tLoss: 4.7648\tLR: 7.450895\n",
      "Training Epoch: 75 [25728/50000]\tLoss: 4.6591\tLR: 7.451151\n",
      "Training Epoch: 75 [25856/50000]\tLoss: 4.8178\tLR: 7.451407\n",
      "Training Epoch: 75 [25984/50000]\tLoss: 4.7854\tLR: 7.451662\n",
      "Training Epoch: 75 [26112/50000]\tLoss: 4.7468\tLR: 7.451918\n",
      "Training Epoch: 75 [26240/50000]\tLoss: 4.7481\tLR: 7.452174\n",
      "Training Epoch: 75 [26368/50000]\tLoss: 4.7130\tLR: 7.452430\n",
      "Training Epoch: 75 [26496/50000]\tLoss: 4.7424\tLR: 7.452685\n",
      "Training Epoch: 75 [26624/50000]\tLoss: 4.7707\tLR: 7.452941\n",
      "Training Epoch: 75 [26752/50000]\tLoss: 4.8113\tLR: 7.453197\n",
      "Training Epoch: 75 [26880/50000]\tLoss: 4.7520\tLR: 7.453453\n",
      "Training Epoch: 75 [27008/50000]\tLoss: 4.7796\tLR: 7.453708\n",
      "Training Epoch: 75 [27136/50000]\tLoss: 4.7323\tLR: 7.453964\n",
      "Training Epoch: 75 [27264/50000]\tLoss: 4.6502\tLR: 7.454220\n",
      "Training Epoch: 75 [27392/50000]\tLoss: 4.8086\tLR: 7.454476\n",
      "Training Epoch: 75 [27520/50000]\tLoss: 4.7049\tLR: 7.454731\n",
      "Training Epoch: 75 [27648/50000]\tLoss: 4.7428\tLR: 7.454987\n",
      "Training Epoch: 75 [27776/50000]\tLoss: 4.8092\tLR: 7.455243\n",
      "Training Epoch: 75 [27904/50000]\tLoss: 4.7534\tLR: 7.455499\n",
      "Training Epoch: 75 [28032/50000]\tLoss: 4.7544\tLR: 7.455754\n",
      "Training Epoch: 75 [28160/50000]\tLoss: 4.7673\tLR: 7.456010\n",
      "Training Epoch: 75 [28288/50000]\tLoss: 4.7473\tLR: 7.456266\n",
      "Training Epoch: 75 [28416/50000]\tLoss: 4.7110\tLR: 7.456522\n",
      "Training Epoch: 75 [28544/50000]\tLoss: 4.7507\tLR: 7.456777\n",
      "Training Epoch: 75 [28672/50000]\tLoss: 4.6960\tLR: 7.457033\n",
      "Training Epoch: 75 [28800/50000]\tLoss: 4.7993\tLR: 7.457289\n",
      "Training Epoch: 75 [28928/50000]\tLoss: 4.7938\tLR: 7.457545\n",
      "Training Epoch: 75 [29056/50000]\tLoss: 4.8082\tLR: 7.457801\n",
      "Training Epoch: 75 [29184/50000]\tLoss: 4.8383\tLR: 7.458056\n",
      "Training Epoch: 75 [29312/50000]\tLoss: 4.8341\tLR: 7.458312\n",
      "Training Epoch: 75 [29440/50000]\tLoss: 4.7025\tLR: 7.458568\n",
      "Training Epoch: 75 [29568/50000]\tLoss: 4.7541\tLR: 7.458824\n",
      "Training Epoch: 75 [29696/50000]\tLoss: 4.6730\tLR: 7.459079\n",
      "Training Epoch: 75 [29824/50000]\tLoss: 4.6895\tLR: 7.459335\n",
      "Training Epoch: 75 [29952/50000]\tLoss: 4.8252\tLR: 7.459591\n",
      "Training Epoch: 75 [30080/50000]\tLoss: 4.7635\tLR: 7.459847\n",
      "Training Epoch: 75 [30208/50000]\tLoss: 4.6819\tLR: 7.460102\n",
      "Training Epoch: 75 [30336/50000]\tLoss: 4.7359\tLR: 7.460358\n",
      "Training Epoch: 75 [30464/50000]\tLoss: 4.7317\tLR: 7.460614\n",
      "Training Epoch: 75 [30592/50000]\tLoss: 4.8157\tLR: 7.460870\n",
      "Training Epoch: 75 [30720/50000]\tLoss: 4.7260\tLR: 7.461125\n",
      "Training Epoch: 75 [30848/50000]\tLoss: 4.8021\tLR: 7.461381\n",
      "Training Epoch: 75 [30976/50000]\tLoss: 4.7793\tLR: 7.461637\n",
      "Training Epoch: 75 [31104/50000]\tLoss: 4.8360\tLR: 7.461893\n",
      "Training Epoch: 75 [31232/50000]\tLoss: 4.7614\tLR: 7.462148\n",
      "Training Epoch: 75 [31360/50000]\tLoss: 4.7220\tLR: 7.462404\n",
      "Training Epoch: 75 [31488/50000]\tLoss: 4.7465\tLR: 7.462660\n",
      "Training Epoch: 75 [31616/50000]\tLoss: 4.8039\tLR: 7.462916\n",
      "Training Epoch: 75 [31744/50000]\tLoss: 4.7951\tLR: 7.463171\n",
      "Training Epoch: 75 [31872/50000]\tLoss: 4.7987\tLR: 7.463427\n",
      "Training Epoch: 75 [32000/50000]\tLoss: 4.8539\tLR: 7.463683\n",
      "Training Epoch: 75 [32128/50000]\tLoss: 4.6917\tLR: 7.463939\n",
      "Training Epoch: 75 [32256/50000]\tLoss: 4.7806\tLR: 7.464194\n",
      "Training Epoch: 75 [32384/50000]\tLoss: 4.6757\tLR: 7.464450\n",
      "Training Epoch: 75 [32512/50000]\tLoss: 4.7018\tLR: 7.464706\n",
      "Training Epoch: 75 [32640/50000]\tLoss: 4.7668\tLR: 7.464962\n",
      "Training Epoch: 75 [32768/50000]\tLoss: 4.7250\tLR: 7.465217\n",
      "Training Epoch: 75 [32896/50000]\tLoss: 4.8327\tLR: 7.465473\n",
      "Training Epoch: 75 [33024/50000]\tLoss: 4.7343\tLR: 7.465729\n",
      "Training Epoch: 75 [33152/50000]\tLoss: 4.7155\tLR: 7.465985\n",
      "Training Epoch: 75 [33280/50000]\tLoss: 4.6994\tLR: 7.466240\n",
      "Training Epoch: 75 [33408/50000]\tLoss: 4.7606\tLR: 7.466496\n",
      "Training Epoch: 75 [33536/50000]\tLoss: 4.7911\tLR: 7.466752\n",
      "Training Epoch: 75 [33664/50000]\tLoss: 4.6636\tLR: 7.467008\n",
      "Training Epoch: 75 [33792/50000]\tLoss: 4.7613\tLR: 7.467263\n",
      "Training Epoch: 75 [33920/50000]\tLoss: 4.8001\tLR: 7.467519\n",
      "Training Epoch: 75 [34048/50000]\tLoss: 4.7909\tLR: 7.467775\n",
      "Training Epoch: 75 [34176/50000]\tLoss: 4.7188\tLR: 7.468031\n",
      "Training Epoch: 75 [34304/50000]\tLoss: 4.7446\tLR: 7.468286\n",
      "Training Epoch: 75 [34432/50000]\tLoss: 4.7234\tLR: 7.468542\n",
      "Training Epoch: 75 [34560/50000]\tLoss: 4.7854\tLR: 7.468798\n",
      "Training Epoch: 75 [34688/50000]\tLoss: 4.7665\tLR: 7.469054\n",
      "Training Epoch: 75 [34816/50000]\tLoss: 4.8015\tLR: 7.469309\n",
      "Training Epoch: 75 [34944/50000]\tLoss: 4.8461\tLR: 7.469565\n",
      "Training Epoch: 75 [35072/50000]\tLoss: 4.7543\tLR: 7.469821\n",
      "Training Epoch: 75 [35200/50000]\tLoss: 4.7014\tLR: 7.470077\n",
      "Training Epoch: 75 [35328/50000]\tLoss: 4.6744\tLR: 7.470332\n",
      "Training Epoch: 75 [35456/50000]\tLoss: 4.6134\tLR: 7.470588\n",
      "Training Epoch: 75 [35584/50000]\tLoss: 4.7844\tLR: 7.470844\n",
      "Training Epoch: 75 [35712/50000]\tLoss: 4.6844\tLR: 7.471100\n",
      "Training Epoch: 75 [35840/50000]\tLoss: 4.7579\tLR: 7.471355\n",
      "Training Epoch: 75 [35968/50000]\tLoss: 4.7117\tLR: 7.471611\n",
      "Training Epoch: 75 [36096/50000]\tLoss: 4.6854\tLR: 7.471867\n",
      "Training Epoch: 75 [36224/50000]\tLoss: 4.7777\tLR: 7.472123\n",
      "Training Epoch: 75 [36352/50000]\tLoss: 4.8226\tLR: 7.472379\n",
      "Training Epoch: 75 [36480/50000]\tLoss: 4.7738\tLR: 7.472634\n",
      "Training Epoch: 75 [36608/50000]\tLoss: 4.8147\tLR: 7.472890\n",
      "Training Epoch: 75 [36736/50000]\tLoss: 4.7589\tLR: 7.473146\n",
      "Training Epoch: 75 [36864/50000]\tLoss: 4.7124\tLR: 7.473402\n",
      "Training Epoch: 75 [36992/50000]\tLoss: 4.6984\tLR: 7.473657\n",
      "Training Epoch: 75 [37120/50000]\tLoss: 4.7019\tLR: 7.473913\n",
      "Training Epoch: 75 [37248/50000]\tLoss: 4.6413\tLR: 7.474169\n",
      "Training Epoch: 75 [37376/50000]\tLoss: 4.7600\tLR: 7.474425\n",
      "Training Epoch: 75 [37504/50000]\tLoss: 4.7429\tLR: 7.474680\n",
      "Training Epoch: 75 [37632/50000]\tLoss: 4.8499\tLR: 7.474936\n",
      "Training Epoch: 75 [37760/50000]\tLoss: 4.8008\tLR: 7.475192\n",
      "Training Epoch: 75 [37888/50000]\tLoss: 4.7349\tLR: 7.475448\n",
      "Training Epoch: 75 [38016/50000]\tLoss: 4.7261\tLR: 7.475703\n",
      "Training Epoch: 75 [38144/50000]\tLoss: 4.6750\tLR: 7.475959\n",
      "Training Epoch: 75 [38272/50000]\tLoss: 4.8767\tLR: 7.476215\n",
      "Training Epoch: 75 [38400/50000]\tLoss: 4.7025\tLR: 7.476471\n",
      "Training Epoch: 75 [38528/50000]\tLoss: 4.8085\tLR: 7.476726\n",
      "Training Epoch: 75 [38656/50000]\tLoss: 4.7091\tLR: 7.476982\n",
      "Training Epoch: 75 [38784/50000]\tLoss: 4.8197\tLR: 7.477238\n",
      "Training Epoch: 75 [38912/50000]\tLoss: 4.7342\tLR: 7.477494\n",
      "Training Epoch: 75 [39040/50000]\tLoss: 4.8799\tLR: 7.477749\n",
      "Training Epoch: 75 [39168/50000]\tLoss: 4.7461\tLR: 7.478005\n",
      "Training Epoch: 75 [39296/50000]\tLoss: 4.7533\tLR: 7.478261\n",
      "Training Epoch: 75 [39424/50000]\tLoss: 4.7574\tLR: 7.478517\n",
      "Training Epoch: 75 [39552/50000]\tLoss: 4.7032\tLR: 7.478772\n",
      "Training Epoch: 75 [39680/50000]\tLoss: 4.7514\tLR: 7.479028\n",
      "Training Epoch: 75 [39808/50000]\tLoss: 4.8150\tLR: 7.479284\n",
      "Training Epoch: 75 [39936/50000]\tLoss: 4.7235\tLR: 7.479540\n",
      "Training Epoch: 75 [40064/50000]\tLoss: 4.7811\tLR: 7.479795\n",
      "Training Epoch: 75 [40192/50000]\tLoss: 4.7271\tLR: 7.480051\n",
      "Training Epoch: 75 [40320/50000]\tLoss: 4.7611\tLR: 7.480307\n",
      "Training Epoch: 75 [40448/50000]\tLoss: 4.7327\tLR: 7.480563\n",
      "Training Epoch: 75 [40576/50000]\tLoss: 4.7385\tLR: 7.480818\n",
      "Training Epoch: 75 [40704/50000]\tLoss: 4.6959\tLR: 7.481074\n",
      "Training Epoch: 75 [40832/50000]\tLoss: 4.7484\tLR: 7.481330\n",
      "Training Epoch: 75 [40960/50000]\tLoss: 4.8480\tLR: 7.481586\n",
      "Training Epoch: 75 [41088/50000]\tLoss: 4.7818\tLR: 7.481841\n",
      "Training Epoch: 75 [41216/50000]\tLoss: 4.8196\tLR: 7.482097\n",
      "Training Epoch: 75 [41344/50000]\tLoss: 4.8253\tLR: 7.482353\n",
      "Training Epoch: 75 [41472/50000]\tLoss: 4.7668\tLR: 7.482609\n",
      "Training Epoch: 75 [41600/50000]\tLoss: 4.6998\tLR: 7.482864\n",
      "Training Epoch: 75 [41728/50000]\tLoss: 4.7595\tLR: 7.483120\n",
      "Training Epoch: 75 [41856/50000]\tLoss: 4.7337\tLR: 7.483376\n",
      "Training Epoch: 75 [41984/50000]\tLoss: 4.7248\tLR: 7.483632\n",
      "Training Epoch: 75 [42112/50000]\tLoss: 4.8352\tLR: 7.483887\n",
      "Training Epoch: 75 [42240/50000]\tLoss: 4.8203\tLR: 7.484143\n",
      "Training Epoch: 75 [42368/50000]\tLoss: 4.7793\tLR: 7.484399\n",
      "Training Epoch: 75 [42496/50000]\tLoss: 4.7388\tLR: 7.484655\n",
      "Training Epoch: 75 [42624/50000]\tLoss: 4.7840\tLR: 7.484910\n",
      "Training Epoch: 75 [42752/50000]\tLoss: 4.8496\tLR: 7.485166\n",
      "Training Epoch: 75 [42880/50000]\tLoss: 4.7736\tLR: 7.485422\n",
      "Training Epoch: 75 [43008/50000]\tLoss: 4.8203\tLR: 7.485678\n",
      "Training Epoch: 75 [43136/50000]\tLoss: 4.8111\tLR: 7.485934\n",
      "Training Epoch: 75 [43264/50000]\tLoss: 4.7000\tLR: 7.486189\n",
      "Training Epoch: 75 [43392/50000]\tLoss: 4.8045\tLR: 7.486445\n",
      "Training Epoch: 75 [43520/50000]\tLoss: 4.7655\tLR: 7.486701\n",
      "Training Epoch: 75 [43648/50000]\tLoss: 4.6728\tLR: 7.486957\n",
      "Training Epoch: 75 [43776/50000]\tLoss: 4.7127\tLR: 7.487212\n",
      "Training Epoch: 75 [43904/50000]\tLoss: 4.7549\tLR: 7.487468\n",
      "Training Epoch: 75 [44032/50000]\tLoss: 4.8542\tLR: 7.487724\n",
      "Training Epoch: 75 [44160/50000]\tLoss: 4.6995\tLR: 7.487980\n",
      "Training Epoch: 75 [44288/50000]\tLoss: 4.8266\tLR: 7.488235\n",
      "Training Epoch: 75 [44416/50000]\tLoss: 4.7328\tLR: 7.488491\n",
      "Training Epoch: 75 [44544/50000]\tLoss: 4.7239\tLR: 7.488747\n",
      "Training Epoch: 75 [44672/50000]\tLoss: 4.6660\tLR: 7.489003\n",
      "Training Epoch: 75 [44800/50000]\tLoss: 4.7718\tLR: 7.489258\n",
      "Training Epoch: 75 [44928/50000]\tLoss: 4.6989\tLR: 7.489514\n",
      "Training Epoch: 75 [45056/50000]\tLoss: 4.6282\tLR: 7.489770\n",
      "Training Epoch: 75 [45184/50000]\tLoss: 4.7772\tLR: 7.490026\n",
      "Training Epoch: 75 [45312/50000]\tLoss: 4.6925\tLR: 7.490281\n",
      "Training Epoch: 75 [45440/50000]\tLoss: 4.7282\tLR: 7.490537\n",
      "Training Epoch: 75 [45568/50000]\tLoss: 4.8001\tLR: 7.490793\n",
      "Training Epoch: 75 [45696/50000]\tLoss: 4.7712\tLR: 7.491049\n",
      "Training Epoch: 75 [45824/50000]\tLoss: 4.7330\tLR: 7.491304\n",
      "Training Epoch: 75 [45952/50000]\tLoss: 4.7600\tLR: 7.491560\n",
      "Training Epoch: 75 [46080/50000]\tLoss: 4.7375\tLR: 7.491816\n",
      "Training Epoch: 75 [46208/50000]\tLoss: 4.6635\tLR: 7.492072\n",
      "Training Epoch: 75 [46336/50000]\tLoss: 4.6991\tLR: 7.492327\n",
      "Training Epoch: 75 [46464/50000]\tLoss: 4.6239\tLR: 7.492583\n",
      "Training Epoch: 75 [46592/50000]\tLoss: 4.7709\tLR: 7.492839\n",
      "Training Epoch: 75 [46720/50000]\tLoss: 4.7148\tLR: 7.493095\n",
      "Training Epoch: 75 [46848/50000]\tLoss: 4.7544\tLR: 7.493350\n",
      "Training Epoch: 75 [46976/50000]\tLoss: 4.7573\tLR: 7.493606\n",
      "Training Epoch: 75 [47104/50000]\tLoss: 4.8103\tLR: 7.493862\n",
      "Training Epoch: 75 [47232/50000]\tLoss: 4.8074\tLR: 7.494118\n",
      "Training Epoch: 75 [47360/50000]\tLoss: 4.6780\tLR: 7.494373\n",
      "Training Epoch: 75 [47488/50000]\tLoss: 4.7461\tLR: 7.494629\n",
      "Training Epoch: 75 [47616/50000]\tLoss: 4.8037\tLR: 7.494885\n",
      "Training Epoch: 75 [47744/50000]\tLoss: 4.7767\tLR: 7.495141\n",
      "Training Epoch: 75 [47872/50000]\tLoss: 4.7277\tLR: 7.495396\n",
      "Training Epoch: 75 [48000/50000]\tLoss: 4.7773\tLR: 7.495652\n",
      "Training Epoch: 75 [48128/50000]\tLoss: 4.7029\tLR: 7.495908\n",
      "Training Epoch: 75 [48256/50000]\tLoss: 4.8443\tLR: 7.496164\n",
      "Training Epoch: 75 [48384/50000]\tLoss: 4.7275\tLR: 7.496419\n",
      "Training Epoch: 75 [48512/50000]\tLoss: 4.8160\tLR: 7.496675\n",
      "Training Epoch: 75 [48640/50000]\tLoss: 4.7032\tLR: 7.496931\n",
      "Training Epoch: 75 [48768/50000]\tLoss: 4.7992\tLR: 7.497187\n",
      "Training Epoch: 75 [48896/50000]\tLoss: 4.7556\tLR: 7.497442\n",
      "Training Epoch: 75 [49024/50000]\tLoss: 4.7314\tLR: 7.497698\n",
      "Training Epoch: 75 [49152/50000]\tLoss: 4.6986\tLR: 7.497954\n",
      "Training Epoch: 75 [49280/50000]\tLoss: 4.6488\tLR: 7.498210\n",
      "Training Epoch: 75 [49408/50000]\tLoss: 4.6325\tLR: 7.498465\n",
      "Training Epoch: 75 [49536/50000]\tLoss: 4.7036\tLR: 7.498721\n",
      "Training Epoch: 75 [49664/50000]\tLoss: 4.7453\tLR: 7.498977\n",
      "Training Epoch: 75 [49792/50000]\tLoss: 4.8512\tLR: 7.499233\n",
      "Training Epoch: 75 [49920/50000]\tLoss: 4.7386\tLR: 7.499488\n",
      "Training Epoch: 75 [50000/50000]\tLoss: 4.6476\tLR: 7.499744\n",
      "epoch 75 training time consumed: 489.09s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  105145 GB |  105145 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  104822 GB |  104822 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     322 GB |     322 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  105145 GB |  105145 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  104822 GB |  104822 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     322 GB |     322 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  103667 GB |  103667 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  103344 GB |  103344 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     322 GB |     322 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   11149 K  |   11148 K  |\n",
      "|       from large pool |      24    |      65    |    4752 K  |    4752 K  |\n",
      "|       from small pool |     231    |     274    |    6396 K  |    6396 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   11149 K  |   11148 K  |\n",
      "|       from large pool |      24    |      65    |    4752 K  |    4752 K  |\n",
      "|       from small pool |     231    |     274    |    6396 K  |    6396 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      38    |      47    |    6462 K  |    6462 K  |\n",
      "|       from large pool |      10    |      23    |    2284 K  |    2284 K  |\n",
      "|       from small pool |      28    |      35    |    4178 K  |    4177 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 75, Average loss: 0.0376, Accuracy: 0.0100, Time consumed:31.25s\n",
      "\n",
      "Training Epoch: 76 [128/50000]\tLoss: 4.7280\tLR: 0.020000\n",
      "Training Epoch: 76 [256/50000]\tLoss: 4.8273\tLR: 7.500256\n",
      "Training Epoch: 76 [384/50000]\tLoss: 4.7552\tLR: 7.500512\n",
      "Training Epoch: 76 [512/50000]\tLoss: 4.8135\tLR: 7.500767\n",
      "Training Epoch: 76 [640/50000]\tLoss: 4.7903\tLR: 7.501023\n",
      "Training Epoch: 76 [768/50000]\tLoss: 4.7602\tLR: 7.501279\n",
      "Training Epoch: 76 [896/50000]\tLoss: 4.7256\tLR: 7.501535\n",
      "Training Epoch: 76 [1024/50000]\tLoss: 4.7248\tLR: 7.501790\n",
      "Training Epoch: 76 [1152/50000]\tLoss: 4.7775\tLR: 7.502046\n",
      "Training Epoch: 76 [1280/50000]\tLoss: 4.7161\tLR: 7.502302\n",
      "Training Epoch: 76 [1408/50000]\tLoss: 4.7704\tLR: 7.502558\n",
      "Training Epoch: 76 [1536/50000]\tLoss: 4.7230\tLR: 7.502813\n",
      "Training Epoch: 76 [1664/50000]\tLoss: 4.8878\tLR: 7.503069\n",
      "Training Epoch: 76 [1792/50000]\tLoss: 4.7629\tLR: 7.503325\n",
      "Training Epoch: 76 [1920/50000]\tLoss: 4.8105\tLR: 7.503581\n",
      "Training Epoch: 76 [2048/50000]\tLoss: 4.7467\tLR: 7.503836\n",
      "Training Epoch: 76 [2176/50000]\tLoss: 4.8202\tLR: 7.504092\n",
      "Training Epoch: 76 [2304/50000]\tLoss: 4.7683\tLR: 7.504348\n",
      "Training Epoch: 76 [2432/50000]\tLoss: 4.6908\tLR: 7.504604\n",
      "Training Epoch: 76 [2560/50000]\tLoss: 4.6792\tLR: 7.504859\n",
      "Training Epoch: 76 [2688/50000]\tLoss: 4.8045\tLR: 7.505115\n",
      "Training Epoch: 76 [2816/50000]\tLoss: 4.8029\tLR: 7.505371\n",
      "Training Epoch: 76 [2944/50000]\tLoss: 4.7644\tLR: 7.505627\n",
      "Training Epoch: 76 [3072/50000]\tLoss: 4.7564\tLR: 7.505882\n",
      "Training Epoch: 76 [3200/50000]\tLoss: 4.7217\tLR: 7.506138\n",
      "Training Epoch: 76 [3328/50000]\tLoss: 4.7419\tLR: 7.506394\n",
      "Training Epoch: 76 [3456/50000]\tLoss: 4.7047\tLR: 7.506650\n",
      "Training Epoch: 76 [3584/50000]\tLoss: 4.7586\tLR: 7.506905\n",
      "Training Epoch: 76 [3712/50000]\tLoss: 4.8138\tLR: 7.507161\n",
      "Training Epoch: 76 [3840/50000]\tLoss: 4.7009\tLR: 7.507417\n",
      "Training Epoch: 76 [3968/50000]\tLoss: 4.7352\tLR: 7.507673\n",
      "Training Epoch: 76 [4096/50000]\tLoss: 4.7949\tLR: 7.507928\n",
      "Training Epoch: 76 [4224/50000]\tLoss: 4.7892\tLR: 7.508184\n",
      "Training Epoch: 76 [4352/50000]\tLoss: 4.7365\tLR: 7.508440\n",
      "Training Epoch: 76 [4480/50000]\tLoss: 4.8068\tLR: 7.508696\n",
      "Training Epoch: 76 [4608/50000]\tLoss: 4.8392\tLR: 7.508951\n",
      "Training Epoch: 76 [4736/50000]\tLoss: 4.7178\tLR: 7.509207\n",
      "Training Epoch: 76 [4864/50000]\tLoss: 4.7989\tLR: 7.509463\n",
      "Training Epoch: 76 [4992/50000]\tLoss: 4.8215\tLR: 7.509719\n",
      "Training Epoch: 76 [5120/50000]\tLoss: 4.6292\tLR: 7.509974\n",
      "Training Epoch: 76 [5248/50000]\tLoss: 4.7275\tLR: 7.510230\n",
      "Training Epoch: 76 [5376/50000]\tLoss: 4.7197\tLR: 7.510486\n",
      "Training Epoch: 76 [5504/50000]\tLoss: 4.7244\tLR: 7.510742\n",
      "Training Epoch: 76 [5632/50000]\tLoss: 4.7672\tLR: 7.510997\n",
      "Training Epoch: 76 [5760/50000]\tLoss: 4.8057\tLR: 7.511253\n",
      "Training Epoch: 76 [5888/50000]\tLoss: 4.6908\tLR: 7.511509\n",
      "Training Epoch: 76 [6016/50000]\tLoss: 4.6762\tLR: 7.511765\n",
      "Training Epoch: 76 [6144/50000]\tLoss: 4.7430\tLR: 7.512020\n",
      "Training Epoch: 76 [6272/50000]\tLoss: 4.7141\tLR: 7.512276\n",
      "Training Epoch: 76 [6400/50000]\tLoss: 4.7094\tLR: 7.512532\n",
      "Training Epoch: 76 [6528/50000]\tLoss: 4.7075\tLR: 7.512788\n",
      "Training Epoch: 76 [6656/50000]\tLoss: 4.7685\tLR: 7.513043\n",
      "Training Epoch: 76 [6784/50000]\tLoss: 4.7933\tLR: 7.513299\n",
      "Training Epoch: 76 [6912/50000]\tLoss: 4.6827\tLR: 7.513555\n",
      "Training Epoch: 76 [7040/50000]\tLoss: 4.8000\tLR: 7.513811\n",
      "Training Epoch: 76 [7168/50000]\tLoss: 4.7194\tLR: 7.514066\n",
      "Training Epoch: 76 [7296/50000]\tLoss: 4.6769\tLR: 7.514322\n",
      "Training Epoch: 76 [7424/50000]\tLoss: 4.6283\tLR: 7.514578\n",
      "Training Epoch: 76 [7552/50000]\tLoss: 4.7401\tLR: 7.514834\n",
      "Training Epoch: 76 [7680/50000]\tLoss: 4.6695\tLR: 7.515090\n",
      "Training Epoch: 76 [7808/50000]\tLoss: 4.7742\tLR: 7.515345\n",
      "Training Epoch: 76 [7936/50000]\tLoss: 4.7423\tLR: 7.515601\n",
      "Training Epoch: 76 [8064/50000]\tLoss: 4.7087\tLR: 7.515857\n",
      "Training Epoch: 76 [8192/50000]\tLoss: 4.8034\tLR: 7.516113\n",
      "Training Epoch: 76 [8320/50000]\tLoss: 4.6914\tLR: 7.516368\n",
      "Training Epoch: 76 [8448/50000]\tLoss: 4.7777\tLR: 7.516624\n",
      "Training Epoch: 76 [8576/50000]\tLoss: 4.7190\tLR: 7.516880\n",
      "Training Epoch: 76 [8704/50000]\tLoss: 4.7578\tLR: 7.517136\n",
      "Training Epoch: 76 [8832/50000]\tLoss: 4.7232\tLR: 7.517391\n",
      "Training Epoch: 76 [8960/50000]\tLoss: 4.7299\tLR: 7.517647\n",
      "Training Epoch: 76 [9088/50000]\tLoss: 4.7420\tLR: 7.517903\n",
      "Training Epoch: 76 [9216/50000]\tLoss: 4.6887\tLR: 7.518159\n",
      "Training Epoch: 76 [9344/50000]\tLoss: 4.7811\tLR: 7.518414\n",
      "Training Epoch: 76 [9472/50000]\tLoss: 4.7227\tLR: 7.518670\n",
      "Training Epoch: 76 [9600/50000]\tLoss: 4.7135\tLR: 7.518926\n",
      "Training Epoch: 76 [9728/50000]\tLoss: 4.7021\tLR: 7.519182\n",
      "Training Epoch: 76 [9856/50000]\tLoss: 4.7563\tLR: 7.519437\n",
      "Training Epoch: 76 [9984/50000]\tLoss: 4.7630\tLR: 7.519693\n",
      "Training Epoch: 76 [10112/50000]\tLoss: 4.6611\tLR: 7.519949\n",
      "Training Epoch: 76 [10240/50000]\tLoss: 4.7062\tLR: 7.520205\n",
      "Training Epoch: 76 [10368/50000]\tLoss: 4.7244\tLR: 7.520460\n",
      "Training Epoch: 76 [10496/50000]\tLoss: 4.7407\tLR: 7.520716\n",
      "Training Epoch: 76 [10624/50000]\tLoss: 4.7612\tLR: 7.520972\n",
      "Training Epoch: 76 [10752/50000]\tLoss: 4.6985\tLR: 7.521228\n",
      "Training Epoch: 76 [10880/50000]\tLoss: 4.7525\tLR: 7.521483\n",
      "Training Epoch: 76 [11008/50000]\tLoss: 4.6942\tLR: 7.521739\n",
      "Training Epoch: 76 [11136/50000]\tLoss: 4.6826\tLR: 7.521995\n",
      "Training Epoch: 76 [11264/50000]\tLoss: 4.7974\tLR: 7.522251\n",
      "Training Epoch: 76 [11392/50000]\tLoss: 4.7355\tLR: 7.522506\n",
      "Training Epoch: 76 [11520/50000]\tLoss: 4.7432\tLR: 7.522762\n",
      "Training Epoch: 76 [11648/50000]\tLoss: 4.7526\tLR: 7.523018\n",
      "Training Epoch: 76 [11776/50000]\tLoss: 4.7430\tLR: 7.523274\n",
      "Training Epoch: 76 [11904/50000]\tLoss: 4.6762\tLR: 7.523529\n",
      "Training Epoch: 76 [12032/50000]\tLoss: 4.6551\tLR: 7.523785\n",
      "Training Epoch: 76 [12160/50000]\tLoss: 4.7284\tLR: 7.524041\n",
      "Training Epoch: 76 [12288/50000]\tLoss: 4.7102\tLR: 7.524297\n",
      "Training Epoch: 76 [12416/50000]\tLoss: 4.7038\tLR: 7.524552\n",
      "Training Epoch: 76 [12544/50000]\tLoss: 4.7706\tLR: 7.524808\n",
      "Training Epoch: 76 [12672/50000]\tLoss: 4.6878\tLR: 7.525064\n",
      "Training Epoch: 76 [12800/50000]\tLoss: 4.7282\tLR: 7.525320\n",
      "Training Epoch: 76 [12928/50000]\tLoss: 4.6783\tLR: 7.525575\n",
      "Training Epoch: 76 [13056/50000]\tLoss: 4.6550\tLR: 7.525831\n",
      "Training Epoch: 76 [13184/50000]\tLoss: 4.8627\tLR: 7.526087\n",
      "Training Epoch: 76 [13312/50000]\tLoss: 4.7912\tLR: 7.526343\n",
      "Training Epoch: 76 [13440/50000]\tLoss: 4.8308\tLR: 7.526598\n",
      "Training Epoch: 76 [13568/50000]\tLoss: 4.7287\tLR: 7.526854\n",
      "Training Epoch: 76 [13696/50000]\tLoss: 4.7372\tLR: 7.527110\n",
      "Training Epoch: 76 [13824/50000]\tLoss: 4.6745\tLR: 7.527366\n",
      "Training Epoch: 76 [13952/50000]\tLoss: 4.7828\tLR: 7.527621\n",
      "Training Epoch: 76 [14080/50000]\tLoss: 4.7472\tLR: 7.527877\n",
      "Training Epoch: 76 [14208/50000]\tLoss: 4.8100\tLR: 7.528133\n",
      "Training Epoch: 76 [14336/50000]\tLoss: 4.6492\tLR: 7.528389\n",
      "Training Epoch: 76 [14464/50000]\tLoss: 4.7166\tLR: 7.528645\n",
      "Training Epoch: 76 [14592/50000]\tLoss: 4.7026\tLR: 7.528900\n",
      "Training Epoch: 76 [14720/50000]\tLoss: 4.7543\tLR: 7.529156\n",
      "Training Epoch: 76 [14848/50000]\tLoss: 4.6371\tLR: 7.529412\n",
      "Training Epoch: 76 [14976/50000]\tLoss: 4.8161\tLR: 7.529668\n",
      "Training Epoch: 76 [15104/50000]\tLoss: 4.7254\tLR: 7.529923\n",
      "Training Epoch: 76 [15232/50000]\tLoss: 4.8163\tLR: 7.530179\n",
      "Training Epoch: 76 [15360/50000]\tLoss: 4.8015\tLR: 7.530435\n",
      "Training Epoch: 76 [15488/50000]\tLoss: 4.7534\tLR: 7.530691\n",
      "Training Epoch: 76 [15616/50000]\tLoss: 4.7005\tLR: 7.530946\n",
      "Training Epoch: 76 [15744/50000]\tLoss: 4.7070\tLR: 7.531202\n",
      "Training Epoch: 76 [15872/50000]\tLoss: 4.7117\tLR: 7.531458\n",
      "Training Epoch: 76 [16000/50000]\tLoss: 4.7089\tLR: 7.531714\n",
      "Training Epoch: 76 [16128/50000]\tLoss: 4.7927\tLR: 7.531969\n",
      "Training Epoch: 76 [16256/50000]\tLoss: 4.7511\tLR: 7.532225\n",
      "Training Epoch: 76 [16384/50000]\tLoss: 4.7654\tLR: 7.532481\n",
      "Training Epoch: 76 [16512/50000]\tLoss: 4.8610\tLR: 7.532737\n",
      "Training Epoch: 76 [16640/50000]\tLoss: 4.6973\tLR: 7.532992\n",
      "Training Epoch: 76 [16768/50000]\tLoss: 4.7033\tLR: 7.533248\n",
      "Training Epoch: 76 [16896/50000]\tLoss: 4.7459\tLR: 7.533504\n",
      "Training Epoch: 76 [17024/50000]\tLoss: 4.7685\tLR: 7.533760\n",
      "Training Epoch: 76 [17152/50000]\tLoss: 4.7062\tLR: 7.534015\n",
      "Training Epoch: 76 [17280/50000]\tLoss: 4.7590\tLR: 7.534271\n",
      "Training Epoch: 76 [17408/50000]\tLoss: 4.8024\tLR: 7.534527\n",
      "Training Epoch: 76 [17536/50000]\tLoss: 4.7029\tLR: 7.534783\n",
      "Training Epoch: 76 [17664/50000]\tLoss: 4.7349\tLR: 7.535038\n",
      "Training Epoch: 76 [17792/50000]\tLoss: 4.7213\tLR: 7.535294\n",
      "Training Epoch: 76 [17920/50000]\tLoss: 4.8208\tLR: 7.535550\n",
      "Training Epoch: 76 [18048/50000]\tLoss: 4.7621\tLR: 7.535806\n",
      "Training Epoch: 76 [18176/50000]\tLoss: 4.7778\tLR: 7.536061\n",
      "Training Epoch: 76 [18304/50000]\tLoss: 4.7828\tLR: 7.536317\n",
      "Training Epoch: 76 [18432/50000]\tLoss: 4.6949\tLR: 7.536573\n",
      "Training Epoch: 76 [18560/50000]\tLoss: 4.7707\tLR: 7.536829\n",
      "Training Epoch: 76 [18688/50000]\tLoss: 4.7100\tLR: 7.537084\n",
      "Training Epoch: 76 [18816/50000]\tLoss: 4.8239\tLR: 7.537340\n",
      "Training Epoch: 76 [18944/50000]\tLoss: 4.7106\tLR: 7.537596\n",
      "Training Epoch: 76 [19072/50000]\tLoss: 4.7466\tLR: 7.537852\n",
      "Training Epoch: 76 [19200/50000]\tLoss: 4.7102\tLR: 7.538107\n",
      "Training Epoch: 76 [19328/50000]\tLoss: 4.6727\tLR: 7.538363\n",
      "Training Epoch: 76 [19456/50000]\tLoss: 4.8090\tLR: 7.538619\n",
      "Training Epoch: 76 [19584/50000]\tLoss: 4.6652\tLR: 7.538875\n",
      "Training Epoch: 76 [19712/50000]\tLoss: 4.6014\tLR: 7.539130\n",
      "Training Epoch: 76 [19840/50000]\tLoss: 4.6121\tLR: 7.539386\n",
      "Training Epoch: 76 [19968/50000]\tLoss: 4.7469\tLR: 7.539642\n",
      "Training Epoch: 76 [20096/50000]\tLoss: 4.7912\tLR: 7.539898\n",
      "Training Epoch: 76 [20224/50000]\tLoss: 4.7677\tLR: 7.540153\n",
      "Training Epoch: 76 [20352/50000]\tLoss: 4.6997\tLR: 7.540409\n",
      "Training Epoch: 76 [20480/50000]\tLoss: 4.7947\tLR: 7.540665\n",
      "Training Epoch: 76 [20608/50000]\tLoss: 4.6926\tLR: 7.540921\n",
      "Training Epoch: 76 [20736/50000]\tLoss: 4.7342\tLR: 7.541176\n",
      "Training Epoch: 76 [20864/50000]\tLoss: 4.7450\tLR: 7.541432\n",
      "Training Epoch: 76 [20992/50000]\tLoss: 4.7681\tLR: 7.541688\n",
      "Training Epoch: 76 [21120/50000]\tLoss: 4.7388\tLR: 7.541944\n",
      "Training Epoch: 76 [21248/50000]\tLoss: 4.7684\tLR: 7.542199\n",
      "Training Epoch: 76 [21376/50000]\tLoss: 4.7538\tLR: 7.542455\n",
      "Training Epoch: 76 [21504/50000]\tLoss: 4.8404\tLR: 7.542711\n",
      "Training Epoch: 76 [21632/50000]\tLoss: 4.7404\tLR: 7.542967\n",
      "Training Epoch: 76 [21760/50000]\tLoss: 4.6938\tLR: 7.543223\n",
      "Training Epoch: 76 [21888/50000]\tLoss: 4.6429\tLR: 7.543478\n",
      "Training Epoch: 76 [22016/50000]\tLoss: 4.7526\tLR: 7.543734\n",
      "Training Epoch: 76 [22144/50000]\tLoss: 4.6682\tLR: 7.543990\n",
      "Training Epoch: 76 [22272/50000]\tLoss: 4.8290\tLR: 7.544246\n",
      "Training Epoch: 76 [22400/50000]\tLoss: 4.7635\tLR: 7.544501\n",
      "Training Epoch: 76 [22528/50000]\tLoss: 4.7379\tLR: 7.544757\n",
      "Training Epoch: 76 [22656/50000]\tLoss: 4.8147\tLR: 7.545013\n",
      "Training Epoch: 76 [22784/50000]\tLoss: 4.8078\tLR: 7.545269\n",
      "Training Epoch: 76 [22912/50000]\tLoss: 4.7060\tLR: 7.545524\n",
      "Training Epoch: 76 [23040/50000]\tLoss: 4.7126\tLR: 7.545780\n",
      "Training Epoch: 76 [23168/50000]\tLoss: 4.7532\tLR: 7.546036\n",
      "Training Epoch: 76 [23296/50000]\tLoss: 4.7475\tLR: 7.546292\n",
      "Training Epoch: 76 [23424/50000]\tLoss: 4.7721\tLR: 7.546547\n",
      "Training Epoch: 76 [23552/50000]\tLoss: 4.6881\tLR: 7.546803\n",
      "Training Epoch: 76 [23680/50000]\tLoss: 4.7606\tLR: 7.547059\n",
      "Training Epoch: 76 [23808/50000]\tLoss: 4.8066\tLR: 7.547315\n",
      "Training Epoch: 76 [23936/50000]\tLoss: 4.7389\tLR: 7.547570\n",
      "Training Epoch: 76 [24064/50000]\tLoss: 4.7065\tLR: 7.547826\n",
      "Training Epoch: 76 [24192/50000]\tLoss: 4.7349\tLR: 7.548082\n",
      "Training Epoch: 76 [24320/50000]\tLoss: 4.7693\tLR: 7.548338\n",
      "Training Epoch: 76 [24448/50000]\tLoss: 4.9699\tLR: 7.548593\n",
      "Training Epoch: 76 [24576/50000]\tLoss: 4.7586\tLR: 7.548849\n",
      "Training Epoch: 76 [24704/50000]\tLoss: 4.7540\tLR: 7.549105\n",
      "Training Epoch: 76 [24832/50000]\tLoss: 4.7493\tLR: 7.549361\n",
      "Training Epoch: 76 [24960/50000]\tLoss: 4.8489\tLR: 7.549616\n",
      "Training Epoch: 76 [25088/50000]\tLoss: 4.7503\tLR: 7.549872\n",
      "Training Epoch: 76 [25216/50000]\tLoss: 4.8140\tLR: 7.550128\n",
      "Training Epoch: 76 [25344/50000]\tLoss: 4.7739\tLR: 7.550384\n",
      "Training Epoch: 76 [25472/50000]\tLoss: 4.7917\tLR: 7.550639\n",
      "Training Epoch: 76 [25600/50000]\tLoss: 4.7874\tLR: 7.550895\n",
      "Training Epoch: 76 [25728/50000]\tLoss: 4.6854\tLR: 7.551151\n",
      "Training Epoch: 76 [25856/50000]\tLoss: 4.7322\tLR: 7.551407\n",
      "Training Epoch: 76 [25984/50000]\tLoss: 4.7300\tLR: 7.551662\n",
      "Training Epoch: 76 [26112/50000]\tLoss: 4.7722\tLR: 7.551918\n",
      "Training Epoch: 76 [26240/50000]\tLoss: 4.7799\tLR: 7.552174\n",
      "Training Epoch: 76 [26368/50000]\tLoss: 4.7821\tLR: 7.552430\n",
      "Training Epoch: 76 [26496/50000]\tLoss: 4.7311\tLR: 7.552685\n",
      "Training Epoch: 76 [26624/50000]\tLoss: 4.6678\tLR: 7.552941\n",
      "Training Epoch: 76 [26752/50000]\tLoss: 4.7918\tLR: 7.553197\n",
      "Training Epoch: 76 [26880/50000]\tLoss: 4.7809\tLR: 7.553453\n",
      "Training Epoch: 76 [27008/50000]\tLoss: 4.8141\tLR: 7.553708\n",
      "Training Epoch: 76 [27136/50000]\tLoss: 4.5480\tLR: 7.553964\n",
      "Training Epoch: 76 [27264/50000]\tLoss: 4.8395\tLR: 7.554220\n",
      "Training Epoch: 76 [27392/50000]\tLoss: 4.7944\tLR: 7.554476\n",
      "Training Epoch: 76 [27520/50000]\tLoss: 4.8648\tLR: 7.554731\n",
      "Training Epoch: 76 [27648/50000]\tLoss: 4.8228\tLR: 7.554987\n",
      "Training Epoch: 76 [27776/50000]\tLoss: 4.8200\tLR: 7.555243\n",
      "Training Epoch: 76 [27904/50000]\tLoss: 4.7841\tLR: 7.555499\n",
      "Training Epoch: 76 [28032/50000]\tLoss: 4.7269\tLR: 7.555754\n",
      "Training Epoch: 76 [28160/50000]\tLoss: 4.7190\tLR: 7.556010\n",
      "Training Epoch: 76 [28288/50000]\tLoss: 4.7530\tLR: 7.556266\n",
      "Training Epoch: 76 [28416/50000]\tLoss: 4.7793\tLR: 7.556522\n",
      "Training Epoch: 76 [28544/50000]\tLoss: 4.7467\tLR: 7.556777\n",
      "Training Epoch: 76 [28672/50000]\tLoss: 4.8639\tLR: 7.557033\n",
      "Training Epoch: 76 [28800/50000]\tLoss: 4.8062\tLR: 7.557289\n",
      "Training Epoch: 76 [28928/50000]\tLoss: 4.7862\tLR: 7.557545\n",
      "Training Epoch: 76 [29056/50000]\tLoss: 4.7302\tLR: 7.557801\n",
      "Training Epoch: 76 [29184/50000]\tLoss: 4.6745\tLR: 7.558056\n",
      "Training Epoch: 76 [29312/50000]\tLoss: 4.7732\tLR: 7.558312\n",
      "Training Epoch: 76 [29440/50000]\tLoss: 4.6731\tLR: 7.558568\n",
      "Training Epoch: 76 [29568/50000]\tLoss: 4.7740\tLR: 7.558824\n",
      "Training Epoch: 76 [29696/50000]\tLoss: 4.7309\tLR: 7.559079\n",
      "Training Epoch: 76 [29824/50000]\tLoss: 4.8694\tLR: 7.559335\n",
      "Training Epoch: 76 [29952/50000]\tLoss: 4.7299\tLR: 7.559591\n",
      "Training Epoch: 76 [30080/50000]\tLoss: 4.7779\tLR: 7.559847\n",
      "Training Epoch: 76 [30208/50000]\tLoss: 4.6647\tLR: 7.560102\n",
      "Training Epoch: 76 [30336/50000]\tLoss: 4.7306\tLR: 7.560358\n",
      "Training Epoch: 76 [30464/50000]\tLoss: 4.7972\tLR: 7.560614\n",
      "Training Epoch: 76 [30592/50000]\tLoss: 4.6988\tLR: 7.560870\n",
      "Training Epoch: 76 [30720/50000]\tLoss: 4.8027\tLR: 7.561125\n",
      "Training Epoch: 76 [30848/50000]\tLoss: 4.7695\tLR: 7.561381\n",
      "Training Epoch: 76 [30976/50000]\tLoss: 4.7701\tLR: 7.561637\n",
      "Training Epoch: 76 [31104/50000]\tLoss: 4.7376\tLR: 7.561893\n",
      "Training Epoch: 76 [31232/50000]\tLoss: 4.7466\tLR: 7.562148\n",
      "Training Epoch: 76 [31360/50000]\tLoss: 4.7524\tLR: 7.562404\n",
      "Training Epoch: 76 [31488/50000]\tLoss: 4.7339\tLR: 7.562660\n",
      "Training Epoch: 76 [31616/50000]\tLoss: 4.6664\tLR: 7.562916\n",
      "Training Epoch: 76 [31744/50000]\tLoss: 4.7200\tLR: 7.563171\n",
      "Training Epoch: 76 [31872/50000]\tLoss: 4.6998\tLR: 7.563427\n",
      "Training Epoch: 76 [32000/50000]\tLoss: 4.6909\tLR: 7.563683\n",
      "Training Epoch: 76 [32128/50000]\tLoss: 4.6793\tLR: 7.563939\n",
      "Training Epoch: 76 [32256/50000]\tLoss: 4.7326\tLR: 7.564194\n",
      "Training Epoch: 76 [32384/50000]\tLoss: 4.7819\tLR: 7.564450\n",
      "Training Epoch: 76 [32512/50000]\tLoss: 4.7542\tLR: 7.564706\n",
      "Training Epoch: 76 [32640/50000]\tLoss: 4.7409\tLR: 7.564962\n",
      "Training Epoch: 76 [32768/50000]\tLoss: 4.7440\tLR: 7.565217\n",
      "Training Epoch: 76 [32896/50000]\tLoss: 4.7282\tLR: 7.565473\n",
      "Training Epoch: 76 [33024/50000]\tLoss: 4.6837\tLR: 7.565729\n",
      "Training Epoch: 76 [33152/50000]\tLoss: 4.7741\tLR: 7.565985\n",
      "Training Epoch: 76 [33280/50000]\tLoss: 4.7544\tLR: 7.566240\n",
      "Training Epoch: 76 [33408/50000]\tLoss: 4.7652\tLR: 7.566496\n",
      "Training Epoch: 76 [33536/50000]\tLoss: 4.8086\tLR: 7.566752\n",
      "Training Epoch: 76 [33664/50000]\tLoss: 4.8049\tLR: 7.567008\n",
      "Training Epoch: 76 [33792/50000]\tLoss: 4.8373\tLR: 7.567263\n",
      "Training Epoch: 76 [33920/50000]\tLoss: 4.7014\tLR: 7.567519\n",
      "Training Epoch: 76 [34048/50000]\tLoss: 4.6363\tLR: 7.567775\n",
      "Training Epoch: 76 [34176/50000]\tLoss: 4.7860\tLR: 7.568031\n",
      "Training Epoch: 76 [34304/50000]\tLoss: 4.8522\tLR: 7.568286\n",
      "Training Epoch: 76 [34432/50000]\tLoss: 4.7140\tLR: 7.568542\n",
      "Training Epoch: 76 [34560/50000]\tLoss: 4.8203\tLR: 7.568798\n",
      "Training Epoch: 76 [34688/50000]\tLoss: 4.6969\tLR: 7.569054\n",
      "Training Epoch: 76 [34816/50000]\tLoss: 4.8281\tLR: 7.569309\n",
      "Training Epoch: 76 [34944/50000]\tLoss: 4.7495\tLR: 7.569565\n",
      "Training Epoch: 76 [35072/50000]\tLoss: 4.7590\tLR: 7.569821\n",
      "Training Epoch: 76 [35200/50000]\tLoss: 4.7466\tLR: 7.570077\n",
      "Training Epoch: 76 [35328/50000]\tLoss: 4.6741\tLR: 7.570332\n",
      "Training Epoch: 76 [35456/50000]\tLoss: 4.7514\tLR: 7.570588\n",
      "Training Epoch: 76 [35584/50000]\tLoss: 4.8264\tLR: 7.570844\n",
      "Training Epoch: 76 [35712/50000]\tLoss: 4.7724\tLR: 7.571100\n",
      "Training Epoch: 76 [35840/50000]\tLoss: 4.7524\tLR: 7.571355\n",
      "Training Epoch: 76 [35968/50000]\tLoss: 4.8628\tLR: 7.571611\n",
      "Training Epoch: 76 [36096/50000]\tLoss: 4.7070\tLR: 7.571867\n",
      "Training Epoch: 76 [36224/50000]\tLoss: 4.7738\tLR: 7.572123\n",
      "Training Epoch: 76 [36352/50000]\tLoss: 4.7794\tLR: 7.572379\n",
      "Training Epoch: 76 [36480/50000]\tLoss: 4.7327\tLR: 7.572634\n",
      "Training Epoch: 76 [36608/50000]\tLoss: 4.7910\tLR: 7.572890\n",
      "Training Epoch: 76 [36736/50000]\tLoss: 4.7198\tLR: 7.573146\n",
      "Training Epoch: 76 [36864/50000]\tLoss: 4.7386\tLR: 7.573402\n",
      "Training Epoch: 76 [36992/50000]\tLoss: 4.8282\tLR: 7.573657\n",
      "Training Epoch: 76 [37120/50000]\tLoss: 4.7979\tLR: 7.573913\n",
      "Training Epoch: 76 [37248/50000]\tLoss: 4.7471\tLR: 7.574169\n",
      "Training Epoch: 76 [37376/50000]\tLoss: 4.7581\tLR: 7.574425\n",
      "Training Epoch: 76 [37504/50000]\tLoss: 4.7076\tLR: 7.574680\n",
      "Training Epoch: 76 [37632/50000]\tLoss: 4.7686\tLR: 7.574936\n",
      "Training Epoch: 76 [37760/50000]\tLoss: 4.7506\tLR: 7.575192\n",
      "Training Epoch: 76 [37888/50000]\tLoss: 4.7860\tLR: 7.575448\n",
      "Training Epoch: 76 [38016/50000]\tLoss: 4.7671\tLR: 7.575703\n",
      "Training Epoch: 76 [38144/50000]\tLoss: 4.6778\tLR: 7.575959\n",
      "Training Epoch: 76 [38272/50000]\tLoss: 4.7397\tLR: 7.576215\n",
      "Training Epoch: 76 [38400/50000]\tLoss: 4.8057\tLR: 7.576471\n",
      "Training Epoch: 76 [38528/50000]\tLoss: 4.7071\tLR: 7.576726\n",
      "Training Epoch: 76 [38656/50000]\tLoss: 4.8084\tLR: 7.576982\n",
      "Training Epoch: 76 [38784/50000]\tLoss: 4.7135\tLR: 7.577238\n",
      "Training Epoch: 76 [38912/50000]\tLoss: 4.7607\tLR: 7.577494\n",
      "Training Epoch: 76 [39040/50000]\tLoss: 4.7206\tLR: 7.577749\n",
      "Training Epoch: 76 [39168/50000]\tLoss: 4.7428\tLR: 7.578005\n",
      "Training Epoch: 76 [39296/50000]\tLoss: 4.7703\tLR: 7.578261\n",
      "Training Epoch: 76 [39424/50000]\tLoss: 4.8024\tLR: 7.578517\n",
      "Training Epoch: 76 [39552/50000]\tLoss: 4.7419\tLR: 7.578772\n",
      "Training Epoch: 76 [39680/50000]\tLoss: 4.6949\tLR: 7.579028\n",
      "Training Epoch: 76 [39808/50000]\tLoss: 4.7004\tLR: 7.579284\n",
      "Training Epoch: 76 [39936/50000]\tLoss: 4.7579\tLR: 7.579540\n",
      "Training Epoch: 76 [40064/50000]\tLoss: 4.8127\tLR: 7.579795\n",
      "Training Epoch: 76 [40192/50000]\tLoss: 4.8299\tLR: 7.580051\n",
      "Training Epoch: 76 [40320/50000]\tLoss: 4.7172\tLR: 7.580307\n",
      "Training Epoch: 76 [40448/50000]\tLoss: 4.7047\tLR: 7.580563\n",
      "Training Epoch: 76 [40576/50000]\tLoss: 4.7682\tLR: 7.580818\n",
      "Training Epoch: 76 [40704/50000]\tLoss: 4.6549\tLR: 7.581074\n",
      "Training Epoch: 76 [40832/50000]\tLoss: 4.7747\tLR: 7.581330\n",
      "Training Epoch: 76 [40960/50000]\tLoss: 4.7273\tLR: 7.581586\n",
      "Training Epoch: 76 [41088/50000]\tLoss: 4.7162\tLR: 7.581841\n",
      "Training Epoch: 76 [41216/50000]\tLoss: 4.7784\tLR: 7.582097\n",
      "Training Epoch: 76 [41344/50000]\tLoss: 4.7181\tLR: 7.582353\n",
      "Training Epoch: 76 [41472/50000]\tLoss: 4.7658\tLR: 7.582609\n",
      "Training Epoch: 76 [41600/50000]\tLoss: 4.6988\tLR: 7.582864\n",
      "Training Epoch: 76 [41728/50000]\tLoss: 4.6768\tLR: 7.583120\n",
      "Training Epoch: 76 [41856/50000]\tLoss: 4.6409\tLR: 7.583376\n",
      "Training Epoch: 76 [41984/50000]\tLoss: 4.7928\tLR: 7.583632\n",
      "Training Epoch: 76 [42112/50000]\tLoss: 4.7640\tLR: 7.583887\n",
      "Training Epoch: 76 [42240/50000]\tLoss: 4.6438\tLR: 7.584143\n",
      "Training Epoch: 76 [42368/50000]\tLoss: 4.6943\tLR: 7.584399\n",
      "Training Epoch: 76 [42496/50000]\tLoss: 4.7408\tLR: 7.584655\n",
      "Training Epoch: 76 [42624/50000]\tLoss: 4.7255\tLR: 7.584910\n",
      "Training Epoch: 76 [42752/50000]\tLoss: 4.7347\tLR: 7.585166\n",
      "Training Epoch: 76 [42880/50000]\tLoss: 4.6772\tLR: 7.585422\n",
      "Training Epoch: 76 [43008/50000]\tLoss: 4.7661\tLR: 7.585678\n",
      "Training Epoch: 76 [43136/50000]\tLoss: 4.7113\tLR: 7.585934\n",
      "Training Epoch: 76 [43264/50000]\tLoss: 4.7542\tLR: 7.586189\n",
      "Training Epoch: 76 [43392/50000]\tLoss: 4.7711\tLR: 7.586445\n",
      "Training Epoch: 76 [43520/50000]\tLoss: 4.6758\tLR: 7.586701\n",
      "Training Epoch: 76 [43648/50000]\tLoss: 4.7816\tLR: 7.586957\n",
      "Training Epoch: 76 [43776/50000]\tLoss: 4.7565\tLR: 7.587212\n",
      "Training Epoch: 76 [43904/50000]\tLoss: 4.8319\tLR: 7.587468\n",
      "Training Epoch: 76 [44032/50000]\tLoss: 4.7348\tLR: 7.587724\n",
      "Training Epoch: 76 [44160/50000]\tLoss: 4.7034\tLR: 7.587980\n",
      "Training Epoch: 76 [44288/50000]\tLoss: 4.7300\tLR: 7.588235\n",
      "Training Epoch: 76 [44416/50000]\tLoss: 4.7084\tLR: 7.588491\n",
      "Training Epoch: 76 [44544/50000]\tLoss: 4.7910\tLR: 7.588747\n",
      "Training Epoch: 76 [44672/50000]\tLoss: 4.7116\tLR: 7.589003\n",
      "Training Epoch: 76 [44800/50000]\tLoss: 4.7734\tLR: 7.589258\n",
      "Training Epoch: 76 [44928/50000]\tLoss: 4.8092\tLR: 7.589514\n",
      "Training Epoch: 76 [45056/50000]\tLoss: 4.6507\tLR: 7.589770\n",
      "Training Epoch: 76 [45184/50000]\tLoss: 4.6958\tLR: 7.590026\n",
      "Training Epoch: 76 [45312/50000]\tLoss: 4.6931\tLR: 7.590281\n",
      "Training Epoch: 76 [45440/50000]\tLoss: 4.7295\tLR: 7.590537\n",
      "Training Epoch: 76 [45568/50000]\tLoss: 4.6978\tLR: 7.590793\n",
      "Training Epoch: 76 [45696/50000]\tLoss: 4.6419\tLR: 7.591049\n",
      "Training Epoch: 76 [45824/50000]\tLoss: 4.7747\tLR: 7.591304\n",
      "Training Epoch: 76 [45952/50000]\tLoss: 4.7389\tLR: 7.591560\n",
      "Training Epoch: 76 [46080/50000]\tLoss: 4.8013\tLR: 7.591816\n",
      "Training Epoch: 76 [46208/50000]\tLoss: 4.7632\tLR: 7.592072\n",
      "Training Epoch: 76 [46336/50000]\tLoss: 4.6898\tLR: 7.592327\n",
      "Training Epoch: 76 [46464/50000]\tLoss: 4.6690\tLR: 7.592583\n",
      "Training Epoch: 76 [46592/50000]\tLoss: 4.6952\tLR: 7.592839\n",
      "Training Epoch: 76 [46720/50000]\tLoss: 4.6293\tLR: 7.593095\n",
      "Training Epoch: 76 [46848/50000]\tLoss: 4.7265\tLR: 7.593350\n",
      "Training Epoch: 76 [46976/50000]\tLoss: 4.7185\tLR: 7.593606\n",
      "Training Epoch: 76 [47104/50000]\tLoss: 4.8603\tLR: 7.593862\n",
      "Training Epoch: 76 [47232/50000]\tLoss: 4.7390\tLR: 7.594118\n",
      "Training Epoch: 76 [47360/50000]\tLoss: 4.8043\tLR: 7.594373\n",
      "Training Epoch: 76 [47488/50000]\tLoss: 4.7499\tLR: 7.594629\n",
      "Training Epoch: 76 [47616/50000]\tLoss: 4.7472\tLR: 7.594885\n",
      "Training Epoch: 76 [47744/50000]\tLoss: 4.7438\tLR: 7.595141\n",
      "Training Epoch: 76 [47872/50000]\tLoss: 4.6351\tLR: 7.595396\n",
      "Training Epoch: 76 [48000/50000]\tLoss: 4.7553\tLR: 7.595652\n",
      "Training Epoch: 76 [48128/50000]\tLoss: 4.8096\tLR: 7.595908\n",
      "Training Epoch: 76 [48256/50000]\tLoss: 4.7556\tLR: 7.596164\n",
      "Training Epoch: 76 [48384/50000]\tLoss: 4.7572\tLR: 7.596419\n",
      "Training Epoch: 76 [48512/50000]\tLoss: 4.6781\tLR: 7.596675\n",
      "Training Epoch: 76 [48640/50000]\tLoss: 4.6711\tLR: 7.596931\n",
      "Training Epoch: 76 [48768/50000]\tLoss: 4.7040\tLR: 7.597187\n",
      "Training Epoch: 76 [48896/50000]\tLoss: 4.7473\tLR: 7.597442\n",
      "Training Epoch: 76 [49024/50000]\tLoss: 4.8644\tLR: 7.597698\n",
      "Training Epoch: 76 [49152/50000]\tLoss: 4.7052\tLR: 7.597954\n",
      "Training Epoch: 76 [49280/50000]\tLoss: 4.8063\tLR: 7.598210\n",
      "Training Epoch: 76 [49408/50000]\tLoss: 4.8049\tLR: 7.598465\n",
      "Training Epoch: 76 [49536/50000]\tLoss: 4.6983\tLR: 7.598721\n",
      "Training Epoch: 76 [49664/50000]\tLoss: 4.7401\tLR: 7.598977\n",
      "Training Epoch: 76 [49792/50000]\tLoss: 4.7025\tLR: 7.599233\n",
      "Training Epoch: 76 [49920/50000]\tLoss: 4.7912\tLR: 7.599488\n",
      "Training Epoch: 76 [50000/50000]\tLoss: 4.7734\tLR: 7.599744\n",
      "epoch 76 training time consumed: 489.24s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  106547 GB |  106547 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  106220 GB |  106219 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     327 GB |     327 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  106547 GB |  106547 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  106220 GB |  106219 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     327 GB |     327 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  105049 GB |  105049 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  104722 GB |  104722 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     327 GB |     327 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   11297 K  |   11297 K  |\n",
      "|       from large pool |      24    |      65    |    4816 K  |    4816 K  |\n",
      "|       from small pool |     231    |     274    |    6481 K  |    6481 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   11297 K  |   11297 K  |\n",
      "|       from large pool |      24    |      65    |    4816 K  |    4816 K  |\n",
      "|       from small pool |     231    |     274    |    6481 K  |    6481 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      47    |    6548 K  |    6548 K  |\n",
      "|       from large pool |      10    |      23    |    2314 K  |    2314 K  |\n",
      "|       from small pool |      27    |      35    |    4233 K  |    4233 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 76, Average loss: 0.0375, Accuracy: 0.0100, Time consumed:31.34s\n",
      "\n",
      "Training Epoch: 77 [128/50000]\tLoss: 4.7160\tLR: 0.020000\n",
      "Training Epoch: 77 [256/50000]\tLoss: 4.7406\tLR: 7.600256\n",
      "Training Epoch: 77 [384/50000]\tLoss: 4.8368\tLR: 7.600512\n",
      "Training Epoch: 77 [512/50000]\tLoss: 4.7342\tLR: 7.600767\n",
      "Training Epoch: 77 [640/50000]\tLoss: 4.7537\tLR: 7.601023\n",
      "Training Epoch: 77 [768/50000]\tLoss: 4.8350\tLR: 7.601279\n",
      "Training Epoch: 77 [896/50000]\tLoss: 4.7578\tLR: 7.601535\n",
      "Training Epoch: 77 [1024/50000]\tLoss: 4.7175\tLR: 7.601790\n",
      "Training Epoch: 77 [1152/50000]\tLoss: 4.7653\tLR: 7.602046\n",
      "Training Epoch: 77 [1280/50000]\tLoss: 4.7818\tLR: 7.602302\n",
      "Training Epoch: 77 [1408/50000]\tLoss: 4.7592\tLR: 7.602558\n",
      "Training Epoch: 77 [1536/50000]\tLoss: 4.7416\tLR: 7.602813\n",
      "Training Epoch: 77 [1664/50000]\tLoss: 4.8120\tLR: 7.603069\n",
      "Training Epoch: 77 [1792/50000]\tLoss: 4.7143\tLR: 7.603325\n",
      "Training Epoch: 77 [1920/50000]\tLoss: 4.7403\tLR: 7.603581\n",
      "Training Epoch: 77 [2048/50000]\tLoss: 4.7781\tLR: 7.603836\n",
      "Training Epoch: 77 [2176/50000]\tLoss: 4.6897\tLR: 7.604092\n",
      "Training Epoch: 77 [2304/50000]\tLoss: 4.7893\tLR: 7.604348\n",
      "Training Epoch: 77 [2432/50000]\tLoss: 4.7497\tLR: 7.604604\n",
      "Training Epoch: 77 [2560/50000]\tLoss: 4.7545\tLR: 7.604859\n",
      "Training Epoch: 77 [2688/50000]\tLoss: 4.6620\tLR: 7.605115\n",
      "Training Epoch: 77 [2816/50000]\tLoss: 4.7919\tLR: 7.605371\n",
      "Training Epoch: 77 [2944/50000]\tLoss: 4.6782\tLR: 7.605627\n",
      "Training Epoch: 77 [3072/50000]\tLoss: 4.6792\tLR: 7.605882\n",
      "Training Epoch: 77 [3200/50000]\tLoss: 4.7214\tLR: 7.606138\n",
      "Training Epoch: 77 [3328/50000]\tLoss: 4.7108\tLR: 7.606394\n",
      "Training Epoch: 77 [3456/50000]\tLoss: 4.7241\tLR: 7.606650\n",
      "Training Epoch: 77 [3584/50000]\tLoss: 4.8671\tLR: 7.606905\n",
      "Training Epoch: 77 [3712/50000]\tLoss: 4.8474\tLR: 7.607161\n",
      "Training Epoch: 77 [3840/50000]\tLoss: 4.7856\tLR: 7.607417\n",
      "Training Epoch: 77 [3968/50000]\tLoss: 4.7718\tLR: 7.607673\n",
      "Training Epoch: 77 [4096/50000]\tLoss: 4.6589\tLR: 7.607928\n",
      "Training Epoch: 77 [4224/50000]\tLoss: 4.6800\tLR: 7.608184\n",
      "Training Epoch: 77 [4352/50000]\tLoss: 4.7660\tLR: 7.608440\n",
      "Training Epoch: 77 [4480/50000]\tLoss: 4.7550\tLR: 7.608696\n",
      "Training Epoch: 77 [4608/50000]\tLoss: 4.8260\tLR: 7.608951\n",
      "Training Epoch: 77 [4736/50000]\tLoss: 4.7247\tLR: 7.609207\n",
      "Training Epoch: 77 [4864/50000]\tLoss: 4.8198\tLR: 7.609463\n",
      "Training Epoch: 77 [4992/50000]\tLoss: 4.7488\tLR: 7.609719\n",
      "Training Epoch: 77 [5120/50000]\tLoss: 4.8303\tLR: 7.609974\n",
      "Training Epoch: 77 [5248/50000]\tLoss: 4.7727\tLR: 7.610230\n",
      "Training Epoch: 77 [5376/50000]\tLoss: 4.6783\tLR: 7.610486\n",
      "Training Epoch: 77 [5504/50000]\tLoss: 4.8376\tLR: 7.610742\n",
      "Training Epoch: 77 [5632/50000]\tLoss: 4.7602\tLR: 7.610997\n",
      "Training Epoch: 77 [5760/50000]\tLoss: 4.8489\tLR: 7.611253\n",
      "Training Epoch: 77 [5888/50000]\tLoss: 4.7682\tLR: 7.611509\n",
      "Training Epoch: 77 [6016/50000]\tLoss: 4.7765\tLR: 7.611765\n",
      "Training Epoch: 77 [6144/50000]\tLoss: 4.8087\tLR: 7.612020\n",
      "Training Epoch: 77 [6272/50000]\tLoss: 4.7222\tLR: 7.612276\n",
      "Training Epoch: 77 [6400/50000]\tLoss: 4.7726\tLR: 7.612532\n",
      "Training Epoch: 77 [6528/50000]\tLoss: 4.6827\tLR: 7.612788\n",
      "Training Epoch: 77 [6656/50000]\tLoss: 4.7796\tLR: 7.613043\n",
      "Training Epoch: 77 [6784/50000]\tLoss: 4.7700\tLR: 7.613299\n",
      "Training Epoch: 77 [6912/50000]\tLoss: 4.7827\tLR: 7.613555\n",
      "Training Epoch: 77 [7040/50000]\tLoss: 4.7030\tLR: 7.613811\n",
      "Training Epoch: 77 [7168/50000]\tLoss: 4.7686\tLR: 7.614066\n",
      "Training Epoch: 77 [7296/50000]\tLoss: 4.7629\tLR: 7.614322\n",
      "Training Epoch: 77 [7424/50000]\tLoss: 4.8134\tLR: 7.614578\n",
      "Training Epoch: 77 [7552/50000]\tLoss: 4.7539\tLR: 7.614834\n",
      "Training Epoch: 77 [7680/50000]\tLoss: 4.7359\tLR: 7.615090\n",
      "Training Epoch: 77 [7808/50000]\tLoss: 4.7852\tLR: 7.615345\n",
      "Training Epoch: 77 [7936/50000]\tLoss: 4.6983\tLR: 7.615601\n",
      "Training Epoch: 77 [8064/50000]\tLoss: 4.6939\tLR: 7.615857\n",
      "Training Epoch: 77 [8192/50000]\tLoss: 4.7383\tLR: 7.616113\n",
      "Training Epoch: 77 [8320/50000]\tLoss: 4.6924\tLR: 7.616368\n",
      "Training Epoch: 77 [8448/50000]\tLoss: 4.8052\tLR: 7.616624\n",
      "Training Epoch: 77 [8576/50000]\tLoss: 4.6491\tLR: 7.616880\n",
      "Training Epoch: 77 [8704/50000]\tLoss: 4.7653\tLR: 7.617136\n",
      "Training Epoch: 77 [8832/50000]\tLoss: 4.7872\tLR: 7.617391\n",
      "Training Epoch: 77 [8960/50000]\tLoss: 4.7727\tLR: 7.617647\n",
      "Training Epoch: 77 [9088/50000]\tLoss: 4.6357\tLR: 7.617903\n",
      "Training Epoch: 77 [9216/50000]\tLoss: 4.7978\tLR: 7.618159\n",
      "Training Epoch: 77 [9344/50000]\tLoss: 4.7183\tLR: 7.618414\n",
      "Training Epoch: 77 [9472/50000]\tLoss: 4.6744\tLR: 7.618670\n",
      "Training Epoch: 77 [9600/50000]\tLoss: 4.7988\tLR: 7.618926\n",
      "Training Epoch: 77 [9728/50000]\tLoss: 4.6997\tLR: 7.619182\n",
      "Training Epoch: 77 [9856/50000]\tLoss: 4.7789\tLR: 7.619437\n",
      "Training Epoch: 77 [9984/50000]\tLoss: 4.8075\tLR: 7.619693\n",
      "Training Epoch: 77 [10112/50000]\tLoss: 4.7606\tLR: 7.619949\n",
      "Training Epoch: 77 [10240/50000]\tLoss: 4.7321\tLR: 7.620205\n",
      "Training Epoch: 77 [10368/50000]\tLoss: 4.7110\tLR: 7.620460\n",
      "Training Epoch: 77 [10496/50000]\tLoss: 4.7149\tLR: 7.620716\n",
      "Training Epoch: 77 [10624/50000]\tLoss: 4.8276\tLR: 7.620972\n",
      "Training Epoch: 77 [10752/50000]\tLoss: 4.7310\tLR: 7.621228\n",
      "Training Epoch: 77 [10880/50000]\tLoss: 4.8092\tLR: 7.621483\n",
      "Training Epoch: 77 [11008/50000]\tLoss: 4.6629\tLR: 7.621739\n",
      "Training Epoch: 77 [11136/50000]\tLoss: 4.7597\tLR: 7.621995\n",
      "Training Epoch: 77 [11264/50000]\tLoss: 4.7221\tLR: 7.622251\n",
      "Training Epoch: 77 [11392/50000]\tLoss: 4.8129\tLR: 7.622506\n",
      "Training Epoch: 77 [11520/50000]\tLoss: 4.7830\tLR: 7.622762\n",
      "Training Epoch: 77 [11648/50000]\tLoss: 4.7993\tLR: 7.623018\n",
      "Training Epoch: 77 [11776/50000]\tLoss: 4.7820\tLR: 7.623274\n",
      "Training Epoch: 77 [11904/50000]\tLoss: 4.6850\tLR: 7.623529\n",
      "Training Epoch: 77 [12032/50000]\tLoss: 4.7908\tLR: 7.623785\n",
      "Training Epoch: 77 [12160/50000]\tLoss: 4.7197\tLR: 7.624041\n",
      "Training Epoch: 77 [12288/50000]\tLoss: 4.7183\tLR: 7.624297\n",
      "Training Epoch: 77 [12416/50000]\tLoss: 4.7560\tLR: 7.624552\n",
      "Training Epoch: 77 [12544/50000]\tLoss: 4.7424\tLR: 7.624808\n",
      "Training Epoch: 77 [12672/50000]\tLoss: 4.7624\tLR: 7.625064\n",
      "Training Epoch: 77 [12800/50000]\tLoss: 4.7418\tLR: 7.625320\n",
      "Training Epoch: 77 [12928/50000]\tLoss: 4.7685\tLR: 7.625575\n",
      "Training Epoch: 77 [13056/50000]\tLoss: 4.7604\tLR: 7.625831\n",
      "Training Epoch: 77 [13184/50000]\tLoss: 4.6941\tLR: 7.626087\n",
      "Training Epoch: 77 [13312/50000]\tLoss: 4.7142\tLR: 7.626343\n",
      "Training Epoch: 77 [13440/50000]\tLoss: 4.8083\tLR: 7.626598\n",
      "Training Epoch: 77 [13568/50000]\tLoss: 4.6840\tLR: 7.626854\n",
      "Training Epoch: 77 [13696/50000]\tLoss: 4.7748\tLR: 7.627110\n",
      "Training Epoch: 77 [13824/50000]\tLoss: 4.8178\tLR: 7.627366\n",
      "Training Epoch: 77 [13952/50000]\tLoss: 4.7471\tLR: 7.627621\n",
      "Training Epoch: 77 [14080/50000]\tLoss: 4.8167\tLR: 7.627877\n",
      "Training Epoch: 77 [14208/50000]\tLoss: 4.7475\tLR: 7.628133\n",
      "Training Epoch: 77 [14336/50000]\tLoss: 4.8126\tLR: 7.628389\n",
      "Training Epoch: 77 [14464/50000]\tLoss: 4.7307\tLR: 7.628645\n",
      "Training Epoch: 77 [14592/50000]\tLoss: 4.7115\tLR: 7.628900\n",
      "Training Epoch: 77 [14720/50000]\tLoss: 4.6871\tLR: 7.629156\n",
      "Training Epoch: 77 [14848/50000]\tLoss: 4.7108\tLR: 7.629412\n",
      "Training Epoch: 77 [14976/50000]\tLoss: 4.7078\tLR: 7.629668\n",
      "Training Epoch: 77 [15104/50000]\tLoss: 4.7207\tLR: 7.629923\n",
      "Training Epoch: 77 [15232/50000]\tLoss: 4.7698\tLR: 7.630179\n",
      "Training Epoch: 77 [15360/50000]\tLoss: 4.6952\tLR: 7.630435\n",
      "Training Epoch: 77 [15488/50000]\tLoss: 4.6722\tLR: 7.630691\n",
      "Training Epoch: 77 [15616/50000]\tLoss: 4.7694\tLR: 7.630946\n",
      "Training Epoch: 77 [15744/50000]\tLoss: 4.7588\tLR: 7.631202\n",
      "Training Epoch: 77 [15872/50000]\tLoss: 4.7457\tLR: 7.631458\n",
      "Training Epoch: 77 [16000/50000]\tLoss: 4.7555\tLR: 7.631714\n",
      "Training Epoch: 77 [16128/50000]\tLoss: 4.6712\tLR: 7.631969\n",
      "Training Epoch: 77 [16256/50000]\tLoss: 4.8141\tLR: 7.632225\n",
      "Training Epoch: 77 [16384/50000]\tLoss: 4.7406\tLR: 7.632481\n",
      "Training Epoch: 77 [16512/50000]\tLoss: 4.7391\tLR: 7.632737\n",
      "Training Epoch: 77 [16640/50000]\tLoss: 4.7290\tLR: 7.632992\n",
      "Training Epoch: 77 [16768/50000]\tLoss: 4.8316\tLR: 7.633248\n",
      "Training Epoch: 77 [16896/50000]\tLoss: 4.7616\tLR: 7.633504\n",
      "Training Epoch: 77 [17024/50000]\tLoss: 4.6547\tLR: 7.633760\n",
      "Training Epoch: 77 [17152/50000]\tLoss: 4.7608\tLR: 7.634015\n",
      "Training Epoch: 77 [17280/50000]\tLoss: 4.6845\tLR: 7.634271\n",
      "Training Epoch: 77 [17408/50000]\tLoss: 4.7127\tLR: 7.634527\n",
      "Training Epoch: 77 [17536/50000]\tLoss: 4.8338\tLR: 7.634783\n",
      "Training Epoch: 77 [17664/50000]\tLoss: 4.7232\tLR: 7.635038\n",
      "Training Epoch: 77 [17792/50000]\tLoss: 4.7305\tLR: 7.635294\n",
      "Training Epoch: 77 [17920/50000]\tLoss: 4.8201\tLR: 7.635550\n",
      "Training Epoch: 77 [18048/50000]\tLoss: 4.6441\tLR: 7.635806\n",
      "Training Epoch: 77 [18176/50000]\tLoss: 4.6809\tLR: 7.636061\n",
      "Training Epoch: 77 [18304/50000]\tLoss: 4.6861\tLR: 7.636317\n",
      "Training Epoch: 77 [18432/50000]\tLoss: 4.6777\tLR: 7.636573\n",
      "Training Epoch: 77 [18560/50000]\tLoss: 4.7895\tLR: 7.636829\n",
      "Training Epoch: 77 [18688/50000]\tLoss: 4.6916\tLR: 7.637084\n",
      "Training Epoch: 77 [18816/50000]\tLoss: 4.6480\tLR: 7.637340\n",
      "Training Epoch: 77 [18944/50000]\tLoss: 4.6647\tLR: 7.637596\n",
      "Training Epoch: 77 [19072/50000]\tLoss: 4.8112\tLR: 7.637852\n",
      "Training Epoch: 77 [19200/50000]\tLoss: 4.7038\tLR: 7.638107\n",
      "Training Epoch: 77 [19328/50000]\tLoss: 4.7482\tLR: 7.638363\n",
      "Training Epoch: 77 [19456/50000]\tLoss: 4.6928\tLR: 7.638619\n",
      "Training Epoch: 77 [19584/50000]\tLoss: 4.6079\tLR: 7.638875\n",
      "Training Epoch: 77 [19712/50000]\tLoss: 4.7567\tLR: 7.639130\n",
      "Training Epoch: 77 [19840/50000]\tLoss: 4.8780\tLR: 7.639386\n",
      "Training Epoch: 77 [19968/50000]\tLoss: 4.6798\tLR: 7.639642\n",
      "Training Epoch: 77 [20096/50000]\tLoss: 4.7757\tLR: 7.639898\n",
      "Training Epoch: 77 [20224/50000]\tLoss: 4.7112\tLR: 7.640153\n",
      "Training Epoch: 77 [20352/50000]\tLoss: 4.6809\tLR: 7.640409\n",
      "Training Epoch: 77 [20480/50000]\tLoss: 4.7295\tLR: 7.640665\n",
      "Training Epoch: 77 [20608/50000]\tLoss: 4.7311\tLR: 7.640921\n",
      "Training Epoch: 77 [20736/50000]\tLoss: 4.6934\tLR: 7.641176\n",
      "Training Epoch: 77 [20864/50000]\tLoss: 4.7267\tLR: 7.641432\n",
      "Training Epoch: 77 [20992/50000]\tLoss: 4.7153\tLR: 7.641688\n",
      "Training Epoch: 77 [21120/50000]\tLoss: 4.7509\tLR: 7.641944\n",
      "Training Epoch: 77 [21248/50000]\tLoss: 4.7281\tLR: 7.642199\n",
      "Training Epoch: 77 [21376/50000]\tLoss: 4.7532\tLR: 7.642455\n",
      "Training Epoch: 77 [21504/50000]\tLoss: 4.7847\tLR: 7.642711\n",
      "Training Epoch: 77 [21632/50000]\tLoss: 4.8007\tLR: 7.642967\n",
      "Training Epoch: 77 [21760/50000]\tLoss: 4.7780\tLR: 7.643223\n",
      "Training Epoch: 77 [21888/50000]\tLoss: 4.8342\tLR: 7.643478\n",
      "Training Epoch: 77 [22016/50000]\tLoss: 4.7900\tLR: 7.643734\n",
      "Training Epoch: 77 [22144/50000]\tLoss: 4.7114\tLR: 7.643990\n",
      "Training Epoch: 77 [22272/50000]\tLoss: 4.8004\tLR: 7.644246\n",
      "Training Epoch: 77 [22400/50000]\tLoss: 4.8088\tLR: 7.644501\n",
      "Training Epoch: 77 [22528/50000]\tLoss: 4.7377\tLR: 7.644757\n",
      "Training Epoch: 77 [22656/50000]\tLoss: 4.6837\tLR: 7.645013\n",
      "Training Epoch: 77 [22784/50000]\tLoss: 4.7418\tLR: 7.645269\n",
      "Training Epoch: 77 [22912/50000]\tLoss: 4.7749\tLR: 7.645524\n",
      "Training Epoch: 77 [23040/50000]\tLoss: 4.7837\tLR: 7.645780\n",
      "Training Epoch: 77 [23168/50000]\tLoss: 4.8773\tLR: 7.646036\n",
      "Training Epoch: 77 [23296/50000]\tLoss: 4.7652\tLR: 7.646292\n",
      "Training Epoch: 77 [23424/50000]\tLoss: 4.7337\tLR: 7.646547\n",
      "Training Epoch: 77 [23552/50000]\tLoss: 4.7342\tLR: 7.646803\n",
      "Training Epoch: 77 [23680/50000]\tLoss: 4.6902\tLR: 7.647059\n",
      "Training Epoch: 77 [23808/50000]\tLoss: 4.7141\tLR: 7.647315\n",
      "Training Epoch: 77 [23936/50000]\tLoss: 4.7201\tLR: 7.647570\n",
      "Training Epoch: 77 [24064/50000]\tLoss: 4.8160\tLR: 7.647826\n",
      "Training Epoch: 77 [24192/50000]\tLoss: 4.7261\tLR: 7.648082\n",
      "Training Epoch: 77 [24320/50000]\tLoss: 4.6970\tLR: 7.648338\n",
      "Training Epoch: 77 [24448/50000]\tLoss: 4.7891\tLR: 7.648593\n",
      "Training Epoch: 77 [24576/50000]\tLoss: 4.7667\tLR: 7.648849\n",
      "Training Epoch: 77 [24704/50000]\tLoss: 4.8011\tLR: 7.649105\n",
      "Training Epoch: 77 [24832/50000]\tLoss: 4.7090\tLR: 7.649361\n",
      "Training Epoch: 77 [24960/50000]\tLoss: 4.7861\tLR: 7.649616\n",
      "Training Epoch: 77 [25088/50000]\tLoss: 4.7907\tLR: 7.649872\n",
      "Training Epoch: 77 [25216/50000]\tLoss: 4.7725\tLR: 7.650128\n",
      "Training Epoch: 77 [25344/50000]\tLoss: 4.7200\tLR: 7.650384\n",
      "Training Epoch: 77 [25472/50000]\tLoss: 4.7259\tLR: 7.650639\n",
      "Training Epoch: 77 [25600/50000]\tLoss: 4.6852\tLR: 7.650895\n",
      "Training Epoch: 77 [25728/50000]\tLoss: 4.7099\tLR: 7.651151\n",
      "Training Epoch: 77 [25856/50000]\tLoss: 4.8251\tLR: 7.651407\n",
      "Training Epoch: 77 [25984/50000]\tLoss: 4.6918\tLR: 7.651662\n",
      "Training Epoch: 77 [26112/50000]\tLoss: 4.7068\tLR: 7.651918\n",
      "Training Epoch: 77 [26240/50000]\tLoss: 4.8035\tLR: 7.652174\n",
      "Training Epoch: 77 [26368/50000]\tLoss: 4.6868\tLR: 7.652430\n",
      "Training Epoch: 77 [26496/50000]\tLoss: 4.7031\tLR: 7.652685\n",
      "Training Epoch: 77 [26624/50000]\tLoss: 4.7714\tLR: 7.652941\n",
      "Training Epoch: 77 [26752/50000]\tLoss: 4.7199\tLR: 7.653197\n",
      "Training Epoch: 77 [26880/50000]\tLoss: 4.7053\tLR: 7.653453\n",
      "Training Epoch: 77 [27008/50000]\tLoss: 4.7419\tLR: 7.653708\n",
      "Training Epoch: 77 [27136/50000]\tLoss: 4.7798\tLR: 7.653964\n",
      "Training Epoch: 77 [27264/50000]\tLoss: 4.8657\tLR: 7.654220\n",
      "Training Epoch: 77 [27392/50000]\tLoss: 4.6593\tLR: 7.654476\n",
      "Training Epoch: 77 [27520/50000]\tLoss: 4.6973\tLR: 7.654731\n",
      "Training Epoch: 77 [27648/50000]\tLoss: 4.7396\tLR: 7.654987\n",
      "Training Epoch: 77 [27776/50000]\tLoss: 4.6931\tLR: 7.655243\n",
      "Training Epoch: 77 [27904/50000]\tLoss: 4.7980\tLR: 7.655499\n",
      "Training Epoch: 77 [28032/50000]\tLoss: 4.8098\tLR: 7.655754\n",
      "Training Epoch: 77 [28160/50000]\tLoss: 4.7984\tLR: 7.656010\n",
      "Training Epoch: 77 [28288/50000]\tLoss: 4.7888\tLR: 7.656266\n",
      "Training Epoch: 77 [28416/50000]\tLoss: 4.7966\tLR: 7.656522\n",
      "Training Epoch: 77 [28544/50000]\tLoss: 4.6744\tLR: 7.656777\n",
      "Training Epoch: 77 [28672/50000]\tLoss: 4.7025\tLR: 7.657033\n",
      "Training Epoch: 77 [28800/50000]\tLoss: 4.8559\tLR: 7.657289\n",
      "Training Epoch: 77 [28928/50000]\tLoss: 4.6858\tLR: 7.657545\n",
      "Training Epoch: 77 [29056/50000]\tLoss: 4.7469\tLR: 7.657801\n",
      "Training Epoch: 77 [29184/50000]\tLoss: 4.7122\tLR: 7.658056\n",
      "Training Epoch: 77 [29312/50000]\tLoss: 4.7621\tLR: 7.658312\n",
      "Training Epoch: 77 [29440/50000]\tLoss: 4.7861\tLR: 7.658568\n",
      "Training Epoch: 77 [29568/50000]\tLoss: 4.6410\tLR: 7.658824\n",
      "Training Epoch: 77 [29696/50000]\tLoss: 4.6701\tLR: 7.659079\n",
      "Training Epoch: 77 [29824/50000]\tLoss: 4.7430\tLR: 7.659335\n",
      "Training Epoch: 77 [29952/50000]\tLoss: 4.7472\tLR: 7.659591\n",
      "Training Epoch: 77 [30080/50000]\tLoss: 4.8720\tLR: 7.659847\n",
      "Training Epoch: 77 [30208/50000]\tLoss: 4.7394\tLR: 7.660102\n",
      "Training Epoch: 77 [30336/50000]\tLoss: 4.6974\tLR: 7.660358\n",
      "Training Epoch: 77 [30464/50000]\tLoss: 4.7900\tLR: 7.660614\n",
      "Training Epoch: 77 [30592/50000]\tLoss: 4.7342\tLR: 7.660870\n",
      "Training Epoch: 77 [30720/50000]\tLoss: 4.7958\tLR: 7.661125\n",
      "Training Epoch: 77 [30848/50000]\tLoss: 4.8144\tLR: 7.661381\n",
      "Training Epoch: 77 [30976/50000]\tLoss: 4.7285\tLR: 7.661637\n",
      "Training Epoch: 77 [31104/50000]\tLoss: 4.6390\tLR: 7.661893\n",
      "Training Epoch: 77 [31232/50000]\tLoss: 4.8396\tLR: 7.662148\n",
      "Training Epoch: 77 [31360/50000]\tLoss: 4.7957\tLR: 7.662404\n",
      "Training Epoch: 77 [31488/50000]\tLoss: 4.7799\tLR: 7.662660\n",
      "Training Epoch: 77 [31616/50000]\tLoss: 4.7417\tLR: 7.662916\n",
      "Training Epoch: 77 [31744/50000]\tLoss: 4.7232\tLR: 7.663171\n",
      "Training Epoch: 77 [31872/50000]\tLoss: 4.7229\tLR: 7.663427\n",
      "Training Epoch: 77 [32000/50000]\tLoss: 4.7090\tLR: 7.663683\n",
      "Training Epoch: 77 [32128/50000]\tLoss: 4.7095\tLR: 7.663939\n",
      "Training Epoch: 77 [32256/50000]\tLoss: 4.7455\tLR: 7.664194\n",
      "Training Epoch: 77 [32384/50000]\tLoss: 4.6616\tLR: 7.664450\n",
      "Training Epoch: 77 [32512/50000]\tLoss: 4.7810\tLR: 7.664706\n",
      "Training Epoch: 77 [32640/50000]\tLoss: 4.7437\tLR: 7.664962\n",
      "Training Epoch: 77 [32768/50000]\tLoss: 4.8358\tLR: 7.665217\n",
      "Training Epoch: 77 [32896/50000]\tLoss: 4.7023\tLR: 7.665473\n",
      "Training Epoch: 77 [33024/50000]\tLoss: 4.7434\tLR: 7.665729\n",
      "Training Epoch: 77 [33152/50000]\tLoss: 4.7529\tLR: 7.665985\n",
      "Training Epoch: 77 [33280/50000]\tLoss: 4.7614\tLR: 7.666240\n",
      "Training Epoch: 77 [33408/50000]\tLoss: 4.6641\tLR: 7.666496\n",
      "Training Epoch: 77 [33536/50000]\tLoss: 4.7565\tLR: 7.666752\n",
      "Training Epoch: 77 [33664/50000]\tLoss: 4.6907\tLR: 7.667008\n",
      "Training Epoch: 77 [33792/50000]\tLoss: 4.7865\tLR: 7.667263\n",
      "Training Epoch: 77 [33920/50000]\tLoss: 4.7299\tLR: 7.667519\n",
      "Training Epoch: 77 [34048/50000]\tLoss: 4.7575\tLR: 7.667775\n",
      "Training Epoch: 77 [34176/50000]\tLoss: 4.6973\tLR: 7.668031\n",
      "Training Epoch: 77 [34304/50000]\tLoss: 4.7703\tLR: 7.668286\n",
      "Training Epoch: 77 [34432/50000]\tLoss: 4.7364\tLR: 7.668542\n",
      "Training Epoch: 77 [34560/50000]\tLoss: 4.7203\tLR: 7.668798\n",
      "Training Epoch: 77 [34688/50000]\tLoss: 4.7563\tLR: 7.669054\n",
      "Training Epoch: 77 [34816/50000]\tLoss: 4.7082\tLR: 7.669309\n",
      "Training Epoch: 77 [34944/50000]\tLoss: 4.8068\tLR: 7.669565\n",
      "Training Epoch: 77 [35072/50000]\tLoss: 4.8743\tLR: 7.669821\n",
      "Training Epoch: 77 [35200/50000]\tLoss: 4.6740\tLR: 7.670077\n",
      "Training Epoch: 77 [35328/50000]\tLoss: 4.7117\tLR: 7.670332\n",
      "Training Epoch: 77 [35456/50000]\tLoss: 4.7619\tLR: 7.670588\n",
      "Training Epoch: 77 [35584/50000]\tLoss: 4.7150\tLR: 7.670844\n",
      "Training Epoch: 77 [35712/50000]\tLoss: 4.7097\tLR: 7.671100\n",
      "Training Epoch: 77 [35840/50000]\tLoss: 4.7474\tLR: 7.671355\n",
      "Training Epoch: 77 [35968/50000]\tLoss: 4.7464\tLR: 7.671611\n",
      "Training Epoch: 77 [36096/50000]\tLoss: 4.7405\tLR: 7.671867\n",
      "Training Epoch: 77 [36224/50000]\tLoss: 4.7251\tLR: 7.672123\n",
      "Training Epoch: 77 [36352/50000]\tLoss: 4.6691\tLR: 7.672379\n",
      "Training Epoch: 77 [36480/50000]\tLoss: 4.8543\tLR: 7.672634\n",
      "Training Epoch: 77 [36608/50000]\tLoss: 4.7490\tLR: 7.672890\n",
      "Training Epoch: 77 [36736/50000]\tLoss: 4.7964\tLR: 7.673146\n",
      "Training Epoch: 77 [36864/50000]\tLoss: 4.7329\tLR: 7.673402\n",
      "Training Epoch: 77 [36992/50000]\tLoss: 4.8973\tLR: 7.673657\n",
      "Training Epoch: 77 [37120/50000]\tLoss: 4.7044\tLR: 7.673913\n",
      "Training Epoch: 77 [37248/50000]\tLoss: 4.7383\tLR: 7.674169\n",
      "Training Epoch: 77 [37376/50000]\tLoss: 4.6615\tLR: 7.674425\n",
      "Training Epoch: 77 [37504/50000]\tLoss: 4.7512\tLR: 7.674680\n",
      "Training Epoch: 77 [37632/50000]\tLoss: 4.7601\tLR: 7.674936\n",
      "Training Epoch: 77 [37760/50000]\tLoss: 4.7905\tLR: 7.675192\n",
      "Training Epoch: 77 [37888/50000]\tLoss: 4.8119\tLR: 7.675448\n",
      "Training Epoch: 77 [38016/50000]\tLoss: 4.7712\tLR: 7.675703\n",
      "Training Epoch: 77 [38144/50000]\tLoss: 4.8288\tLR: 7.675959\n",
      "Training Epoch: 77 [38272/50000]\tLoss: 4.7479\tLR: 7.676215\n",
      "Training Epoch: 77 [38400/50000]\tLoss: 4.7382\tLR: 7.676471\n",
      "Training Epoch: 77 [38528/50000]\tLoss: 4.7295\tLR: 7.676726\n",
      "Training Epoch: 77 [38656/50000]\tLoss: 4.7715\tLR: 7.676982\n",
      "Training Epoch: 77 [38784/50000]\tLoss: 4.8001\tLR: 7.677238\n",
      "Training Epoch: 77 [38912/50000]\tLoss: 4.8058\tLR: 7.677494\n",
      "Training Epoch: 77 [39040/50000]\tLoss: 4.7423\tLR: 7.677749\n",
      "Training Epoch: 77 [39168/50000]\tLoss: 4.7402\tLR: 7.678005\n",
      "Training Epoch: 77 [39296/50000]\tLoss: 4.6558\tLR: 7.678261\n",
      "Training Epoch: 77 [39424/50000]\tLoss: 4.7471\tLR: 7.678517\n",
      "Training Epoch: 77 [39552/50000]\tLoss: 4.7192\tLR: 7.678772\n",
      "Training Epoch: 77 [39680/50000]\tLoss: 4.8001\tLR: 7.679028\n",
      "Training Epoch: 77 [39808/50000]\tLoss: 4.7612\tLR: 7.679284\n",
      "Training Epoch: 77 [39936/50000]\tLoss: 4.7602\tLR: 7.679540\n",
      "Training Epoch: 77 [40064/50000]\tLoss: 4.7106\tLR: 7.679795\n",
      "Training Epoch: 77 [40192/50000]\tLoss: 4.6920\tLR: 7.680051\n",
      "Training Epoch: 77 [40320/50000]\tLoss: 4.6856\tLR: 7.680307\n",
      "Training Epoch: 77 [40448/50000]\tLoss: 4.7485\tLR: 7.680563\n",
      "Training Epoch: 77 [40576/50000]\tLoss: 4.7026\tLR: 7.680818\n",
      "Training Epoch: 77 [40704/50000]\tLoss: 4.8075\tLR: 7.681074\n",
      "Training Epoch: 77 [40832/50000]\tLoss: 4.7171\tLR: 7.681330\n",
      "Training Epoch: 77 [40960/50000]\tLoss: 4.8013\tLR: 7.681586\n",
      "Training Epoch: 77 [41088/50000]\tLoss: 4.7520\tLR: 7.681841\n",
      "Training Epoch: 77 [41216/50000]\tLoss: 4.7467\tLR: 7.682097\n",
      "Training Epoch: 77 [41344/50000]\tLoss: 4.8426\tLR: 7.682353\n",
      "Training Epoch: 77 [41472/50000]\tLoss: 4.8007\tLR: 7.682609\n",
      "Training Epoch: 77 [41600/50000]\tLoss: 4.7238\tLR: 7.682864\n",
      "Training Epoch: 77 [41728/50000]\tLoss: 4.7061\tLR: 7.683120\n",
      "Training Epoch: 77 [41856/50000]\tLoss: 4.7248\tLR: 7.683376\n",
      "Training Epoch: 77 [41984/50000]\tLoss: 4.7700\tLR: 7.683632\n",
      "Training Epoch: 77 [42112/50000]\tLoss: 4.8015\tLR: 7.683887\n",
      "Training Epoch: 77 [42240/50000]\tLoss: 4.8783\tLR: 7.684143\n",
      "Training Epoch: 77 [42368/50000]\tLoss: 4.7950\tLR: 7.684399\n",
      "Training Epoch: 77 [42496/50000]\tLoss: 4.7797\tLR: 7.684655\n",
      "Training Epoch: 77 [42624/50000]\tLoss: 4.6620\tLR: 7.684910\n",
      "Training Epoch: 77 [42752/50000]\tLoss: 4.8505\tLR: 7.685166\n",
      "Training Epoch: 77 [42880/50000]\tLoss: 4.7384\tLR: 7.685422\n",
      "Training Epoch: 77 [43008/50000]\tLoss: 4.6515\tLR: 7.685678\n",
      "Training Epoch: 77 [43136/50000]\tLoss: 4.7440\tLR: 7.685934\n",
      "Training Epoch: 77 [43264/50000]\tLoss: 4.7812\tLR: 7.686189\n",
      "Training Epoch: 77 [43392/50000]\tLoss: 4.7865\tLR: 7.686445\n",
      "Training Epoch: 77 [43520/50000]\tLoss: 4.7646\tLR: 7.686701\n",
      "Training Epoch: 77 [43648/50000]\tLoss: 4.8033\tLR: 7.686957\n",
      "Training Epoch: 77 [43776/50000]\tLoss: 4.7410\tLR: 7.687212\n",
      "Training Epoch: 77 [43904/50000]\tLoss: 4.7720\tLR: 7.687468\n",
      "Training Epoch: 77 [44032/50000]\tLoss: 4.7075\tLR: 7.687724\n",
      "Training Epoch: 77 [44160/50000]\tLoss: 4.7519\tLR: 7.687980\n",
      "Training Epoch: 77 [44288/50000]\tLoss: 4.6765\tLR: 7.688235\n",
      "Training Epoch: 77 [44416/50000]\tLoss: 4.7859\tLR: 7.688491\n",
      "Training Epoch: 77 [44544/50000]\tLoss: 4.7705\tLR: 7.688747\n",
      "Training Epoch: 77 [44672/50000]\tLoss: 4.8541\tLR: 7.689003\n",
      "Training Epoch: 77 [44800/50000]\tLoss: 4.8597\tLR: 7.689258\n",
      "Training Epoch: 77 [44928/50000]\tLoss: 4.7203\tLR: 7.689514\n",
      "Training Epoch: 77 [45056/50000]\tLoss: 4.8249\tLR: 7.689770\n",
      "Training Epoch: 77 [45184/50000]\tLoss: 4.7167\tLR: 7.690026\n",
      "Training Epoch: 77 [45312/50000]\tLoss: 4.7085\tLR: 7.690281\n",
      "Training Epoch: 77 [45440/50000]\tLoss: 4.7643\tLR: 7.690537\n",
      "Training Epoch: 77 [45568/50000]\tLoss: 4.7651\tLR: 7.690793\n",
      "Training Epoch: 77 [45696/50000]\tLoss: 4.7559\tLR: 7.691049\n",
      "Training Epoch: 77 [45824/50000]\tLoss: 4.6866\tLR: 7.691304\n",
      "Training Epoch: 77 [45952/50000]\tLoss: 4.7636\tLR: 7.691560\n",
      "Training Epoch: 77 [46080/50000]\tLoss: 4.7836\tLR: 7.691816\n",
      "Training Epoch: 77 [46208/50000]\tLoss: 4.7827\tLR: 7.692072\n",
      "Training Epoch: 77 [46336/50000]\tLoss: 4.8038\tLR: 7.692327\n",
      "Training Epoch: 77 [46464/50000]\tLoss: 4.7411\tLR: 7.692583\n",
      "Training Epoch: 77 [46592/50000]\tLoss: 4.7066\tLR: 7.692839\n",
      "Training Epoch: 77 [46720/50000]\tLoss: 4.6785\tLR: 7.693095\n",
      "Training Epoch: 77 [46848/50000]\tLoss: 4.8830\tLR: 7.693350\n",
      "Training Epoch: 77 [46976/50000]\tLoss: 4.7186\tLR: 7.693606\n",
      "Training Epoch: 77 [47104/50000]\tLoss: 4.7871\tLR: 7.693862\n",
      "Training Epoch: 77 [47232/50000]\tLoss: 4.7810\tLR: 7.694118\n",
      "Training Epoch: 77 [47360/50000]\tLoss: 4.7689\tLR: 7.694373\n",
      "Training Epoch: 77 [47488/50000]\tLoss: 4.7146\tLR: 7.694629\n",
      "Training Epoch: 77 [47616/50000]\tLoss: 4.7056\tLR: 7.694885\n",
      "Training Epoch: 77 [47744/50000]\tLoss: 4.7317\tLR: 7.695141\n",
      "Training Epoch: 77 [47872/50000]\tLoss: 4.8189\tLR: 7.695396\n",
      "Training Epoch: 77 [48000/50000]\tLoss: 4.7687\tLR: 7.695652\n",
      "Training Epoch: 77 [48128/50000]\tLoss: 4.7089\tLR: 7.695908\n",
      "Training Epoch: 77 [48256/50000]\tLoss: 4.7258\tLR: 7.696164\n",
      "Training Epoch: 77 [48384/50000]\tLoss: 4.7392\tLR: 7.696419\n",
      "Training Epoch: 77 [48512/50000]\tLoss: 4.9200\tLR: 7.696675\n",
      "Training Epoch: 77 [48640/50000]\tLoss: 4.8281\tLR: 7.696931\n",
      "Training Epoch: 77 [48768/50000]\tLoss: 4.6986\tLR: 7.697187\n",
      "Training Epoch: 77 [48896/50000]\tLoss: 4.8072\tLR: 7.697442\n",
      "Training Epoch: 77 [49024/50000]\tLoss: 4.8455\tLR: 7.697698\n",
      "Training Epoch: 77 [49152/50000]\tLoss: 4.6843\tLR: 7.697954\n",
      "Training Epoch: 77 [49280/50000]\tLoss: 4.6760\tLR: 7.698210\n",
      "Training Epoch: 77 [49408/50000]\tLoss: 4.7193\tLR: 7.698465\n",
      "Training Epoch: 77 [49536/50000]\tLoss: 4.6660\tLR: 7.698721\n",
      "Training Epoch: 77 [49664/50000]\tLoss: 4.7647\tLR: 7.698977\n",
      "Training Epoch: 77 [49792/50000]\tLoss: 4.7967\tLR: 7.699233\n",
      "Training Epoch: 77 [49920/50000]\tLoss: 4.7686\tLR: 7.699488\n",
      "Training Epoch: 77 [50000/50000]\tLoss: 4.6689\tLR: 7.699744\n",
      "epoch 77 training time consumed: 489.29s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  107949 GB |  107948 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  107617 GB |  107617 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     331 GB |     331 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  107949 GB |  107948 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  107617 GB |  107617 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     331 GB |     331 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  106431 GB |  106431 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  106100 GB |  106100 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     331 GB |     331 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   11446 K  |   11446 K  |\n",
      "|       from large pool |      24    |      65    |    4879 K  |    4879 K  |\n",
      "|       from small pool |     231    |     274    |    6566 K  |    6566 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   11446 K  |   11446 K  |\n",
      "|       from large pool |      24    |      65    |    4879 K  |    4879 K  |\n",
      "|       from small pool |     231    |     274    |    6566 K  |    6566 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      47    |    6634 K  |    6634 K  |\n",
      "|       from large pool |      10    |      23    |    2345 K  |    2345 K  |\n",
      "|       from small pool |      27    |      35    |    4289 K  |    4289 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 77, Average loss: 0.0375, Accuracy: 0.0100, Time consumed:31.26s\n",
      "\n",
      "Training Epoch: 78 [128/50000]\tLoss: 4.7817\tLR: 0.020000\n",
      "Training Epoch: 78 [256/50000]\tLoss: 4.8005\tLR: 7.700256\n",
      "Training Epoch: 78 [384/50000]\tLoss: 4.6766\tLR: 7.700512\n",
      "Training Epoch: 78 [512/50000]\tLoss: 4.7364\tLR: 7.700767\n",
      "Training Epoch: 78 [640/50000]\tLoss: 4.7442\tLR: 7.701023\n",
      "Training Epoch: 78 [768/50000]\tLoss: 4.7985\tLR: 7.701279\n",
      "Training Epoch: 78 [896/50000]\tLoss: 4.7967\tLR: 7.701535\n",
      "Training Epoch: 78 [1024/50000]\tLoss: 4.7807\tLR: 7.701790\n",
      "Training Epoch: 78 [1152/50000]\tLoss: 4.6595\tLR: 7.702046\n",
      "Training Epoch: 78 [1280/50000]\tLoss: 4.7350\tLR: 7.702302\n",
      "Training Epoch: 78 [1408/50000]\tLoss: 4.7170\tLR: 7.702558\n",
      "Training Epoch: 78 [1536/50000]\tLoss: 4.7945\tLR: 7.702813\n",
      "Training Epoch: 78 [1664/50000]\tLoss: 4.7293\tLR: 7.703069\n",
      "Training Epoch: 78 [1792/50000]\tLoss: 4.7329\tLR: 7.703325\n",
      "Training Epoch: 78 [1920/50000]\tLoss: 4.8619\tLR: 7.703581\n",
      "Training Epoch: 78 [2048/50000]\tLoss: 4.7957\tLR: 7.703836\n",
      "Training Epoch: 78 [2176/50000]\tLoss: 4.7555\tLR: 7.704092\n",
      "Training Epoch: 78 [2304/50000]\tLoss: 4.8043\tLR: 7.704348\n",
      "Training Epoch: 78 [2432/50000]\tLoss: 4.8993\tLR: 7.704604\n",
      "Training Epoch: 78 [2560/50000]\tLoss: 4.6926\tLR: 7.704859\n",
      "Training Epoch: 78 [2688/50000]\tLoss: 4.7281\tLR: 7.705115\n",
      "Training Epoch: 78 [2816/50000]\tLoss: 4.7216\tLR: 7.705371\n",
      "Training Epoch: 78 [2944/50000]\tLoss: 4.7924\tLR: 7.705627\n",
      "Training Epoch: 78 [3072/50000]\tLoss: 4.7569\tLR: 7.705882\n",
      "Training Epoch: 78 [3200/50000]\tLoss: 4.8539\tLR: 7.706138\n",
      "Training Epoch: 78 [3328/50000]\tLoss: 4.6824\tLR: 7.706394\n",
      "Training Epoch: 78 [3456/50000]\tLoss: 4.6746\tLR: 7.706650\n",
      "Training Epoch: 78 [3584/50000]\tLoss: 4.7220\tLR: 7.706905\n",
      "Training Epoch: 78 [3712/50000]\tLoss: 4.8093\tLR: 7.707161\n",
      "Training Epoch: 78 [3840/50000]\tLoss: 4.8323\tLR: 7.707417\n",
      "Training Epoch: 78 [3968/50000]\tLoss: 4.8087\tLR: 7.707673\n",
      "Training Epoch: 78 [4096/50000]\tLoss: 4.7178\tLR: 7.707928\n",
      "Training Epoch: 78 [4224/50000]\tLoss: 4.7600\tLR: 7.708184\n",
      "Training Epoch: 78 [4352/50000]\tLoss: 4.8231\tLR: 7.708440\n",
      "Training Epoch: 78 [4480/50000]\tLoss: 4.7693\tLR: 7.708696\n",
      "Training Epoch: 78 [4608/50000]\tLoss: 4.7521\tLR: 7.708951\n",
      "Training Epoch: 78 [4736/50000]\tLoss: 4.7164\tLR: 7.709207\n",
      "Training Epoch: 78 [4864/50000]\tLoss: 4.7124\tLR: 7.709463\n",
      "Training Epoch: 78 [4992/50000]\tLoss: 4.7690\tLR: 7.709719\n",
      "Training Epoch: 78 [5120/50000]\tLoss: 4.7314\tLR: 7.709974\n",
      "Training Epoch: 78 [5248/50000]\tLoss: 4.7566\tLR: 7.710230\n",
      "Training Epoch: 78 [5376/50000]\tLoss: 4.8273\tLR: 7.710486\n",
      "Training Epoch: 78 [5504/50000]\tLoss: 4.8230\tLR: 7.710742\n",
      "Training Epoch: 78 [5632/50000]\tLoss: 4.7592\tLR: 7.710997\n",
      "Training Epoch: 78 [5760/50000]\tLoss: 4.7285\tLR: 7.711253\n",
      "Training Epoch: 78 [5888/50000]\tLoss: 4.7956\tLR: 7.711509\n",
      "Training Epoch: 78 [6016/50000]\tLoss: 4.7767\tLR: 7.711765\n",
      "Training Epoch: 78 [6144/50000]\tLoss: 4.7553\tLR: 7.712020\n",
      "Training Epoch: 78 [6272/50000]\tLoss: 4.7505\tLR: 7.712276\n",
      "Training Epoch: 78 [6400/50000]\tLoss: 4.6261\tLR: 7.712532\n",
      "Training Epoch: 78 [6528/50000]\tLoss: 4.7408\tLR: 7.712788\n",
      "Training Epoch: 78 [6656/50000]\tLoss: 4.6799\tLR: 7.713043\n",
      "Training Epoch: 78 [6784/50000]\tLoss: 4.7802\tLR: 7.713299\n",
      "Training Epoch: 78 [6912/50000]\tLoss: 4.6694\tLR: 7.713555\n",
      "Training Epoch: 78 [7040/50000]\tLoss: 4.8176\tLR: 7.713811\n",
      "Training Epoch: 78 [7168/50000]\tLoss: 4.7534\tLR: 7.714066\n",
      "Training Epoch: 78 [7296/50000]\tLoss: 4.7632\tLR: 7.714322\n",
      "Training Epoch: 78 [7424/50000]\tLoss: 4.7102\tLR: 7.714578\n",
      "Training Epoch: 78 [7552/50000]\tLoss: 4.8422\tLR: 7.714834\n",
      "Training Epoch: 78 [7680/50000]\tLoss: 4.7422\tLR: 7.715090\n",
      "Training Epoch: 78 [7808/50000]\tLoss: 4.6431\tLR: 7.715345\n",
      "Training Epoch: 78 [7936/50000]\tLoss: 4.7011\tLR: 7.715601\n",
      "Training Epoch: 78 [8064/50000]\tLoss: 4.7439\tLR: 7.715857\n",
      "Training Epoch: 78 [8192/50000]\tLoss: 4.6784\tLR: 7.716113\n",
      "Training Epoch: 78 [8320/50000]\tLoss: 4.8294\tLR: 7.716368\n",
      "Training Epoch: 78 [8448/50000]\tLoss: 4.7846\tLR: 7.716624\n",
      "Training Epoch: 78 [8576/50000]\tLoss: 4.7818\tLR: 7.716880\n",
      "Training Epoch: 78 [8704/50000]\tLoss: 4.7471\tLR: 7.717136\n",
      "Training Epoch: 78 [8832/50000]\tLoss: 4.8301\tLR: 7.717391\n",
      "Training Epoch: 78 [8960/50000]\tLoss: 4.7091\tLR: 7.717647\n",
      "Training Epoch: 78 [9088/50000]\tLoss: 4.6760\tLR: 7.717903\n",
      "Training Epoch: 78 [9216/50000]\tLoss: 4.7850\tLR: 7.718159\n",
      "Training Epoch: 78 [9344/50000]\tLoss: 4.8362\tLR: 7.718414\n",
      "Training Epoch: 78 [9472/50000]\tLoss: 4.8197\tLR: 7.718670\n",
      "Training Epoch: 78 [9600/50000]\tLoss: 4.7966\tLR: 7.718926\n",
      "Training Epoch: 78 [9728/50000]\tLoss: 4.8183\tLR: 7.719182\n",
      "Training Epoch: 78 [9856/50000]\tLoss: 4.7662\tLR: 7.719437\n",
      "Training Epoch: 78 [9984/50000]\tLoss: 4.8294\tLR: 7.719693\n",
      "Training Epoch: 78 [10112/50000]\tLoss: 4.6764\tLR: 7.719949\n",
      "Training Epoch: 78 [10240/50000]\tLoss: 4.7328\tLR: 7.720205\n",
      "Training Epoch: 78 [10368/50000]\tLoss: 4.7766\tLR: 7.720460\n",
      "Training Epoch: 78 [10496/50000]\tLoss: 4.6823\tLR: 7.720716\n",
      "Training Epoch: 78 [10624/50000]\tLoss: 4.7046\tLR: 7.720972\n",
      "Training Epoch: 78 [10752/50000]\tLoss: 4.8230\tLR: 7.721228\n",
      "Training Epoch: 78 [10880/50000]\tLoss: 4.7600\tLR: 7.721483\n",
      "Training Epoch: 78 [11008/50000]\tLoss: 4.7866\tLR: 7.721739\n",
      "Training Epoch: 78 [11136/50000]\tLoss: 4.7871\tLR: 7.721995\n",
      "Training Epoch: 78 [11264/50000]\tLoss: 4.7942\tLR: 7.722251\n",
      "Training Epoch: 78 [11392/50000]\tLoss: 4.7741\tLR: 7.722506\n",
      "Training Epoch: 78 [11520/50000]\tLoss: 4.7918\tLR: 7.722762\n",
      "Training Epoch: 78 [11648/50000]\tLoss: 4.7747\tLR: 7.723018\n",
      "Training Epoch: 78 [11776/50000]\tLoss: 4.7955\tLR: 7.723274\n",
      "Training Epoch: 78 [11904/50000]\tLoss: 4.7542\tLR: 7.723529\n",
      "Training Epoch: 78 [12032/50000]\tLoss: 4.8133\tLR: 7.723785\n",
      "Training Epoch: 78 [12160/50000]\tLoss: 4.7270\tLR: 7.724041\n",
      "Training Epoch: 78 [12288/50000]\tLoss: 4.8269\tLR: 7.724297\n",
      "Training Epoch: 78 [12416/50000]\tLoss: 4.8605\tLR: 7.724552\n",
      "Training Epoch: 78 [12544/50000]\tLoss: 4.7312\tLR: 7.724808\n",
      "Training Epoch: 78 [12672/50000]\tLoss: 4.7079\tLR: 7.725064\n",
      "Training Epoch: 78 [12800/50000]\tLoss: 4.7966\tLR: 7.725320\n",
      "Training Epoch: 78 [12928/50000]\tLoss: 4.6751\tLR: 7.725575\n",
      "Training Epoch: 78 [13056/50000]\tLoss: 4.7809\tLR: 7.725831\n",
      "Training Epoch: 78 [13184/50000]\tLoss: 4.6646\tLR: 7.726087\n",
      "Training Epoch: 78 [13312/50000]\tLoss: 4.7938\tLR: 7.726343\n",
      "Training Epoch: 78 [13440/50000]\tLoss: 4.7939\tLR: 7.726598\n",
      "Training Epoch: 78 [13568/50000]\tLoss: 4.7008\tLR: 7.726854\n",
      "Training Epoch: 78 [13696/50000]\tLoss: 4.6987\tLR: 7.727110\n",
      "Training Epoch: 78 [13824/50000]\tLoss: 4.7239\tLR: 7.727366\n",
      "Training Epoch: 78 [13952/50000]\tLoss: 4.7459\tLR: 7.727621\n",
      "Training Epoch: 78 [14080/50000]\tLoss: 4.7152\tLR: 7.727877\n",
      "Training Epoch: 78 [14208/50000]\tLoss: 4.6689\tLR: 7.728133\n",
      "Training Epoch: 78 [14336/50000]\tLoss: 4.7720\tLR: 7.728389\n",
      "Training Epoch: 78 [14464/50000]\tLoss: 4.7038\tLR: 7.728645\n",
      "Training Epoch: 78 [14592/50000]\tLoss: 4.7508\tLR: 7.728900\n",
      "Training Epoch: 78 [14720/50000]\tLoss: 4.7455\tLR: 7.729156\n",
      "Training Epoch: 78 [14848/50000]\tLoss: 4.6987\tLR: 7.729412\n",
      "Training Epoch: 78 [14976/50000]\tLoss: 4.7616\tLR: 7.729668\n",
      "Training Epoch: 78 [15104/50000]\tLoss: 4.7635\tLR: 7.729923\n",
      "Training Epoch: 78 [15232/50000]\tLoss: 4.7495\tLR: 7.730179\n",
      "Training Epoch: 78 [15360/50000]\tLoss: 4.7527\tLR: 7.730435\n",
      "Training Epoch: 78 [15488/50000]\tLoss: 4.7453\tLR: 7.730691\n",
      "Training Epoch: 78 [15616/50000]\tLoss: 4.7408\tLR: 7.730946\n",
      "Training Epoch: 78 [15744/50000]\tLoss: 4.7479\tLR: 7.731202\n",
      "Training Epoch: 78 [15872/50000]\tLoss: 4.7973\tLR: 7.731458\n",
      "Training Epoch: 78 [16000/50000]\tLoss: 4.7991\tLR: 7.731714\n",
      "Training Epoch: 78 [16128/50000]\tLoss: 4.7556\tLR: 7.731969\n",
      "Training Epoch: 78 [16256/50000]\tLoss: 4.7469\tLR: 7.732225\n",
      "Training Epoch: 78 [16384/50000]\tLoss: 4.7979\tLR: 7.732481\n",
      "Training Epoch: 78 [16512/50000]\tLoss: 4.7133\tLR: 7.732737\n",
      "Training Epoch: 78 [16640/50000]\tLoss: 4.7827\tLR: 7.732992\n",
      "Training Epoch: 78 [16768/50000]\tLoss: 4.7487\tLR: 7.733248\n",
      "Training Epoch: 78 [16896/50000]\tLoss: 4.7042\tLR: 7.733504\n",
      "Training Epoch: 78 [17024/50000]\tLoss: 4.7489\tLR: 7.733760\n",
      "Training Epoch: 78 [17152/50000]\tLoss: 4.7920\tLR: 7.734015\n",
      "Training Epoch: 78 [17280/50000]\tLoss: 4.8515\tLR: 7.734271\n",
      "Training Epoch: 78 [17408/50000]\tLoss: 4.8400\tLR: 7.734527\n",
      "Training Epoch: 78 [17536/50000]\tLoss: 4.7741\tLR: 7.734783\n",
      "Training Epoch: 78 [17664/50000]\tLoss: 4.7172\tLR: 7.735038\n",
      "Training Epoch: 78 [17792/50000]\tLoss: 4.6946\tLR: 7.735294\n",
      "Training Epoch: 78 [17920/50000]\tLoss: 4.7216\tLR: 7.735550\n",
      "Training Epoch: 78 [18048/50000]\tLoss: 4.7703\tLR: 7.735806\n",
      "Training Epoch: 78 [18176/50000]\tLoss: 4.6748\tLR: 7.736061\n",
      "Training Epoch: 78 [18304/50000]\tLoss: 4.7436\tLR: 7.736317\n",
      "Training Epoch: 78 [18432/50000]\tLoss: 4.7440\tLR: 7.736573\n",
      "Training Epoch: 78 [18560/50000]\tLoss: 4.6811\tLR: 7.736829\n",
      "Training Epoch: 78 [18688/50000]\tLoss: 4.7788\tLR: 7.737084\n",
      "Training Epoch: 78 [18816/50000]\tLoss: 4.7521\tLR: 7.737340\n",
      "Training Epoch: 78 [18944/50000]\tLoss: 4.6834\tLR: 7.737596\n",
      "Training Epoch: 78 [19072/50000]\tLoss: 4.8507\tLR: 7.737852\n",
      "Training Epoch: 78 [19200/50000]\tLoss: 4.8328\tLR: 7.738107\n",
      "Training Epoch: 78 [19328/50000]\tLoss: 4.7802\tLR: 7.738363\n",
      "Training Epoch: 78 [19456/50000]\tLoss: 4.7314\tLR: 7.738619\n",
      "Training Epoch: 78 [19584/50000]\tLoss: 4.6761\tLR: 7.738875\n",
      "Training Epoch: 78 [19712/50000]\tLoss: 4.7877\tLR: 7.739130\n",
      "Training Epoch: 78 [19840/50000]\tLoss: 4.7040\tLR: 7.739386\n",
      "Training Epoch: 78 [19968/50000]\tLoss: 4.7708\tLR: 7.739642\n",
      "Training Epoch: 78 [20096/50000]\tLoss: 4.7631\tLR: 7.739898\n",
      "Training Epoch: 78 [20224/50000]\tLoss: 4.7423\tLR: 7.740153\n",
      "Training Epoch: 78 [20352/50000]\tLoss: 4.7632\tLR: 7.740409\n",
      "Training Epoch: 78 [20480/50000]\tLoss: 4.7165\tLR: 7.740665\n",
      "Training Epoch: 78 [20608/50000]\tLoss: 4.8771\tLR: 7.740921\n",
      "Training Epoch: 78 [20736/50000]\tLoss: 4.7664\tLR: 7.741176\n",
      "Training Epoch: 78 [20864/50000]\tLoss: 4.7302\tLR: 7.741432\n",
      "Training Epoch: 78 [20992/50000]\tLoss: 4.7223\tLR: 7.741688\n",
      "Training Epoch: 78 [21120/50000]\tLoss: 4.7323\tLR: 7.741944\n",
      "Training Epoch: 78 [21248/50000]\tLoss: 4.7256\tLR: 7.742199\n",
      "Training Epoch: 78 [21376/50000]\tLoss: 4.7616\tLR: 7.742455\n",
      "Training Epoch: 78 [21504/50000]\tLoss: 4.7074\tLR: 7.742711\n",
      "Training Epoch: 78 [21632/50000]\tLoss: 4.7497\tLR: 7.742967\n",
      "Training Epoch: 78 [21760/50000]\tLoss: 4.7798\tLR: 7.743223\n",
      "Training Epoch: 78 [21888/50000]\tLoss: 4.7342\tLR: 7.743478\n",
      "Training Epoch: 78 [22016/50000]\tLoss: 4.8449\tLR: 7.743734\n",
      "Training Epoch: 78 [22144/50000]\tLoss: 4.6815\tLR: 7.743990\n",
      "Training Epoch: 78 [22272/50000]\tLoss: 4.7856\tLR: 7.744246\n",
      "Training Epoch: 78 [22400/50000]\tLoss: 4.7315\tLR: 7.744501\n",
      "Training Epoch: 78 [22528/50000]\tLoss: 4.6952\tLR: 7.744757\n",
      "Training Epoch: 78 [22656/50000]\tLoss: 4.7324\tLR: 7.745013\n",
      "Training Epoch: 78 [22784/50000]\tLoss: 4.7602\tLR: 7.745269\n",
      "Training Epoch: 78 [22912/50000]\tLoss: 4.7572\tLR: 7.745524\n",
      "Training Epoch: 78 [23040/50000]\tLoss: 4.6990\tLR: 7.745780\n",
      "Training Epoch: 78 [23168/50000]\tLoss: 4.7569\tLR: 7.746036\n",
      "Training Epoch: 78 [23296/50000]\tLoss: 4.7248\tLR: 7.746292\n",
      "Training Epoch: 78 [23424/50000]\tLoss: 4.7324\tLR: 7.746547\n",
      "Training Epoch: 78 [23552/50000]\tLoss: 4.6867\tLR: 7.746803\n",
      "Training Epoch: 78 [23680/50000]\tLoss: 4.7165\tLR: 7.747059\n",
      "Training Epoch: 78 [23808/50000]\tLoss: 4.7113\tLR: 7.747315\n",
      "Training Epoch: 78 [23936/50000]\tLoss: 4.7355\tLR: 7.747570\n",
      "Training Epoch: 78 [24064/50000]\tLoss: 4.7502\tLR: 7.747826\n",
      "Training Epoch: 78 [24192/50000]\tLoss: 4.8066\tLR: 7.748082\n",
      "Training Epoch: 78 [24320/50000]\tLoss: 4.7251\tLR: 7.748338\n",
      "Training Epoch: 78 [24448/50000]\tLoss: 4.7632\tLR: 7.748593\n",
      "Training Epoch: 78 [24576/50000]\tLoss: 4.7351\tLR: 7.748849\n",
      "Training Epoch: 78 [24704/50000]\tLoss: 4.7193\tLR: 7.749105\n",
      "Training Epoch: 78 [24832/50000]\tLoss: 4.8283\tLR: 7.749361\n",
      "Training Epoch: 78 [24960/50000]\tLoss: 4.7242\tLR: 7.749616\n",
      "Training Epoch: 78 [25088/50000]\tLoss: 4.7186\tLR: 7.749872\n",
      "Training Epoch: 78 [25216/50000]\tLoss: 4.6690\tLR: 7.750128\n",
      "Training Epoch: 78 [25344/50000]\tLoss: 4.7155\tLR: 7.750384\n",
      "Training Epoch: 78 [25472/50000]\tLoss: 4.8301\tLR: 7.750639\n",
      "Training Epoch: 78 [25600/50000]\tLoss: 4.6877\tLR: 7.750895\n",
      "Training Epoch: 78 [25728/50000]\tLoss: 4.7246\tLR: 7.751151\n",
      "Training Epoch: 78 [25856/50000]\tLoss: 4.7016\tLR: 7.751407\n",
      "Training Epoch: 78 [25984/50000]\tLoss: 4.7436\tLR: 7.751662\n",
      "Training Epoch: 78 [26112/50000]\tLoss: 4.7969\tLR: 7.751918\n",
      "Training Epoch: 78 [26240/50000]\tLoss: 4.7042\tLR: 7.752174\n",
      "Training Epoch: 78 [26368/50000]\tLoss: 4.7429\tLR: 7.752430\n",
      "Training Epoch: 78 [26496/50000]\tLoss: 4.6872\tLR: 7.752685\n",
      "Training Epoch: 78 [26624/50000]\tLoss: 4.7670\tLR: 7.752941\n",
      "Training Epoch: 78 [26752/50000]\tLoss: 4.7700\tLR: 7.753197\n",
      "Training Epoch: 78 [26880/50000]\tLoss: 4.7674\tLR: 7.753453\n",
      "Training Epoch: 78 [27008/50000]\tLoss: 4.7363\tLR: 7.753708\n",
      "Training Epoch: 78 [27136/50000]\tLoss: 4.7629\tLR: 7.753964\n",
      "Training Epoch: 78 [27264/50000]\tLoss: 4.6280\tLR: 7.754220\n",
      "Training Epoch: 78 [27392/50000]\tLoss: 4.6833\tLR: 7.754476\n",
      "Training Epoch: 78 [27520/50000]\tLoss: 4.6995\tLR: 7.754731\n",
      "Training Epoch: 78 [27648/50000]\tLoss: 4.8123\tLR: 7.754987\n",
      "Training Epoch: 78 [27776/50000]\tLoss: 4.7117\tLR: 7.755243\n",
      "Training Epoch: 78 [27904/50000]\tLoss: 4.7962\tLR: 7.755499\n",
      "Training Epoch: 78 [28032/50000]\tLoss: 4.7849\tLR: 7.755754\n",
      "Training Epoch: 78 [28160/50000]\tLoss: 4.7986\tLR: 7.756010\n",
      "Training Epoch: 78 [28288/50000]\tLoss: 4.7933\tLR: 7.756266\n",
      "Training Epoch: 78 [28416/50000]\tLoss: 4.6965\tLR: 7.756522\n",
      "Training Epoch: 78 [28544/50000]\tLoss: 4.7869\tLR: 7.756777\n",
      "Training Epoch: 78 [28672/50000]\tLoss: 4.7660\tLR: 7.757033\n",
      "Training Epoch: 78 [28800/50000]\tLoss: 4.7657\tLR: 7.757289\n",
      "Training Epoch: 78 [28928/50000]\tLoss: 4.6375\tLR: 7.757545\n",
      "Training Epoch: 78 [29056/50000]\tLoss: 4.7027\tLR: 7.757801\n",
      "Training Epoch: 78 [29184/50000]\tLoss: 4.7677\tLR: 7.758056\n",
      "Training Epoch: 78 [29312/50000]\tLoss: 4.7794\tLR: 7.758312\n",
      "Training Epoch: 78 [29440/50000]\tLoss: 4.7135\tLR: 7.758568\n",
      "Training Epoch: 78 [29568/50000]\tLoss: 4.7584\tLR: 7.758824\n",
      "Training Epoch: 78 [29696/50000]\tLoss: 4.7379\tLR: 7.759079\n",
      "Training Epoch: 78 [29824/50000]\tLoss: 4.6239\tLR: 7.759335\n",
      "Training Epoch: 78 [29952/50000]\tLoss: 4.7697\tLR: 7.759591\n",
      "Training Epoch: 78 [30080/50000]\tLoss: 4.8115\tLR: 7.759847\n",
      "Training Epoch: 78 [30208/50000]\tLoss: 4.7444\tLR: 7.760102\n",
      "Training Epoch: 78 [30336/50000]\tLoss: 4.7018\tLR: 7.760358\n",
      "Training Epoch: 78 [30464/50000]\tLoss: 4.7428\tLR: 7.760614\n",
      "Training Epoch: 78 [30592/50000]\tLoss: 4.7406\tLR: 7.760870\n",
      "Training Epoch: 78 [30720/50000]\tLoss: 4.7292\tLR: 7.761125\n",
      "Training Epoch: 78 [30848/50000]\tLoss: 4.6897\tLR: 7.761381\n",
      "Training Epoch: 78 [30976/50000]\tLoss: 4.8296\tLR: 7.761637\n",
      "Training Epoch: 78 [31104/50000]\tLoss: 4.7642\tLR: 7.761893\n",
      "Training Epoch: 78 [31232/50000]\tLoss: 4.8049\tLR: 7.762148\n",
      "Training Epoch: 78 [31360/50000]\tLoss: 4.7847\tLR: 7.762404\n",
      "Training Epoch: 78 [31488/50000]\tLoss: 4.7025\tLR: 7.762660\n",
      "Training Epoch: 78 [31616/50000]\tLoss: 4.7101\tLR: 7.762916\n",
      "Training Epoch: 78 [31744/50000]\tLoss: 4.7844\tLR: 7.763171\n",
      "Training Epoch: 78 [31872/50000]\tLoss: 4.7719\tLR: 7.763427\n",
      "Training Epoch: 78 [32000/50000]\tLoss: 4.6820\tLR: 7.763683\n",
      "Training Epoch: 78 [32128/50000]\tLoss: 4.7328\tLR: 7.763939\n",
      "Training Epoch: 78 [32256/50000]\tLoss: 4.7724\tLR: 7.764194\n",
      "Training Epoch: 78 [32384/50000]\tLoss: 4.7527\tLR: 7.764450\n",
      "Training Epoch: 78 [32512/50000]\tLoss: 4.7916\tLR: 7.764706\n",
      "Training Epoch: 78 [32640/50000]\tLoss: 4.7181\tLR: 7.764962\n",
      "Training Epoch: 78 [32768/50000]\tLoss: 4.6790\tLR: 7.765217\n",
      "Training Epoch: 78 [32896/50000]\tLoss: 4.7398\tLR: 7.765473\n",
      "Training Epoch: 78 [33024/50000]\tLoss: 4.8604\tLR: 7.765729\n",
      "Training Epoch: 78 [33152/50000]\tLoss: 4.7478\tLR: 7.765985\n",
      "Training Epoch: 78 [33280/50000]\tLoss: 4.7218\tLR: 7.766240\n",
      "Training Epoch: 78 [33408/50000]\tLoss: 4.8160\tLR: 7.766496\n",
      "Training Epoch: 78 [33536/50000]\tLoss: 4.7602\tLR: 7.766752\n",
      "Training Epoch: 78 [33664/50000]\tLoss: 4.7502\tLR: 7.767008\n",
      "Training Epoch: 78 [33792/50000]\tLoss: 4.7474\tLR: 7.767263\n",
      "Training Epoch: 78 [33920/50000]\tLoss: 4.6701\tLR: 7.767519\n",
      "Training Epoch: 78 [34048/50000]\tLoss: 4.7450\tLR: 7.767775\n",
      "Training Epoch: 78 [34176/50000]\tLoss: 4.8002\tLR: 7.768031\n",
      "Training Epoch: 78 [34304/50000]\tLoss: 4.9269\tLR: 7.768286\n",
      "Training Epoch: 78 [34432/50000]\tLoss: 4.7937\tLR: 7.768542\n",
      "Training Epoch: 78 [34560/50000]\tLoss: 4.7436\tLR: 7.768798\n",
      "Training Epoch: 78 [34688/50000]\tLoss: 4.7211\tLR: 7.769054\n",
      "Training Epoch: 78 [34816/50000]\tLoss: 4.6959\tLR: 7.769309\n",
      "Training Epoch: 78 [34944/50000]\tLoss: 4.8804\tLR: 7.769565\n",
      "Training Epoch: 78 [35072/50000]\tLoss: 4.8300\tLR: 7.769821\n",
      "Training Epoch: 78 [35200/50000]\tLoss: 4.8339\tLR: 7.770077\n",
      "Training Epoch: 78 [35328/50000]\tLoss: 4.6574\tLR: 7.770332\n",
      "Training Epoch: 78 [35456/50000]\tLoss: 4.7358\tLR: 7.770588\n",
      "Training Epoch: 78 [35584/50000]\tLoss: 4.6944\tLR: 7.770844\n",
      "Training Epoch: 78 [35712/50000]\tLoss: 4.8143\tLR: 7.771100\n",
      "Training Epoch: 78 [35840/50000]\tLoss: 4.7541\tLR: 7.771355\n",
      "Training Epoch: 78 [35968/50000]\tLoss: 4.6602\tLR: 7.771611\n",
      "Training Epoch: 78 [36096/50000]\tLoss: 4.6584\tLR: 7.771867\n",
      "Training Epoch: 78 [36224/50000]\tLoss: 4.7849\tLR: 7.772123\n",
      "Training Epoch: 78 [36352/50000]\tLoss: 4.7106\tLR: 7.772379\n",
      "Training Epoch: 78 [36480/50000]\tLoss: 4.7247\tLR: 7.772634\n",
      "Training Epoch: 78 [36608/50000]\tLoss: 4.9221\tLR: 7.772890\n",
      "Training Epoch: 78 [36736/50000]\tLoss: 4.7532\tLR: 7.773146\n",
      "Training Epoch: 78 [36864/50000]\tLoss: 4.6940\tLR: 7.773402\n",
      "Training Epoch: 78 [36992/50000]\tLoss: 4.6360\tLR: 7.773657\n",
      "Training Epoch: 78 [37120/50000]\tLoss: 4.8184\tLR: 7.773913\n",
      "Training Epoch: 78 [37248/50000]\tLoss: 4.8330\tLR: 7.774169\n",
      "Training Epoch: 78 [37376/50000]\tLoss: 4.7082\tLR: 7.774425\n",
      "Training Epoch: 78 [37504/50000]\tLoss: 4.7292\tLR: 7.774680\n",
      "Training Epoch: 78 [37632/50000]\tLoss: 4.7822\tLR: 7.774936\n",
      "Training Epoch: 78 [37760/50000]\tLoss: 4.7636\tLR: 7.775192\n",
      "Training Epoch: 78 [37888/50000]\tLoss: 4.7370\tLR: 7.775448\n",
      "Training Epoch: 78 [38016/50000]\tLoss: 4.7723\tLR: 7.775703\n",
      "Training Epoch: 78 [38144/50000]\tLoss: 4.6888\tLR: 7.775959\n",
      "Training Epoch: 78 [38272/50000]\tLoss: 4.8147\tLR: 7.776215\n",
      "Training Epoch: 78 [38400/50000]\tLoss: 4.7892\tLR: 7.776471\n",
      "Training Epoch: 78 [38528/50000]\tLoss: 4.6125\tLR: 7.776726\n",
      "Training Epoch: 78 [38656/50000]\tLoss: 4.7294\tLR: 7.776982\n",
      "Training Epoch: 78 [38784/50000]\tLoss: 4.8282\tLR: 7.777238\n",
      "Training Epoch: 78 [38912/50000]\tLoss: 4.7351\tLR: 7.777494\n",
      "Training Epoch: 78 [39040/50000]\tLoss: 4.7997\tLR: 7.777749\n",
      "Training Epoch: 78 [39168/50000]\tLoss: 4.7346\tLR: 7.778005\n",
      "Training Epoch: 78 [39296/50000]\tLoss: 4.7496\tLR: 7.778261\n",
      "Training Epoch: 78 [39424/50000]\tLoss: 4.7232\tLR: 7.778517\n",
      "Training Epoch: 78 [39552/50000]\tLoss: 4.7823\tLR: 7.778772\n",
      "Training Epoch: 78 [39680/50000]\tLoss: 4.8395\tLR: 7.779028\n",
      "Training Epoch: 78 [39808/50000]\tLoss: 4.7994\tLR: 7.779284\n",
      "Training Epoch: 78 [39936/50000]\tLoss: 4.7788\tLR: 7.779540\n",
      "Training Epoch: 78 [40064/50000]\tLoss: 4.7752\tLR: 7.779795\n",
      "Training Epoch: 78 [40192/50000]\tLoss: 4.7020\tLR: 7.780051\n",
      "Training Epoch: 78 [40320/50000]\tLoss: 4.7840\tLR: 7.780307\n",
      "Training Epoch: 78 [40448/50000]\tLoss: 4.8268\tLR: 7.780563\n",
      "Training Epoch: 78 [40576/50000]\tLoss: 4.6827\tLR: 7.780818\n",
      "Training Epoch: 78 [40704/50000]\tLoss: 4.8204\tLR: 7.781074\n",
      "Training Epoch: 78 [40832/50000]\tLoss: 4.7523\tLR: 7.781330\n",
      "Training Epoch: 78 [40960/50000]\tLoss: 4.8358\tLR: 7.781586\n",
      "Training Epoch: 78 [41088/50000]\tLoss: 4.7579\tLR: 7.781841\n",
      "Training Epoch: 78 [41216/50000]\tLoss: 4.8023\tLR: 7.782097\n",
      "Training Epoch: 78 [41344/50000]\tLoss: 4.7105\tLR: 7.782353\n",
      "Training Epoch: 78 [41472/50000]\tLoss: 4.7494\tLR: 7.782609\n",
      "Training Epoch: 78 [41600/50000]\tLoss: 4.7776\tLR: 7.782864\n",
      "Training Epoch: 78 [41728/50000]\tLoss: 4.7394\tLR: 7.783120\n",
      "Training Epoch: 78 [41856/50000]\tLoss: 4.7136\tLR: 7.783376\n",
      "Training Epoch: 78 [41984/50000]\tLoss: 4.7781\tLR: 7.783632\n",
      "Training Epoch: 78 [42112/50000]\tLoss: 4.7305\tLR: 7.783887\n",
      "Training Epoch: 78 [42240/50000]\tLoss: 4.7946\tLR: 7.784143\n",
      "Training Epoch: 78 [42368/50000]\tLoss: 4.7701\tLR: 7.784399\n",
      "Training Epoch: 78 [42496/50000]\tLoss: 4.6811\tLR: 7.784655\n",
      "Training Epoch: 78 [42624/50000]\tLoss: 4.6941\tLR: 7.784910\n",
      "Training Epoch: 78 [42752/50000]\tLoss: 4.6481\tLR: 7.785166\n",
      "Training Epoch: 78 [42880/50000]\tLoss: 4.6840\tLR: 7.785422\n",
      "Training Epoch: 78 [43008/50000]\tLoss: 4.7811\tLR: 7.785678\n",
      "Training Epoch: 78 [43136/50000]\tLoss: 4.7912\tLR: 7.785934\n",
      "Training Epoch: 78 [43264/50000]\tLoss: 4.7075\tLR: 7.786189\n",
      "Training Epoch: 78 [43392/50000]\tLoss: 4.7706\tLR: 7.786445\n",
      "Training Epoch: 78 [43520/50000]\tLoss: 4.7384\tLR: 7.786701\n",
      "Training Epoch: 78 [43648/50000]\tLoss: 4.7802\tLR: 7.786957\n",
      "Training Epoch: 78 [43776/50000]\tLoss: 4.7209\tLR: 7.787212\n",
      "Training Epoch: 78 [43904/50000]\tLoss: 4.7779\tLR: 7.787468\n",
      "Training Epoch: 78 [44032/50000]\tLoss: 4.7794\tLR: 7.787724\n",
      "Training Epoch: 78 [44160/50000]\tLoss: 4.8085\tLR: 7.787980\n",
      "Training Epoch: 78 [44288/50000]\tLoss: 4.7455\tLR: 7.788235\n",
      "Training Epoch: 78 [44416/50000]\tLoss: 4.6958\tLR: 7.788491\n",
      "Training Epoch: 78 [44544/50000]\tLoss: 4.7461\tLR: 7.788747\n",
      "Training Epoch: 78 [44672/50000]\tLoss: 4.6145\tLR: 7.789003\n",
      "Training Epoch: 78 [44800/50000]\tLoss: 4.6793\tLR: 7.789258\n",
      "Training Epoch: 78 [44928/50000]\tLoss: 4.7803\tLR: 7.789514\n",
      "Training Epoch: 78 [45056/50000]\tLoss: 4.8008\tLR: 7.789770\n",
      "Training Epoch: 78 [45184/50000]\tLoss: 4.7234\tLR: 7.790026\n",
      "Training Epoch: 78 [45312/50000]\tLoss: 4.7939\tLR: 7.790281\n",
      "Training Epoch: 78 [45440/50000]\tLoss: 4.7423\tLR: 7.790537\n",
      "Training Epoch: 78 [45568/50000]\tLoss: 4.7255\tLR: 7.790793\n",
      "Training Epoch: 78 [45696/50000]\tLoss: 4.7080\tLR: 7.791049\n",
      "Training Epoch: 78 [45824/50000]\tLoss: 4.6872\tLR: 7.791304\n",
      "Training Epoch: 78 [45952/50000]\tLoss: 4.7089\tLR: 7.791560\n",
      "Training Epoch: 78 [46080/50000]\tLoss: 4.7235\tLR: 7.791816\n",
      "Training Epoch: 78 [46208/50000]\tLoss: 4.7604\tLR: 7.792072\n",
      "Training Epoch: 78 [46336/50000]\tLoss: 4.6737\tLR: 7.792327\n",
      "Training Epoch: 78 [46464/50000]\tLoss: 4.6917\tLR: 7.792583\n",
      "Training Epoch: 78 [46592/50000]\tLoss: 4.7061\tLR: 7.792839\n",
      "Training Epoch: 78 [46720/50000]\tLoss: 4.8221\tLR: 7.793095\n",
      "Training Epoch: 78 [46848/50000]\tLoss: 4.7219\tLR: 7.793350\n",
      "Training Epoch: 78 [46976/50000]\tLoss: 4.7094\tLR: 7.793606\n",
      "Training Epoch: 78 [47104/50000]\tLoss: 4.7047\tLR: 7.793862\n",
      "Training Epoch: 78 [47232/50000]\tLoss: 4.7593\tLR: 7.794118\n",
      "Training Epoch: 78 [47360/50000]\tLoss: 4.6859\tLR: 7.794373\n",
      "Training Epoch: 78 [47488/50000]\tLoss: 4.7293\tLR: 7.794629\n",
      "Training Epoch: 78 [47616/50000]\tLoss: 4.6597\tLR: 7.794885\n",
      "Training Epoch: 78 [47744/50000]\tLoss: 4.7443\tLR: 7.795141\n",
      "Training Epoch: 78 [47872/50000]\tLoss: 4.7194\tLR: 7.795396\n",
      "Training Epoch: 78 [48000/50000]\tLoss: 4.7520\tLR: 7.795652\n",
      "Training Epoch: 78 [48128/50000]\tLoss: 4.8790\tLR: 7.795908\n",
      "Training Epoch: 78 [48256/50000]\tLoss: 4.8135\tLR: 7.796164\n",
      "Training Epoch: 78 [48384/50000]\tLoss: 4.7725\tLR: 7.796419\n",
      "Training Epoch: 78 [48512/50000]\tLoss: 4.6918\tLR: 7.796675\n",
      "Training Epoch: 78 [48640/50000]\tLoss: 4.6904\tLR: 7.796931\n",
      "Training Epoch: 78 [48768/50000]\tLoss: 4.8425\tLR: 7.797187\n",
      "Training Epoch: 78 [48896/50000]\tLoss: 4.8050\tLR: 7.797442\n",
      "Training Epoch: 78 [49024/50000]\tLoss: 4.6941\tLR: 7.797698\n",
      "Training Epoch: 78 [49152/50000]\tLoss: 4.7755\tLR: 7.797954\n",
      "Training Epoch: 78 [49280/50000]\tLoss: 4.8212\tLR: 7.798210\n",
      "Training Epoch: 78 [49408/50000]\tLoss: 4.6895\tLR: 7.798465\n",
      "Training Epoch: 78 [49536/50000]\tLoss: 4.7481\tLR: 7.798721\n",
      "Training Epoch: 78 [49664/50000]\tLoss: 4.7803\tLR: 7.798977\n",
      "Training Epoch: 78 [49792/50000]\tLoss: 4.7889\tLR: 7.799233\n",
      "Training Epoch: 78 [49920/50000]\tLoss: 4.6677\tLR: 7.799488\n",
      "Training Epoch: 78 [50000/50000]\tLoss: 4.6992\tLR: 7.799744\n",
      "epoch 78 training time consumed: 489.20s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  109350 GB |  109350 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  109015 GB |  109015 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     335 GB |     335 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  109350 GB |  109350 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  109015 GB |  109015 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     335 GB |     335 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  107814 GB |  107813 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  107478 GB |  107478 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     335 GB |     335 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   11595 K  |   11594 K  |\n",
      "|       from large pool |      24    |      65    |    4942 K  |    4942 K  |\n",
      "|       from small pool |     231    |     274    |    6652 K  |    6651 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   11595 K  |   11594 K  |\n",
      "|       from large pool |      24    |      65    |    4942 K  |    4942 K  |\n",
      "|       from small pool |     231    |     274    |    6652 K  |    6651 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      47    |    6720 K  |    6720 K  |\n",
      "|       from large pool |      10    |      23    |    2375 K  |    2375 K  |\n",
      "|       from small pool |      26    |      35    |    4345 K  |    4345 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 78, Average loss: 0.0376, Accuracy: 0.0100, Time consumed:31.28s\n",
      "\n",
      "Training Epoch: 79 [128/50000]\tLoss: 4.7873\tLR: 0.020000\n",
      "Training Epoch: 79 [256/50000]\tLoss: 4.6715\tLR: 7.800256\n",
      "Training Epoch: 79 [384/50000]\tLoss: 4.7498\tLR: 7.800512\n",
      "Training Epoch: 79 [512/50000]\tLoss: 4.8359\tLR: 7.800767\n",
      "Training Epoch: 79 [640/50000]\tLoss: 4.7776\tLR: 7.801023\n",
      "Training Epoch: 79 [768/50000]\tLoss: 4.7280\tLR: 7.801279\n",
      "Training Epoch: 79 [896/50000]\tLoss: 4.7267\tLR: 7.801535\n",
      "Training Epoch: 79 [1024/50000]\tLoss: 4.8255\tLR: 7.801790\n",
      "Training Epoch: 79 [1152/50000]\tLoss: 4.6847\tLR: 7.802046\n",
      "Training Epoch: 79 [1280/50000]\tLoss: 4.7200\tLR: 7.802302\n",
      "Training Epoch: 79 [1408/50000]\tLoss: 4.7720\tLR: 7.802558\n",
      "Training Epoch: 79 [1536/50000]\tLoss: 4.7234\tLR: 7.802813\n",
      "Training Epoch: 79 [1664/50000]\tLoss: 4.7225\tLR: 7.803069\n",
      "Training Epoch: 79 [1792/50000]\tLoss: 4.7150\tLR: 7.803325\n",
      "Training Epoch: 79 [1920/50000]\tLoss: 4.7914\tLR: 7.803581\n",
      "Training Epoch: 79 [2048/50000]\tLoss: 4.8552\tLR: 7.803836\n",
      "Training Epoch: 79 [2176/50000]\tLoss: 4.8283\tLR: 7.804092\n",
      "Training Epoch: 79 [2304/50000]\tLoss: 4.7318\tLR: 7.804348\n",
      "Training Epoch: 79 [2432/50000]\tLoss: 4.7400\tLR: 7.804604\n",
      "Training Epoch: 79 [2560/50000]\tLoss: 4.8063\tLR: 7.804859\n",
      "Training Epoch: 79 [2688/50000]\tLoss: 4.8083\tLR: 7.805115\n",
      "Training Epoch: 79 [2816/50000]\tLoss: 4.8085\tLR: 7.805371\n",
      "Training Epoch: 79 [2944/50000]\tLoss: 4.7829\tLR: 7.805627\n",
      "Training Epoch: 79 [3072/50000]\tLoss: 4.7088\tLR: 7.805882\n",
      "Training Epoch: 79 [3200/50000]\tLoss: 4.7717\tLR: 7.806138\n",
      "Training Epoch: 79 [3328/50000]\tLoss: 4.8639\tLR: 7.806394\n",
      "Training Epoch: 79 [3456/50000]\tLoss: 4.8685\tLR: 7.806650\n",
      "Training Epoch: 79 [3584/50000]\tLoss: 4.9287\tLR: 7.806905\n",
      "Training Epoch: 79 [3712/50000]\tLoss: 4.8954\tLR: 7.807161\n",
      "Training Epoch: 79 [3840/50000]\tLoss: 4.7236\tLR: 7.807417\n",
      "Training Epoch: 79 [3968/50000]\tLoss: 4.7570\tLR: 7.807673\n",
      "Training Epoch: 79 [4096/50000]\tLoss: 4.7099\tLR: 7.807928\n",
      "Training Epoch: 79 [4224/50000]\tLoss: 4.7116\tLR: 7.808184\n",
      "Training Epoch: 79 [4352/50000]\tLoss: 4.7741\tLR: 7.808440\n",
      "Training Epoch: 79 [4480/50000]\tLoss: 4.7885\tLR: 7.808696\n",
      "Training Epoch: 79 [4608/50000]\tLoss: 4.7284\tLR: 7.808951\n",
      "Training Epoch: 79 [4736/50000]\tLoss: 4.6960\tLR: 7.809207\n",
      "Training Epoch: 79 [4864/50000]\tLoss: 4.7900\tLR: 7.809463\n",
      "Training Epoch: 79 [4992/50000]\tLoss: 4.7153\tLR: 7.809719\n",
      "Training Epoch: 79 [5120/50000]\tLoss: 4.7801\tLR: 7.809974\n",
      "Training Epoch: 79 [5248/50000]\tLoss: 4.6478\tLR: 7.810230\n",
      "Training Epoch: 79 [5376/50000]\tLoss: 4.8148\tLR: 7.810486\n",
      "Training Epoch: 79 [5504/50000]\tLoss: 4.7639\tLR: 7.810742\n",
      "Training Epoch: 79 [5632/50000]\tLoss: 4.7108\tLR: 7.810997\n",
      "Training Epoch: 79 [5760/50000]\tLoss: 4.6961\tLR: 7.811253\n",
      "Training Epoch: 79 [5888/50000]\tLoss: 4.6590\tLR: 7.811509\n",
      "Training Epoch: 79 [6016/50000]\tLoss: 4.7508\tLR: 7.811765\n",
      "Training Epoch: 79 [6144/50000]\tLoss: 4.7844\tLR: 7.812020\n",
      "Training Epoch: 79 [6272/50000]\tLoss: 4.7761\tLR: 7.812276\n",
      "Training Epoch: 79 [6400/50000]\tLoss: 4.7236\tLR: 7.812532\n",
      "Training Epoch: 79 [6528/50000]\tLoss: 4.7223\tLR: 7.812788\n",
      "Training Epoch: 79 [6656/50000]\tLoss: 4.7905\tLR: 7.813043\n",
      "Training Epoch: 79 [6784/50000]\tLoss: 4.7341\tLR: 7.813299\n",
      "Training Epoch: 79 [6912/50000]\tLoss: 4.7703\tLR: 7.813555\n",
      "Training Epoch: 79 [7040/50000]\tLoss: 4.7272\tLR: 7.813811\n",
      "Training Epoch: 79 [7168/50000]\tLoss: 4.7076\tLR: 7.814066\n",
      "Training Epoch: 79 [7296/50000]\tLoss: 4.6796\tLR: 7.814322\n",
      "Training Epoch: 79 [7424/50000]\tLoss: 4.7715\tLR: 7.814578\n",
      "Training Epoch: 79 [7552/50000]\tLoss: 4.7377\tLR: 7.814834\n",
      "Training Epoch: 79 [7680/50000]\tLoss: 4.7515\tLR: 7.815090\n",
      "Training Epoch: 79 [7808/50000]\tLoss: 4.7046\tLR: 7.815345\n",
      "Training Epoch: 79 [7936/50000]\tLoss: 4.8012\tLR: 7.815601\n",
      "Training Epoch: 79 [8064/50000]\tLoss: 4.8130\tLR: 7.815857\n",
      "Training Epoch: 79 [8192/50000]\tLoss: 4.7340\tLR: 7.816113\n",
      "Training Epoch: 79 [8320/50000]\tLoss: 4.7384\tLR: 7.816368\n",
      "Training Epoch: 79 [8448/50000]\tLoss: 4.7381\tLR: 7.816624\n",
      "Training Epoch: 79 [8576/50000]\tLoss: 4.7632\tLR: 7.816880\n",
      "Training Epoch: 79 [8704/50000]\tLoss: 4.7514\tLR: 7.817136\n",
      "Training Epoch: 79 [8832/50000]\tLoss: 4.7877\tLR: 7.817391\n",
      "Training Epoch: 79 [8960/50000]\tLoss: 4.7486\tLR: 7.817647\n",
      "Training Epoch: 79 [9088/50000]\tLoss: 4.7001\tLR: 7.817903\n",
      "Training Epoch: 79 [9216/50000]\tLoss: 4.7627\tLR: 7.818159\n",
      "Training Epoch: 79 [9344/50000]\tLoss: 4.7415\tLR: 7.818414\n",
      "Training Epoch: 79 [9472/50000]\tLoss: 4.7534\tLR: 7.818670\n",
      "Training Epoch: 79 [9600/50000]\tLoss: 4.6894\tLR: 7.818926\n",
      "Training Epoch: 79 [9728/50000]\tLoss: 4.6330\tLR: 7.819182\n",
      "Training Epoch: 79 [9856/50000]\tLoss: 4.8612\tLR: 7.819437\n",
      "Training Epoch: 79 [9984/50000]\tLoss: 4.7785\tLR: 7.819693\n",
      "Training Epoch: 79 [10112/50000]\tLoss: 4.7272\tLR: 7.819949\n",
      "Training Epoch: 79 [10240/50000]\tLoss: 4.8492\tLR: 7.820205\n",
      "Training Epoch: 79 [10368/50000]\tLoss: 4.7875\tLR: 7.820460\n",
      "Training Epoch: 79 [10496/50000]\tLoss: 4.7758\tLR: 7.820716\n",
      "Training Epoch: 79 [10624/50000]\tLoss: 4.6631\tLR: 7.820972\n",
      "Training Epoch: 79 [10752/50000]\tLoss: 4.8333\tLR: 7.821228\n",
      "Training Epoch: 79 [10880/50000]\tLoss: 4.6813\tLR: 7.821483\n",
      "Training Epoch: 79 [11008/50000]\tLoss: 4.6752\tLR: 7.821739\n",
      "Training Epoch: 79 [11136/50000]\tLoss: 4.7896\tLR: 7.821995\n",
      "Training Epoch: 79 [11264/50000]\tLoss: 4.6184\tLR: 7.822251\n",
      "Training Epoch: 79 [11392/50000]\tLoss: 4.7166\tLR: 7.822506\n",
      "Training Epoch: 79 [11520/50000]\tLoss: 4.8004\tLR: 7.822762\n",
      "Training Epoch: 79 [11648/50000]\tLoss: 4.8832\tLR: 7.823018\n",
      "Training Epoch: 79 [11776/50000]\tLoss: 4.7893\tLR: 7.823274\n",
      "Training Epoch: 79 [11904/50000]\tLoss: 4.7215\tLR: 7.823529\n",
      "Training Epoch: 79 [12032/50000]\tLoss: 4.7844\tLR: 7.823785\n",
      "Training Epoch: 79 [12160/50000]\tLoss: 4.6976\tLR: 7.824041\n",
      "Training Epoch: 79 [12288/50000]\tLoss: 4.7269\tLR: 7.824297\n",
      "Training Epoch: 79 [12416/50000]\tLoss: 4.7021\tLR: 7.824552\n",
      "Training Epoch: 79 [12544/50000]\tLoss: 4.6770\tLR: 7.824808\n",
      "Training Epoch: 79 [12672/50000]\tLoss: 4.7957\tLR: 7.825064\n",
      "Training Epoch: 79 [12800/50000]\tLoss: 4.8045\tLR: 7.825320\n",
      "Training Epoch: 79 [12928/50000]\tLoss: 4.7532\tLR: 7.825575\n",
      "Training Epoch: 79 [13056/50000]\tLoss: 4.8383\tLR: 7.825831\n",
      "Training Epoch: 79 [13184/50000]\tLoss: 4.7898\tLR: 7.826087\n",
      "Training Epoch: 79 [13312/50000]\tLoss: 4.6822\tLR: 7.826343\n",
      "Training Epoch: 79 [13440/50000]\tLoss: 4.7466\tLR: 7.826598\n",
      "Training Epoch: 79 [13568/50000]\tLoss: 4.7301\tLR: 7.826854\n",
      "Training Epoch: 79 [13696/50000]\tLoss: 4.6856\tLR: 7.827110\n",
      "Training Epoch: 79 [13824/50000]\tLoss: 4.8207\tLR: 7.827366\n",
      "Training Epoch: 79 [13952/50000]\tLoss: 4.7782\tLR: 7.827621\n",
      "Training Epoch: 79 [14080/50000]\tLoss: 4.7537\tLR: 7.827877\n",
      "Training Epoch: 79 [14208/50000]\tLoss: 4.8021\tLR: 7.828133\n",
      "Training Epoch: 79 [14336/50000]\tLoss: 4.7554\tLR: 7.828389\n",
      "Training Epoch: 79 [14464/50000]\tLoss: 4.7418\tLR: 7.828645\n",
      "Training Epoch: 79 [14592/50000]\tLoss: 4.7308\tLR: 7.828900\n",
      "Training Epoch: 79 [14720/50000]\tLoss: 4.7138\tLR: 7.829156\n",
      "Training Epoch: 79 [14848/50000]\tLoss: 4.8104\tLR: 7.829412\n",
      "Training Epoch: 79 [14976/50000]\tLoss: 4.7165\tLR: 7.829668\n",
      "Training Epoch: 79 [15104/50000]\tLoss: 4.7436\tLR: 7.829923\n",
      "Training Epoch: 79 [15232/50000]\tLoss: 4.7210\tLR: 7.830179\n",
      "Training Epoch: 79 [15360/50000]\tLoss: 4.7837\tLR: 7.830435\n",
      "Training Epoch: 79 [15488/50000]\tLoss: 4.7535\tLR: 7.830691\n",
      "Training Epoch: 79 [15616/50000]\tLoss: 4.6876\tLR: 7.830946\n",
      "Training Epoch: 79 [15744/50000]\tLoss: 4.7337\tLR: 7.831202\n",
      "Training Epoch: 79 [15872/50000]\tLoss: 4.6978\tLR: 7.831458\n",
      "Training Epoch: 79 [16000/50000]\tLoss: 4.6539\tLR: 7.831714\n",
      "Training Epoch: 79 [16128/50000]\tLoss: 4.7998\tLR: 7.831969\n",
      "Training Epoch: 79 [16256/50000]\tLoss: 4.7536\tLR: 7.832225\n",
      "Training Epoch: 79 [16384/50000]\tLoss: 4.7616\tLR: 7.832481\n",
      "Training Epoch: 79 [16512/50000]\tLoss: 4.7008\tLR: 7.832737\n",
      "Training Epoch: 79 [16640/50000]\tLoss: 4.6186\tLR: 7.832992\n",
      "Training Epoch: 79 [16768/50000]\tLoss: 4.7551\tLR: 7.833248\n",
      "Training Epoch: 79 [16896/50000]\tLoss: 4.7280\tLR: 7.833504\n",
      "Training Epoch: 79 [17024/50000]\tLoss: 4.8000\tLR: 7.833760\n",
      "Training Epoch: 79 [17152/50000]\tLoss: 4.8038\tLR: 7.834015\n",
      "Training Epoch: 79 [17280/50000]\tLoss: 4.7254\tLR: 7.834271\n",
      "Training Epoch: 79 [17408/50000]\tLoss: 4.7968\tLR: 7.834527\n",
      "Training Epoch: 79 [17536/50000]\tLoss: 4.7296\tLR: 7.834783\n",
      "Training Epoch: 79 [17664/50000]\tLoss: 4.7493\tLR: 7.835038\n",
      "Training Epoch: 79 [17792/50000]\tLoss: 4.7963\tLR: 7.835294\n",
      "Training Epoch: 79 [17920/50000]\tLoss: 4.7380\tLR: 7.835550\n",
      "Training Epoch: 79 [18048/50000]\tLoss: 4.7236\tLR: 7.835806\n",
      "Training Epoch: 79 [18176/50000]\tLoss: 4.7340\tLR: 7.836061\n",
      "Training Epoch: 79 [18304/50000]\tLoss: 4.8171\tLR: 7.836317\n",
      "Training Epoch: 79 [18432/50000]\tLoss: 4.8434\tLR: 7.836573\n",
      "Training Epoch: 79 [18560/50000]\tLoss: 4.7511\tLR: 7.836829\n",
      "Training Epoch: 79 [18688/50000]\tLoss: 4.7069\tLR: 7.837084\n",
      "Training Epoch: 79 [18816/50000]\tLoss: 4.7029\tLR: 7.837340\n",
      "Training Epoch: 79 [18944/50000]\tLoss: 4.6875\tLR: 7.837596\n",
      "Training Epoch: 79 [19072/50000]\tLoss: 4.8543\tLR: 7.837852\n",
      "Training Epoch: 79 [19200/50000]\tLoss: 4.7946\tLR: 7.838107\n",
      "Training Epoch: 79 [19328/50000]\tLoss: 4.8016\tLR: 7.838363\n",
      "Training Epoch: 79 [19456/50000]\tLoss: 4.6858\tLR: 7.838619\n",
      "Training Epoch: 79 [19584/50000]\tLoss: 4.7453\tLR: 7.838875\n",
      "Training Epoch: 79 [19712/50000]\tLoss: 4.6826\tLR: 7.839130\n",
      "Training Epoch: 79 [19840/50000]\tLoss: 4.7899\tLR: 7.839386\n",
      "Training Epoch: 79 [19968/50000]\tLoss: 4.8345\tLR: 7.839642\n",
      "Training Epoch: 79 [20096/50000]\tLoss: 4.8042\tLR: 7.839898\n",
      "Training Epoch: 79 [20224/50000]\tLoss: 4.8097\tLR: 7.840153\n",
      "Training Epoch: 79 [20352/50000]\tLoss: 4.6965\tLR: 7.840409\n",
      "Training Epoch: 79 [20480/50000]\tLoss: 4.7502\tLR: 7.840665\n",
      "Training Epoch: 79 [20608/50000]\tLoss: 4.7099\tLR: 7.840921\n",
      "Training Epoch: 79 [20736/50000]\tLoss: 4.6601\tLR: 7.841176\n",
      "Training Epoch: 79 [20864/50000]\tLoss: 4.8639\tLR: 7.841432\n",
      "Training Epoch: 79 [20992/50000]\tLoss: 4.7327\tLR: 7.841688\n",
      "Training Epoch: 79 [21120/50000]\tLoss: 4.8164\tLR: 7.841944\n",
      "Training Epoch: 79 [21248/50000]\tLoss: 4.7078\tLR: 7.842199\n",
      "Training Epoch: 79 [21376/50000]\tLoss: 4.7955\tLR: 7.842455\n",
      "Training Epoch: 79 [21504/50000]\tLoss: 4.8378\tLR: 7.842711\n",
      "Training Epoch: 79 [21632/50000]\tLoss: 4.7977\tLR: 7.842967\n",
      "Training Epoch: 79 [21760/50000]\tLoss: 4.7679\tLR: 7.843223\n",
      "Training Epoch: 79 [21888/50000]\tLoss: 4.6964\tLR: 7.843478\n",
      "Training Epoch: 79 [22016/50000]\tLoss: 4.6882\tLR: 7.843734\n",
      "Training Epoch: 79 [22144/50000]\tLoss: 4.7075\tLR: 7.843990\n",
      "Training Epoch: 79 [22272/50000]\tLoss: 4.8149\tLR: 7.844246\n",
      "Training Epoch: 79 [22400/50000]\tLoss: 4.7678\tLR: 7.844501\n",
      "Training Epoch: 79 [22528/50000]\tLoss: 4.7034\tLR: 7.844757\n",
      "Training Epoch: 79 [22656/50000]\tLoss: 4.7483\tLR: 7.845013\n",
      "Training Epoch: 79 [22784/50000]\tLoss: 4.8172\tLR: 7.845269\n",
      "Training Epoch: 79 [22912/50000]\tLoss: 4.7293\tLR: 7.845524\n",
      "Training Epoch: 79 [23040/50000]\tLoss: 4.8071\tLR: 7.845780\n",
      "Training Epoch: 79 [23168/50000]\tLoss: 4.7127\tLR: 7.846036\n",
      "Training Epoch: 79 [23296/50000]\tLoss: 4.7749\tLR: 7.846292\n",
      "Training Epoch: 79 [23424/50000]\tLoss: 4.7808\tLR: 7.846547\n",
      "Training Epoch: 79 [23552/50000]\tLoss: 4.7464\tLR: 7.846803\n",
      "Training Epoch: 79 [23680/50000]\tLoss: 4.7820\tLR: 7.847059\n",
      "Training Epoch: 79 [23808/50000]\tLoss: 4.7931\tLR: 7.847315\n",
      "Training Epoch: 79 [23936/50000]\tLoss: 4.6915\tLR: 7.847570\n",
      "Training Epoch: 79 [24064/50000]\tLoss: 4.8500\tLR: 7.847826\n",
      "Training Epoch: 79 [24192/50000]\tLoss: 4.7311\tLR: 7.848082\n",
      "Training Epoch: 79 [24320/50000]\tLoss: 4.7682\tLR: 7.848338\n",
      "Training Epoch: 79 [24448/50000]\tLoss: 4.6866\tLR: 7.848593\n",
      "Training Epoch: 79 [24576/50000]\tLoss: 4.7499\tLR: 7.848849\n",
      "Training Epoch: 79 [24704/50000]\tLoss: 4.7123\tLR: 7.849105\n",
      "Training Epoch: 79 [24832/50000]\tLoss: 4.7391\tLR: 7.849361\n",
      "Training Epoch: 79 [24960/50000]\tLoss: 4.7478\tLR: 7.849616\n",
      "Training Epoch: 79 [25088/50000]\tLoss: 4.6990\tLR: 7.849872\n",
      "Training Epoch: 79 [25216/50000]\tLoss: 4.7708\tLR: 7.850128\n",
      "Training Epoch: 79 [25344/50000]\tLoss: 4.7417\tLR: 7.850384\n",
      "Training Epoch: 79 [25472/50000]\tLoss: 4.7683\tLR: 7.850639\n",
      "Training Epoch: 79 [25600/50000]\tLoss: 4.7563\tLR: 7.850895\n",
      "Training Epoch: 79 [25728/50000]\tLoss: 4.7309\tLR: 7.851151\n",
      "Training Epoch: 79 [25856/50000]\tLoss: 4.6876\tLR: 7.851407\n",
      "Training Epoch: 79 [25984/50000]\tLoss: 4.7089\tLR: 7.851662\n",
      "Training Epoch: 79 [26112/50000]\tLoss: 4.7574\tLR: 7.851918\n",
      "Training Epoch: 79 [26240/50000]\tLoss: 4.7060\tLR: 7.852174\n",
      "Training Epoch: 79 [26368/50000]\tLoss: 4.6933\tLR: 7.852430\n",
      "Training Epoch: 79 [26496/50000]\tLoss: 4.7494\tLR: 7.852685\n",
      "Training Epoch: 79 [26624/50000]\tLoss: 4.7070\tLR: 7.852941\n",
      "Training Epoch: 79 [26752/50000]\tLoss: 4.7335\tLR: 7.853197\n",
      "Training Epoch: 79 [26880/50000]\tLoss: 4.7036\tLR: 7.853453\n",
      "Training Epoch: 79 [27008/50000]\tLoss: 4.7919\tLR: 7.853708\n",
      "Training Epoch: 79 [27136/50000]\tLoss: 4.6932\tLR: 7.853964\n",
      "Training Epoch: 79 [27264/50000]\tLoss: 4.7674\tLR: 7.854220\n",
      "Training Epoch: 79 [27392/50000]\tLoss: 4.7254\tLR: 7.854476\n",
      "Training Epoch: 79 [27520/50000]\tLoss: 4.6889\tLR: 7.854731\n",
      "Training Epoch: 79 [27648/50000]\tLoss: 4.6571\tLR: 7.854987\n",
      "Training Epoch: 79 [27776/50000]\tLoss: 4.6847\tLR: 7.855243\n",
      "Training Epoch: 79 [27904/50000]\tLoss: 4.8249\tLR: 7.855499\n",
      "Training Epoch: 79 [28032/50000]\tLoss: 4.7810\tLR: 7.855754\n",
      "Training Epoch: 79 [28160/50000]\tLoss: 4.7379\tLR: 7.856010\n",
      "Training Epoch: 79 [28288/50000]\tLoss: 4.8105\tLR: 7.856266\n",
      "Training Epoch: 79 [28416/50000]\tLoss: 4.8342\tLR: 7.856522\n",
      "Training Epoch: 79 [28544/50000]\tLoss: 4.7963\tLR: 7.856777\n",
      "Training Epoch: 79 [28672/50000]\tLoss: 4.7046\tLR: 7.857033\n",
      "Training Epoch: 79 [28800/50000]\tLoss: 4.7375\tLR: 7.857289\n",
      "Training Epoch: 79 [28928/50000]\tLoss: 4.7880\tLR: 7.857545\n",
      "Training Epoch: 79 [29056/50000]\tLoss: 4.7200\tLR: 7.857801\n",
      "Training Epoch: 79 [29184/50000]\tLoss: 4.7894\tLR: 7.858056\n",
      "Training Epoch: 79 [29312/50000]\tLoss: 4.7910\tLR: 7.858312\n",
      "Training Epoch: 79 [29440/50000]\tLoss: 4.7996\tLR: 7.858568\n",
      "Training Epoch: 79 [29568/50000]\tLoss: 4.6705\tLR: 7.858824\n",
      "Training Epoch: 79 [29696/50000]\tLoss: 4.7272\tLR: 7.859079\n",
      "Training Epoch: 79 [29824/50000]\tLoss: 4.7608\tLR: 7.859335\n",
      "Training Epoch: 79 [29952/50000]\tLoss: 4.7300\tLR: 7.859591\n",
      "Training Epoch: 79 [30080/50000]\tLoss: 4.7029\tLR: 7.859847\n",
      "Training Epoch: 79 [30208/50000]\tLoss: 4.6704\tLR: 7.860102\n",
      "Training Epoch: 79 [30336/50000]\tLoss: 4.7072\tLR: 7.860358\n",
      "Training Epoch: 79 [30464/50000]\tLoss: 4.7405\tLR: 7.860614\n",
      "Training Epoch: 79 [30592/50000]\tLoss: 4.6942\tLR: 7.860870\n",
      "Training Epoch: 79 [30720/50000]\tLoss: 4.6829\tLR: 7.861125\n",
      "Training Epoch: 79 [30848/50000]\tLoss: 4.7213\tLR: 7.861381\n",
      "Training Epoch: 79 [30976/50000]\tLoss: 4.8131\tLR: 7.861637\n",
      "Training Epoch: 79 [31104/50000]\tLoss: 4.7832\tLR: 7.861893\n",
      "Training Epoch: 79 [31232/50000]\tLoss: 4.8148\tLR: 7.862148\n",
      "Training Epoch: 79 [31360/50000]\tLoss: 4.7002\tLR: 7.862404\n",
      "Training Epoch: 79 [31488/50000]\tLoss: 4.8506\tLR: 7.862660\n",
      "Training Epoch: 79 [31616/50000]\tLoss: 4.7996\tLR: 7.862916\n",
      "Training Epoch: 79 [31744/50000]\tLoss: 4.7299\tLR: 7.863171\n",
      "Training Epoch: 79 [31872/50000]\tLoss: 4.7148\tLR: 7.863427\n",
      "Training Epoch: 79 [32000/50000]\tLoss: 4.7982\tLR: 7.863683\n",
      "Training Epoch: 79 [32128/50000]\tLoss: 4.8193\tLR: 7.863939\n",
      "Training Epoch: 79 [32256/50000]\tLoss: 4.7332\tLR: 7.864194\n",
      "Training Epoch: 79 [32384/50000]\tLoss: 4.7986\tLR: 7.864450\n",
      "Training Epoch: 79 [32512/50000]\tLoss: 4.6851\tLR: 7.864706\n",
      "Training Epoch: 79 [32640/50000]\tLoss: 4.7986\tLR: 7.864962\n",
      "Training Epoch: 79 [32768/50000]\tLoss: 4.8232\tLR: 7.865217\n",
      "Training Epoch: 79 [32896/50000]\tLoss: 4.7587\tLR: 7.865473\n",
      "Training Epoch: 79 [33024/50000]\tLoss: 4.7680\tLR: 7.865729\n",
      "Training Epoch: 79 [33152/50000]\tLoss: 4.6794\tLR: 7.865985\n",
      "Training Epoch: 79 [33280/50000]\tLoss: 4.7696\tLR: 7.866240\n",
      "Training Epoch: 79 [33408/50000]\tLoss: 4.7364\tLR: 7.866496\n",
      "Training Epoch: 79 [33536/50000]\tLoss: 4.6972\tLR: 7.866752\n",
      "Training Epoch: 79 [33664/50000]\tLoss: 4.7677\tLR: 7.867008\n",
      "Training Epoch: 79 [33792/50000]\tLoss: 4.7120\tLR: 7.867263\n",
      "Training Epoch: 79 [33920/50000]\tLoss: 4.6926\tLR: 7.867519\n",
      "Training Epoch: 79 [34048/50000]\tLoss: 4.7142\tLR: 7.867775\n",
      "Training Epoch: 79 [34176/50000]\tLoss: 4.7757\tLR: 7.868031\n",
      "Training Epoch: 79 [34304/50000]\tLoss: 4.7690\tLR: 7.868286\n",
      "Training Epoch: 79 [34432/50000]\tLoss: 4.7994\tLR: 7.868542\n",
      "Training Epoch: 79 [34560/50000]\tLoss: 4.7961\tLR: 7.868798\n",
      "Training Epoch: 79 [34688/50000]\tLoss: 4.7534\tLR: 7.869054\n",
      "Training Epoch: 79 [34816/50000]\tLoss: 4.7022\tLR: 7.869309\n",
      "Training Epoch: 79 [34944/50000]\tLoss: 4.7384\tLR: 7.869565\n",
      "Training Epoch: 79 [35072/50000]\tLoss: 4.8106\tLR: 7.869821\n",
      "Training Epoch: 79 [35200/50000]\tLoss: 4.6904\tLR: 7.870077\n",
      "Training Epoch: 79 [35328/50000]\tLoss: 4.7825\tLR: 7.870332\n",
      "Training Epoch: 79 [35456/50000]\tLoss: 4.7498\tLR: 7.870588\n",
      "Training Epoch: 79 [35584/50000]\tLoss: 4.7488\tLR: 7.870844\n",
      "Training Epoch: 79 [35712/50000]\tLoss: 4.8508\tLR: 7.871100\n",
      "Training Epoch: 79 [35840/50000]\tLoss: 4.7776\tLR: 7.871355\n",
      "Training Epoch: 79 [35968/50000]\tLoss: 4.6979\tLR: 7.871611\n",
      "Training Epoch: 79 [36096/50000]\tLoss: 4.7087\tLR: 7.871867\n",
      "Training Epoch: 79 [36224/50000]\tLoss: 4.7499\tLR: 7.872123\n",
      "Training Epoch: 79 [36352/50000]\tLoss: 4.7003\tLR: 7.872379\n",
      "Training Epoch: 79 [36480/50000]\tLoss: 4.7548\tLR: 7.872634\n",
      "Training Epoch: 79 [36608/50000]\tLoss: 4.8414\tLR: 7.872890\n",
      "Training Epoch: 79 [36736/50000]\tLoss: 4.7140\tLR: 7.873146\n",
      "Training Epoch: 79 [36864/50000]\tLoss: 4.7011\tLR: 7.873402\n",
      "Training Epoch: 79 [36992/50000]\tLoss: 4.7271\tLR: 7.873657\n",
      "Training Epoch: 79 [37120/50000]\tLoss: 4.8799\tLR: 7.873913\n",
      "Training Epoch: 79 [37248/50000]\tLoss: 4.7789\tLR: 7.874169\n",
      "Training Epoch: 79 [37376/50000]\tLoss: 4.8320\tLR: 7.874425\n",
      "Training Epoch: 79 [37504/50000]\tLoss: 4.8170\tLR: 7.874680\n",
      "Training Epoch: 79 [37632/50000]\tLoss: 4.7530\tLR: 7.874936\n",
      "Training Epoch: 79 [37760/50000]\tLoss: 4.7650\tLR: 7.875192\n",
      "Training Epoch: 79 [37888/50000]\tLoss: 4.7928\tLR: 7.875448\n",
      "Training Epoch: 79 [38016/50000]\tLoss: 4.7117\tLR: 7.875703\n",
      "Training Epoch: 79 [38144/50000]\tLoss: 4.7744\tLR: 7.875959\n",
      "Training Epoch: 79 [38272/50000]\tLoss: 4.6988\tLR: 7.876215\n",
      "Training Epoch: 79 [38400/50000]\tLoss: 4.7216\tLR: 7.876471\n",
      "Training Epoch: 79 [38528/50000]\tLoss: 4.7360\tLR: 7.876726\n",
      "Training Epoch: 79 [38656/50000]\tLoss: 4.7148\tLR: 7.876982\n",
      "Training Epoch: 79 [38784/50000]\tLoss: 4.7847\tLR: 7.877238\n",
      "Training Epoch: 79 [38912/50000]\tLoss: 4.7397\tLR: 7.877494\n",
      "Training Epoch: 79 [39040/50000]\tLoss: 4.7805\tLR: 7.877749\n",
      "Training Epoch: 79 [39168/50000]\tLoss: 4.7447\tLR: 7.878005\n",
      "Training Epoch: 79 [39296/50000]\tLoss: 4.7134\tLR: 7.878261\n",
      "Training Epoch: 79 [39424/50000]\tLoss: 4.7575\tLR: 7.878517\n",
      "Training Epoch: 79 [39552/50000]\tLoss: 4.8252\tLR: 7.878772\n",
      "Training Epoch: 79 [39680/50000]\tLoss: 4.6305\tLR: 7.879028\n",
      "Training Epoch: 79 [39808/50000]\tLoss: 4.7470\tLR: 7.879284\n",
      "Training Epoch: 79 [39936/50000]\tLoss: 4.7669\tLR: 7.879540\n",
      "Training Epoch: 79 [40064/50000]\tLoss: 4.7742\tLR: 7.879795\n",
      "Training Epoch: 79 [40192/50000]\tLoss: 4.8422\tLR: 7.880051\n",
      "Training Epoch: 79 [40320/50000]\tLoss: 4.8203\tLR: 7.880307\n",
      "Training Epoch: 79 [40448/50000]\tLoss: 4.7076\tLR: 7.880563\n",
      "Training Epoch: 79 [40576/50000]\tLoss: 4.7532\tLR: 7.880818\n",
      "Training Epoch: 79 [40704/50000]\tLoss: 4.7144\tLR: 7.881074\n",
      "Training Epoch: 79 [40832/50000]\tLoss: 4.7491\tLR: 7.881330\n",
      "Training Epoch: 79 [40960/50000]\tLoss: 4.7938\tLR: 7.881586\n",
      "Training Epoch: 79 [41088/50000]\tLoss: 4.8282\tLR: 7.881841\n",
      "Training Epoch: 79 [41216/50000]\tLoss: 4.8598\tLR: 7.882097\n",
      "Training Epoch: 79 [41344/50000]\tLoss: 4.7519\tLR: 7.882353\n",
      "Training Epoch: 79 [41472/50000]\tLoss: 4.8446\tLR: 7.882609\n",
      "Training Epoch: 79 [41600/50000]\tLoss: 4.7146\tLR: 7.882864\n",
      "Training Epoch: 79 [41728/50000]\tLoss: 4.7583\tLR: 7.883120\n",
      "Training Epoch: 79 [41856/50000]\tLoss: 4.7300\tLR: 7.883376\n",
      "Training Epoch: 79 [41984/50000]\tLoss: 4.7073\tLR: 7.883632\n",
      "Training Epoch: 79 [42112/50000]\tLoss: 4.7281\tLR: 7.883887\n",
      "Training Epoch: 79 [42240/50000]\tLoss: 4.7712\tLR: 7.884143\n",
      "Training Epoch: 79 [42368/50000]\tLoss: 4.7472\tLR: 7.884399\n",
      "Training Epoch: 79 [42496/50000]\tLoss: 4.7072\tLR: 7.884655\n",
      "Training Epoch: 79 [42624/50000]\tLoss: 4.7820\tLR: 7.884910\n",
      "Training Epoch: 79 [42752/50000]\tLoss: 4.7813\tLR: 7.885166\n",
      "Training Epoch: 79 [42880/50000]\tLoss: 4.6624\tLR: 7.885422\n",
      "Training Epoch: 79 [43008/50000]\tLoss: 4.6756\tLR: 7.885678\n",
      "Training Epoch: 79 [43136/50000]\tLoss: 4.6800\tLR: 7.885934\n",
      "Training Epoch: 79 [43264/50000]\tLoss: 4.6878\tLR: 7.886189\n",
      "Training Epoch: 79 [43392/50000]\tLoss: 4.7453\tLR: 7.886445\n",
      "Training Epoch: 79 [43520/50000]\tLoss: 4.7743\tLR: 7.886701\n",
      "Training Epoch: 79 [43648/50000]\tLoss: 4.7162\tLR: 7.886957\n",
      "Training Epoch: 79 [43776/50000]\tLoss: 4.7237\tLR: 7.887212\n",
      "Training Epoch: 79 [43904/50000]\tLoss: 4.7438\tLR: 7.887468\n",
      "Training Epoch: 79 [44032/50000]\tLoss: 4.9022\tLR: 7.887724\n",
      "Training Epoch: 79 [44160/50000]\tLoss: 4.7411\tLR: 7.887980\n",
      "Training Epoch: 79 [44288/50000]\tLoss: 4.7653\tLR: 7.888235\n",
      "Training Epoch: 79 [44416/50000]\tLoss: 4.7062\tLR: 7.888491\n",
      "Training Epoch: 79 [44544/50000]\tLoss: 4.6968\tLR: 7.888747\n",
      "Training Epoch: 79 [44672/50000]\tLoss: 4.7779\tLR: 7.889003\n",
      "Training Epoch: 79 [44800/50000]\tLoss: 4.7721\tLR: 7.889258\n",
      "Training Epoch: 79 [44928/50000]\tLoss: 4.6681\tLR: 7.889514\n",
      "Training Epoch: 79 [45056/50000]\tLoss: 4.7700\tLR: 7.889770\n",
      "Training Epoch: 79 [45184/50000]\tLoss: 4.7254\tLR: 7.890026\n",
      "Training Epoch: 79 [45312/50000]\tLoss: 4.7271\tLR: 7.890281\n",
      "Training Epoch: 79 [45440/50000]\tLoss: 4.7912\tLR: 7.890537\n",
      "Training Epoch: 79 [45568/50000]\tLoss: 4.8075\tLR: 7.890793\n",
      "Training Epoch: 79 [45696/50000]\tLoss: 4.7561\tLR: 7.891049\n",
      "Training Epoch: 79 [45824/50000]\tLoss: 4.7549\tLR: 7.891304\n",
      "Training Epoch: 79 [45952/50000]\tLoss: 4.8713\tLR: 7.891560\n",
      "Training Epoch: 79 [46080/50000]\tLoss: 4.7824\tLR: 7.891816\n",
      "Training Epoch: 79 [46208/50000]\tLoss: 4.8822\tLR: 7.892072\n",
      "Training Epoch: 79 [46336/50000]\tLoss: 4.7700\tLR: 7.892327\n",
      "Training Epoch: 79 [46464/50000]\tLoss: 4.8878\tLR: 7.892583\n",
      "Training Epoch: 79 [46592/50000]\tLoss: 4.7274\tLR: 7.892839\n",
      "Training Epoch: 79 [46720/50000]\tLoss: 4.6890\tLR: 7.893095\n",
      "Training Epoch: 79 [46848/50000]\tLoss: 4.7800\tLR: 7.893350\n",
      "Training Epoch: 79 [46976/50000]\tLoss: 4.7809\tLR: 7.893606\n",
      "Training Epoch: 79 [47104/50000]\tLoss: 4.7752\tLR: 7.893862\n",
      "Training Epoch: 79 [47232/50000]\tLoss: 4.7253\tLR: 7.894118\n",
      "Training Epoch: 79 [47360/50000]\tLoss: 4.8415\tLR: 7.894373\n",
      "Training Epoch: 79 [47488/50000]\tLoss: 4.7885\tLR: 7.894629\n",
      "Training Epoch: 79 [47616/50000]\tLoss: 4.8160\tLR: 7.894885\n",
      "Training Epoch: 79 [47744/50000]\tLoss: 4.7658\tLR: 7.895141\n",
      "Training Epoch: 79 [47872/50000]\tLoss: 4.7322\tLR: 7.895396\n",
      "Training Epoch: 79 [48000/50000]\tLoss: 4.7602\tLR: 7.895652\n",
      "Training Epoch: 79 [48128/50000]\tLoss: 4.7611\tLR: 7.895908\n",
      "Training Epoch: 79 [48256/50000]\tLoss: 4.7333\tLR: 7.896164\n",
      "Training Epoch: 79 [48384/50000]\tLoss: 4.8156\tLR: 7.896419\n",
      "Training Epoch: 79 [48512/50000]\tLoss: 4.6939\tLR: 7.896675\n",
      "Training Epoch: 79 [48640/50000]\tLoss: 4.7568\tLR: 7.896931\n",
      "Training Epoch: 79 [48768/50000]\tLoss: 4.7054\tLR: 7.897187\n",
      "Training Epoch: 79 [48896/50000]\tLoss: 4.7129\tLR: 7.897442\n",
      "Training Epoch: 79 [49024/50000]\tLoss: 4.8326\tLR: 7.897698\n",
      "Training Epoch: 79 [49152/50000]\tLoss: 4.7779\tLR: 7.897954\n",
      "Training Epoch: 79 [49280/50000]\tLoss: 4.6957\tLR: 7.898210\n",
      "Training Epoch: 79 [49408/50000]\tLoss: 4.6749\tLR: 7.898465\n",
      "Training Epoch: 79 [49536/50000]\tLoss: 4.7536\tLR: 7.898721\n",
      "Training Epoch: 79 [49664/50000]\tLoss: 4.7346\tLR: 7.898977\n",
      "Training Epoch: 79 [49792/50000]\tLoss: 4.8282\tLR: 7.899233\n",
      "Training Epoch: 79 [49920/50000]\tLoss: 4.7257\tLR: 7.899488\n",
      "Training Epoch: 79 [50000/50000]\tLoss: 4.7781\tLR: 7.899744\n",
      "epoch 79 training time consumed: 489.56s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  110752 GB |  110752 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  110412 GB |  110412 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     339 GB |     339 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  110752 GB |  110752 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  110412 GB |  110412 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     339 GB |     339 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  109196 GB |  109196 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  108856 GB |  108856 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     340 GB |     340 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   11743 K  |   11743 K  |\n",
      "|       from large pool |      24    |      65    |    5006 K  |    5006 K  |\n",
      "|       from small pool |     231    |     274    |    6737 K  |    6737 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   11743 K  |   11743 K  |\n",
      "|       from large pool |      24    |      65    |    5006 K  |    5006 K  |\n",
      "|       from small pool |     231    |     274    |    6737 K  |    6737 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      47    |    6807 K  |    6806 K  |\n",
      "|       from large pool |      10    |      23    |    2406 K  |    2406 K  |\n",
      "|       from small pool |      27    |      35    |    4400 K  |    4400 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 79, Average loss: 0.0375, Accuracy: 0.0100, Time consumed:31.25s\n",
      "\n",
      "Training Epoch: 80 [128/50000]\tLoss: 4.8179\tLR: 0.020000\n",
      "Training Epoch: 80 [256/50000]\tLoss: 4.7188\tLR: 7.900256\n",
      "Training Epoch: 80 [384/50000]\tLoss: 4.7934\tLR: 7.900512\n",
      "Training Epoch: 80 [512/50000]\tLoss: 4.7478\tLR: 7.900767\n",
      "Training Epoch: 80 [640/50000]\tLoss: 4.6846\tLR: 7.901023\n",
      "Training Epoch: 80 [768/50000]\tLoss: 4.7141\tLR: 7.901279\n",
      "Training Epoch: 80 [896/50000]\tLoss: 4.7256\tLR: 7.901535\n",
      "Training Epoch: 80 [1024/50000]\tLoss: 4.8786\tLR: 7.901790\n",
      "Training Epoch: 80 [1152/50000]\tLoss: 4.8264\tLR: 7.902046\n",
      "Training Epoch: 80 [1280/50000]\tLoss: 4.7141\tLR: 7.902302\n",
      "Training Epoch: 80 [1408/50000]\tLoss: 4.7180\tLR: 7.902558\n",
      "Training Epoch: 80 [1536/50000]\tLoss: 4.7910\tLR: 7.902813\n",
      "Training Epoch: 80 [1664/50000]\tLoss: 4.6716\tLR: 7.903069\n",
      "Training Epoch: 80 [1792/50000]\tLoss: 4.8169\tLR: 7.903325\n",
      "Training Epoch: 80 [1920/50000]\tLoss: 4.7795\tLR: 7.903581\n",
      "Training Epoch: 80 [2048/50000]\tLoss: 4.7815\tLR: 7.903836\n",
      "Training Epoch: 80 [2176/50000]\tLoss: 4.7301\tLR: 7.904092\n",
      "Training Epoch: 80 [2304/50000]\tLoss: 4.7138\tLR: 7.904348\n",
      "Training Epoch: 80 [2432/50000]\tLoss: 4.8643\tLR: 7.904604\n",
      "Training Epoch: 80 [2560/50000]\tLoss: 4.7777\tLR: 7.904859\n",
      "Training Epoch: 80 [2688/50000]\tLoss: 4.7605\tLR: 7.905115\n",
      "Training Epoch: 80 [2816/50000]\tLoss: 4.8373\tLR: 7.905371\n",
      "Training Epoch: 80 [2944/50000]\tLoss: 4.7420\tLR: 7.905627\n",
      "Training Epoch: 80 [3072/50000]\tLoss: 4.7259\tLR: 7.905882\n",
      "Training Epoch: 80 [3200/50000]\tLoss: 4.7577\tLR: 7.906138\n",
      "Training Epoch: 80 [3328/50000]\tLoss: 4.7551\tLR: 7.906394\n",
      "Training Epoch: 80 [3456/50000]\tLoss: 4.8004\tLR: 7.906650\n",
      "Training Epoch: 80 [3584/50000]\tLoss: 4.7274\tLR: 7.906905\n",
      "Training Epoch: 80 [3712/50000]\tLoss: 4.7685\tLR: 7.907161\n",
      "Training Epoch: 80 [3840/50000]\tLoss: 4.7131\tLR: 7.907417\n",
      "Training Epoch: 80 [3968/50000]\tLoss: 4.7360\tLR: 7.907673\n",
      "Training Epoch: 80 [4096/50000]\tLoss: 4.7625\tLR: 7.907928\n",
      "Training Epoch: 80 [4224/50000]\tLoss: 4.7560\tLR: 7.908184\n",
      "Training Epoch: 80 [4352/50000]\tLoss: 4.7708\tLR: 7.908440\n",
      "Training Epoch: 80 [4480/50000]\tLoss: 4.6998\tLR: 7.908696\n",
      "Training Epoch: 80 [4608/50000]\tLoss: 4.7195\tLR: 7.908951\n",
      "Training Epoch: 80 [4736/50000]\tLoss: 4.7723\tLR: 7.909207\n",
      "Training Epoch: 80 [4864/50000]\tLoss: 4.7492\tLR: 7.909463\n",
      "Training Epoch: 80 [4992/50000]\tLoss: 4.7464\tLR: 7.909719\n",
      "Training Epoch: 80 [5120/50000]\tLoss: 4.6433\tLR: 7.909974\n",
      "Training Epoch: 80 [5248/50000]\tLoss: 4.7109\tLR: 7.910230\n",
      "Training Epoch: 80 [5376/50000]\tLoss: 4.7315\tLR: 7.910486\n",
      "Training Epoch: 80 [5504/50000]\tLoss: 4.6884\tLR: 7.910742\n",
      "Training Epoch: 80 [5632/50000]\tLoss: 4.7223\tLR: 7.910997\n",
      "Training Epoch: 80 [5760/50000]\tLoss: 4.7688\tLR: 7.911253\n",
      "Training Epoch: 80 [5888/50000]\tLoss: 4.7450\tLR: 7.911509\n",
      "Training Epoch: 80 [6016/50000]\tLoss: 4.7449\tLR: 7.911765\n",
      "Training Epoch: 80 [6144/50000]\tLoss: 4.8159\tLR: 7.912020\n",
      "Training Epoch: 80 [6272/50000]\tLoss: 4.7162\tLR: 7.912276\n",
      "Training Epoch: 80 [6400/50000]\tLoss: 4.7756\tLR: 7.912532\n",
      "Training Epoch: 80 [6528/50000]\tLoss: 4.7150\tLR: 7.912788\n",
      "Training Epoch: 80 [6656/50000]\tLoss: 4.7229\tLR: 7.913043\n",
      "Training Epoch: 80 [6784/50000]\tLoss: 4.7228\tLR: 7.913299\n",
      "Training Epoch: 80 [6912/50000]\tLoss: 4.8423\tLR: 7.913555\n",
      "Training Epoch: 80 [7040/50000]\tLoss: 4.8092\tLR: 7.913811\n",
      "Training Epoch: 80 [7168/50000]\tLoss: 4.8144\tLR: 7.914066\n",
      "Training Epoch: 80 [7296/50000]\tLoss: 4.7614\tLR: 7.914322\n",
      "Training Epoch: 80 [7424/50000]\tLoss: 4.7824\tLR: 7.914578\n",
      "Training Epoch: 80 [7552/50000]\tLoss: 4.7131\tLR: 7.914834\n",
      "Training Epoch: 80 [7680/50000]\tLoss: 4.7880\tLR: 7.915090\n",
      "Training Epoch: 80 [7808/50000]\tLoss: 4.7513\tLR: 7.915345\n",
      "Training Epoch: 80 [7936/50000]\tLoss: 4.6135\tLR: 7.915601\n",
      "Training Epoch: 80 [8064/50000]\tLoss: 4.7614\tLR: 7.915857\n",
      "Training Epoch: 80 [8192/50000]\tLoss: 4.6984\tLR: 7.916113\n",
      "Training Epoch: 80 [8320/50000]\tLoss: 4.8916\tLR: 7.916368\n",
      "Training Epoch: 80 [8448/50000]\tLoss: 4.8136\tLR: 7.916624\n",
      "Training Epoch: 80 [8576/50000]\tLoss: 4.8062\tLR: 7.916880\n",
      "Training Epoch: 80 [8704/50000]\tLoss: 4.7367\tLR: 7.917136\n",
      "Training Epoch: 80 [8832/50000]\tLoss: 4.7720\tLR: 7.917391\n",
      "Training Epoch: 80 [8960/50000]\tLoss: 4.6760\tLR: 7.917647\n",
      "Training Epoch: 80 [9088/50000]\tLoss: 4.6637\tLR: 7.917903\n",
      "Training Epoch: 80 [9216/50000]\tLoss: 4.8091\tLR: 7.918159\n",
      "Training Epoch: 80 [9344/50000]\tLoss: 4.6965\tLR: 7.918414\n",
      "Training Epoch: 80 [9472/50000]\tLoss: 4.8190\tLR: 7.918670\n",
      "Training Epoch: 80 [9600/50000]\tLoss: 4.8032\tLR: 7.918926\n",
      "Training Epoch: 80 [9728/50000]\tLoss: 4.7144\tLR: 7.919182\n",
      "Training Epoch: 80 [9856/50000]\tLoss: 4.6993\tLR: 7.919437\n",
      "Training Epoch: 80 [9984/50000]\tLoss: 4.7137\tLR: 7.919693\n",
      "Training Epoch: 80 [10112/50000]\tLoss: 4.7540\tLR: 7.919949\n",
      "Training Epoch: 80 [10240/50000]\tLoss: 4.8028\tLR: 7.920205\n",
      "Training Epoch: 80 [10368/50000]\tLoss: 4.6984\tLR: 7.920460\n",
      "Training Epoch: 80 [10496/50000]\tLoss: 4.7604\tLR: 7.920716\n",
      "Training Epoch: 80 [10624/50000]\tLoss: 4.7447\tLR: 7.920972\n",
      "Training Epoch: 80 [10752/50000]\tLoss: 4.6569\tLR: 7.921228\n",
      "Training Epoch: 80 [10880/50000]\tLoss: 4.8406\tLR: 7.921483\n",
      "Training Epoch: 80 [11008/50000]\tLoss: 4.8070\tLR: 7.921739\n",
      "Training Epoch: 80 [11136/50000]\tLoss: 4.8070\tLR: 7.921995\n",
      "Training Epoch: 80 [11264/50000]\tLoss: 4.7641\tLR: 7.922251\n",
      "Training Epoch: 80 [11392/50000]\tLoss: 4.7884\tLR: 7.922506\n",
      "Training Epoch: 80 [11520/50000]\tLoss: 4.7554\tLR: 7.922762\n",
      "Training Epoch: 80 [11648/50000]\tLoss: 4.7743\tLR: 7.923018\n",
      "Training Epoch: 80 [11776/50000]\tLoss: 4.6862\tLR: 7.923274\n",
      "Training Epoch: 80 [11904/50000]\tLoss: 4.7240\tLR: 7.923529\n",
      "Training Epoch: 80 [12032/50000]\tLoss: 4.7554\tLR: 7.923785\n",
      "Training Epoch: 80 [12160/50000]\tLoss: 4.7963\tLR: 7.924041\n",
      "Training Epoch: 80 [12288/50000]\tLoss: 4.6825\tLR: 7.924297\n",
      "Training Epoch: 80 [12416/50000]\tLoss: 4.7717\tLR: 7.924552\n",
      "Training Epoch: 80 [12544/50000]\tLoss: 4.7236\tLR: 7.924808\n",
      "Training Epoch: 80 [12672/50000]\tLoss: 4.7113\tLR: 7.925064\n",
      "Training Epoch: 80 [12800/50000]\tLoss: 4.7648\tLR: 7.925320\n",
      "Training Epoch: 80 [12928/50000]\tLoss: 4.7445\tLR: 7.925575\n",
      "Training Epoch: 80 [13056/50000]\tLoss: 4.7796\tLR: 7.925831\n",
      "Training Epoch: 80 [13184/50000]\tLoss: 4.7154\tLR: 7.926087\n",
      "Training Epoch: 80 [13312/50000]\tLoss: 4.7389\tLR: 7.926343\n",
      "Training Epoch: 80 [13440/50000]\tLoss: 4.7172\tLR: 7.926598\n",
      "Training Epoch: 80 [13568/50000]\tLoss: 4.7615\tLR: 7.926854\n",
      "Training Epoch: 80 [13696/50000]\tLoss: 4.7923\tLR: 7.927110\n",
      "Training Epoch: 80 [13824/50000]\tLoss: 4.6943\tLR: 7.927366\n",
      "Training Epoch: 80 [13952/50000]\tLoss: 4.7911\tLR: 7.927621\n",
      "Training Epoch: 80 [14080/50000]\tLoss: 4.7305\tLR: 7.927877\n",
      "Training Epoch: 80 [14208/50000]\tLoss: 4.8939\tLR: 7.928133\n",
      "Training Epoch: 80 [14336/50000]\tLoss: 4.7961\tLR: 7.928389\n",
      "Training Epoch: 80 [14464/50000]\tLoss: 4.6917\tLR: 7.928645\n",
      "Training Epoch: 80 [14592/50000]\tLoss: 4.7421\tLR: 7.928900\n",
      "Training Epoch: 80 [14720/50000]\tLoss: 4.9105\tLR: 7.929156\n",
      "Training Epoch: 80 [14848/50000]\tLoss: 4.6845\tLR: 7.929412\n",
      "Training Epoch: 80 [14976/50000]\tLoss: 4.7792\tLR: 7.929668\n",
      "Training Epoch: 80 [15104/50000]\tLoss: 4.7542\tLR: 7.929923\n",
      "Training Epoch: 80 [15232/50000]\tLoss: 4.7369\tLR: 7.930179\n",
      "Training Epoch: 80 [15360/50000]\tLoss: 4.8070\tLR: 7.930435\n",
      "Training Epoch: 80 [15488/50000]\tLoss: 4.8088\tLR: 7.930691\n",
      "Training Epoch: 80 [15616/50000]\tLoss: 4.7343\tLR: 7.930946\n",
      "Training Epoch: 80 [15744/50000]\tLoss: 4.7551\tLR: 7.931202\n",
      "Training Epoch: 80 [15872/50000]\tLoss: 4.7375\tLR: 7.931458\n",
      "Training Epoch: 80 [16000/50000]\tLoss: 4.6995\tLR: 7.931714\n",
      "Training Epoch: 80 [16128/50000]\tLoss: 4.7091\tLR: 7.931969\n",
      "Training Epoch: 80 [16256/50000]\tLoss: 4.7809\tLR: 7.932225\n",
      "Training Epoch: 80 [16384/50000]\tLoss: 4.7658\tLR: 7.932481\n",
      "Training Epoch: 80 [16512/50000]\tLoss: 4.8049\tLR: 7.932737\n",
      "Training Epoch: 80 [16640/50000]\tLoss: 4.7633\tLR: 7.932992\n",
      "Training Epoch: 80 [16768/50000]\tLoss: 4.9774\tLR: 7.933248\n",
      "Training Epoch: 80 [16896/50000]\tLoss: 4.7859\tLR: 7.933504\n",
      "Training Epoch: 80 [17024/50000]\tLoss: 4.7572\tLR: 7.933760\n",
      "Training Epoch: 80 [17152/50000]\tLoss: 4.7620\tLR: 7.934015\n",
      "Training Epoch: 80 [17280/50000]\tLoss: 4.7342\tLR: 7.934271\n",
      "Training Epoch: 80 [17408/50000]\tLoss: 4.7431\tLR: 7.934527\n",
      "Training Epoch: 80 [17536/50000]\tLoss: 4.7936\tLR: 7.934783\n",
      "Training Epoch: 80 [17664/50000]\tLoss: 4.7474\tLR: 7.935038\n",
      "Training Epoch: 80 [17792/50000]\tLoss: 4.7156\tLR: 7.935294\n",
      "Training Epoch: 80 [17920/50000]\tLoss: 4.8168\tLR: 7.935550\n",
      "Training Epoch: 80 [18048/50000]\tLoss: 4.8312\tLR: 7.935806\n",
      "Training Epoch: 80 [18176/50000]\tLoss: 4.8874\tLR: 7.936061\n",
      "Training Epoch: 80 [18304/50000]\tLoss: 4.7514\tLR: 7.936317\n",
      "Training Epoch: 80 [18432/50000]\tLoss: 4.8039\tLR: 7.936573\n",
      "Training Epoch: 80 [18560/50000]\tLoss: 4.7334\tLR: 7.936829\n",
      "Training Epoch: 80 [18688/50000]\tLoss: 4.7905\tLR: 7.937084\n",
      "Training Epoch: 80 [18816/50000]\tLoss: 4.7528\tLR: 7.937340\n",
      "Training Epoch: 80 [18944/50000]\tLoss: 4.8387\tLR: 7.937596\n",
      "Training Epoch: 80 [19072/50000]\tLoss: 4.7542\tLR: 7.937852\n",
      "Training Epoch: 80 [19200/50000]\tLoss: 4.7875\tLR: 7.938107\n",
      "Training Epoch: 80 [19328/50000]\tLoss: 4.7911\tLR: 7.938363\n",
      "Training Epoch: 80 [19456/50000]\tLoss: 4.7723\tLR: 7.938619\n",
      "Training Epoch: 80 [19584/50000]\tLoss: 4.8376\tLR: 7.938875\n",
      "Training Epoch: 80 [19712/50000]\tLoss: 4.7775\tLR: 7.939130\n",
      "Training Epoch: 80 [19840/50000]\tLoss: 4.7581\tLR: 7.939386\n",
      "Training Epoch: 80 [19968/50000]\tLoss: 4.7080\tLR: 7.939642\n",
      "Training Epoch: 80 [20096/50000]\tLoss: 4.7588\tLR: 7.939898\n",
      "Training Epoch: 80 [20224/50000]\tLoss: 4.7411\tLR: 7.940153\n",
      "Training Epoch: 80 [20352/50000]\tLoss: 4.7891\tLR: 7.940409\n",
      "Training Epoch: 80 [20480/50000]\tLoss: 4.6981\tLR: 7.940665\n",
      "Training Epoch: 80 [20608/50000]\tLoss: 4.8446\tLR: 7.940921\n",
      "Training Epoch: 80 [20736/50000]\tLoss: 4.8545\tLR: 7.941176\n",
      "Training Epoch: 80 [20864/50000]\tLoss: 4.7509\tLR: 7.941432\n",
      "Training Epoch: 80 [20992/50000]\tLoss: 4.7242\tLR: 7.941688\n",
      "Training Epoch: 80 [21120/50000]\tLoss: 4.6524\tLR: 7.941944\n",
      "Training Epoch: 80 [21248/50000]\tLoss: 4.7084\tLR: 7.942199\n",
      "Training Epoch: 80 [21376/50000]\tLoss: 4.7762\tLR: 7.942455\n",
      "Training Epoch: 80 [21504/50000]\tLoss: 4.8275\tLR: 7.942711\n",
      "Training Epoch: 80 [21632/50000]\tLoss: 4.7284\tLR: 7.942967\n",
      "Training Epoch: 80 [21760/50000]\tLoss: 4.7857\tLR: 7.943223\n",
      "Training Epoch: 80 [21888/50000]\tLoss: 4.7857\tLR: 7.943478\n",
      "Training Epoch: 80 [22016/50000]\tLoss: 4.8883\tLR: 7.943734\n",
      "Training Epoch: 80 [22144/50000]\tLoss: 4.6332\tLR: 7.943990\n",
      "Training Epoch: 80 [22272/50000]\tLoss: 4.8011\tLR: 7.944246\n",
      "Training Epoch: 80 [22400/50000]\tLoss: 4.8033\tLR: 7.944501\n",
      "Training Epoch: 80 [22528/50000]\tLoss: 4.8088\tLR: 7.944757\n",
      "Training Epoch: 80 [22656/50000]\tLoss: 4.7665\tLR: 7.945013\n",
      "Training Epoch: 80 [22784/50000]\tLoss: 4.7841\tLR: 7.945269\n",
      "Training Epoch: 80 [22912/50000]\tLoss: 4.7598\tLR: 7.945524\n",
      "Training Epoch: 80 [23040/50000]\tLoss: 4.7101\tLR: 7.945780\n",
      "Training Epoch: 80 [23168/50000]\tLoss: 4.7527\tLR: 7.946036\n",
      "Training Epoch: 80 [23296/50000]\tLoss: 4.7509\tLR: 7.946292\n",
      "Training Epoch: 80 [23424/50000]\tLoss: 4.7061\tLR: 7.946547\n",
      "Training Epoch: 80 [23552/50000]\tLoss: 4.8402\tLR: 7.946803\n",
      "Training Epoch: 80 [23680/50000]\tLoss: 4.7486\tLR: 7.947059\n",
      "Training Epoch: 80 [23808/50000]\tLoss: 4.6616\tLR: 7.947315\n",
      "Training Epoch: 80 [23936/50000]\tLoss: 4.8102\tLR: 7.947570\n",
      "Training Epoch: 80 [24064/50000]\tLoss: 4.7754\tLR: 7.947826\n",
      "Training Epoch: 80 [24192/50000]\tLoss: 4.7959\tLR: 7.948082\n",
      "Training Epoch: 80 [24320/50000]\tLoss: 4.8027\tLR: 7.948338\n",
      "Training Epoch: 80 [24448/50000]\tLoss: 4.7132\tLR: 7.948593\n",
      "Training Epoch: 80 [24576/50000]\tLoss: 4.7741\tLR: 7.948849\n",
      "Training Epoch: 80 [24704/50000]\tLoss: 4.7987\tLR: 7.949105\n",
      "Training Epoch: 80 [24832/50000]\tLoss: 4.8501\tLR: 7.949361\n",
      "Training Epoch: 80 [24960/50000]\tLoss: 4.7523\tLR: 7.949616\n",
      "Training Epoch: 80 [25088/50000]\tLoss: 4.8488\tLR: 7.949872\n",
      "Training Epoch: 80 [25216/50000]\tLoss: 4.6894\tLR: 7.950128\n",
      "Training Epoch: 80 [25344/50000]\tLoss: 4.7912\tLR: 7.950384\n",
      "Training Epoch: 80 [25472/50000]\tLoss: 4.6990\tLR: 7.950639\n",
      "Training Epoch: 80 [25600/50000]\tLoss: 4.7127\tLR: 7.950895\n",
      "Training Epoch: 80 [25728/50000]\tLoss: 4.6833\tLR: 7.951151\n",
      "Training Epoch: 80 [25856/50000]\tLoss: 4.8435\tLR: 7.951407\n",
      "Training Epoch: 80 [25984/50000]\tLoss: 4.7586\tLR: 7.951662\n",
      "Training Epoch: 80 [26112/50000]\tLoss: 4.8403\tLR: 7.951918\n",
      "Training Epoch: 80 [26240/50000]\tLoss: 4.8210\tLR: 7.952174\n",
      "Training Epoch: 80 [26368/50000]\tLoss: 4.6983\tLR: 7.952430\n",
      "Training Epoch: 80 [26496/50000]\tLoss: 4.7948\tLR: 7.952685\n",
      "Training Epoch: 80 [26624/50000]\tLoss: 4.7729\tLR: 7.952941\n",
      "Training Epoch: 80 [26752/50000]\tLoss: 4.6552\tLR: 7.953197\n",
      "Training Epoch: 80 [26880/50000]\tLoss: 4.7352\tLR: 7.953453\n",
      "Training Epoch: 80 [27008/50000]\tLoss: 4.6907\tLR: 7.953708\n",
      "Training Epoch: 80 [27136/50000]\tLoss: 4.8138\tLR: 7.953964\n",
      "Training Epoch: 80 [27264/50000]\tLoss: 4.7891\tLR: 7.954220\n",
      "Training Epoch: 80 [27392/50000]\tLoss: 4.8340\tLR: 7.954476\n",
      "Training Epoch: 80 [27520/50000]\tLoss: 4.8195\tLR: 7.954731\n",
      "Training Epoch: 80 [27648/50000]\tLoss: 4.7581\tLR: 7.954987\n",
      "Training Epoch: 80 [27776/50000]\tLoss: 4.6897\tLR: 7.955243\n",
      "Training Epoch: 80 [27904/50000]\tLoss: 4.7234\tLR: 7.955499\n",
      "Training Epoch: 80 [28032/50000]\tLoss: 4.7700\tLR: 7.955754\n",
      "Training Epoch: 80 [28160/50000]\tLoss: 4.7496\tLR: 7.956010\n",
      "Training Epoch: 80 [28288/50000]\tLoss: 4.7177\tLR: 7.956266\n",
      "Training Epoch: 80 [28416/50000]\tLoss: 4.7414\tLR: 7.956522\n",
      "Training Epoch: 80 [28544/50000]\tLoss: 4.6759\tLR: 7.956777\n",
      "Training Epoch: 80 [28672/50000]\tLoss: 4.7846\tLR: 7.957033\n",
      "Training Epoch: 80 [28800/50000]\tLoss: 4.6317\tLR: 7.957289\n",
      "Training Epoch: 80 [28928/50000]\tLoss: 4.6914\tLR: 7.957545\n",
      "Training Epoch: 80 [29056/50000]\tLoss: 4.7071\tLR: 7.957801\n",
      "Training Epoch: 80 [29184/50000]\tLoss: 4.8293\tLR: 7.958056\n",
      "Training Epoch: 80 [29312/50000]\tLoss: 4.7518\tLR: 7.958312\n",
      "Training Epoch: 80 [29440/50000]\tLoss: 4.7553\tLR: 7.958568\n",
      "Training Epoch: 80 [29568/50000]\tLoss: 4.7319\tLR: 7.958824\n",
      "Training Epoch: 80 [29696/50000]\tLoss: 4.8212\tLR: 7.959079\n",
      "Training Epoch: 80 [29824/50000]\tLoss: 4.6700\tLR: 7.959335\n",
      "Training Epoch: 80 [29952/50000]\tLoss: 4.7103\tLR: 7.959591\n",
      "Training Epoch: 80 [30080/50000]\tLoss: 4.7046\tLR: 7.959847\n",
      "Training Epoch: 80 [30208/50000]\tLoss: 4.6898\tLR: 7.960102\n",
      "Training Epoch: 80 [30336/50000]\tLoss: 4.7632\tLR: 7.960358\n",
      "Training Epoch: 80 [30464/50000]\tLoss: 4.7870\tLR: 7.960614\n",
      "Training Epoch: 80 [30592/50000]\tLoss: 4.7937\tLR: 7.960870\n",
      "Training Epoch: 80 [30720/50000]\tLoss: 4.7567\tLR: 7.961125\n",
      "Training Epoch: 80 [30848/50000]\tLoss: 4.7420\tLR: 7.961381\n",
      "Training Epoch: 80 [30976/50000]\tLoss: 4.7176\tLR: 7.961637\n",
      "Training Epoch: 80 [31104/50000]\tLoss: 4.7036\tLR: 7.961893\n",
      "Training Epoch: 80 [31232/50000]\tLoss: 4.7561\tLR: 7.962148\n",
      "Training Epoch: 80 [31360/50000]\tLoss: 4.7733\tLR: 7.962404\n",
      "Training Epoch: 80 [31488/50000]\tLoss: 4.7595\tLR: 7.962660\n",
      "Training Epoch: 80 [31616/50000]\tLoss: 4.6869\tLR: 7.962916\n",
      "Training Epoch: 80 [31744/50000]\tLoss: 4.7476\tLR: 7.963171\n",
      "Training Epoch: 80 [31872/50000]\tLoss: 4.7060\tLR: 7.963427\n",
      "Training Epoch: 80 [32000/50000]\tLoss: 4.7786\tLR: 7.963683\n",
      "Training Epoch: 80 [32128/50000]\tLoss: 4.7622\tLR: 7.963939\n",
      "Training Epoch: 80 [32256/50000]\tLoss: 4.7828\tLR: 7.964194\n",
      "Training Epoch: 80 [32384/50000]\tLoss: 4.6595\tLR: 7.964450\n",
      "Training Epoch: 80 [32512/50000]\tLoss: 4.7765\tLR: 7.964706\n",
      "Training Epoch: 80 [32640/50000]\tLoss: 4.7293\tLR: 7.964962\n",
      "Training Epoch: 80 [32768/50000]\tLoss: 4.7082\tLR: 7.965217\n",
      "Training Epoch: 80 [32896/50000]\tLoss: 4.7469\tLR: 7.965473\n",
      "Training Epoch: 80 [33024/50000]\tLoss: 4.7229\tLR: 7.965729\n",
      "Training Epoch: 80 [33152/50000]\tLoss: 4.7904\tLR: 7.965985\n",
      "Training Epoch: 80 [33280/50000]\tLoss: 4.7832\tLR: 7.966240\n",
      "Training Epoch: 80 [33408/50000]\tLoss: 4.6540\tLR: 7.966496\n",
      "Training Epoch: 80 [33536/50000]\tLoss: 4.7749\tLR: 7.966752\n",
      "Training Epoch: 80 [33664/50000]\tLoss: 4.7650\tLR: 7.967008\n",
      "Training Epoch: 80 [33792/50000]\tLoss: 4.8592\tLR: 7.967263\n",
      "Training Epoch: 80 [33920/50000]\tLoss: 4.7662\tLR: 7.967519\n",
      "Training Epoch: 80 [34048/50000]\tLoss: 4.7203\tLR: 7.967775\n",
      "Training Epoch: 80 [34176/50000]\tLoss: 4.7196\tLR: 7.968031\n",
      "Training Epoch: 80 [34304/50000]\tLoss: 4.7756\tLR: 7.968286\n",
      "Training Epoch: 80 [34432/50000]\tLoss: 4.7451\tLR: 7.968542\n",
      "Training Epoch: 80 [34560/50000]\tLoss: 4.6897\tLR: 7.968798\n",
      "Training Epoch: 80 [34688/50000]\tLoss: 4.7441\tLR: 7.969054\n",
      "Training Epoch: 80 [34816/50000]\tLoss: 4.7092\tLR: 7.969309\n",
      "Training Epoch: 80 [34944/50000]\tLoss: 4.6692\tLR: 7.969565\n",
      "Training Epoch: 80 [35072/50000]\tLoss: 4.7712\tLR: 7.969821\n",
      "Training Epoch: 80 [35200/50000]\tLoss: 4.7623\tLR: 7.970077\n",
      "Training Epoch: 80 [35328/50000]\tLoss: 4.9346\tLR: 7.970332\n",
      "Training Epoch: 80 [35456/50000]\tLoss: 4.7717\tLR: 7.970588\n",
      "Training Epoch: 80 [35584/50000]\tLoss: 4.7196\tLR: 7.970844\n",
      "Training Epoch: 80 [35712/50000]\tLoss: 4.7828\tLR: 7.971100\n",
      "Training Epoch: 80 [35840/50000]\tLoss: 4.7337\tLR: 7.971355\n",
      "Training Epoch: 80 [35968/50000]\tLoss: 4.7121\tLR: 7.971611\n",
      "Training Epoch: 80 [36096/50000]\tLoss: 4.7235\tLR: 7.971867\n",
      "Training Epoch: 80 [36224/50000]\tLoss: 4.6963\tLR: 7.972123\n",
      "Training Epoch: 80 [36352/50000]\tLoss: 4.7754\tLR: 7.972379\n",
      "Training Epoch: 80 [36480/50000]\tLoss: 4.8029\tLR: 7.972634\n",
      "Training Epoch: 80 [36608/50000]\tLoss: 4.7563\tLR: 7.972890\n",
      "Training Epoch: 80 [36736/50000]\tLoss: 4.7802\tLR: 7.973146\n",
      "Training Epoch: 80 [36864/50000]\tLoss: 4.7807\tLR: 7.973402\n",
      "Training Epoch: 80 [36992/50000]\tLoss: 4.7129\tLR: 7.973657\n",
      "Training Epoch: 80 [37120/50000]\tLoss: 4.6950\tLR: 7.973913\n",
      "Training Epoch: 80 [37248/50000]\tLoss: 4.9057\tLR: 7.974169\n",
      "Training Epoch: 80 [37376/50000]\tLoss: 4.8068\tLR: 7.974425\n",
      "Training Epoch: 80 [37504/50000]\tLoss: 4.6211\tLR: 7.974680\n",
      "Training Epoch: 80 [37632/50000]\tLoss: 4.6697\tLR: 7.974936\n",
      "Training Epoch: 80 [37760/50000]\tLoss: 4.7353\tLR: 7.975192\n",
      "Training Epoch: 80 [37888/50000]\tLoss: 4.8162\tLR: 7.975448\n",
      "Training Epoch: 80 [38016/50000]\tLoss: 4.7862\tLR: 7.975703\n",
      "Training Epoch: 80 [38144/50000]\tLoss: 4.7386\tLR: 7.975959\n",
      "Training Epoch: 80 [38272/50000]\tLoss: 4.8814\tLR: 7.976215\n",
      "Training Epoch: 80 [38400/50000]\tLoss: 4.8103\tLR: 7.976471\n",
      "Training Epoch: 80 [38528/50000]\tLoss: 4.7065\tLR: 7.976726\n",
      "Training Epoch: 80 [38656/50000]\tLoss: 4.6561\tLR: 7.976982\n",
      "Training Epoch: 80 [38784/50000]\tLoss: 4.7712\tLR: 7.977238\n",
      "Training Epoch: 80 [38912/50000]\tLoss: 4.7808\tLR: 7.977494\n",
      "Training Epoch: 80 [39040/50000]\tLoss: 4.7873\tLR: 7.977749\n",
      "Training Epoch: 80 [39168/50000]\tLoss: 4.7887\tLR: 7.978005\n",
      "Training Epoch: 80 [39296/50000]\tLoss: 4.7274\tLR: 7.978261\n",
      "Training Epoch: 80 [39424/50000]\tLoss: 4.7984\tLR: 7.978517\n",
      "Training Epoch: 80 [39552/50000]\tLoss: 4.8222\tLR: 7.978772\n",
      "Training Epoch: 80 [39680/50000]\tLoss: 4.6712\tLR: 7.979028\n",
      "Training Epoch: 80 [39808/50000]\tLoss: 4.6531\tLR: 7.979284\n",
      "Training Epoch: 80 [39936/50000]\tLoss: 4.8400\tLR: 7.979540\n",
      "Training Epoch: 80 [40064/50000]\tLoss: 4.7778\tLR: 7.979795\n",
      "Training Epoch: 80 [40192/50000]\tLoss: 4.8175\tLR: 7.980051\n",
      "Training Epoch: 80 [40320/50000]\tLoss: 4.7749\tLR: 7.980307\n",
      "Training Epoch: 80 [40448/50000]\tLoss: 4.7653\tLR: 7.980563\n",
      "Training Epoch: 80 [40576/50000]\tLoss: 4.7631\tLR: 7.980818\n",
      "Training Epoch: 80 [40704/50000]\tLoss: 4.7694\tLR: 7.981074\n",
      "Training Epoch: 80 [40832/50000]\tLoss: 4.7927\tLR: 7.981330\n",
      "Training Epoch: 80 [40960/50000]\tLoss: 4.7474\tLR: 7.981586\n",
      "Training Epoch: 80 [41088/50000]\tLoss: 4.8164\tLR: 7.981841\n",
      "Training Epoch: 80 [41216/50000]\tLoss: 4.7688\tLR: 7.982097\n",
      "Training Epoch: 80 [41344/50000]\tLoss: 4.6933\tLR: 7.982353\n",
      "Training Epoch: 80 [41472/50000]\tLoss: 4.7707\tLR: 7.982609\n",
      "Training Epoch: 80 [41600/50000]\tLoss: 4.7423\tLR: 7.982864\n",
      "Training Epoch: 80 [41728/50000]\tLoss: 4.8130\tLR: 7.983120\n",
      "Training Epoch: 80 [41856/50000]\tLoss: 4.8511\tLR: 7.983376\n",
      "Training Epoch: 80 [41984/50000]\tLoss: 4.7179\tLR: 7.983632\n",
      "Training Epoch: 80 [42112/50000]\tLoss: 4.6917\tLR: 7.983887\n",
      "Training Epoch: 80 [42240/50000]\tLoss: 4.8396\tLR: 7.984143\n",
      "Training Epoch: 80 [42368/50000]\tLoss: 4.8001\tLR: 7.984399\n",
      "Training Epoch: 80 [42496/50000]\tLoss: 4.6951\tLR: 7.984655\n",
      "Training Epoch: 80 [42624/50000]\tLoss: 4.7380\tLR: 7.984910\n",
      "Training Epoch: 80 [42752/50000]\tLoss: 4.7047\tLR: 7.985166\n",
      "Training Epoch: 80 [42880/50000]\tLoss: 4.7876\tLR: 7.985422\n",
      "Training Epoch: 80 [43008/50000]\tLoss: 4.7551\tLR: 7.985678\n",
      "Training Epoch: 80 [43136/50000]\tLoss: 4.8832\tLR: 7.985934\n",
      "Training Epoch: 80 [43264/50000]\tLoss: 4.7840\tLR: 7.986189\n",
      "Training Epoch: 80 [43392/50000]\tLoss: 4.7622\tLR: 7.986445\n",
      "Training Epoch: 80 [43520/50000]\tLoss: 4.7022\tLR: 7.986701\n",
      "Training Epoch: 80 [43648/50000]\tLoss: 4.7732\tLR: 7.986957\n",
      "Training Epoch: 80 [43776/50000]\tLoss: 4.7153\tLR: 7.987212\n",
      "Training Epoch: 80 [43904/50000]\tLoss: 4.8458\tLR: 7.987468\n",
      "Training Epoch: 80 [44032/50000]\tLoss: 4.7940\tLR: 7.987724\n",
      "Training Epoch: 80 [44160/50000]\tLoss: 4.8086\tLR: 7.987980\n",
      "Training Epoch: 80 [44288/50000]\tLoss: 4.6854\tLR: 7.988235\n",
      "Training Epoch: 80 [44416/50000]\tLoss: 4.7234\tLR: 7.988491\n",
      "Training Epoch: 80 [44544/50000]\tLoss: 4.7155\tLR: 7.988747\n",
      "Training Epoch: 80 [44672/50000]\tLoss: 4.7122\tLR: 7.989003\n",
      "Training Epoch: 80 [44800/50000]\tLoss: 4.7405\tLR: 7.989258\n",
      "Training Epoch: 80 [44928/50000]\tLoss: 4.7441\tLR: 7.989514\n",
      "Training Epoch: 80 [45056/50000]\tLoss: 4.7090\tLR: 7.989770\n",
      "Training Epoch: 80 [45184/50000]\tLoss: 4.6739\tLR: 7.990026\n",
      "Training Epoch: 80 [45312/50000]\tLoss: 4.8149\tLR: 7.990281\n",
      "Training Epoch: 80 [45440/50000]\tLoss: 4.7786\tLR: 7.990537\n",
      "Training Epoch: 80 [45568/50000]\tLoss: 4.7245\tLR: 7.990793\n",
      "Training Epoch: 80 [45696/50000]\tLoss: 4.6925\tLR: 7.991049\n",
      "Training Epoch: 80 [45824/50000]\tLoss: 4.7520\tLR: 7.991304\n",
      "Training Epoch: 80 [45952/50000]\tLoss: 4.7559\tLR: 7.991560\n",
      "Training Epoch: 80 [46080/50000]\tLoss: 4.7911\tLR: 7.991816\n",
      "Training Epoch: 80 [46208/50000]\tLoss: 4.7931\tLR: 7.992072\n",
      "Training Epoch: 80 [46336/50000]\tLoss: 4.7529\tLR: 7.992327\n",
      "Training Epoch: 80 [46464/50000]\tLoss: 4.7672\tLR: 7.992583\n",
      "Training Epoch: 80 [46592/50000]\tLoss: 4.7838\tLR: 7.992839\n",
      "Training Epoch: 80 [46720/50000]\tLoss: 4.8087\tLR: 7.993095\n",
      "Training Epoch: 80 [46848/50000]\tLoss: 4.7365\tLR: 7.993350\n",
      "Training Epoch: 80 [46976/50000]\tLoss: 4.7038\tLR: 7.993606\n",
      "Training Epoch: 80 [47104/50000]\tLoss: 4.7358\tLR: 7.993862\n",
      "Training Epoch: 80 [47232/50000]\tLoss: 4.7639\tLR: 7.994118\n",
      "Training Epoch: 80 [47360/50000]\tLoss: 4.6705\tLR: 7.994373\n",
      "Training Epoch: 80 [47488/50000]\tLoss: 4.7748\tLR: 7.994629\n",
      "Training Epoch: 80 [47616/50000]\tLoss: 4.8328\tLR: 7.994885\n",
      "Training Epoch: 80 [47744/50000]\tLoss: 4.7437\tLR: 7.995141\n",
      "Training Epoch: 80 [47872/50000]\tLoss: 4.7327\tLR: 7.995396\n",
      "Training Epoch: 80 [48000/50000]\tLoss: 4.7153\tLR: 7.995652\n",
      "Training Epoch: 80 [48128/50000]\tLoss: 4.7061\tLR: 7.995908\n",
      "Training Epoch: 80 [48256/50000]\tLoss: 4.7075\tLR: 7.996164\n",
      "Training Epoch: 80 [48384/50000]\tLoss: 4.6541\tLR: 7.996419\n",
      "Training Epoch: 80 [48512/50000]\tLoss: 4.7366\tLR: 7.996675\n",
      "Training Epoch: 80 [48640/50000]\tLoss: 4.8123\tLR: 7.996931\n",
      "Training Epoch: 80 [48768/50000]\tLoss: 4.7652\tLR: 7.997187\n",
      "Training Epoch: 80 [48896/50000]\tLoss: 4.7455\tLR: 7.997442\n",
      "Training Epoch: 80 [49024/50000]\tLoss: 4.7194\tLR: 7.997698\n",
      "Training Epoch: 80 [49152/50000]\tLoss: 4.7843\tLR: 7.997954\n",
      "Training Epoch: 80 [49280/50000]\tLoss: 4.7279\tLR: 7.998210\n",
      "Training Epoch: 80 [49408/50000]\tLoss: 4.7308\tLR: 7.998465\n",
      "Training Epoch: 80 [49536/50000]\tLoss: 4.8095\tLR: 7.998721\n",
      "Training Epoch: 80 [49664/50000]\tLoss: 4.8007\tLR: 7.998977\n",
      "Training Epoch: 80 [49792/50000]\tLoss: 4.7566\tLR: 7.999233\n",
      "Training Epoch: 80 [49920/50000]\tLoss: 4.7243\tLR: 7.999488\n",
      "Training Epoch: 80 [50000/50000]\tLoss: 4.6567\tLR: 7.999744\n",
      "epoch 80 training time consumed: 489.08s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  112154 GB |  112154 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  111810 GB |  111810 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     344 GB |     344 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  112154 GB |  112154 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  111810 GB |  111810 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     344 GB |     344 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  110578 GB |  110578 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  110234 GB |  110234 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     344 GB |     344 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   11892 K  |   11892 K  |\n",
      "|       from large pool |      24    |      65    |    5069 K  |    5069 K  |\n",
      "|       from small pool |     231    |     274    |    6822 K  |    6822 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   11892 K  |   11892 K  |\n",
      "|       from large pool |      24    |      65    |    5069 K  |    5069 K  |\n",
      "|       from small pool |     231    |     274    |    6822 K  |    6822 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      47    |    6893 K  |    6893 K  |\n",
      "|       from large pool |      10    |      23    |    2436 K  |    2436 K  |\n",
      "|       from small pool |      26    |      35    |    4456 K  |    4456 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 80, Average loss: 0.0375, Accuracy: 0.0100, Time consumed:31.29s\n",
      "\n",
      "saving weights file to checkpoint\\resnet18\\Monday_25_July_2022_16h_56m_47s\\resnet18-80-regular.pth\n",
      "Training Epoch: 81 [128/50000]\tLoss: 4.6668\tLR: 0.020000\n",
      "Training Epoch: 81 [256/50000]\tLoss: 4.7926\tLR: 8.000256\n",
      "Training Epoch: 81 [384/50000]\tLoss: 4.6831\tLR: 8.000512\n",
      "Training Epoch: 81 [512/50000]\tLoss: 4.7631\tLR: 8.000767\n",
      "Training Epoch: 81 [640/50000]\tLoss: 4.7373\tLR: 8.001023\n",
      "Training Epoch: 81 [768/50000]\tLoss: 4.7233\tLR: 8.001279\n",
      "Training Epoch: 81 [896/50000]\tLoss: 4.8594\tLR: 8.001535\n",
      "Training Epoch: 81 [1024/50000]\tLoss: 4.7735\tLR: 8.001790\n",
      "Training Epoch: 81 [1152/50000]\tLoss: 4.6867\tLR: 8.002046\n",
      "Training Epoch: 81 [1280/50000]\tLoss: 4.7707\tLR: 8.002302\n",
      "Training Epoch: 81 [1408/50000]\tLoss: 4.7675\tLR: 8.002558\n",
      "Training Epoch: 81 [1536/50000]\tLoss: 4.7192\tLR: 8.002813\n",
      "Training Epoch: 81 [1664/50000]\tLoss: 4.7181\tLR: 8.003069\n",
      "Training Epoch: 81 [1792/50000]\tLoss: 4.7371\tLR: 8.003325\n",
      "Training Epoch: 81 [1920/50000]\tLoss: 4.7951\tLR: 8.003581\n",
      "Training Epoch: 81 [2048/50000]\tLoss: 4.7114\tLR: 8.003836\n",
      "Training Epoch: 81 [2176/50000]\tLoss: 4.8041\tLR: 8.004092\n",
      "Training Epoch: 81 [2304/50000]\tLoss: 4.8649\tLR: 8.004348\n",
      "Training Epoch: 81 [2432/50000]\tLoss: 4.7287\tLR: 8.004604\n",
      "Training Epoch: 81 [2560/50000]\tLoss: 4.7517\tLR: 8.004859\n",
      "Training Epoch: 81 [2688/50000]\tLoss: 4.6562\tLR: 8.005115\n",
      "Training Epoch: 81 [2816/50000]\tLoss: 4.7971\tLR: 8.005371\n",
      "Training Epoch: 81 [2944/50000]\tLoss: 4.8299\tLR: 8.005627\n",
      "Training Epoch: 81 [3072/50000]\tLoss: 4.7300\tLR: 8.005882\n",
      "Training Epoch: 81 [3200/50000]\tLoss: 4.7298\tLR: 8.006138\n",
      "Training Epoch: 81 [3328/50000]\tLoss: 4.7902\tLR: 8.006394\n",
      "Training Epoch: 81 [3456/50000]\tLoss: 4.7421\tLR: 8.006650\n",
      "Training Epoch: 81 [3584/50000]\tLoss: 4.8713\tLR: 8.006905\n",
      "Training Epoch: 81 [3712/50000]\tLoss: 4.8017\tLR: 8.007161\n",
      "Training Epoch: 81 [3840/50000]\tLoss: 4.7193\tLR: 8.007417\n",
      "Training Epoch: 81 [3968/50000]\tLoss: 4.7706\tLR: 8.007673\n",
      "Training Epoch: 81 [4096/50000]\tLoss: 4.7364\tLR: 8.007928\n",
      "Training Epoch: 81 [4224/50000]\tLoss: 4.7835\tLR: 8.008184\n",
      "Training Epoch: 81 [4352/50000]\tLoss: 4.6976\tLR: 8.008440\n",
      "Training Epoch: 81 [4480/50000]\tLoss: 4.7172\tLR: 8.008696\n",
      "Training Epoch: 81 [4608/50000]\tLoss: 4.8144\tLR: 8.008951\n",
      "Training Epoch: 81 [4736/50000]\tLoss: 4.7673\tLR: 8.009207\n",
      "Training Epoch: 81 [4864/50000]\tLoss: 4.8287\tLR: 8.009463\n",
      "Training Epoch: 81 [4992/50000]\tLoss: 4.7677\tLR: 8.009719\n",
      "Training Epoch: 81 [5120/50000]\tLoss: 4.7108\tLR: 8.009974\n",
      "Training Epoch: 81 [5248/50000]\tLoss: 4.7493\tLR: 8.010230\n",
      "Training Epoch: 81 [5376/50000]\tLoss: 4.7960\tLR: 8.010486\n",
      "Training Epoch: 81 [5504/50000]\tLoss: 4.7643\tLR: 8.010742\n",
      "Training Epoch: 81 [5632/50000]\tLoss: 4.7446\tLR: 8.010997\n",
      "Training Epoch: 81 [5760/50000]\tLoss: 4.7735\tLR: 8.011253\n",
      "Training Epoch: 81 [5888/50000]\tLoss: 4.7164\tLR: 8.011509\n",
      "Training Epoch: 81 [6016/50000]\tLoss: 4.7946\tLR: 8.011765\n",
      "Training Epoch: 81 [6144/50000]\tLoss: 4.8441\tLR: 8.012020\n",
      "Training Epoch: 81 [6272/50000]\tLoss: 4.7763\tLR: 8.012276\n",
      "Training Epoch: 81 [6400/50000]\tLoss: 4.7457\tLR: 8.012532\n",
      "Training Epoch: 81 [6528/50000]\tLoss: 4.6949\tLR: 8.012788\n",
      "Training Epoch: 81 [6656/50000]\tLoss: 4.8142\tLR: 8.013043\n",
      "Training Epoch: 81 [6784/50000]\tLoss: 4.6988\tLR: 8.013299\n",
      "Training Epoch: 81 [6912/50000]\tLoss: 4.8207\tLR: 8.013555\n",
      "Training Epoch: 81 [7040/50000]\tLoss: 4.8667\tLR: 8.013811\n",
      "Training Epoch: 81 [7168/50000]\tLoss: 4.7208\tLR: 8.014066\n",
      "Training Epoch: 81 [7296/50000]\tLoss: 4.7629\tLR: 8.014322\n",
      "Training Epoch: 81 [7424/50000]\tLoss: 4.7925\tLR: 8.014578\n",
      "Training Epoch: 81 [7552/50000]\tLoss: 4.7197\tLR: 8.014834\n",
      "Training Epoch: 81 [7680/50000]\tLoss: 4.7093\tLR: 8.015090\n",
      "Training Epoch: 81 [7808/50000]\tLoss: 4.7093\tLR: 8.015345\n",
      "Training Epoch: 81 [7936/50000]\tLoss: 4.7620\tLR: 8.015601\n",
      "Training Epoch: 81 [8064/50000]\tLoss: 4.7634\tLR: 8.015857\n",
      "Training Epoch: 81 [8192/50000]\tLoss: 4.7156\tLR: 8.016113\n",
      "Training Epoch: 81 [8320/50000]\tLoss: 4.7291\tLR: 8.016368\n",
      "Training Epoch: 81 [8448/50000]\tLoss: 4.7692\tLR: 8.016624\n",
      "Training Epoch: 81 [8576/50000]\tLoss: 4.8082\tLR: 8.016880\n",
      "Training Epoch: 81 [8704/50000]\tLoss: 4.6688\tLR: 8.017136\n",
      "Training Epoch: 81 [8832/50000]\tLoss: 4.7184\tLR: 8.017391\n",
      "Training Epoch: 81 [8960/50000]\tLoss: 4.7601\tLR: 8.017647\n",
      "Training Epoch: 81 [9088/50000]\tLoss: 4.7387\tLR: 8.017903\n",
      "Training Epoch: 81 [9216/50000]\tLoss: 4.7235\tLR: 8.018159\n",
      "Training Epoch: 81 [9344/50000]\tLoss: 4.6683\tLR: 8.018414\n",
      "Training Epoch: 81 [9472/50000]\tLoss: 4.7584\tLR: 8.018670\n",
      "Training Epoch: 81 [9600/50000]\tLoss: 4.7939\tLR: 8.018926\n",
      "Training Epoch: 81 [9728/50000]\tLoss: 4.7433\tLR: 8.019182\n",
      "Training Epoch: 81 [9856/50000]\tLoss: 4.7961\tLR: 8.019437\n",
      "Training Epoch: 81 [9984/50000]\tLoss: 4.7548\tLR: 8.019693\n",
      "Training Epoch: 81 [10112/50000]\tLoss: 4.7683\tLR: 8.019949\n",
      "Training Epoch: 81 [10240/50000]\tLoss: 4.7202\tLR: 8.020205\n",
      "Training Epoch: 81 [10368/50000]\tLoss: 4.7086\tLR: 8.020460\n",
      "Training Epoch: 81 [10496/50000]\tLoss: 4.7901\tLR: 8.020716\n",
      "Training Epoch: 81 [10624/50000]\tLoss: 4.7539\tLR: 8.020972\n",
      "Training Epoch: 81 [10752/50000]\tLoss: 4.7542\tLR: 8.021228\n",
      "Training Epoch: 81 [10880/50000]\tLoss: 4.8143\tLR: 8.021483\n",
      "Training Epoch: 81 [11008/50000]\tLoss: 4.7057\tLR: 8.021739\n",
      "Training Epoch: 81 [11136/50000]\tLoss: 4.7441\tLR: 8.021995\n",
      "Training Epoch: 81 [11264/50000]\tLoss: 4.6508\tLR: 8.022251\n",
      "Training Epoch: 81 [11392/50000]\tLoss: 4.7523\tLR: 8.022506\n",
      "Training Epoch: 81 [11520/50000]\tLoss: 4.8159\tLR: 8.022762\n",
      "Training Epoch: 81 [11648/50000]\tLoss: 4.7805\tLR: 8.023018\n",
      "Training Epoch: 81 [11776/50000]\tLoss: 4.8106\tLR: 8.023274\n",
      "Training Epoch: 81 [11904/50000]\tLoss: 4.8552\tLR: 8.023529\n",
      "Training Epoch: 81 [12032/50000]\tLoss: 4.7598\tLR: 8.023785\n",
      "Training Epoch: 81 [12160/50000]\tLoss: 4.7048\tLR: 8.024041\n",
      "Training Epoch: 81 [12288/50000]\tLoss: 4.7658\tLR: 8.024297\n",
      "Training Epoch: 81 [12416/50000]\tLoss: 4.6542\tLR: 8.024552\n",
      "Training Epoch: 81 [12544/50000]\tLoss: 4.7511\tLR: 8.024808\n",
      "Training Epoch: 81 [12672/50000]\tLoss: 4.7728\tLR: 8.025064\n",
      "Training Epoch: 81 [12800/50000]\tLoss: 4.7918\tLR: 8.025320\n",
      "Training Epoch: 81 [12928/50000]\tLoss: 4.7273\tLR: 8.025575\n",
      "Training Epoch: 81 [13056/50000]\tLoss: 4.8302\tLR: 8.025831\n",
      "Training Epoch: 81 [13184/50000]\tLoss: 4.6994\tLR: 8.026087\n",
      "Training Epoch: 81 [13312/50000]\tLoss: 4.6116\tLR: 8.026343\n",
      "Training Epoch: 81 [13440/50000]\tLoss: 4.7205\tLR: 8.026598\n",
      "Training Epoch: 81 [13568/50000]\tLoss: 4.6745\tLR: 8.026854\n",
      "Training Epoch: 81 [13696/50000]\tLoss: 4.8020\tLR: 8.027110\n",
      "Training Epoch: 81 [13824/50000]\tLoss: 4.7614\tLR: 8.027366\n",
      "Training Epoch: 81 [13952/50000]\tLoss: 4.7814\tLR: 8.027621\n",
      "Training Epoch: 81 [14080/50000]\tLoss: 4.8680\tLR: 8.027877\n",
      "Training Epoch: 81 [14208/50000]\tLoss: 4.8199\tLR: 8.028133\n",
      "Training Epoch: 81 [14336/50000]\tLoss: 4.7941\tLR: 8.028389\n",
      "Training Epoch: 81 [14464/50000]\tLoss: 4.7009\tLR: 8.028645\n",
      "Training Epoch: 81 [14592/50000]\tLoss: 4.7004\tLR: 8.028900\n",
      "Training Epoch: 81 [14720/50000]\tLoss: 4.7130\tLR: 8.029156\n",
      "Training Epoch: 81 [14848/50000]\tLoss: 4.6693\tLR: 8.029412\n",
      "Training Epoch: 81 [14976/50000]\tLoss: 4.8057\tLR: 8.029668\n",
      "Training Epoch: 81 [15104/50000]\tLoss: 4.7646\tLR: 8.029923\n",
      "Training Epoch: 81 [15232/50000]\tLoss: 4.7913\tLR: 8.030179\n",
      "Training Epoch: 81 [15360/50000]\tLoss: 4.8187\tLR: 8.030435\n",
      "Training Epoch: 81 [15488/50000]\tLoss: 4.7135\tLR: 8.030691\n",
      "Training Epoch: 81 [15616/50000]\tLoss: 4.7203\tLR: 8.030946\n",
      "Training Epoch: 81 [15744/50000]\tLoss: 4.8388\tLR: 8.031202\n",
      "Training Epoch: 81 [15872/50000]\tLoss: 4.6759\tLR: 8.031458\n",
      "Training Epoch: 81 [16000/50000]\tLoss: 4.7094\tLR: 8.031714\n",
      "Training Epoch: 81 [16128/50000]\tLoss: 4.8091\tLR: 8.031969\n",
      "Training Epoch: 81 [16256/50000]\tLoss: 4.7755\tLR: 8.032225\n",
      "Training Epoch: 81 [16384/50000]\tLoss: 4.7463\tLR: 8.032481\n",
      "Training Epoch: 81 [16512/50000]\tLoss: 4.7395\tLR: 8.032737\n",
      "Training Epoch: 81 [16640/50000]\tLoss: 4.7281\tLR: 8.032992\n",
      "Training Epoch: 81 [16768/50000]\tLoss: 4.7182\tLR: 8.033248\n",
      "Training Epoch: 81 [16896/50000]\tLoss: 4.7468\tLR: 8.033504\n",
      "Training Epoch: 81 [17024/50000]\tLoss: 4.8340\tLR: 8.033760\n",
      "Training Epoch: 81 [17152/50000]\tLoss: 4.6651\tLR: 8.034015\n",
      "Training Epoch: 81 [17280/50000]\tLoss: 4.7068\tLR: 8.034271\n",
      "Training Epoch: 81 [17408/50000]\tLoss: 4.7659\tLR: 8.034527\n",
      "Training Epoch: 81 [17536/50000]\tLoss: 4.7478\tLR: 8.034783\n",
      "Training Epoch: 81 [17664/50000]\tLoss: 4.7539\tLR: 8.035038\n",
      "Training Epoch: 81 [17792/50000]\tLoss: 4.7077\tLR: 8.035294\n",
      "Training Epoch: 81 [17920/50000]\tLoss: 4.7573\tLR: 8.035550\n",
      "Training Epoch: 81 [18048/50000]\tLoss: 4.6750\tLR: 8.035806\n",
      "Training Epoch: 81 [18176/50000]\tLoss: 4.8990\tLR: 8.036061\n",
      "Training Epoch: 81 [18304/50000]\tLoss: 4.8191\tLR: 8.036317\n",
      "Training Epoch: 81 [18432/50000]\tLoss: 4.8249\tLR: 8.036573\n",
      "Training Epoch: 81 [18560/50000]\tLoss: 4.7989\tLR: 8.036829\n",
      "Training Epoch: 81 [18688/50000]\tLoss: 4.7116\tLR: 8.037084\n",
      "Training Epoch: 81 [18816/50000]\tLoss: 4.7847\tLR: 8.037340\n",
      "Training Epoch: 81 [18944/50000]\tLoss: 4.7584\tLR: 8.037596\n",
      "Training Epoch: 81 [19072/50000]\tLoss: 4.8199\tLR: 8.037852\n",
      "Training Epoch: 81 [19200/50000]\tLoss: 4.7033\tLR: 8.038107\n",
      "Training Epoch: 81 [19328/50000]\tLoss: 4.7541\tLR: 8.038363\n",
      "Training Epoch: 81 [19456/50000]\tLoss: 4.7626\tLR: 8.038619\n",
      "Training Epoch: 81 [19584/50000]\tLoss: 4.8415\tLR: 8.038875\n",
      "Training Epoch: 81 [19712/50000]\tLoss: 4.7991\tLR: 8.039130\n",
      "Training Epoch: 81 [19840/50000]\tLoss: 4.7207\tLR: 8.039386\n",
      "Training Epoch: 81 [19968/50000]\tLoss: 4.6999\tLR: 8.039642\n",
      "Training Epoch: 81 [20096/50000]\tLoss: 4.7216\tLR: 8.039898\n",
      "Training Epoch: 81 [20224/50000]\tLoss: 4.7643\tLR: 8.040153\n",
      "Training Epoch: 81 [20352/50000]\tLoss: 4.7713\tLR: 8.040409\n",
      "Training Epoch: 81 [20480/50000]\tLoss: 4.7454\tLR: 8.040665\n",
      "Training Epoch: 81 [20608/50000]\tLoss: 4.7043\tLR: 8.040921\n",
      "Training Epoch: 81 [20736/50000]\tLoss: 4.7756\tLR: 8.041176\n",
      "Training Epoch: 81 [20864/50000]\tLoss: 4.7346\tLR: 8.041432\n",
      "Training Epoch: 81 [20992/50000]\tLoss: 4.6729\tLR: 8.041688\n",
      "Training Epoch: 81 [21120/50000]\tLoss: 4.7326\tLR: 8.041944\n",
      "Training Epoch: 81 [21248/50000]\tLoss: 4.8937\tLR: 8.042199\n",
      "Training Epoch: 81 [21376/50000]\tLoss: 4.6705\tLR: 8.042455\n",
      "Training Epoch: 81 [21504/50000]\tLoss: 4.7234\tLR: 8.042711\n",
      "Training Epoch: 81 [21632/50000]\tLoss: 4.7104\tLR: 8.042967\n",
      "Training Epoch: 81 [21760/50000]\tLoss: 4.7299\tLR: 8.043223\n",
      "Training Epoch: 81 [21888/50000]\tLoss: 4.7670\tLR: 8.043478\n",
      "Training Epoch: 81 [22016/50000]\tLoss: 4.7485\tLR: 8.043734\n",
      "Training Epoch: 81 [22144/50000]\tLoss: 4.7182\tLR: 8.043990\n",
      "Training Epoch: 81 [22272/50000]\tLoss: 4.7846\tLR: 8.044246\n",
      "Training Epoch: 81 [22400/50000]\tLoss: 4.7724\tLR: 8.044501\n",
      "Training Epoch: 81 [22528/50000]\tLoss: 4.7415\tLR: 8.044757\n",
      "Training Epoch: 81 [22656/50000]\tLoss: 4.7103\tLR: 8.045013\n",
      "Training Epoch: 81 [22784/50000]\tLoss: 4.8598\tLR: 8.045269\n",
      "Training Epoch: 81 [22912/50000]\tLoss: 4.7181\tLR: 8.045524\n",
      "Training Epoch: 81 [23040/50000]\tLoss: 4.7569\tLR: 8.045780\n",
      "Training Epoch: 81 [23168/50000]\tLoss: 4.7868\tLR: 8.046036\n",
      "Training Epoch: 81 [23296/50000]\tLoss: 4.7595\tLR: 8.046292\n",
      "Training Epoch: 81 [23424/50000]\tLoss: 4.7448\tLR: 8.046547\n",
      "Training Epoch: 81 [23552/50000]\tLoss: 4.7772\tLR: 8.046803\n",
      "Training Epoch: 81 [23680/50000]\tLoss: 4.7389\tLR: 8.047059\n",
      "Training Epoch: 81 [23808/50000]\tLoss: 4.7061\tLR: 8.047315\n",
      "Training Epoch: 81 [23936/50000]\tLoss: 4.7363\tLR: 8.047570\n",
      "Training Epoch: 81 [24064/50000]\tLoss: 4.7216\tLR: 8.047826\n",
      "Training Epoch: 81 [24192/50000]\tLoss: 4.6802\tLR: 8.048082\n",
      "Training Epoch: 81 [24320/50000]\tLoss: 4.6537\tLR: 8.048338\n",
      "Training Epoch: 81 [24448/50000]\tLoss: 4.8396\tLR: 8.048593\n",
      "Training Epoch: 81 [24576/50000]\tLoss: 4.7739\tLR: 8.048849\n",
      "Training Epoch: 81 [24704/50000]\tLoss: 4.8224\tLR: 8.049105\n",
      "Training Epoch: 81 [24832/50000]\tLoss: 4.7270\tLR: 8.049361\n",
      "Training Epoch: 81 [24960/50000]\tLoss: 4.7574\tLR: 8.049616\n",
      "Training Epoch: 81 [25088/50000]\tLoss: 4.6598\tLR: 8.049872\n",
      "Training Epoch: 81 [25216/50000]\tLoss: 4.7599\tLR: 8.050128\n",
      "Training Epoch: 81 [25344/50000]\tLoss: 4.7647\tLR: 8.050384\n",
      "Training Epoch: 81 [25472/50000]\tLoss: 4.7701\tLR: 8.050639\n",
      "Training Epoch: 81 [25600/50000]\tLoss: 4.8555\tLR: 8.050895\n",
      "Training Epoch: 81 [25728/50000]\tLoss: 4.7492\tLR: 8.051151\n",
      "Training Epoch: 81 [25856/50000]\tLoss: 4.8076\tLR: 8.051407\n",
      "Training Epoch: 81 [25984/50000]\tLoss: 4.7529\tLR: 8.051662\n",
      "Training Epoch: 81 [26112/50000]\tLoss: 4.8805\tLR: 8.051918\n",
      "Training Epoch: 81 [26240/50000]\tLoss: 4.7940\tLR: 8.052174\n",
      "Training Epoch: 81 [26368/50000]\tLoss: 4.7137\tLR: 8.052430\n",
      "Training Epoch: 81 [26496/50000]\tLoss: 4.8157\tLR: 8.052685\n",
      "Training Epoch: 81 [26624/50000]\tLoss: 4.7680\tLR: 8.052941\n",
      "Training Epoch: 81 [26752/50000]\tLoss: 4.7545\tLR: 8.053197\n",
      "Training Epoch: 81 [26880/50000]\tLoss: 4.7610\tLR: 8.053453\n",
      "Training Epoch: 81 [27008/50000]\tLoss: 4.8038\tLR: 8.053708\n",
      "Training Epoch: 81 [27136/50000]\tLoss: 4.7980\tLR: 8.053964\n",
      "Training Epoch: 81 [27264/50000]\tLoss: 4.8760\tLR: 8.054220\n",
      "Training Epoch: 81 [27392/50000]\tLoss: 4.7974\tLR: 8.054476\n",
      "Training Epoch: 81 [27520/50000]\tLoss: 4.8339\tLR: 8.054731\n",
      "Training Epoch: 81 [27648/50000]\tLoss: 4.7661\tLR: 8.054987\n",
      "Training Epoch: 81 [27776/50000]\tLoss: 4.6800\tLR: 8.055243\n",
      "Training Epoch: 81 [27904/50000]\tLoss: 4.7773\tLR: 8.055499\n",
      "Training Epoch: 81 [28032/50000]\tLoss: 4.6673\tLR: 8.055754\n",
      "Training Epoch: 81 [28160/50000]\tLoss: 4.7194\tLR: 8.056010\n",
      "Training Epoch: 81 [28288/50000]\tLoss: 4.7571\tLR: 8.056266\n",
      "Training Epoch: 81 [28416/50000]\tLoss: 4.8140\tLR: 8.056522\n",
      "Training Epoch: 81 [28544/50000]\tLoss: 4.7830\tLR: 8.056777\n",
      "Training Epoch: 81 [28672/50000]\tLoss: 4.8000\tLR: 8.057033\n",
      "Training Epoch: 81 [28800/50000]\tLoss: 4.7214\tLR: 8.057289\n",
      "Training Epoch: 81 [28928/50000]\tLoss: 4.7899\tLR: 8.057545\n",
      "Training Epoch: 81 [29056/50000]\tLoss: 4.7446\tLR: 8.057801\n",
      "Training Epoch: 81 [29184/50000]\tLoss: 4.7608\tLR: 8.058056\n",
      "Training Epoch: 81 [29312/50000]\tLoss: 4.7569\tLR: 8.058312\n",
      "Training Epoch: 81 [29440/50000]\tLoss: 4.7231\tLR: 8.058568\n",
      "Training Epoch: 81 [29568/50000]\tLoss: 4.7881\tLR: 8.058824\n",
      "Training Epoch: 81 [29696/50000]\tLoss: 4.6845\tLR: 8.059079\n",
      "Training Epoch: 81 [29824/50000]\tLoss: 4.7009\tLR: 8.059335\n",
      "Training Epoch: 81 [29952/50000]\tLoss: 4.8423\tLR: 8.059591\n",
      "Training Epoch: 81 [30080/50000]\tLoss: 4.8181\tLR: 8.059847\n",
      "Training Epoch: 81 [30208/50000]\tLoss: 4.8046\tLR: 8.060102\n",
      "Training Epoch: 81 [30336/50000]\tLoss: 4.8835\tLR: 8.060358\n",
      "Training Epoch: 81 [30464/50000]\tLoss: 4.7962\tLR: 8.060614\n",
      "Training Epoch: 81 [30592/50000]\tLoss: 4.7207\tLR: 8.060870\n",
      "Training Epoch: 81 [30720/50000]\tLoss: 4.7726\tLR: 8.061125\n",
      "Training Epoch: 81 [30848/50000]\tLoss: 4.7599\tLR: 8.061381\n",
      "Training Epoch: 81 [30976/50000]\tLoss: 4.7843\tLR: 8.061637\n",
      "Training Epoch: 81 [31104/50000]\tLoss: 4.7565\tLR: 8.061893\n",
      "Training Epoch: 81 [31232/50000]\tLoss: 4.7613\tLR: 8.062148\n",
      "Training Epoch: 81 [31360/50000]\tLoss: 4.7934\tLR: 8.062404\n",
      "Training Epoch: 81 [31488/50000]\tLoss: 4.7331\tLR: 8.062660\n",
      "Training Epoch: 81 [31616/50000]\tLoss: 4.8233\tLR: 8.062916\n",
      "Training Epoch: 81 [31744/50000]\tLoss: 4.7024\tLR: 8.063171\n",
      "Training Epoch: 81 [31872/50000]\tLoss: 4.7916\tLR: 8.063427\n",
      "Training Epoch: 81 [32000/50000]\tLoss: 4.6963\tLR: 8.063683\n",
      "Training Epoch: 81 [32128/50000]\tLoss: 4.6860\tLR: 8.063939\n",
      "Training Epoch: 81 [32256/50000]\tLoss: 4.7899\tLR: 8.064194\n",
      "Training Epoch: 81 [32384/50000]\tLoss: 4.6738\tLR: 8.064450\n",
      "Training Epoch: 81 [32512/50000]\tLoss: 4.8290\tLR: 8.064706\n",
      "Training Epoch: 81 [32640/50000]\tLoss: 4.7674\tLR: 8.064962\n",
      "Training Epoch: 81 [32768/50000]\tLoss: 4.7652\tLR: 8.065217\n",
      "Training Epoch: 81 [32896/50000]\tLoss: 4.7066\tLR: 8.065473\n",
      "Training Epoch: 81 [33024/50000]\tLoss: 4.6884\tLR: 8.065729\n",
      "Training Epoch: 81 [33152/50000]\tLoss: 4.8053\tLR: 8.065985\n",
      "Training Epoch: 81 [33280/50000]\tLoss: 4.7113\tLR: 8.066240\n",
      "Training Epoch: 81 [33408/50000]\tLoss: 4.7294\tLR: 8.066496\n",
      "Training Epoch: 81 [33536/50000]\tLoss: 4.7353\tLR: 8.066752\n",
      "Training Epoch: 81 [33664/50000]\tLoss: 4.8054\tLR: 8.067008\n",
      "Training Epoch: 81 [33792/50000]\tLoss: 4.9558\tLR: 8.067263\n",
      "Training Epoch: 81 [33920/50000]\tLoss: 4.7695\tLR: 8.067519\n",
      "Training Epoch: 81 [34048/50000]\tLoss: 4.6953\tLR: 8.067775\n",
      "Training Epoch: 81 [34176/50000]\tLoss: 4.7254\tLR: 8.068031\n",
      "Training Epoch: 81 [34304/50000]\tLoss: 4.7839\tLR: 8.068286\n",
      "Training Epoch: 81 [34432/50000]\tLoss: 4.7638\tLR: 8.068542\n",
      "Training Epoch: 81 [34560/50000]\tLoss: 4.6941\tLR: 8.068798\n",
      "Training Epoch: 81 [34688/50000]\tLoss: 4.6971\tLR: 8.069054\n",
      "Training Epoch: 81 [34816/50000]\tLoss: 4.7379\tLR: 8.069309\n",
      "Training Epoch: 81 [34944/50000]\tLoss: 4.7669\tLR: 8.069565\n",
      "Training Epoch: 81 [35072/50000]\tLoss: 4.8181\tLR: 8.069821\n",
      "Training Epoch: 81 [35200/50000]\tLoss: 4.7825\tLR: 8.070077\n",
      "Training Epoch: 81 [35328/50000]\tLoss: 4.8736\tLR: 8.070332\n",
      "Training Epoch: 81 [35456/50000]\tLoss: 4.6963\tLR: 8.070588\n",
      "Training Epoch: 81 [35584/50000]\tLoss: 4.6550\tLR: 8.070844\n",
      "Training Epoch: 81 [35712/50000]\tLoss: 4.7164\tLR: 8.071100\n",
      "Training Epoch: 81 [35840/50000]\tLoss: 4.7786\tLR: 8.071355\n",
      "Training Epoch: 81 [35968/50000]\tLoss: 4.7688\tLR: 8.071611\n",
      "Training Epoch: 81 [36096/50000]\tLoss: 4.7667\tLR: 8.071867\n",
      "Training Epoch: 81 [36224/50000]\tLoss: 4.7712\tLR: 8.072123\n",
      "Training Epoch: 81 [36352/50000]\tLoss: 4.7739\tLR: 8.072379\n",
      "Training Epoch: 81 [36480/50000]\tLoss: 4.7528\tLR: 8.072634\n",
      "Training Epoch: 81 [36608/50000]\tLoss: 4.6664\tLR: 8.072890\n",
      "Training Epoch: 81 [36736/50000]\tLoss: 4.8039\tLR: 8.073146\n",
      "Training Epoch: 81 [36864/50000]\tLoss: 4.7622\tLR: 8.073402\n",
      "Training Epoch: 81 [36992/50000]\tLoss: 4.7832\tLR: 8.073657\n",
      "Training Epoch: 81 [37120/50000]\tLoss: 4.7456\tLR: 8.073913\n",
      "Training Epoch: 81 [37248/50000]\tLoss: 4.7835\tLR: 8.074169\n",
      "Training Epoch: 81 [37376/50000]\tLoss: 4.7770\tLR: 8.074425\n",
      "Training Epoch: 81 [37504/50000]\tLoss: 4.8078\tLR: 8.074680\n",
      "Training Epoch: 81 [37632/50000]\tLoss: 4.7334\tLR: 8.074936\n",
      "Training Epoch: 81 [37760/50000]\tLoss: 4.7851\tLR: 8.075192\n",
      "Training Epoch: 81 [37888/50000]\tLoss: 4.7159\tLR: 8.075448\n",
      "Training Epoch: 81 [38016/50000]\tLoss: 4.8061\tLR: 8.075703\n",
      "Training Epoch: 81 [38144/50000]\tLoss: 4.7265\tLR: 8.075959\n",
      "Training Epoch: 81 [38272/50000]\tLoss: 4.8256\tLR: 8.076215\n",
      "Training Epoch: 81 [38400/50000]\tLoss: 4.7186\tLR: 8.076471\n",
      "Training Epoch: 81 [38528/50000]\tLoss: 4.7653\tLR: 8.076726\n",
      "Training Epoch: 81 [38656/50000]\tLoss: 4.7303\tLR: 8.076982\n",
      "Training Epoch: 81 [38784/50000]\tLoss: 4.7292\tLR: 8.077238\n",
      "Training Epoch: 81 [38912/50000]\tLoss: 4.7004\tLR: 8.077494\n",
      "Training Epoch: 81 [39040/50000]\tLoss: 4.8209\tLR: 8.077749\n",
      "Training Epoch: 81 [39168/50000]\tLoss: 4.8981\tLR: 8.078005\n",
      "Training Epoch: 81 [39296/50000]\tLoss: 4.7652\tLR: 8.078261\n",
      "Training Epoch: 81 [39424/50000]\tLoss: 4.7354\tLR: 8.078517\n",
      "Training Epoch: 81 [39552/50000]\tLoss: 4.7307\tLR: 8.078772\n",
      "Training Epoch: 81 [39680/50000]\tLoss: 4.7240\tLR: 8.079028\n",
      "Training Epoch: 81 [39808/50000]\tLoss: 4.7559\tLR: 8.079284\n",
      "Training Epoch: 81 [39936/50000]\tLoss: 4.6486\tLR: 8.079540\n",
      "Training Epoch: 81 [40064/50000]\tLoss: 4.8084\tLR: 8.079795\n",
      "Training Epoch: 81 [40192/50000]\tLoss: 4.6956\tLR: 8.080051\n",
      "Training Epoch: 81 [40320/50000]\tLoss: 4.7131\tLR: 8.080307\n",
      "Training Epoch: 81 [40448/50000]\tLoss: 4.8284\tLR: 8.080563\n",
      "Training Epoch: 81 [40576/50000]\tLoss: 4.7532\tLR: 8.080818\n",
      "Training Epoch: 81 [40704/50000]\tLoss: 4.6849\tLR: 8.081074\n",
      "Training Epoch: 81 [40832/50000]\tLoss: 4.6751\tLR: 8.081330\n",
      "Training Epoch: 81 [40960/50000]\tLoss: 4.8216\tLR: 8.081586\n",
      "Training Epoch: 81 [41088/50000]\tLoss: 4.8299\tLR: 8.081841\n",
      "Training Epoch: 81 [41216/50000]\tLoss: 4.7870\tLR: 8.082097\n",
      "Training Epoch: 81 [41344/50000]\tLoss: 4.8137\tLR: 8.082353\n",
      "Training Epoch: 81 [41472/50000]\tLoss: 4.7563\tLR: 8.082609\n",
      "Training Epoch: 81 [41600/50000]\tLoss: 4.6828\tLR: 8.082864\n",
      "Training Epoch: 81 [41728/50000]\tLoss: 4.7655\tLR: 8.083120\n",
      "Training Epoch: 81 [41856/50000]\tLoss: 4.7159\tLR: 8.083376\n",
      "Training Epoch: 81 [41984/50000]\tLoss: 4.8077\tLR: 8.083632\n",
      "Training Epoch: 81 [42112/50000]\tLoss: 4.7713\tLR: 8.083887\n",
      "Training Epoch: 81 [42240/50000]\tLoss: 4.8056\tLR: 8.084143\n",
      "Training Epoch: 81 [42368/50000]\tLoss: 4.6978\tLR: 8.084399\n",
      "Training Epoch: 81 [42496/50000]\tLoss: 4.8408\tLR: 8.084655\n",
      "Training Epoch: 81 [42624/50000]\tLoss: 4.7222\tLR: 8.084910\n",
      "Training Epoch: 81 [42752/50000]\tLoss: 4.8350\tLR: 8.085166\n",
      "Training Epoch: 81 [42880/50000]\tLoss: 4.6693\tLR: 8.085422\n",
      "Training Epoch: 81 [43008/50000]\tLoss: 4.7609\tLR: 8.085678\n",
      "Training Epoch: 81 [43136/50000]\tLoss: 4.7381\tLR: 8.085934\n",
      "Training Epoch: 81 [43264/50000]\tLoss: 4.7455\tLR: 8.086189\n",
      "Training Epoch: 81 [43392/50000]\tLoss: 4.7951\tLR: 8.086445\n",
      "Training Epoch: 81 [43520/50000]\tLoss: 4.7755\tLR: 8.086701\n",
      "Training Epoch: 81 [43648/50000]\tLoss: 4.7385\tLR: 8.086957\n",
      "Training Epoch: 81 [43776/50000]\tLoss: 4.7359\tLR: 8.087212\n",
      "Training Epoch: 81 [43904/50000]\tLoss: 4.7993\tLR: 8.087468\n",
      "Training Epoch: 81 [44032/50000]\tLoss: 4.7699\tLR: 8.087724\n",
      "Training Epoch: 81 [44160/50000]\tLoss: 4.8493\tLR: 8.087980\n",
      "Training Epoch: 81 [44288/50000]\tLoss: 4.6873\tLR: 8.088235\n",
      "Training Epoch: 81 [44416/50000]\tLoss: 4.7053\tLR: 8.088491\n",
      "Training Epoch: 81 [44544/50000]\tLoss: 4.7770\tLR: 8.088747\n",
      "Training Epoch: 81 [44672/50000]\tLoss: 4.7558\tLR: 8.089003\n",
      "Training Epoch: 81 [44800/50000]\tLoss: 4.7467\tLR: 8.089258\n",
      "Training Epoch: 81 [44928/50000]\tLoss: 4.6561\tLR: 8.089514\n",
      "Training Epoch: 81 [45056/50000]\tLoss: 4.7436\tLR: 8.089770\n",
      "Training Epoch: 81 [45184/50000]\tLoss: 4.7999\tLR: 8.090026\n",
      "Training Epoch: 81 [45312/50000]\tLoss: 4.7401\tLR: 8.090281\n",
      "Training Epoch: 81 [45440/50000]\tLoss: 4.7077\tLR: 8.090537\n",
      "Training Epoch: 81 [45568/50000]\tLoss: 4.7889\tLR: 8.090793\n",
      "Training Epoch: 81 [45696/50000]\tLoss: 4.7311\tLR: 8.091049\n",
      "Training Epoch: 81 [45824/50000]\tLoss: 4.7676\tLR: 8.091304\n",
      "Training Epoch: 81 [45952/50000]\tLoss: 4.7622\tLR: 8.091560\n",
      "Training Epoch: 81 [46080/50000]\tLoss: 4.6386\tLR: 8.091816\n",
      "Training Epoch: 81 [46208/50000]\tLoss: 4.7567\tLR: 8.092072\n",
      "Training Epoch: 81 [46336/50000]\tLoss: 4.7005\tLR: 8.092327\n",
      "Training Epoch: 81 [46464/50000]\tLoss: 4.7576\tLR: 8.092583\n",
      "Training Epoch: 81 [46592/50000]\tLoss: 4.7256\tLR: 8.092839\n",
      "Training Epoch: 81 [46720/50000]\tLoss: 4.7886\tLR: 8.093095\n",
      "Training Epoch: 81 [46848/50000]\tLoss: 4.7510\tLR: 8.093350\n",
      "Training Epoch: 81 [46976/50000]\tLoss: 4.7054\tLR: 8.093606\n",
      "Training Epoch: 81 [47104/50000]\tLoss: 4.8236\tLR: 8.093862\n",
      "Training Epoch: 81 [47232/50000]\tLoss: 4.7580\tLR: 8.094118\n",
      "Training Epoch: 81 [47360/50000]\tLoss: 4.8951\tLR: 8.094373\n",
      "Training Epoch: 81 [47488/50000]\tLoss: 4.7369\tLR: 8.094629\n",
      "Training Epoch: 81 [47616/50000]\tLoss: 4.6603\tLR: 8.094885\n",
      "Training Epoch: 81 [47744/50000]\tLoss: 4.7301\tLR: 8.095141\n",
      "Training Epoch: 81 [47872/50000]\tLoss: 4.6968\tLR: 8.095396\n",
      "Training Epoch: 81 [48000/50000]\tLoss: 4.7432\tLR: 8.095652\n",
      "Training Epoch: 81 [48128/50000]\tLoss: 4.7115\tLR: 8.095908\n",
      "Training Epoch: 81 [48256/50000]\tLoss: 4.7075\tLR: 8.096164\n",
      "Training Epoch: 81 [48384/50000]\tLoss: 4.7832\tLR: 8.096419\n",
      "Training Epoch: 81 [48512/50000]\tLoss: 4.8138\tLR: 8.096675\n",
      "Training Epoch: 81 [48640/50000]\tLoss: 4.7704\tLR: 8.096931\n",
      "Training Epoch: 81 [48768/50000]\tLoss: 4.7577\tLR: 8.097187\n",
      "Training Epoch: 81 [48896/50000]\tLoss: 4.7189\tLR: 8.097442\n",
      "Training Epoch: 81 [49024/50000]\tLoss: 4.6954\tLR: 8.097698\n",
      "Training Epoch: 81 [49152/50000]\tLoss: 4.7367\tLR: 8.097954\n",
      "Training Epoch: 81 [49280/50000]\tLoss: 4.6642\tLR: 8.098210\n",
      "Training Epoch: 81 [49408/50000]\tLoss: 4.7269\tLR: 8.098465\n",
      "Training Epoch: 81 [49536/50000]\tLoss: 4.7529\tLR: 8.098721\n",
      "Training Epoch: 81 [49664/50000]\tLoss: 4.7130\tLR: 8.098977\n",
      "Training Epoch: 81 [49792/50000]\tLoss: 4.7933\tLR: 8.099233\n",
      "Training Epoch: 81 [49920/50000]\tLoss: 4.8257\tLR: 8.099488\n",
      "Training Epoch: 81 [50000/50000]\tLoss: 4.7262\tLR: 8.099744\n",
      "epoch 81 training time consumed: 489.51s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  113556 GB |  113556 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  113208 GB |  113208 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     348 GB |     348 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  113556 GB |  113556 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  113208 GB |  113208 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     348 GB |     348 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  111960 GB |  111960 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  111612 GB |  111612 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     348 GB |     348 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   12040 K  |   12040 K  |\n",
      "|       from large pool |      24    |      65    |    5132 K  |    5132 K  |\n",
      "|       from small pool |     231    |     274    |    6908 K  |    6907 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   12040 K  |   12040 K  |\n",
      "|       from large pool |      24    |      65    |    5132 K  |    5132 K  |\n",
      "|       from small pool |     231    |     274    |    6908 K  |    6907 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      47    |    6979 K  |    6979 K  |\n",
      "|       from large pool |      10    |      23    |    2467 K  |    2467 K  |\n",
      "|       from small pool |      25    |      35    |    4512 K  |    4511 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 81, Average loss: 0.0372, Accuracy: 0.0100, Time consumed:31.19s\n",
      "\n",
      "Training Epoch: 82 [128/50000]\tLoss: 4.7519\tLR: 0.020000\n",
      "Training Epoch: 82 [256/50000]\tLoss: 4.6418\tLR: 8.100256\n",
      "Training Epoch: 82 [384/50000]\tLoss: 4.7292\tLR: 8.100512\n",
      "Training Epoch: 82 [512/50000]\tLoss: 4.6549\tLR: 8.100767\n",
      "Training Epoch: 82 [640/50000]\tLoss: 4.7117\tLR: 8.101023\n",
      "Training Epoch: 82 [768/50000]\tLoss: 4.7588\tLR: 8.101279\n",
      "Training Epoch: 82 [896/50000]\tLoss: 4.7761\tLR: 8.101535\n",
      "Training Epoch: 82 [1024/50000]\tLoss: 4.7198\tLR: 8.101790\n",
      "Training Epoch: 82 [1152/50000]\tLoss: 4.7223\tLR: 8.102046\n",
      "Training Epoch: 82 [1280/50000]\tLoss: 4.7209\tLR: 8.102302\n",
      "Training Epoch: 82 [1408/50000]\tLoss: 4.7814\tLR: 8.102558\n",
      "Training Epoch: 82 [1536/50000]\tLoss: 4.7301\tLR: 8.102813\n",
      "Training Epoch: 82 [1664/50000]\tLoss: 4.7623\tLR: 8.103069\n",
      "Training Epoch: 82 [1792/50000]\tLoss: 4.6954\tLR: 8.103325\n",
      "Training Epoch: 82 [1920/50000]\tLoss: 4.7503\tLR: 8.103581\n",
      "Training Epoch: 82 [2048/50000]\tLoss: 4.7411\tLR: 8.103836\n",
      "Training Epoch: 82 [2176/50000]\tLoss: 4.6383\tLR: 8.104092\n",
      "Training Epoch: 82 [2304/50000]\tLoss: 4.7602\tLR: 8.104348\n",
      "Training Epoch: 82 [2432/50000]\tLoss: 4.7235\tLR: 8.104604\n",
      "Training Epoch: 82 [2560/50000]\tLoss: 4.8464\tLR: 8.104859\n",
      "Training Epoch: 82 [2688/50000]\tLoss: 4.8366\tLR: 8.105115\n",
      "Training Epoch: 82 [2816/50000]\tLoss: 4.7900\tLR: 8.105371\n",
      "Training Epoch: 82 [2944/50000]\tLoss: 4.7730\tLR: 8.105627\n",
      "Training Epoch: 82 [3072/50000]\tLoss: 4.7063\tLR: 8.105882\n",
      "Training Epoch: 82 [3200/50000]\tLoss: 4.7939\tLR: 8.106138\n",
      "Training Epoch: 82 [3328/50000]\tLoss: 4.6147\tLR: 8.106394\n",
      "Training Epoch: 82 [3456/50000]\tLoss: 4.7558\tLR: 8.106650\n",
      "Training Epoch: 82 [3584/50000]\tLoss: 4.7350\tLR: 8.106905\n",
      "Training Epoch: 82 [3712/50000]\tLoss: 4.7160\tLR: 8.107161\n",
      "Training Epoch: 82 [3840/50000]\tLoss: 4.8334\tLR: 8.107417\n",
      "Training Epoch: 82 [3968/50000]\tLoss: 4.7470\tLR: 8.107673\n",
      "Training Epoch: 82 [4096/50000]\tLoss: 4.7721\tLR: 8.107928\n",
      "Training Epoch: 82 [4224/50000]\tLoss: 4.7814\tLR: 8.108184\n",
      "Training Epoch: 82 [4352/50000]\tLoss: 4.7887\tLR: 8.108440\n",
      "Training Epoch: 82 [4480/50000]\tLoss: 4.8228\tLR: 8.108696\n",
      "Training Epoch: 82 [4608/50000]\tLoss: 4.7574\tLR: 8.108951\n",
      "Training Epoch: 82 [4736/50000]\tLoss: 4.7388\tLR: 8.109207\n",
      "Training Epoch: 82 [4864/50000]\tLoss: 4.7023\tLR: 8.109463\n",
      "Training Epoch: 82 [4992/50000]\tLoss: 4.6966\tLR: 8.109719\n",
      "Training Epoch: 82 [5120/50000]\tLoss: 4.7264\tLR: 8.109974\n",
      "Training Epoch: 82 [5248/50000]\tLoss: 4.8266\tLR: 8.110230\n",
      "Training Epoch: 82 [5376/50000]\tLoss: 4.6656\tLR: 8.110486\n",
      "Training Epoch: 82 [5504/50000]\tLoss: 4.7730\tLR: 8.110742\n",
      "Training Epoch: 82 [5632/50000]\tLoss: 4.8346\tLR: 8.110997\n",
      "Training Epoch: 82 [5760/50000]\tLoss: 4.8506\tLR: 8.111253\n",
      "Training Epoch: 82 [5888/50000]\tLoss: 4.7385\tLR: 8.111509\n",
      "Training Epoch: 82 [6016/50000]\tLoss: 4.8424\tLR: 8.111765\n",
      "Training Epoch: 82 [6144/50000]\tLoss: 4.7625\tLR: 8.112020\n",
      "Training Epoch: 82 [6272/50000]\tLoss: 4.7924\tLR: 8.112276\n",
      "Training Epoch: 82 [6400/50000]\tLoss: 4.7960\tLR: 8.112532\n",
      "Training Epoch: 82 [6528/50000]\tLoss: 4.7213\tLR: 8.112788\n",
      "Training Epoch: 82 [6656/50000]\tLoss: 4.6751\tLR: 8.113043\n",
      "Training Epoch: 82 [6784/50000]\tLoss: 4.6504\tLR: 8.113299\n",
      "Training Epoch: 82 [6912/50000]\tLoss: 4.7794\tLR: 8.113555\n",
      "Training Epoch: 82 [7040/50000]\tLoss: 4.7774\tLR: 8.113811\n",
      "Training Epoch: 82 [7168/50000]\tLoss: 4.8210\tLR: 8.114066\n",
      "Training Epoch: 82 [7296/50000]\tLoss: 4.8236\tLR: 8.114322\n",
      "Training Epoch: 82 [7424/50000]\tLoss: 4.8041\tLR: 8.114578\n",
      "Training Epoch: 82 [7552/50000]\tLoss: 4.7588\tLR: 8.114834\n",
      "Training Epoch: 82 [7680/50000]\tLoss: 4.7641\tLR: 8.115090\n",
      "Training Epoch: 82 [7808/50000]\tLoss: 4.8045\tLR: 8.115345\n",
      "Training Epoch: 82 [7936/50000]\tLoss: 4.7059\tLR: 8.115601\n",
      "Training Epoch: 82 [8064/50000]\tLoss: 4.7602\tLR: 8.115857\n",
      "Training Epoch: 82 [8192/50000]\tLoss: 4.7580\tLR: 8.116113\n",
      "Training Epoch: 82 [8320/50000]\tLoss: 4.8792\tLR: 8.116368\n",
      "Training Epoch: 82 [8448/50000]\tLoss: 4.7979\tLR: 8.116624\n",
      "Training Epoch: 82 [8576/50000]\tLoss: 4.7190\tLR: 8.116880\n",
      "Training Epoch: 82 [8704/50000]\tLoss: 4.7243\tLR: 8.117136\n",
      "Training Epoch: 82 [8832/50000]\tLoss: 4.8307\tLR: 8.117391\n",
      "Training Epoch: 82 [8960/50000]\tLoss: 4.7314\tLR: 8.117647\n",
      "Training Epoch: 82 [9088/50000]\tLoss: 4.8904\tLR: 8.117903\n",
      "Training Epoch: 82 [9216/50000]\tLoss: 4.8251\tLR: 8.118159\n",
      "Training Epoch: 82 [9344/50000]\tLoss: 4.7935\tLR: 8.118414\n",
      "Training Epoch: 82 [9472/50000]\tLoss: 4.7378\tLR: 8.118670\n",
      "Training Epoch: 82 [9600/50000]\tLoss: 4.6804\tLR: 8.118926\n",
      "Training Epoch: 82 [9728/50000]\tLoss: 4.8733\tLR: 8.119182\n",
      "Training Epoch: 82 [9856/50000]\tLoss: 4.7711\tLR: 8.119437\n",
      "Training Epoch: 82 [9984/50000]\tLoss: 4.7553\tLR: 8.119693\n",
      "Training Epoch: 82 [10112/50000]\tLoss: 4.7788\tLR: 8.119949\n",
      "Training Epoch: 82 [10240/50000]\tLoss: 4.7247\tLR: 8.120205\n",
      "Training Epoch: 82 [10368/50000]\tLoss: 4.7651\tLR: 8.120460\n",
      "Training Epoch: 82 [10496/50000]\tLoss: 4.8002\tLR: 8.120716\n",
      "Training Epoch: 82 [10624/50000]\tLoss: 4.7778\tLR: 8.120972\n",
      "Training Epoch: 82 [10752/50000]\tLoss: 4.7888\tLR: 8.121228\n",
      "Training Epoch: 82 [10880/50000]\tLoss: 4.8215\tLR: 8.121483\n",
      "Training Epoch: 82 [11008/50000]\tLoss: 4.7626\tLR: 8.121739\n",
      "Training Epoch: 82 [11136/50000]\tLoss: 4.6966\tLR: 8.121995\n",
      "Training Epoch: 82 [11264/50000]\tLoss: 4.6859\tLR: 8.122251\n",
      "Training Epoch: 82 [11392/50000]\tLoss: 4.7626\tLR: 8.122506\n",
      "Training Epoch: 82 [11520/50000]\tLoss: 4.8164\tLR: 8.122762\n",
      "Training Epoch: 82 [11648/50000]\tLoss: 4.8075\tLR: 8.123018\n",
      "Training Epoch: 82 [11776/50000]\tLoss: 4.8267\tLR: 8.123274\n",
      "Training Epoch: 82 [11904/50000]\tLoss: 4.7859\tLR: 8.123529\n",
      "Training Epoch: 82 [12032/50000]\tLoss: 4.7955\tLR: 8.123785\n",
      "Training Epoch: 82 [12160/50000]\tLoss: 4.8442\tLR: 8.124041\n",
      "Training Epoch: 82 [12288/50000]\tLoss: 4.7849\tLR: 8.124297\n",
      "Training Epoch: 82 [12416/50000]\tLoss: 4.6814\tLR: 8.124552\n",
      "Training Epoch: 82 [12544/50000]\tLoss: 4.8520\tLR: 8.124808\n",
      "Training Epoch: 82 [12672/50000]\tLoss: 4.7245\tLR: 8.125064\n",
      "Training Epoch: 82 [12800/50000]\tLoss: 4.8897\tLR: 8.125320\n",
      "Training Epoch: 82 [12928/50000]\tLoss: 4.9351\tLR: 8.125575\n",
      "Training Epoch: 82 [13056/50000]\tLoss: 4.8763\tLR: 8.125831\n",
      "Training Epoch: 82 [13184/50000]\tLoss: 4.7741\tLR: 8.126087\n",
      "Training Epoch: 82 [13312/50000]\tLoss: 4.7955\tLR: 8.126343\n",
      "Training Epoch: 82 [13440/50000]\tLoss: 4.8154\tLR: 8.126598\n",
      "Training Epoch: 82 [13568/50000]\tLoss: 4.7497\tLR: 8.126854\n",
      "Training Epoch: 82 [13696/50000]\tLoss: 4.7988\tLR: 8.127110\n",
      "Training Epoch: 82 [13824/50000]\tLoss: 4.7809\tLR: 8.127366\n",
      "Training Epoch: 82 [13952/50000]\tLoss: 4.8579\tLR: 8.127621\n",
      "Training Epoch: 82 [14080/50000]\tLoss: 4.7951\tLR: 8.127877\n",
      "Training Epoch: 82 [14208/50000]\tLoss: 4.8008\tLR: 8.128133\n",
      "Training Epoch: 82 [14336/50000]\tLoss: 4.7779\tLR: 8.128389\n",
      "Training Epoch: 82 [14464/50000]\tLoss: 4.6781\tLR: 8.128645\n",
      "Training Epoch: 82 [14592/50000]\tLoss: 4.7370\tLR: 8.128900\n",
      "Training Epoch: 82 [14720/50000]\tLoss: 4.7604\tLR: 8.129156\n",
      "Training Epoch: 82 [14848/50000]\tLoss: 4.7670\tLR: 8.129412\n",
      "Training Epoch: 82 [14976/50000]\tLoss: 4.8221\tLR: 8.129668\n",
      "Training Epoch: 82 [15104/50000]\tLoss: 4.7227\tLR: 8.129923\n",
      "Training Epoch: 82 [15232/50000]\tLoss: 4.7552\tLR: 8.130179\n",
      "Training Epoch: 82 [15360/50000]\tLoss: 4.7051\tLR: 8.130435\n",
      "Training Epoch: 82 [15488/50000]\tLoss: 4.8056\tLR: 8.130691\n",
      "Training Epoch: 82 [15616/50000]\tLoss: 4.7291\tLR: 8.130946\n",
      "Training Epoch: 82 [15744/50000]\tLoss: 4.7290\tLR: 8.131202\n",
      "Training Epoch: 82 [15872/50000]\tLoss: 4.7963\tLR: 8.131458\n",
      "Training Epoch: 82 [16000/50000]\tLoss: 4.7677\tLR: 8.131714\n",
      "Training Epoch: 82 [16128/50000]\tLoss: 4.7687\tLR: 8.131969\n",
      "Training Epoch: 82 [16256/50000]\tLoss: 4.8175\tLR: 8.132225\n",
      "Training Epoch: 82 [16384/50000]\tLoss: 4.6835\tLR: 8.132481\n",
      "Training Epoch: 82 [16512/50000]\tLoss: 4.7655\tLR: 8.132737\n",
      "Training Epoch: 82 [16640/50000]\tLoss: 4.9151\tLR: 8.132992\n",
      "Training Epoch: 82 [16768/50000]\tLoss: 4.8585\tLR: 8.133248\n",
      "Training Epoch: 82 [16896/50000]\tLoss: 4.7112\tLR: 8.133504\n",
      "Training Epoch: 82 [17024/50000]\tLoss: 4.7913\tLR: 8.133760\n",
      "Training Epoch: 82 [17152/50000]\tLoss: 4.7456\tLR: 8.134015\n",
      "Training Epoch: 82 [17280/50000]\tLoss: 4.7150\tLR: 8.134271\n",
      "Training Epoch: 82 [17408/50000]\tLoss: 4.7723\tLR: 8.134527\n",
      "Training Epoch: 82 [17536/50000]\tLoss: 4.7982\tLR: 8.134783\n",
      "Training Epoch: 82 [17664/50000]\tLoss: 4.7353\tLR: 8.135038\n",
      "Training Epoch: 82 [17792/50000]\tLoss: 4.8016\tLR: 8.135294\n",
      "Training Epoch: 82 [17920/50000]\tLoss: 4.8771\tLR: 8.135550\n",
      "Training Epoch: 82 [18048/50000]\tLoss: 4.7351\tLR: 8.135806\n",
      "Training Epoch: 82 [18176/50000]\tLoss: 4.6567\tLR: 8.136061\n",
      "Training Epoch: 82 [18304/50000]\tLoss: 4.7864\tLR: 8.136317\n",
      "Training Epoch: 82 [18432/50000]\tLoss: 4.7226\tLR: 8.136573\n",
      "Training Epoch: 82 [18560/50000]\tLoss: 4.8249\tLR: 8.136829\n",
      "Training Epoch: 82 [18688/50000]\tLoss: 4.7644\tLR: 8.137084\n",
      "Training Epoch: 82 [18816/50000]\tLoss: 4.8411\tLR: 8.137340\n",
      "Training Epoch: 82 [18944/50000]\tLoss: 4.7009\tLR: 8.137596\n",
      "Training Epoch: 82 [19072/50000]\tLoss: 4.6676\tLR: 8.137852\n",
      "Training Epoch: 82 [19200/50000]\tLoss: 4.7193\tLR: 8.138107\n",
      "Training Epoch: 82 [19328/50000]\tLoss: 4.7289\tLR: 8.138363\n",
      "Training Epoch: 82 [19456/50000]\tLoss: 4.8350\tLR: 8.138619\n",
      "Training Epoch: 82 [19584/50000]\tLoss: 4.7800\tLR: 8.138875\n",
      "Training Epoch: 82 [19712/50000]\tLoss: 4.7961\tLR: 8.139130\n",
      "Training Epoch: 82 [19840/50000]\tLoss: 4.7044\tLR: 8.139386\n",
      "Training Epoch: 82 [19968/50000]\tLoss: 4.8307\tLR: 8.139642\n",
      "Training Epoch: 82 [20096/50000]\tLoss: 4.6773\tLR: 8.139898\n",
      "Training Epoch: 82 [20224/50000]\tLoss: 4.7504\tLR: 8.140153\n",
      "Training Epoch: 82 [20352/50000]\tLoss: 4.6393\tLR: 8.140409\n",
      "Training Epoch: 82 [20480/50000]\tLoss: 4.6867\tLR: 8.140665\n",
      "Training Epoch: 82 [20608/50000]\tLoss: 4.7594\tLR: 8.140921\n",
      "Training Epoch: 82 [20736/50000]\tLoss: 4.7518\tLR: 8.141176\n",
      "Training Epoch: 82 [20864/50000]\tLoss: 4.7338\tLR: 8.141432\n",
      "Training Epoch: 82 [20992/50000]\tLoss: 4.7672\tLR: 8.141688\n",
      "Training Epoch: 82 [21120/50000]\tLoss: 4.6409\tLR: 8.141944\n",
      "Training Epoch: 82 [21248/50000]\tLoss: 4.7490\tLR: 8.142199\n",
      "Training Epoch: 82 [21376/50000]\tLoss: 4.7160\tLR: 8.142455\n",
      "Training Epoch: 82 [21504/50000]\tLoss: 4.8112\tLR: 8.142711\n",
      "Training Epoch: 82 [21632/50000]\tLoss: 4.8484\tLR: 8.142967\n",
      "Training Epoch: 82 [21760/50000]\tLoss: 4.8874\tLR: 8.143223\n",
      "Training Epoch: 82 [21888/50000]\tLoss: 4.7061\tLR: 8.143478\n",
      "Training Epoch: 82 [22016/50000]\tLoss: 4.7448\tLR: 8.143734\n",
      "Training Epoch: 82 [22144/50000]\tLoss: 4.6965\tLR: 8.143990\n",
      "Training Epoch: 82 [22272/50000]\tLoss: 4.7509\tLR: 8.144246\n",
      "Training Epoch: 82 [22400/50000]\tLoss: 4.7820\tLR: 8.144501\n",
      "Training Epoch: 82 [22528/50000]\tLoss: 4.7686\tLR: 8.144757\n",
      "Training Epoch: 82 [22656/50000]\tLoss: 4.7716\tLR: 8.145013\n",
      "Training Epoch: 82 [22784/50000]\tLoss: 4.8855\tLR: 8.145269\n",
      "Training Epoch: 82 [22912/50000]\tLoss: 4.8053\tLR: 8.145524\n",
      "Training Epoch: 82 [23040/50000]\tLoss: 4.7124\tLR: 8.145780\n",
      "Training Epoch: 82 [23168/50000]\tLoss: 4.7755\tLR: 8.146036\n",
      "Training Epoch: 82 [23296/50000]\tLoss: 4.7420\tLR: 8.146292\n",
      "Training Epoch: 82 [23424/50000]\tLoss: 4.7735\tLR: 8.146547\n",
      "Training Epoch: 82 [23552/50000]\tLoss: 4.8577\tLR: 8.146803\n",
      "Training Epoch: 82 [23680/50000]\tLoss: 4.7983\tLR: 8.147059\n",
      "Training Epoch: 82 [23808/50000]\tLoss: 4.6735\tLR: 8.147315\n",
      "Training Epoch: 82 [23936/50000]\tLoss: 4.6794\tLR: 8.147570\n",
      "Training Epoch: 82 [24064/50000]\tLoss: 4.8375\tLR: 8.147826\n",
      "Training Epoch: 82 [24192/50000]\tLoss: 4.7641\tLR: 8.148082\n",
      "Training Epoch: 82 [24320/50000]\tLoss: 4.8446\tLR: 8.148338\n",
      "Training Epoch: 82 [24448/50000]\tLoss: 4.7678\tLR: 8.148593\n",
      "Training Epoch: 82 [24576/50000]\tLoss: 4.7451\tLR: 8.148849\n",
      "Training Epoch: 82 [24704/50000]\tLoss: 4.9049\tLR: 8.149105\n",
      "Training Epoch: 82 [24832/50000]\tLoss: 4.8082\tLR: 8.149361\n",
      "Training Epoch: 82 [24960/50000]\tLoss: 4.8261\tLR: 8.149616\n",
      "Training Epoch: 82 [25088/50000]\tLoss: 4.7314\tLR: 8.149872\n",
      "Training Epoch: 82 [25216/50000]\tLoss: 4.7532\tLR: 8.150128\n",
      "Training Epoch: 82 [25344/50000]\tLoss: 4.7853\tLR: 8.150384\n",
      "Training Epoch: 82 [25472/50000]\tLoss: 4.7951\tLR: 8.150639\n",
      "Training Epoch: 82 [25600/50000]\tLoss: 4.7899\tLR: 8.150895\n",
      "Training Epoch: 82 [25728/50000]\tLoss: 4.9724\tLR: 8.151151\n",
      "Training Epoch: 82 [25856/50000]\tLoss: 4.8031\tLR: 8.151407\n",
      "Training Epoch: 82 [25984/50000]\tLoss: 4.7761\tLR: 8.151662\n",
      "Training Epoch: 82 [26112/50000]\tLoss: 4.6931\tLR: 8.151918\n",
      "Training Epoch: 82 [26240/50000]\tLoss: 4.8283\tLR: 8.152174\n",
      "Training Epoch: 82 [26368/50000]\tLoss: 4.8224\tLR: 8.152430\n",
      "Training Epoch: 82 [26496/50000]\tLoss: 4.7406\tLR: 8.152685\n",
      "Training Epoch: 82 [26624/50000]\tLoss: 4.7184\tLR: 8.152941\n",
      "Training Epoch: 82 [26752/50000]\tLoss: 4.7449\tLR: 8.153197\n",
      "Training Epoch: 82 [26880/50000]\tLoss: 4.6811\tLR: 8.153453\n",
      "Training Epoch: 82 [27008/50000]\tLoss: 4.7376\tLR: 8.153708\n",
      "Training Epoch: 82 [27136/50000]\tLoss: 4.7475\tLR: 8.153964\n",
      "Training Epoch: 82 [27264/50000]\tLoss: 4.7867\tLR: 8.154220\n",
      "Training Epoch: 82 [27392/50000]\tLoss: 4.7464\tLR: 8.154476\n",
      "Training Epoch: 82 [27520/50000]\tLoss: 4.8078\tLR: 8.154731\n",
      "Training Epoch: 82 [27648/50000]\tLoss: 4.7248\tLR: 8.154987\n",
      "Training Epoch: 82 [27776/50000]\tLoss: 4.7409\tLR: 8.155243\n",
      "Training Epoch: 82 [27904/50000]\tLoss: 4.7825\tLR: 8.155499\n",
      "Training Epoch: 82 [28032/50000]\tLoss: 4.7559\tLR: 8.155754\n",
      "Training Epoch: 82 [28160/50000]\tLoss: 4.8715\tLR: 8.156010\n",
      "Training Epoch: 82 [28288/50000]\tLoss: 4.7352\tLR: 8.156266\n",
      "Training Epoch: 82 [28416/50000]\tLoss: 4.7665\tLR: 8.156522\n",
      "Training Epoch: 82 [28544/50000]\tLoss: 4.7646\tLR: 8.156777\n",
      "Training Epoch: 82 [28672/50000]\tLoss: 4.6979\tLR: 8.157033\n",
      "Training Epoch: 82 [28800/50000]\tLoss: 4.7782\tLR: 8.157289\n",
      "Training Epoch: 82 [28928/50000]\tLoss: 4.7603\tLR: 8.157545\n",
      "Training Epoch: 82 [29056/50000]\tLoss: 4.7562\tLR: 8.157801\n",
      "Training Epoch: 82 [29184/50000]\tLoss: 4.7755\tLR: 8.158056\n",
      "Training Epoch: 82 [29312/50000]\tLoss: 4.7961\tLR: 8.158312\n",
      "Training Epoch: 82 [29440/50000]\tLoss: 4.6866\tLR: 8.158568\n",
      "Training Epoch: 82 [29568/50000]\tLoss: 4.7438\tLR: 8.158824\n",
      "Training Epoch: 82 [29696/50000]\tLoss: 4.7396\tLR: 8.159079\n",
      "Training Epoch: 82 [29824/50000]\tLoss: 4.7607\tLR: 8.159335\n",
      "Training Epoch: 82 [29952/50000]\tLoss: 4.7648\tLR: 8.159591\n",
      "Training Epoch: 82 [30080/50000]\tLoss: 4.8182\tLR: 8.159847\n",
      "Training Epoch: 82 [30208/50000]\tLoss: 4.7861\tLR: 8.160102\n",
      "Training Epoch: 82 [30336/50000]\tLoss: 4.7772\tLR: 8.160358\n",
      "Training Epoch: 82 [30464/50000]\tLoss: 4.7279\tLR: 8.160614\n",
      "Training Epoch: 82 [30592/50000]\tLoss: 4.7733\tLR: 8.160870\n",
      "Training Epoch: 82 [30720/50000]\tLoss: 4.7896\tLR: 8.161125\n",
      "Training Epoch: 82 [30848/50000]\tLoss: 4.7937\tLR: 8.161381\n",
      "Training Epoch: 82 [30976/50000]\tLoss: 4.7665\tLR: 8.161637\n",
      "Training Epoch: 82 [31104/50000]\tLoss: 4.7586\tLR: 8.161893\n",
      "Training Epoch: 82 [31232/50000]\tLoss: 4.7226\tLR: 8.162148\n",
      "Training Epoch: 82 [31360/50000]\tLoss: 4.7824\tLR: 8.162404\n",
      "Training Epoch: 82 [31488/50000]\tLoss: 4.7148\tLR: 8.162660\n",
      "Training Epoch: 82 [31616/50000]\tLoss: 4.8103\tLR: 8.162916\n",
      "Training Epoch: 82 [31744/50000]\tLoss: 4.6741\tLR: 8.163171\n",
      "Training Epoch: 82 [31872/50000]\tLoss: 4.6987\tLR: 8.163427\n",
      "Training Epoch: 82 [32000/50000]\tLoss: 4.8691\tLR: 8.163683\n",
      "Training Epoch: 82 [32128/50000]\tLoss: 4.7847\tLR: 8.163939\n",
      "Training Epoch: 82 [32256/50000]\tLoss: 4.7690\tLR: 8.164194\n",
      "Training Epoch: 82 [32384/50000]\tLoss: 4.7821\tLR: 8.164450\n",
      "Training Epoch: 82 [32512/50000]\tLoss: 4.7684\tLR: 8.164706\n",
      "Training Epoch: 82 [32640/50000]\tLoss: 4.7922\tLR: 8.164962\n",
      "Training Epoch: 82 [32768/50000]\tLoss: 4.8354\tLR: 8.165217\n",
      "Training Epoch: 82 [32896/50000]\tLoss: 4.7212\tLR: 8.165473\n",
      "Training Epoch: 82 [33024/50000]\tLoss: 4.7341\tLR: 8.165729\n",
      "Training Epoch: 82 [33152/50000]\tLoss: 4.7619\tLR: 8.165985\n",
      "Training Epoch: 82 [33280/50000]\tLoss: 4.7501\tLR: 8.166240\n",
      "Training Epoch: 82 [33408/50000]\tLoss: 4.7106\tLR: 8.166496\n",
      "Training Epoch: 82 [33536/50000]\tLoss: 4.7206\tLR: 8.166752\n",
      "Training Epoch: 82 [33664/50000]\tLoss: 4.7859\tLR: 8.167008\n",
      "Training Epoch: 82 [33792/50000]\tLoss: 4.7896\tLR: 8.167263\n",
      "Training Epoch: 82 [33920/50000]\tLoss: 4.8489\tLR: 8.167519\n",
      "Training Epoch: 82 [34048/50000]\tLoss: 4.7784\tLR: 8.167775\n",
      "Training Epoch: 82 [34176/50000]\tLoss: 4.8059\tLR: 8.168031\n",
      "Training Epoch: 82 [34304/50000]\tLoss: 4.8507\tLR: 8.168286\n",
      "Training Epoch: 82 [34432/50000]\tLoss: 4.8034\tLR: 8.168542\n",
      "Training Epoch: 82 [34560/50000]\tLoss: 4.7278\tLR: 8.168798\n",
      "Training Epoch: 82 [34688/50000]\tLoss: 4.6891\tLR: 8.169054\n",
      "Training Epoch: 82 [34816/50000]\tLoss: 4.8367\tLR: 8.169309\n",
      "Training Epoch: 82 [34944/50000]\tLoss: 4.8249\tLR: 8.169565\n",
      "Training Epoch: 82 [35072/50000]\tLoss: 4.7706\tLR: 8.169821\n",
      "Training Epoch: 82 [35200/50000]\tLoss: 4.7271\tLR: 8.170077\n",
      "Training Epoch: 82 [35328/50000]\tLoss: 4.7408\tLR: 8.170332\n",
      "Training Epoch: 82 [35456/50000]\tLoss: 4.7630\tLR: 8.170588\n",
      "Training Epoch: 82 [35584/50000]\tLoss: 4.7475\tLR: 8.170844\n",
      "Training Epoch: 82 [35712/50000]\tLoss: 4.6923\tLR: 8.171100\n",
      "Training Epoch: 82 [35840/50000]\tLoss: 4.6676\tLR: 8.171355\n",
      "Training Epoch: 82 [35968/50000]\tLoss: 4.7462\tLR: 8.171611\n",
      "Training Epoch: 82 [36096/50000]\tLoss: 4.7677\tLR: 8.171867\n",
      "Training Epoch: 82 [36224/50000]\tLoss: 4.7371\tLR: 8.172123\n",
      "Training Epoch: 82 [36352/50000]\tLoss: 4.7053\tLR: 8.172379\n",
      "Training Epoch: 82 [36480/50000]\tLoss: 4.6799\tLR: 8.172634\n",
      "Training Epoch: 82 [36608/50000]\tLoss: 4.7437\tLR: 8.172890\n",
      "Training Epoch: 82 [36736/50000]\tLoss: 4.6701\tLR: 8.173146\n",
      "Training Epoch: 82 [36864/50000]\tLoss: 4.7307\tLR: 8.173402\n",
      "Training Epoch: 82 [36992/50000]\tLoss: 4.8310\tLR: 8.173657\n",
      "Training Epoch: 82 [37120/50000]\tLoss: 4.6977\tLR: 8.173913\n",
      "Training Epoch: 82 [37248/50000]\tLoss: 4.7550\tLR: 8.174169\n",
      "Training Epoch: 82 [37376/50000]\tLoss: 4.6611\tLR: 8.174425\n",
      "Training Epoch: 82 [37504/50000]\tLoss: 4.8855\tLR: 8.174680\n",
      "Training Epoch: 82 [37632/50000]\tLoss: 4.6065\tLR: 8.174936\n",
      "Training Epoch: 82 [37760/50000]\tLoss: 4.8992\tLR: 8.175192\n",
      "Training Epoch: 82 [37888/50000]\tLoss: 4.7797\tLR: 8.175448\n",
      "Training Epoch: 82 [38016/50000]\tLoss: 4.7456\tLR: 8.175703\n",
      "Training Epoch: 82 [38144/50000]\tLoss: 4.7772\tLR: 8.175959\n",
      "Training Epoch: 82 [38272/50000]\tLoss: 4.7295\tLR: 8.176215\n",
      "Training Epoch: 82 [38400/50000]\tLoss: 4.7965\tLR: 8.176471\n",
      "Training Epoch: 82 [38528/50000]\tLoss: 4.7065\tLR: 8.176726\n",
      "Training Epoch: 82 [38656/50000]\tLoss: 4.7397\tLR: 8.176982\n",
      "Training Epoch: 82 [38784/50000]\tLoss: 4.7983\tLR: 8.177238\n",
      "Training Epoch: 82 [38912/50000]\tLoss: 4.7170\tLR: 8.177494\n",
      "Training Epoch: 82 [39040/50000]\tLoss: 4.7710\tLR: 8.177749\n",
      "Training Epoch: 82 [39168/50000]\tLoss: 4.6623\tLR: 8.178005\n",
      "Training Epoch: 82 [39296/50000]\tLoss: 4.7806\tLR: 8.178261\n",
      "Training Epoch: 82 [39424/50000]\tLoss: 4.8028\tLR: 8.178517\n",
      "Training Epoch: 82 [39552/50000]\tLoss: 4.7588\tLR: 8.178772\n",
      "Training Epoch: 82 [39680/50000]\tLoss: 4.6863\tLR: 8.179028\n",
      "Training Epoch: 82 [39808/50000]\tLoss: 4.8294\tLR: 8.179284\n",
      "Training Epoch: 82 [39936/50000]\tLoss: 4.6612\tLR: 8.179540\n",
      "Training Epoch: 82 [40064/50000]\tLoss: 4.6850\tLR: 8.179795\n",
      "Training Epoch: 82 [40192/50000]\tLoss: 4.7739\tLR: 8.180051\n",
      "Training Epoch: 82 [40320/50000]\tLoss: 4.7357\tLR: 8.180307\n",
      "Training Epoch: 82 [40448/50000]\tLoss: 4.6766\tLR: 8.180563\n",
      "Training Epoch: 82 [40576/50000]\tLoss: 4.7112\tLR: 8.180818\n",
      "Training Epoch: 82 [40704/50000]\tLoss: 4.9221\tLR: 8.181074\n",
      "Training Epoch: 82 [40832/50000]\tLoss: 4.8362\tLR: 8.181330\n",
      "Training Epoch: 82 [40960/50000]\tLoss: 4.8627\tLR: 8.181586\n",
      "Training Epoch: 82 [41088/50000]\tLoss: 4.7144\tLR: 8.181841\n",
      "Training Epoch: 82 [41216/50000]\tLoss: 4.6661\tLR: 8.182097\n",
      "Training Epoch: 82 [41344/50000]\tLoss: 4.7483\tLR: 8.182353\n",
      "Training Epoch: 82 [41472/50000]\tLoss: 4.8078\tLR: 8.182609\n",
      "Training Epoch: 82 [41600/50000]\tLoss: 4.7285\tLR: 8.182864\n",
      "Training Epoch: 82 [41728/50000]\tLoss: 4.7387\tLR: 8.183120\n",
      "Training Epoch: 82 [41856/50000]\tLoss: 4.7596\tLR: 8.183376\n",
      "Training Epoch: 82 [41984/50000]\tLoss: 4.7714\tLR: 8.183632\n",
      "Training Epoch: 82 [42112/50000]\tLoss: 4.7022\tLR: 8.183887\n",
      "Training Epoch: 82 [42240/50000]\tLoss: 4.7810\tLR: 8.184143\n",
      "Training Epoch: 82 [42368/50000]\tLoss: 4.8108\tLR: 8.184399\n",
      "Training Epoch: 82 [42496/50000]\tLoss: 4.7741\tLR: 8.184655\n",
      "Training Epoch: 82 [42624/50000]\tLoss: 4.7434\tLR: 8.184910\n",
      "Training Epoch: 82 [42752/50000]\tLoss: 4.7145\tLR: 8.185166\n",
      "Training Epoch: 82 [42880/50000]\tLoss: 4.7565\tLR: 8.185422\n",
      "Training Epoch: 82 [43008/50000]\tLoss: 4.7888\tLR: 8.185678\n",
      "Training Epoch: 82 [43136/50000]\tLoss: 4.6715\tLR: 8.185934\n",
      "Training Epoch: 82 [43264/50000]\tLoss: 4.6778\tLR: 8.186189\n",
      "Training Epoch: 82 [43392/50000]\tLoss: 4.7218\tLR: 8.186445\n",
      "Training Epoch: 82 [43520/50000]\tLoss: 4.8032\tLR: 8.186701\n",
      "Training Epoch: 82 [43648/50000]\tLoss: 4.7148\tLR: 8.186957\n",
      "Training Epoch: 82 [43776/50000]\tLoss: 4.7296\tLR: 8.187212\n",
      "Training Epoch: 82 [43904/50000]\tLoss: 4.7860\tLR: 8.187468\n",
      "Training Epoch: 82 [44032/50000]\tLoss: 4.7336\tLR: 8.187724\n",
      "Training Epoch: 82 [44160/50000]\tLoss: 4.8212\tLR: 8.187980\n",
      "Training Epoch: 82 [44288/50000]\tLoss: 4.8865\tLR: 8.188235\n",
      "Training Epoch: 82 [44416/50000]\tLoss: 4.7395\tLR: 8.188491\n",
      "Training Epoch: 82 [44544/50000]\tLoss: 4.8659\tLR: 8.188747\n",
      "Training Epoch: 82 [44672/50000]\tLoss: 4.7612\tLR: 8.189003\n",
      "Training Epoch: 82 [44800/50000]\tLoss: 4.8540\tLR: 8.189258\n",
      "Training Epoch: 82 [44928/50000]\tLoss: 4.8047\tLR: 8.189514\n",
      "Training Epoch: 82 [45056/50000]\tLoss: 4.7126\tLR: 8.189770\n",
      "Training Epoch: 82 [45184/50000]\tLoss: 4.8303\tLR: 8.190026\n",
      "Training Epoch: 82 [45312/50000]\tLoss: 4.7990\tLR: 8.190281\n",
      "Training Epoch: 82 [45440/50000]\tLoss: 4.7637\tLR: 8.190537\n",
      "Training Epoch: 82 [45568/50000]\tLoss: 4.7766\tLR: 8.190793\n",
      "Training Epoch: 82 [45696/50000]\tLoss: 4.7156\tLR: 8.191049\n",
      "Training Epoch: 82 [45824/50000]\tLoss: 4.8457\tLR: 8.191304\n",
      "Training Epoch: 82 [45952/50000]\tLoss: 4.7944\tLR: 8.191560\n",
      "Training Epoch: 82 [46080/50000]\tLoss: 4.7199\tLR: 8.191816\n",
      "Training Epoch: 82 [46208/50000]\tLoss: 4.6637\tLR: 8.192072\n",
      "Training Epoch: 82 [46336/50000]\tLoss: 4.7131\tLR: 8.192327\n",
      "Training Epoch: 82 [46464/50000]\tLoss: 4.6927\tLR: 8.192583\n",
      "Training Epoch: 82 [46592/50000]\tLoss: 4.7739\tLR: 8.192839\n",
      "Training Epoch: 82 [46720/50000]\tLoss: 4.8593\tLR: 8.193095\n",
      "Training Epoch: 82 [46848/50000]\tLoss: 4.8023\tLR: 8.193350\n",
      "Training Epoch: 82 [46976/50000]\tLoss: 4.7802\tLR: 8.193606\n",
      "Training Epoch: 82 [47104/50000]\tLoss: 4.8131\tLR: 8.193862\n",
      "Training Epoch: 82 [47232/50000]\tLoss: 4.7312\tLR: 8.194118\n",
      "Training Epoch: 82 [47360/50000]\tLoss: 4.7674\tLR: 8.194373\n",
      "Training Epoch: 82 [47488/50000]\tLoss: 4.7255\tLR: 8.194629\n",
      "Training Epoch: 82 [47616/50000]\tLoss: 4.7414\tLR: 8.194885\n",
      "Training Epoch: 82 [47744/50000]\tLoss: 4.7187\tLR: 8.195141\n",
      "Training Epoch: 82 [47872/50000]\tLoss: 4.7852\tLR: 8.195396\n",
      "Training Epoch: 82 [48000/50000]\tLoss: 4.6399\tLR: 8.195652\n",
      "Training Epoch: 82 [48128/50000]\tLoss: 4.7511\tLR: 8.195908\n",
      "Training Epoch: 82 [48256/50000]\tLoss: 4.8303\tLR: 8.196164\n",
      "Training Epoch: 82 [48384/50000]\tLoss: 4.7220\tLR: 8.196419\n",
      "Training Epoch: 82 [48512/50000]\tLoss: 4.7328\tLR: 8.196675\n",
      "Training Epoch: 82 [48640/50000]\tLoss: 4.7379\tLR: 8.196931\n",
      "Training Epoch: 82 [48768/50000]\tLoss: 4.7495\tLR: 8.197187\n",
      "Training Epoch: 82 [48896/50000]\tLoss: 4.6422\tLR: 8.197442\n",
      "Training Epoch: 82 [49024/50000]\tLoss: 4.7340\tLR: 8.197698\n",
      "Training Epoch: 82 [49152/50000]\tLoss: 4.6833\tLR: 8.197954\n",
      "Training Epoch: 82 [49280/50000]\tLoss: 4.8171\tLR: 8.198210\n",
      "Training Epoch: 82 [49408/50000]\tLoss: 4.7103\tLR: 8.198465\n",
      "Training Epoch: 82 [49536/50000]\tLoss: 4.7508\tLR: 8.198721\n",
      "Training Epoch: 82 [49664/50000]\tLoss: 4.7475\tLR: 8.198977\n",
      "Training Epoch: 82 [49792/50000]\tLoss: 4.7969\tLR: 8.199233\n",
      "Training Epoch: 82 [49920/50000]\tLoss: 4.7948\tLR: 8.199488\n",
      "Training Epoch: 82 [50000/50000]\tLoss: 4.7970\tLR: 8.199744\n",
      "epoch 82 training time consumed: 489.28s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  114958 GB |  114958 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  114605 GB |  114605 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     352 GB |     352 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  114958 GB |  114958 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  114605 GB |  114605 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     352 GB |     352 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  113343 GB |  113343 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  112990 GB |  112990 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     352 GB |     352 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   12189 K  |   12189 K  |\n",
      "|       from large pool |      24    |      65    |    5196 K  |    5196 K  |\n",
      "|       from small pool |     231    |     274    |    6993 K  |    6993 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   12189 K  |   12189 K  |\n",
      "|       from large pool |      24    |      65    |    5196 K  |    5196 K  |\n",
      "|       from small pool |     231    |     274    |    6993 K  |    6993 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      47    |    7065 K  |    7065 K  |\n",
      "|       from large pool |      10    |      23    |    2497 K  |    2497 K  |\n",
      "|       from small pool |      26    |      35    |    4567 K  |    4567 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 82, Average loss: 0.0374, Accuracy: 0.0100, Time consumed:31.37s\n",
      "\n",
      "Training Epoch: 83 [128/50000]\tLoss: 4.7292\tLR: 0.020000\n",
      "Training Epoch: 83 [256/50000]\tLoss: 4.8037\tLR: 8.200256\n",
      "Training Epoch: 83 [384/50000]\tLoss: 4.7391\tLR: 8.200512\n",
      "Training Epoch: 83 [512/50000]\tLoss: 4.7183\tLR: 8.200767\n",
      "Training Epoch: 83 [640/50000]\tLoss: 4.7146\tLR: 8.201023\n",
      "Training Epoch: 83 [768/50000]\tLoss: 4.8049\tLR: 8.201279\n",
      "Training Epoch: 83 [896/50000]\tLoss: 4.7605\tLR: 8.201535\n",
      "Training Epoch: 83 [1024/50000]\tLoss: 4.8068\tLR: 8.201790\n",
      "Training Epoch: 83 [1152/50000]\tLoss: 4.8340\tLR: 8.202046\n",
      "Training Epoch: 83 [1280/50000]\tLoss: 4.8914\tLR: 8.202302\n",
      "Training Epoch: 83 [1408/50000]\tLoss: 4.7499\tLR: 8.202558\n",
      "Training Epoch: 83 [1536/50000]\tLoss: 4.8133\tLR: 8.202813\n",
      "Training Epoch: 83 [1664/50000]\tLoss: 4.7270\tLR: 8.203069\n",
      "Training Epoch: 83 [1792/50000]\tLoss: 4.7430\tLR: 8.203325\n",
      "Training Epoch: 83 [1920/50000]\tLoss: 4.7173\tLR: 8.203581\n",
      "Training Epoch: 83 [2048/50000]\tLoss: 4.8284\tLR: 8.203836\n",
      "Training Epoch: 83 [2176/50000]\tLoss: 4.8159\tLR: 8.204092\n",
      "Training Epoch: 83 [2304/50000]\tLoss: 4.7696\tLR: 8.204348\n",
      "Training Epoch: 83 [2432/50000]\tLoss: 4.7968\tLR: 8.204604\n",
      "Training Epoch: 83 [2560/50000]\tLoss: 4.7629\tLR: 8.204859\n",
      "Training Epoch: 83 [2688/50000]\tLoss: 4.7154\tLR: 8.205115\n",
      "Training Epoch: 83 [2816/50000]\tLoss: 4.8002\tLR: 8.205371\n",
      "Training Epoch: 83 [2944/50000]\tLoss: 4.8000\tLR: 8.205627\n",
      "Training Epoch: 83 [3072/50000]\tLoss: 4.8689\tLR: 8.205882\n",
      "Training Epoch: 83 [3200/50000]\tLoss: 4.7533\tLR: 8.206138\n",
      "Training Epoch: 83 [3328/50000]\tLoss: 4.6708\tLR: 8.206394\n",
      "Training Epoch: 83 [3456/50000]\tLoss: 4.7547\tLR: 8.206650\n",
      "Training Epoch: 83 [3584/50000]\tLoss: 4.7453\tLR: 8.206905\n",
      "Training Epoch: 83 [3712/50000]\tLoss: 4.7951\tLR: 8.207161\n",
      "Training Epoch: 83 [3840/50000]\tLoss: 4.7412\tLR: 8.207417\n",
      "Training Epoch: 83 [3968/50000]\tLoss: 4.7274\tLR: 8.207673\n",
      "Training Epoch: 83 [4096/50000]\tLoss: 4.7905\tLR: 8.207928\n",
      "Training Epoch: 83 [4224/50000]\tLoss: 4.7792\tLR: 8.208184\n",
      "Training Epoch: 83 [4352/50000]\tLoss: 4.7798\tLR: 8.208440\n",
      "Training Epoch: 83 [4480/50000]\tLoss: 4.7378\tLR: 8.208696\n",
      "Training Epoch: 83 [4608/50000]\tLoss: 4.8556\tLR: 8.208951\n",
      "Training Epoch: 83 [4736/50000]\tLoss: 4.7375\tLR: 8.209207\n",
      "Training Epoch: 83 [4864/50000]\tLoss: 4.7835\tLR: 8.209463\n",
      "Training Epoch: 83 [4992/50000]\tLoss: 4.7400\tLR: 8.209719\n",
      "Training Epoch: 83 [5120/50000]\tLoss: 4.7951\tLR: 8.209974\n",
      "Training Epoch: 83 [5248/50000]\tLoss: 4.7292\tLR: 8.210230\n",
      "Training Epoch: 83 [5376/50000]\tLoss: 4.9012\tLR: 8.210486\n",
      "Training Epoch: 83 [5504/50000]\tLoss: 4.7598\tLR: 8.210742\n",
      "Training Epoch: 83 [5632/50000]\tLoss: 4.6831\tLR: 8.210997\n",
      "Training Epoch: 83 [5760/50000]\tLoss: 4.8086\tLR: 8.211253\n",
      "Training Epoch: 83 [5888/50000]\tLoss: 4.6864\tLR: 8.211509\n",
      "Training Epoch: 83 [6016/50000]\tLoss: 4.7006\tLR: 8.211765\n",
      "Training Epoch: 83 [6144/50000]\tLoss: 4.7593\tLR: 8.212020\n",
      "Training Epoch: 83 [6272/50000]\tLoss: 4.6793\tLR: 8.212276\n",
      "Training Epoch: 83 [6400/50000]\tLoss: 4.7662\tLR: 8.212532\n",
      "Training Epoch: 83 [6528/50000]\tLoss: 4.7197\tLR: 8.212788\n",
      "Training Epoch: 83 [6656/50000]\tLoss: 4.7220\tLR: 8.213043\n",
      "Training Epoch: 83 [6784/50000]\tLoss: 4.7627\tLR: 8.213299\n",
      "Training Epoch: 83 [6912/50000]\tLoss: 4.8184\tLR: 8.213555\n",
      "Training Epoch: 83 [7040/50000]\tLoss: 4.7875\tLR: 8.213811\n",
      "Training Epoch: 83 [7168/50000]\tLoss: 4.7777\tLR: 8.214066\n",
      "Training Epoch: 83 [7296/50000]\tLoss: 4.7557\tLR: 8.214322\n",
      "Training Epoch: 83 [7424/50000]\tLoss: 4.7014\tLR: 8.214578\n",
      "Training Epoch: 83 [7552/50000]\tLoss: 4.7560\tLR: 8.214834\n",
      "Training Epoch: 83 [7680/50000]\tLoss: 4.6816\tLR: 8.215090\n",
      "Training Epoch: 83 [7808/50000]\tLoss: 4.7457\tLR: 8.215345\n",
      "Training Epoch: 83 [7936/50000]\tLoss: 4.7973\tLR: 8.215601\n",
      "Training Epoch: 83 [8064/50000]\tLoss: 4.7448\tLR: 8.215857\n",
      "Training Epoch: 83 [8192/50000]\tLoss: 4.7758\tLR: 8.216113\n",
      "Training Epoch: 83 [8320/50000]\tLoss: 4.8381\tLR: 8.216368\n",
      "Training Epoch: 83 [8448/50000]\tLoss: 4.7887\tLR: 8.216624\n",
      "Training Epoch: 83 [8576/50000]\tLoss: 4.6881\tLR: 8.216880\n",
      "Training Epoch: 83 [8704/50000]\tLoss: 4.7775\tLR: 8.217136\n",
      "Training Epoch: 83 [8832/50000]\tLoss: 4.7190\tLR: 8.217391\n",
      "Training Epoch: 83 [8960/50000]\tLoss: 4.7060\tLR: 8.217647\n",
      "Training Epoch: 83 [9088/50000]\tLoss: 4.8376\tLR: 8.217903\n",
      "Training Epoch: 83 [9216/50000]\tLoss: 4.8106\tLR: 8.218159\n",
      "Training Epoch: 83 [9344/50000]\tLoss: 4.8145\tLR: 8.218414\n",
      "Training Epoch: 83 [9472/50000]\tLoss: 4.8482\tLR: 8.218670\n",
      "Training Epoch: 83 [9600/50000]\tLoss: 4.7098\tLR: 8.218926\n",
      "Training Epoch: 83 [9728/50000]\tLoss: 4.8311\tLR: 8.219182\n",
      "Training Epoch: 83 [9856/50000]\tLoss: 4.8313\tLR: 8.219437\n",
      "Training Epoch: 83 [9984/50000]\tLoss: 4.6976\tLR: 8.219693\n",
      "Training Epoch: 83 [10112/50000]\tLoss: 4.8515\tLR: 8.219949\n",
      "Training Epoch: 83 [10240/50000]\tLoss: 4.7746\tLR: 8.220205\n",
      "Training Epoch: 83 [10368/50000]\tLoss: 4.7343\tLR: 8.220460\n",
      "Training Epoch: 83 [10496/50000]\tLoss: 4.8035\tLR: 8.220716\n",
      "Training Epoch: 83 [10624/50000]\tLoss: 4.7654\tLR: 8.220972\n",
      "Training Epoch: 83 [10752/50000]\tLoss: 4.7862\tLR: 8.221228\n",
      "Training Epoch: 83 [10880/50000]\tLoss: 4.7819\tLR: 8.221483\n",
      "Training Epoch: 83 [11008/50000]\tLoss: 4.7501\tLR: 8.221739\n",
      "Training Epoch: 83 [11136/50000]\tLoss: 4.7843\tLR: 8.221995\n",
      "Training Epoch: 83 [11264/50000]\tLoss: 4.7711\tLR: 8.222251\n",
      "Training Epoch: 83 [11392/50000]\tLoss: 4.7869\tLR: 8.222506\n",
      "Training Epoch: 83 [11520/50000]\tLoss: 4.7334\tLR: 8.222762\n",
      "Training Epoch: 83 [11648/50000]\tLoss: 4.7689\tLR: 8.223018\n",
      "Training Epoch: 83 [11776/50000]\tLoss: 4.6463\tLR: 8.223274\n",
      "Training Epoch: 83 [11904/50000]\tLoss: 4.7765\tLR: 8.223529\n",
      "Training Epoch: 83 [12032/50000]\tLoss: 4.7583\tLR: 8.223785\n",
      "Training Epoch: 83 [12160/50000]\tLoss: 4.7507\tLR: 8.224041\n",
      "Training Epoch: 83 [12288/50000]\tLoss: 4.7440\tLR: 8.224297\n",
      "Training Epoch: 83 [12416/50000]\tLoss: 4.8039\tLR: 8.224552\n",
      "Training Epoch: 83 [12544/50000]\tLoss: 4.8320\tLR: 8.224808\n",
      "Training Epoch: 83 [12672/50000]\tLoss: 4.6630\tLR: 8.225064\n",
      "Training Epoch: 83 [12800/50000]\tLoss: 4.8022\tLR: 8.225320\n",
      "Training Epoch: 83 [12928/50000]\tLoss: 4.7498\tLR: 8.225575\n",
      "Training Epoch: 83 [13056/50000]\tLoss: 4.7124\tLR: 8.225831\n",
      "Training Epoch: 83 [13184/50000]\tLoss: 4.6467\tLR: 8.226087\n",
      "Training Epoch: 83 [13312/50000]\tLoss: 4.6513\tLR: 8.226343\n",
      "Training Epoch: 83 [13440/50000]\tLoss: 4.6981\tLR: 8.226598\n",
      "Training Epoch: 83 [13568/50000]\tLoss: 4.7597\tLR: 8.226854\n",
      "Training Epoch: 83 [13696/50000]\tLoss: 4.9040\tLR: 8.227110\n",
      "Training Epoch: 83 [13824/50000]\tLoss: 4.6860\tLR: 8.227366\n",
      "Training Epoch: 83 [13952/50000]\tLoss: 4.7244\tLR: 8.227621\n",
      "Training Epoch: 83 [14080/50000]\tLoss: 4.7884\tLR: 8.227877\n",
      "Training Epoch: 83 [14208/50000]\tLoss: 4.7874\tLR: 8.228133\n",
      "Training Epoch: 83 [14336/50000]\tLoss: 4.7756\tLR: 8.228389\n",
      "Training Epoch: 83 [14464/50000]\tLoss: 4.7362\tLR: 8.228645\n",
      "Training Epoch: 83 [14592/50000]\tLoss: 4.7405\tLR: 8.228900\n",
      "Training Epoch: 83 [14720/50000]\tLoss: 4.7839\tLR: 8.229156\n",
      "Training Epoch: 83 [14848/50000]\tLoss: 4.7727\tLR: 8.229412\n",
      "Training Epoch: 83 [14976/50000]\tLoss: 4.7990\tLR: 8.229668\n",
      "Training Epoch: 83 [15104/50000]\tLoss: 4.7677\tLR: 8.229923\n",
      "Training Epoch: 83 [15232/50000]\tLoss: 4.8417\tLR: 8.230179\n",
      "Training Epoch: 83 [15360/50000]\tLoss: 4.7338\tLR: 8.230435\n",
      "Training Epoch: 83 [15488/50000]\tLoss: 4.7809\tLR: 8.230691\n",
      "Training Epoch: 83 [15616/50000]\tLoss: 4.6731\tLR: 8.230946\n",
      "Training Epoch: 83 [15744/50000]\tLoss: 4.8358\tLR: 8.231202\n",
      "Training Epoch: 83 [15872/50000]\tLoss: 4.8227\tLR: 8.231458\n",
      "Training Epoch: 83 [16000/50000]\tLoss: 4.7536\tLR: 8.231714\n",
      "Training Epoch: 83 [16128/50000]\tLoss: 4.8287\tLR: 8.231969\n",
      "Training Epoch: 83 [16256/50000]\tLoss: 4.7651\tLR: 8.232225\n",
      "Training Epoch: 83 [16384/50000]\tLoss: 4.8316\tLR: 8.232481\n",
      "Training Epoch: 83 [16512/50000]\tLoss: 4.8521\tLR: 8.232737\n",
      "Training Epoch: 83 [16640/50000]\tLoss: 4.6998\tLR: 8.232992\n",
      "Training Epoch: 83 [16768/50000]\tLoss: 4.7782\tLR: 8.233248\n",
      "Training Epoch: 83 [16896/50000]\tLoss: 4.7715\tLR: 8.233504\n",
      "Training Epoch: 83 [17024/50000]\tLoss: 4.7832\tLR: 8.233760\n",
      "Training Epoch: 83 [17152/50000]\tLoss: 4.7806\tLR: 8.234015\n",
      "Training Epoch: 83 [17280/50000]\tLoss: 4.7464\tLR: 8.234271\n",
      "Training Epoch: 83 [17408/50000]\tLoss: 4.7322\tLR: 8.234527\n",
      "Training Epoch: 83 [17536/50000]\tLoss: 4.8187\tLR: 8.234783\n",
      "Training Epoch: 83 [17664/50000]\tLoss: 4.7620\tLR: 8.235038\n",
      "Training Epoch: 83 [17792/50000]\tLoss: 4.8732\tLR: 8.235294\n",
      "Training Epoch: 83 [17920/50000]\tLoss: 4.8532\tLR: 8.235550\n",
      "Training Epoch: 83 [18048/50000]\tLoss: 4.7817\tLR: 8.235806\n",
      "Training Epoch: 83 [18176/50000]\tLoss: 4.7352\tLR: 8.236061\n",
      "Training Epoch: 83 [18304/50000]\tLoss: 4.7380\tLR: 8.236317\n",
      "Training Epoch: 83 [18432/50000]\tLoss: 4.7536\tLR: 8.236573\n",
      "Training Epoch: 83 [18560/50000]\tLoss: 4.8079\tLR: 8.236829\n",
      "Training Epoch: 83 [18688/50000]\tLoss: 4.7313\tLR: 8.237084\n",
      "Training Epoch: 83 [18816/50000]\tLoss: 4.7085\tLR: 8.237340\n",
      "Training Epoch: 83 [18944/50000]\tLoss: 4.7564\tLR: 8.237596\n",
      "Training Epoch: 83 [19072/50000]\tLoss: 4.8145\tLR: 8.237852\n",
      "Training Epoch: 83 [19200/50000]\tLoss: 4.8694\tLR: 8.238107\n",
      "Training Epoch: 83 [19328/50000]\tLoss: 4.7393\tLR: 8.238363\n",
      "Training Epoch: 83 [19456/50000]\tLoss: 4.8015\tLR: 8.238619\n",
      "Training Epoch: 83 [19584/50000]\tLoss: 4.7862\tLR: 8.238875\n",
      "Training Epoch: 83 [19712/50000]\tLoss: 4.8500\tLR: 8.239130\n",
      "Training Epoch: 83 [19840/50000]\tLoss: 4.7109\tLR: 8.239386\n",
      "Training Epoch: 83 [19968/50000]\tLoss: 4.7388\tLR: 8.239642\n",
      "Training Epoch: 83 [20096/50000]\tLoss: 4.7298\tLR: 8.239898\n",
      "Training Epoch: 83 [20224/50000]\tLoss: 4.7182\tLR: 8.240153\n",
      "Training Epoch: 83 [20352/50000]\tLoss: 4.8243\tLR: 8.240409\n",
      "Training Epoch: 83 [20480/50000]\tLoss: 4.8060\tLR: 8.240665\n",
      "Training Epoch: 83 [20608/50000]\tLoss: 4.8766\tLR: 8.240921\n",
      "Training Epoch: 83 [20736/50000]\tLoss: 4.7037\tLR: 8.241176\n",
      "Training Epoch: 83 [20864/50000]\tLoss: 4.7453\tLR: 8.241432\n",
      "Training Epoch: 83 [20992/50000]\tLoss: 4.8353\tLR: 8.241688\n",
      "Training Epoch: 83 [21120/50000]\tLoss: 4.7584\tLR: 8.241944\n",
      "Training Epoch: 83 [21248/50000]\tLoss: 4.7350\tLR: 8.242199\n",
      "Training Epoch: 83 [21376/50000]\tLoss: 4.8606\tLR: 8.242455\n",
      "Training Epoch: 83 [21504/50000]\tLoss: 4.8195\tLR: 8.242711\n",
      "Training Epoch: 83 [21632/50000]\tLoss: 4.8267\tLR: 8.242967\n",
      "Training Epoch: 83 [21760/50000]\tLoss: 4.8692\tLR: 8.243223\n",
      "Training Epoch: 83 [21888/50000]\tLoss: 4.7605\tLR: 8.243478\n",
      "Training Epoch: 83 [22016/50000]\tLoss: 4.7841\tLR: 8.243734\n",
      "Training Epoch: 83 [22144/50000]\tLoss: 4.8243\tLR: 8.243990\n",
      "Training Epoch: 83 [22272/50000]\tLoss: 4.7962\tLR: 8.244246\n",
      "Training Epoch: 83 [22400/50000]\tLoss: 4.8830\tLR: 8.244501\n",
      "Training Epoch: 83 [22528/50000]\tLoss: 4.7773\tLR: 8.244757\n",
      "Training Epoch: 83 [22656/50000]\tLoss: 4.7967\tLR: 8.245013\n",
      "Training Epoch: 83 [22784/50000]\tLoss: 4.8111\tLR: 8.245269\n",
      "Training Epoch: 83 [22912/50000]\tLoss: 4.8231\tLR: 8.245524\n",
      "Training Epoch: 83 [23040/50000]\tLoss: 4.8430\tLR: 8.245780\n",
      "Training Epoch: 83 [23168/50000]\tLoss: 4.8401\tLR: 8.246036\n",
      "Training Epoch: 83 [23296/50000]\tLoss: 4.7799\tLR: 8.246292\n",
      "Training Epoch: 83 [23424/50000]\tLoss: 4.7397\tLR: 8.246547\n",
      "Training Epoch: 83 [23552/50000]\tLoss: 4.7155\tLR: 8.246803\n",
      "Training Epoch: 83 [23680/50000]\tLoss: 4.8478\tLR: 8.247059\n",
      "Training Epoch: 83 [23808/50000]\tLoss: 4.8748\tLR: 8.247315\n",
      "Training Epoch: 83 [23936/50000]\tLoss: 4.7799\tLR: 8.247570\n",
      "Training Epoch: 83 [24064/50000]\tLoss: 4.8217\tLR: 8.247826\n",
      "Training Epoch: 83 [24192/50000]\tLoss: 4.7721\tLR: 8.248082\n",
      "Training Epoch: 83 [24320/50000]\tLoss: 4.7292\tLR: 8.248338\n",
      "Training Epoch: 83 [24448/50000]\tLoss: 4.7430\tLR: 8.248593\n",
      "Training Epoch: 83 [24576/50000]\tLoss: 4.7626\tLR: 8.248849\n",
      "Training Epoch: 83 [24704/50000]\tLoss: 4.7579\tLR: 8.249105\n",
      "Training Epoch: 83 [24832/50000]\tLoss: 4.8597\tLR: 8.249361\n",
      "Training Epoch: 83 [24960/50000]\tLoss: 4.8484\tLR: 8.249616\n",
      "Training Epoch: 83 [25088/50000]\tLoss: 4.8406\tLR: 8.249872\n",
      "Training Epoch: 83 [25216/50000]\tLoss: 4.7943\tLR: 8.250128\n",
      "Training Epoch: 83 [25344/50000]\tLoss: 4.7920\tLR: 8.250384\n",
      "Training Epoch: 83 [25472/50000]\tLoss: 4.7664\tLR: 8.250639\n",
      "Training Epoch: 83 [25600/50000]\tLoss: 4.7332\tLR: 8.250895\n",
      "Training Epoch: 83 [25728/50000]\tLoss: 4.8346\tLR: 8.251151\n",
      "Training Epoch: 83 [25856/50000]\tLoss: 4.8758\tLR: 8.251407\n",
      "Training Epoch: 83 [25984/50000]\tLoss: 4.7518\tLR: 8.251662\n",
      "Training Epoch: 83 [26112/50000]\tLoss: 4.7523\tLR: 8.251918\n",
      "Training Epoch: 83 [26240/50000]\tLoss: 4.7872\tLR: 8.252174\n",
      "Training Epoch: 83 [26368/50000]\tLoss: 4.8280\tLR: 8.252430\n",
      "Training Epoch: 83 [26496/50000]\tLoss: 4.7684\tLR: 8.252685\n",
      "Training Epoch: 83 [26624/50000]\tLoss: 4.8225\tLR: 8.252941\n",
      "Training Epoch: 83 [26752/50000]\tLoss: 4.7812\tLR: 8.253197\n",
      "Training Epoch: 83 [26880/50000]\tLoss: 4.7330\tLR: 8.253453\n",
      "Training Epoch: 83 [27008/50000]\tLoss: 4.7227\tLR: 8.253708\n",
      "Training Epoch: 83 [27136/50000]\tLoss: 4.8299\tLR: 8.253964\n",
      "Training Epoch: 83 [27264/50000]\tLoss: 4.7275\tLR: 8.254220\n",
      "Training Epoch: 83 [27392/50000]\tLoss: 4.7573\tLR: 8.254476\n",
      "Training Epoch: 83 [27520/50000]\tLoss: 4.7564\tLR: 8.254731\n",
      "Training Epoch: 83 [27648/50000]\tLoss: 4.7774\tLR: 8.254987\n",
      "Training Epoch: 83 [27776/50000]\tLoss: 4.7346\tLR: 8.255243\n",
      "Training Epoch: 83 [27904/50000]\tLoss: 4.7359\tLR: 8.255499\n",
      "Training Epoch: 83 [28032/50000]\tLoss: 4.8555\tLR: 8.255754\n",
      "Training Epoch: 83 [28160/50000]\tLoss: 4.7425\tLR: 8.256010\n",
      "Training Epoch: 83 [28288/50000]\tLoss: 4.8220\tLR: 8.256266\n",
      "Training Epoch: 83 [28416/50000]\tLoss: 4.7315\tLR: 8.256522\n",
      "Training Epoch: 83 [28544/50000]\tLoss: 4.7695\tLR: 8.256777\n",
      "Training Epoch: 83 [28672/50000]\tLoss: 4.7068\tLR: 8.257033\n",
      "Training Epoch: 83 [28800/50000]\tLoss: 4.6968\tLR: 8.257289\n",
      "Training Epoch: 83 [28928/50000]\tLoss: 4.7089\tLR: 8.257545\n",
      "Training Epoch: 83 [29056/50000]\tLoss: 4.7190\tLR: 8.257801\n",
      "Training Epoch: 83 [29184/50000]\tLoss: 4.8050\tLR: 8.258056\n",
      "Training Epoch: 83 [29312/50000]\tLoss: 4.8246\tLR: 8.258312\n",
      "Training Epoch: 83 [29440/50000]\tLoss: 4.8122\tLR: 8.258568\n",
      "Training Epoch: 83 [29568/50000]\tLoss: 4.8488\tLR: 8.258824\n",
      "Training Epoch: 83 [29696/50000]\tLoss: 4.7627\tLR: 8.259079\n",
      "Training Epoch: 83 [29824/50000]\tLoss: 4.7855\tLR: 8.259335\n",
      "Training Epoch: 83 [29952/50000]\tLoss: 4.7293\tLR: 8.259591\n",
      "Training Epoch: 83 [30080/50000]\tLoss: 4.7144\tLR: 8.259847\n",
      "Training Epoch: 83 [30208/50000]\tLoss: 4.6461\tLR: 8.260102\n",
      "Training Epoch: 83 [30336/50000]\tLoss: 4.8604\tLR: 8.260358\n",
      "Training Epoch: 83 [30464/50000]\tLoss: 4.7827\tLR: 8.260614\n",
      "Training Epoch: 83 [30592/50000]\tLoss: 4.7842\tLR: 8.260870\n",
      "Training Epoch: 83 [30720/50000]\tLoss: 4.7442\tLR: 8.261125\n",
      "Training Epoch: 83 [30848/50000]\tLoss: 4.7141\tLR: 8.261381\n",
      "Training Epoch: 83 [30976/50000]\tLoss: 4.8497\tLR: 8.261637\n",
      "Training Epoch: 83 [31104/50000]\tLoss: 4.7578\tLR: 8.261893\n",
      "Training Epoch: 83 [31232/50000]\tLoss: 4.8671\tLR: 8.262148\n",
      "Training Epoch: 83 [31360/50000]\tLoss: 4.7601\tLR: 8.262404\n",
      "Training Epoch: 83 [31488/50000]\tLoss: 4.8231\tLR: 8.262660\n",
      "Training Epoch: 83 [31616/50000]\tLoss: 4.7883\tLR: 8.262916\n",
      "Training Epoch: 83 [31744/50000]\tLoss: 4.7760\tLR: 8.263171\n",
      "Training Epoch: 83 [31872/50000]\tLoss: 4.8268\tLR: 8.263427\n",
      "Training Epoch: 83 [32000/50000]\tLoss: 4.6904\tLR: 8.263683\n",
      "Training Epoch: 83 [32128/50000]\tLoss: 4.8085\tLR: 8.263939\n",
      "Training Epoch: 83 [32256/50000]\tLoss: 4.7425\tLR: 8.264194\n",
      "Training Epoch: 83 [32384/50000]\tLoss: 4.8408\tLR: 8.264450\n",
      "Training Epoch: 83 [32512/50000]\tLoss: 4.7840\tLR: 8.264706\n",
      "Training Epoch: 83 [32640/50000]\tLoss: 4.8285\tLR: 8.264962\n",
      "Training Epoch: 83 [32768/50000]\tLoss: 4.7343\tLR: 8.265217\n",
      "Training Epoch: 83 [32896/50000]\tLoss: 4.7232\tLR: 8.265473\n",
      "Training Epoch: 83 [33024/50000]\tLoss: 4.8824\tLR: 8.265729\n",
      "Training Epoch: 83 [33152/50000]\tLoss: 4.7757\tLR: 8.265985\n",
      "Training Epoch: 83 [33280/50000]\tLoss: 4.8298\tLR: 8.266240\n",
      "Training Epoch: 83 [33408/50000]\tLoss: 4.7544\tLR: 8.266496\n",
      "Training Epoch: 83 [33536/50000]\tLoss: 4.7971\tLR: 8.266752\n",
      "Training Epoch: 83 [33664/50000]\tLoss: 4.7031\tLR: 8.267008\n",
      "Training Epoch: 83 [33792/50000]\tLoss: 4.7139\tLR: 8.267263\n",
      "Training Epoch: 83 [33920/50000]\tLoss: 4.7727\tLR: 8.267519\n",
      "Training Epoch: 83 [34048/50000]\tLoss: 4.7816\tLR: 8.267775\n",
      "Training Epoch: 83 [34176/50000]\tLoss: 4.8036\tLR: 8.268031\n",
      "Training Epoch: 83 [34304/50000]\tLoss: 4.8967\tLR: 8.268286\n",
      "Training Epoch: 83 [34432/50000]\tLoss: 4.8246\tLR: 8.268542\n",
      "Training Epoch: 83 [34560/50000]\tLoss: 4.7800\tLR: 8.268798\n",
      "Training Epoch: 83 [34688/50000]\tLoss: 4.7780\tLR: 8.269054\n",
      "Training Epoch: 83 [34816/50000]\tLoss: 4.6671\tLR: 8.269309\n",
      "Training Epoch: 83 [34944/50000]\tLoss: 4.6738\tLR: 8.269565\n",
      "Training Epoch: 83 [35072/50000]\tLoss: 4.8087\tLR: 8.269821\n",
      "Training Epoch: 83 [35200/50000]\tLoss: 4.8320\tLR: 8.270077\n",
      "Training Epoch: 83 [35328/50000]\tLoss: 4.7595\tLR: 8.270332\n",
      "Training Epoch: 83 [35456/50000]\tLoss: 4.7627\tLR: 8.270588\n",
      "Training Epoch: 83 [35584/50000]\tLoss: 4.7922\tLR: 8.270844\n",
      "Training Epoch: 83 [35712/50000]\tLoss: 4.8188\tLR: 8.271100\n",
      "Training Epoch: 83 [35840/50000]\tLoss: 4.7227\tLR: 8.271355\n",
      "Training Epoch: 83 [35968/50000]\tLoss: 4.7850\tLR: 8.271611\n",
      "Training Epoch: 83 [36096/50000]\tLoss: 4.9005\tLR: 8.271867\n",
      "Training Epoch: 83 [36224/50000]\tLoss: 4.7415\tLR: 8.272123\n",
      "Training Epoch: 83 [36352/50000]\tLoss: 4.7575\tLR: 8.272379\n",
      "Training Epoch: 83 [36480/50000]\tLoss: 4.7530\tLR: 8.272634\n",
      "Training Epoch: 83 [36608/50000]\tLoss: 4.8304\tLR: 8.272890\n",
      "Training Epoch: 83 [36736/50000]\tLoss: 4.9282\tLR: 8.273146\n",
      "Training Epoch: 83 [36864/50000]\tLoss: 4.7467\tLR: 8.273402\n",
      "Training Epoch: 83 [36992/50000]\tLoss: 4.7479\tLR: 8.273657\n",
      "Training Epoch: 83 [37120/50000]\tLoss: 4.8038\tLR: 8.273913\n",
      "Training Epoch: 83 [37248/50000]\tLoss: 4.8259\tLR: 8.274169\n",
      "Training Epoch: 83 [37376/50000]\tLoss: 4.6950\tLR: 8.274425\n",
      "Training Epoch: 83 [37504/50000]\tLoss: 4.8154\tLR: 8.274680\n",
      "Training Epoch: 83 [37632/50000]\tLoss: 4.7318\tLR: 8.274936\n",
      "Training Epoch: 83 [37760/50000]\tLoss: 4.7824\tLR: 8.275192\n",
      "Training Epoch: 83 [37888/50000]\tLoss: 4.7461\tLR: 8.275448\n",
      "Training Epoch: 83 [38016/50000]\tLoss: 4.7816\tLR: 8.275703\n",
      "Training Epoch: 83 [38144/50000]\tLoss: 4.7919\tLR: 8.275959\n",
      "Training Epoch: 83 [38272/50000]\tLoss: 4.7659\tLR: 8.276215\n",
      "Training Epoch: 83 [38400/50000]\tLoss: 4.7019\tLR: 8.276471\n",
      "Training Epoch: 83 [38528/50000]\tLoss: 4.7768\tLR: 8.276726\n",
      "Training Epoch: 83 [38656/50000]\tLoss: 4.7197\tLR: 8.276982\n",
      "Training Epoch: 83 [38784/50000]\tLoss: 4.8250\tLR: 8.277238\n",
      "Training Epoch: 83 [38912/50000]\tLoss: 4.7133\tLR: 8.277494\n",
      "Training Epoch: 83 [39040/50000]\tLoss: 4.7546\tLR: 8.277749\n",
      "Training Epoch: 83 [39168/50000]\tLoss: 4.7744\tLR: 8.278005\n",
      "Training Epoch: 83 [39296/50000]\tLoss: 4.7551\tLR: 8.278261\n",
      "Training Epoch: 83 [39424/50000]\tLoss: 4.7399\tLR: 8.278517\n",
      "Training Epoch: 83 [39552/50000]\tLoss: 4.7989\tLR: 8.278772\n",
      "Training Epoch: 83 [39680/50000]\tLoss: 4.8695\tLR: 8.279028\n",
      "Training Epoch: 83 [39808/50000]\tLoss: 4.7393\tLR: 8.279284\n",
      "Training Epoch: 83 [39936/50000]\tLoss: 4.8307\tLR: 8.279540\n",
      "Training Epoch: 83 [40064/50000]\tLoss: 4.8163\tLR: 8.279795\n",
      "Training Epoch: 83 [40192/50000]\tLoss: 4.7471\tLR: 8.280051\n",
      "Training Epoch: 83 [40320/50000]\tLoss: 4.6612\tLR: 8.280307\n",
      "Training Epoch: 83 [40448/50000]\tLoss: 4.6770\tLR: 8.280563\n",
      "Training Epoch: 83 [40576/50000]\tLoss: 4.8322\tLR: 8.280818\n",
      "Training Epoch: 83 [40704/50000]\tLoss: 4.8563\tLR: 8.281074\n",
      "Training Epoch: 83 [40832/50000]\tLoss: 4.7267\tLR: 8.281330\n",
      "Training Epoch: 83 [40960/50000]\tLoss: 4.8666\tLR: 8.281586\n",
      "Training Epoch: 83 [41088/50000]\tLoss: 4.7176\tLR: 8.281841\n",
      "Training Epoch: 83 [41216/50000]\tLoss: 4.7287\tLR: 8.282097\n",
      "Training Epoch: 83 [41344/50000]\tLoss: 4.7323\tLR: 8.282353\n",
      "Training Epoch: 83 [41472/50000]\tLoss: 4.7161\tLR: 8.282609\n",
      "Training Epoch: 83 [41600/50000]\tLoss: 4.7102\tLR: 8.282864\n",
      "Training Epoch: 83 [41728/50000]\tLoss: 4.7682\tLR: 8.283120\n",
      "Training Epoch: 83 [41856/50000]\tLoss: 4.7945\tLR: 8.283376\n",
      "Training Epoch: 83 [41984/50000]\tLoss: 4.7974\tLR: 8.283632\n",
      "Training Epoch: 83 [42112/50000]\tLoss: 4.8366\tLR: 8.283887\n",
      "Training Epoch: 83 [42240/50000]\tLoss: 4.8089\tLR: 8.284143\n",
      "Training Epoch: 83 [42368/50000]\tLoss: 4.7359\tLR: 8.284399\n",
      "Training Epoch: 83 [42496/50000]\tLoss: 4.8180\tLR: 8.284655\n",
      "Training Epoch: 83 [42624/50000]\tLoss: 4.7315\tLR: 8.284910\n",
      "Training Epoch: 83 [42752/50000]\tLoss: 4.7360\tLR: 8.285166\n",
      "Training Epoch: 83 [42880/50000]\tLoss: 4.7536\tLR: 8.285422\n",
      "Training Epoch: 83 [43008/50000]\tLoss: 4.7379\tLR: 8.285678\n",
      "Training Epoch: 83 [43136/50000]\tLoss: 4.7826\tLR: 8.285934\n",
      "Training Epoch: 83 [43264/50000]\tLoss: 4.8305\tLR: 8.286189\n",
      "Training Epoch: 83 [43392/50000]\tLoss: 4.8105\tLR: 8.286445\n",
      "Training Epoch: 83 [43520/50000]\tLoss: 4.7724\tLR: 8.286701\n",
      "Training Epoch: 83 [43648/50000]\tLoss: 4.7443\tLR: 8.286957\n",
      "Training Epoch: 83 [43776/50000]\tLoss: 4.7417\tLR: 8.287212\n",
      "Training Epoch: 83 [43904/50000]\tLoss: 4.7843\tLR: 8.287468\n",
      "Training Epoch: 83 [44032/50000]\tLoss: 4.7216\tLR: 8.287724\n",
      "Training Epoch: 83 [44160/50000]\tLoss: 4.7584\tLR: 8.287980\n",
      "Training Epoch: 83 [44288/50000]\tLoss: 4.7980\tLR: 8.288235\n",
      "Training Epoch: 83 [44416/50000]\tLoss: 4.7646\tLR: 8.288491\n",
      "Training Epoch: 83 [44544/50000]\tLoss: 4.6965\tLR: 8.288747\n",
      "Training Epoch: 83 [44672/50000]\tLoss: 4.7896\tLR: 8.289003\n",
      "Training Epoch: 83 [44800/50000]\tLoss: 4.7330\tLR: 8.289258\n",
      "Training Epoch: 83 [44928/50000]\tLoss: 4.9105\tLR: 8.289514\n",
      "Training Epoch: 83 [45056/50000]\tLoss: 4.8254\tLR: 8.289770\n",
      "Training Epoch: 83 [45184/50000]\tLoss: 4.8068\tLR: 8.290026\n",
      "Training Epoch: 83 [45312/50000]\tLoss: 4.8423\tLR: 8.290281\n",
      "Training Epoch: 83 [45440/50000]\tLoss: 4.7240\tLR: 8.290537\n",
      "Training Epoch: 83 [45568/50000]\tLoss: 4.8051\tLR: 8.290793\n",
      "Training Epoch: 83 [45696/50000]\tLoss: 4.8613\tLR: 8.291049\n",
      "Training Epoch: 83 [45824/50000]\tLoss: 4.6754\tLR: 8.291304\n",
      "Training Epoch: 83 [45952/50000]\tLoss: 4.8025\tLR: 8.291560\n",
      "Training Epoch: 83 [46080/50000]\tLoss: 4.7822\tLR: 8.291816\n",
      "Training Epoch: 83 [46208/50000]\tLoss: 4.9119\tLR: 8.292072\n",
      "Training Epoch: 83 [46336/50000]\tLoss: 4.9073\tLR: 8.292327\n",
      "Training Epoch: 83 [46464/50000]\tLoss: 4.7830\tLR: 8.292583\n",
      "Training Epoch: 83 [46592/50000]\tLoss: 4.7749\tLR: 8.292839\n",
      "Training Epoch: 83 [46720/50000]\tLoss: 4.7544\tLR: 8.293095\n",
      "Training Epoch: 83 [46848/50000]\tLoss: 4.7600\tLR: 8.293350\n",
      "Training Epoch: 83 [46976/50000]\tLoss: 4.6731\tLR: 8.293606\n",
      "Training Epoch: 83 [47104/50000]\tLoss: 4.7081\tLR: 8.293862\n",
      "Training Epoch: 83 [47232/50000]\tLoss: 4.7440\tLR: 8.294118\n",
      "Training Epoch: 83 [47360/50000]\tLoss: 4.7307\tLR: 8.294373\n",
      "Training Epoch: 83 [47488/50000]\tLoss: 4.7206\tLR: 8.294629\n",
      "Training Epoch: 83 [47616/50000]\tLoss: 4.7994\tLR: 8.294885\n",
      "Training Epoch: 83 [47744/50000]\tLoss: 4.7873\tLR: 8.295141\n",
      "Training Epoch: 83 [47872/50000]\tLoss: 4.7757\tLR: 8.295396\n",
      "Training Epoch: 83 [48000/50000]\tLoss: 4.7454\tLR: 8.295652\n",
      "Training Epoch: 83 [48128/50000]\tLoss: 4.7615\tLR: 8.295908\n",
      "Training Epoch: 83 [48256/50000]\tLoss: 4.7554\tLR: 8.296164\n",
      "Training Epoch: 83 [48384/50000]\tLoss: 4.7798\tLR: 8.296419\n",
      "Training Epoch: 83 [48512/50000]\tLoss: 4.7243\tLR: 8.296675\n",
      "Training Epoch: 83 [48640/50000]\tLoss: 4.7671\tLR: 8.296931\n",
      "Training Epoch: 83 [48768/50000]\tLoss: 4.7432\tLR: 8.297187\n",
      "Training Epoch: 83 [48896/50000]\tLoss: 4.7420\tLR: 8.297442\n",
      "Training Epoch: 83 [49024/50000]\tLoss: 4.8108\tLR: 8.297698\n",
      "Training Epoch: 83 [49152/50000]\tLoss: 4.7686\tLR: 8.297954\n",
      "Training Epoch: 83 [49280/50000]\tLoss: 4.8473\tLR: 8.298210\n",
      "Training Epoch: 83 [49408/50000]\tLoss: 4.7720\tLR: 8.298465\n",
      "Training Epoch: 83 [49536/50000]\tLoss: 4.7549\tLR: 8.298721\n",
      "Training Epoch: 83 [49664/50000]\tLoss: 4.7190\tLR: 8.298977\n",
      "Training Epoch: 83 [49792/50000]\tLoss: 4.6846\tLR: 8.299233\n",
      "Training Epoch: 83 [49920/50000]\tLoss: 4.6582\tLR: 8.299488\n",
      "Training Epoch: 83 [50000/50000]\tLoss: 4.8284\tLR: 8.299744\n",
      "epoch 83 training time consumed: 489.10s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  116360 GB |  116360 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  116003 GB |  116003 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     357 GB |     357 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  116360 GB |  116360 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  116003 GB |  116003 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     357 GB |     357 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  114725 GB |  114725 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  114368 GB |  114368 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     357 GB |     357 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   12338 K  |   12338 K  |\n",
      "|       from large pool |      24    |      65    |    5259 K  |    5259 K  |\n",
      "|       from small pool |     231    |     274    |    7078 K  |    7078 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   12338 K  |   12338 K  |\n",
      "|       from large pool |      24    |      65    |    5259 K  |    5259 K  |\n",
      "|       from small pool |     231    |     274    |    7078 K  |    7078 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      47    |    7151 K  |    7151 K  |\n",
      "|       from large pool |      10    |      23    |    2528 K  |    2528 K  |\n",
      "|       from small pool |      25    |      35    |    4623 K  |    4623 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 83, Average loss: 0.0377, Accuracy: 0.0100, Time consumed:31.30s\n",
      "\n",
      "Training Epoch: 84 [128/50000]\tLoss: 4.7670\tLR: 0.020000\n",
      "Training Epoch: 84 [256/50000]\tLoss: 4.7876\tLR: 8.300256\n",
      "Training Epoch: 84 [384/50000]\tLoss: 4.8060\tLR: 8.300512\n",
      "Training Epoch: 84 [512/50000]\tLoss: 4.7430\tLR: 8.300767\n",
      "Training Epoch: 84 [640/50000]\tLoss: 4.6789\tLR: 8.301023\n",
      "Training Epoch: 84 [768/50000]\tLoss: 4.6419\tLR: 8.301279\n",
      "Training Epoch: 84 [896/50000]\tLoss: 4.7016\tLR: 8.301535\n",
      "Training Epoch: 84 [1024/50000]\tLoss: 4.7730\tLR: 8.301790\n",
      "Training Epoch: 84 [1152/50000]\tLoss: 4.7759\tLR: 8.302046\n",
      "Training Epoch: 84 [1280/50000]\tLoss: 4.7571\tLR: 8.302302\n",
      "Training Epoch: 84 [1408/50000]\tLoss: 4.7492\tLR: 8.302558\n",
      "Training Epoch: 84 [1536/50000]\tLoss: 4.7180\tLR: 8.302813\n",
      "Training Epoch: 84 [1664/50000]\tLoss: 4.7047\tLR: 8.303069\n",
      "Training Epoch: 84 [1792/50000]\tLoss: 4.7818\tLR: 8.303325\n",
      "Training Epoch: 84 [1920/50000]\tLoss: 4.7272\tLR: 8.303581\n",
      "Training Epoch: 84 [2048/50000]\tLoss: 4.7692\tLR: 8.303836\n",
      "Training Epoch: 84 [2176/50000]\tLoss: 4.8086\tLR: 8.304092\n",
      "Training Epoch: 84 [2304/50000]\tLoss: 4.7576\tLR: 8.304348\n",
      "Training Epoch: 84 [2432/50000]\tLoss: 4.8786\tLR: 8.304604\n",
      "Training Epoch: 84 [2560/50000]\tLoss: 4.8225\tLR: 8.304859\n",
      "Training Epoch: 84 [2688/50000]\tLoss: 4.7015\tLR: 8.305115\n",
      "Training Epoch: 84 [2816/50000]\tLoss: 4.7159\tLR: 8.305371\n",
      "Training Epoch: 84 [2944/50000]\tLoss: 4.7737\tLR: 8.305627\n",
      "Training Epoch: 84 [3072/50000]\tLoss: 4.7958\tLR: 8.305882\n",
      "Training Epoch: 84 [3200/50000]\tLoss: 4.8319\tLR: 8.306138\n",
      "Training Epoch: 84 [3328/50000]\tLoss: 4.7449\tLR: 8.306394\n",
      "Training Epoch: 84 [3456/50000]\tLoss: 4.8352\tLR: 8.306650\n",
      "Training Epoch: 84 [3584/50000]\tLoss: 4.7102\tLR: 8.306905\n",
      "Training Epoch: 84 [3712/50000]\tLoss: 4.7705\tLR: 8.307161\n",
      "Training Epoch: 84 [3840/50000]\tLoss: 4.7115\tLR: 8.307417\n",
      "Training Epoch: 84 [3968/50000]\tLoss: 4.6956\tLR: 8.307673\n",
      "Training Epoch: 84 [4096/50000]\tLoss: 4.7933\tLR: 8.307928\n",
      "Training Epoch: 84 [4224/50000]\tLoss: 4.7414\tLR: 8.308184\n",
      "Training Epoch: 84 [4352/50000]\tLoss: 4.8039\tLR: 8.308440\n",
      "Training Epoch: 84 [4480/50000]\tLoss: 4.7972\tLR: 8.308696\n",
      "Training Epoch: 84 [4608/50000]\tLoss: 4.6499\tLR: 8.308951\n",
      "Training Epoch: 84 [4736/50000]\tLoss: 4.7411\tLR: 8.309207\n",
      "Training Epoch: 84 [4864/50000]\tLoss: 4.7511\tLR: 8.309463\n",
      "Training Epoch: 84 [4992/50000]\tLoss: 4.7351\tLR: 8.309719\n",
      "Training Epoch: 84 [5120/50000]\tLoss: 4.7358\tLR: 8.309974\n",
      "Training Epoch: 84 [5248/50000]\tLoss: 4.7305\tLR: 8.310230\n",
      "Training Epoch: 84 [5376/50000]\tLoss: 4.7917\tLR: 8.310486\n",
      "Training Epoch: 84 [5504/50000]\tLoss: 4.7624\tLR: 8.310742\n",
      "Training Epoch: 84 [5632/50000]\tLoss: 4.7583\tLR: 8.310997\n",
      "Training Epoch: 84 [5760/50000]\tLoss: 4.6997\tLR: 8.311253\n",
      "Training Epoch: 84 [5888/50000]\tLoss: 4.6671\tLR: 8.311509\n",
      "Training Epoch: 84 [6016/50000]\tLoss: 4.7232\tLR: 8.311765\n",
      "Training Epoch: 84 [6144/50000]\tLoss: 4.7049\tLR: 8.312020\n",
      "Training Epoch: 84 [6272/50000]\tLoss: 4.8719\tLR: 8.312276\n",
      "Training Epoch: 84 [6400/50000]\tLoss: 4.7457\tLR: 8.312532\n",
      "Training Epoch: 84 [6528/50000]\tLoss: 4.7808\tLR: 8.312788\n",
      "Training Epoch: 84 [6656/50000]\tLoss: 4.7282\tLR: 8.313043\n",
      "Training Epoch: 84 [6784/50000]\tLoss: 4.8317\tLR: 8.313299\n",
      "Training Epoch: 84 [6912/50000]\tLoss: 4.7249\tLR: 8.313555\n",
      "Training Epoch: 84 [7040/50000]\tLoss: 4.7361\tLR: 8.313811\n",
      "Training Epoch: 84 [7168/50000]\tLoss: 4.6552\tLR: 8.314066\n",
      "Training Epoch: 84 [7296/50000]\tLoss: 4.7976\tLR: 8.314322\n",
      "Training Epoch: 84 [7424/50000]\tLoss: 4.8179\tLR: 8.314578\n",
      "Training Epoch: 84 [7552/50000]\tLoss: 4.8959\tLR: 8.314834\n",
      "Training Epoch: 84 [7680/50000]\tLoss: 4.8613\tLR: 8.315090\n",
      "Training Epoch: 84 [7808/50000]\tLoss: 4.7854\tLR: 8.315345\n",
      "Training Epoch: 84 [7936/50000]\tLoss: 4.7275\tLR: 8.315601\n",
      "Training Epoch: 84 [8064/50000]\tLoss: 4.7145\tLR: 8.315857\n",
      "Training Epoch: 84 [8192/50000]\tLoss: 4.7815\tLR: 8.316113\n",
      "Training Epoch: 84 [8320/50000]\tLoss: 4.7470\tLR: 8.316368\n",
      "Training Epoch: 84 [8448/50000]\tLoss: 4.7878\tLR: 8.316624\n",
      "Training Epoch: 84 [8576/50000]\tLoss: 4.7019\tLR: 8.316880\n",
      "Training Epoch: 84 [8704/50000]\tLoss: 4.7251\tLR: 8.317136\n",
      "Training Epoch: 84 [8832/50000]\tLoss: 4.9530\tLR: 8.317391\n",
      "Training Epoch: 84 [8960/50000]\tLoss: 4.9839\tLR: 8.317647\n",
      "Training Epoch: 84 [9088/50000]\tLoss: 4.7785\tLR: 8.317903\n",
      "Training Epoch: 84 [9216/50000]\tLoss: 4.7043\tLR: 8.318159\n",
      "Training Epoch: 84 [9344/50000]\tLoss: 4.7457\tLR: 8.318414\n",
      "Training Epoch: 84 [9472/50000]\tLoss: 4.7396\tLR: 8.318670\n",
      "Training Epoch: 84 [9600/50000]\tLoss: 4.8622\tLR: 8.318926\n",
      "Training Epoch: 84 [9728/50000]\tLoss: 4.7184\tLR: 8.319182\n",
      "Training Epoch: 84 [9856/50000]\tLoss: 4.7165\tLR: 8.319437\n",
      "Training Epoch: 84 [9984/50000]\tLoss: 4.8118\tLR: 8.319693\n",
      "Training Epoch: 84 [10112/50000]\tLoss: 4.7772\tLR: 8.319949\n",
      "Training Epoch: 84 [10240/50000]\tLoss: 4.8450\tLR: 8.320205\n",
      "Training Epoch: 84 [10368/50000]\tLoss: 4.6965\tLR: 8.320460\n",
      "Training Epoch: 84 [10496/50000]\tLoss: 4.6222\tLR: 8.320716\n",
      "Training Epoch: 84 [10624/50000]\tLoss: 4.7902\tLR: 8.320972\n",
      "Training Epoch: 84 [10752/50000]\tLoss: 4.8194\tLR: 8.321228\n",
      "Training Epoch: 84 [10880/50000]\tLoss: 4.7764\tLR: 8.321483\n",
      "Training Epoch: 84 [11008/50000]\tLoss: 4.7484\tLR: 8.321739\n",
      "Training Epoch: 84 [11136/50000]\tLoss: 4.7142\tLR: 8.321995\n",
      "Training Epoch: 84 [11264/50000]\tLoss: 4.7771\tLR: 8.322251\n",
      "Training Epoch: 84 [11392/50000]\tLoss: 4.7558\tLR: 8.322506\n",
      "Training Epoch: 84 [11520/50000]\tLoss: 4.7420\tLR: 8.322762\n",
      "Training Epoch: 84 [11648/50000]\tLoss: 4.8212\tLR: 8.323018\n",
      "Training Epoch: 84 [11776/50000]\tLoss: 4.7327\tLR: 8.323274\n",
      "Training Epoch: 84 [11904/50000]\tLoss: 4.6775\tLR: 8.323529\n",
      "Training Epoch: 84 [12032/50000]\tLoss: 4.7173\tLR: 8.323785\n",
      "Training Epoch: 84 [12160/50000]\tLoss: 4.7587\tLR: 8.324041\n",
      "Training Epoch: 84 [12288/50000]\tLoss: 4.6908\tLR: 8.324297\n",
      "Training Epoch: 84 [12416/50000]\tLoss: 4.8653\tLR: 8.324552\n",
      "Training Epoch: 84 [12544/50000]\tLoss: 4.8599\tLR: 8.324808\n",
      "Training Epoch: 84 [12672/50000]\tLoss: 4.8399\tLR: 8.325064\n",
      "Training Epoch: 84 [12800/50000]\tLoss: 4.8046\tLR: 8.325320\n",
      "Training Epoch: 84 [12928/50000]\tLoss: 4.6915\tLR: 8.325575\n",
      "Training Epoch: 84 [13056/50000]\tLoss: 4.7506\tLR: 8.325831\n",
      "Training Epoch: 84 [13184/50000]\tLoss: 4.8497\tLR: 8.326087\n",
      "Training Epoch: 84 [13312/50000]\tLoss: 4.7667\tLR: 8.326343\n",
      "Training Epoch: 84 [13440/50000]\tLoss: 4.8499\tLR: 8.326598\n",
      "Training Epoch: 84 [13568/50000]\tLoss: 4.7717\tLR: 8.326854\n",
      "Training Epoch: 84 [13696/50000]\tLoss: 4.7277\tLR: 8.327110\n",
      "Training Epoch: 84 [13824/50000]\tLoss: 4.7804\tLR: 8.327366\n",
      "Training Epoch: 84 [13952/50000]\tLoss: 4.8000\tLR: 8.327621\n",
      "Training Epoch: 84 [14080/50000]\tLoss: 4.7599\tLR: 8.327877\n",
      "Training Epoch: 84 [14208/50000]\tLoss: 4.6905\tLR: 8.328133\n",
      "Training Epoch: 84 [14336/50000]\tLoss: 4.8645\tLR: 8.328389\n",
      "Training Epoch: 84 [14464/50000]\tLoss: 4.7169\tLR: 8.328645\n",
      "Training Epoch: 84 [14592/50000]\tLoss: 4.7929\tLR: 8.328900\n",
      "Training Epoch: 84 [14720/50000]\tLoss: 4.8129\tLR: 8.329156\n",
      "Training Epoch: 84 [14848/50000]\tLoss: 4.7966\tLR: 8.329412\n",
      "Training Epoch: 84 [14976/50000]\tLoss: 4.7429\tLR: 8.329668\n",
      "Training Epoch: 84 [15104/50000]\tLoss: 4.8938\tLR: 8.329923\n",
      "Training Epoch: 84 [15232/50000]\tLoss: 4.7462\tLR: 8.330179\n",
      "Training Epoch: 84 [15360/50000]\tLoss: 4.8114\tLR: 8.330435\n",
      "Training Epoch: 84 [15488/50000]\tLoss: 4.8614\tLR: 8.330691\n",
      "Training Epoch: 84 [15616/50000]\tLoss: 4.7824\tLR: 8.330946\n",
      "Training Epoch: 84 [15744/50000]\tLoss: 4.8518\tLR: 8.331202\n",
      "Training Epoch: 84 [15872/50000]\tLoss: 4.8113\tLR: 8.331458\n",
      "Training Epoch: 84 [16000/50000]\tLoss: 4.7600\tLR: 8.331714\n",
      "Training Epoch: 84 [16128/50000]\tLoss: 4.7393\tLR: 8.331969\n",
      "Training Epoch: 84 [16256/50000]\tLoss: 4.8258\tLR: 8.332225\n",
      "Training Epoch: 84 [16384/50000]\tLoss: 4.7551\tLR: 8.332481\n",
      "Training Epoch: 84 [16512/50000]\tLoss: 4.7284\tLR: 8.332737\n",
      "Training Epoch: 84 [16640/50000]\tLoss: 4.8097\tLR: 8.332992\n",
      "Training Epoch: 84 [16768/50000]\tLoss: 4.8608\tLR: 8.333248\n",
      "Training Epoch: 84 [16896/50000]\tLoss: 4.7057\tLR: 8.333504\n",
      "Training Epoch: 84 [17024/50000]\tLoss: 4.6891\tLR: 8.333760\n",
      "Training Epoch: 84 [17152/50000]\tLoss: 4.7745\tLR: 8.334015\n",
      "Training Epoch: 84 [17280/50000]\tLoss: 4.7703\tLR: 8.334271\n",
      "Training Epoch: 84 [17408/50000]\tLoss: 4.7342\tLR: 8.334527\n",
      "Training Epoch: 84 [17536/50000]\tLoss: 4.8757\tLR: 8.334783\n",
      "Training Epoch: 84 [17664/50000]\tLoss: 4.7301\tLR: 8.335038\n",
      "Training Epoch: 84 [17792/50000]\tLoss: 4.8247\tLR: 8.335294\n",
      "Training Epoch: 84 [17920/50000]\tLoss: 4.7457\tLR: 8.335550\n",
      "Training Epoch: 84 [18048/50000]\tLoss: 4.8259\tLR: 8.335806\n",
      "Training Epoch: 84 [18176/50000]\tLoss: 4.7342\tLR: 8.336061\n",
      "Training Epoch: 84 [18304/50000]\tLoss: 4.7781\tLR: 8.336317\n",
      "Training Epoch: 84 [18432/50000]\tLoss: 4.8917\tLR: 8.336573\n",
      "Training Epoch: 84 [18560/50000]\tLoss: 4.7647\tLR: 8.336829\n",
      "Training Epoch: 84 [18688/50000]\tLoss: 4.7027\tLR: 8.337084\n",
      "Training Epoch: 84 [18816/50000]\tLoss: 4.7615\tLR: 8.337340\n",
      "Training Epoch: 84 [18944/50000]\tLoss: 4.8665\tLR: 8.337596\n",
      "Training Epoch: 84 [19072/50000]\tLoss: 4.8395\tLR: 8.337852\n",
      "Training Epoch: 84 [19200/50000]\tLoss: 4.7558\tLR: 8.338107\n",
      "Training Epoch: 84 [19328/50000]\tLoss: 4.8775\tLR: 8.338363\n",
      "Training Epoch: 84 [19456/50000]\tLoss: 4.7505\tLR: 8.338619\n",
      "Training Epoch: 84 [19584/50000]\tLoss: 4.7865\tLR: 8.338875\n",
      "Training Epoch: 84 [19712/50000]\tLoss: 4.8517\tLR: 8.339130\n",
      "Training Epoch: 84 [19840/50000]\tLoss: 4.8875\tLR: 8.339386\n",
      "Training Epoch: 84 [19968/50000]\tLoss: 4.8046\tLR: 8.339642\n",
      "Training Epoch: 84 [20096/50000]\tLoss: 4.7927\tLR: 8.339898\n",
      "Training Epoch: 84 [20224/50000]\tLoss: 4.7894\tLR: 8.340153\n",
      "Training Epoch: 84 [20352/50000]\tLoss: 4.7828\tLR: 8.340409\n",
      "Training Epoch: 84 [20480/50000]\tLoss: 4.7895\tLR: 8.340665\n",
      "Training Epoch: 84 [20608/50000]\tLoss: 4.6462\tLR: 8.340921\n",
      "Training Epoch: 84 [20736/50000]\tLoss: 4.8766\tLR: 8.341176\n",
      "Training Epoch: 84 [20864/50000]\tLoss: 4.8592\tLR: 8.341432\n",
      "Training Epoch: 84 [20992/50000]\tLoss: 4.6893\tLR: 8.341688\n",
      "Training Epoch: 84 [21120/50000]\tLoss: 4.7958\tLR: 8.341944\n",
      "Training Epoch: 84 [21248/50000]\tLoss: 4.7045\tLR: 8.342199\n",
      "Training Epoch: 84 [21376/50000]\tLoss: 4.7234\tLR: 8.342455\n",
      "Training Epoch: 84 [21504/50000]\tLoss: 4.8133\tLR: 8.342711\n",
      "Training Epoch: 84 [21632/50000]\tLoss: 4.8820\tLR: 8.342967\n",
      "Training Epoch: 84 [21760/50000]\tLoss: 4.7583\tLR: 8.343223\n",
      "Training Epoch: 84 [21888/50000]\tLoss: 4.6972\tLR: 8.343478\n",
      "Training Epoch: 84 [22016/50000]\tLoss: 4.7123\tLR: 8.343734\n",
      "Training Epoch: 84 [22144/50000]\tLoss: 4.7669\tLR: 8.343990\n",
      "Training Epoch: 84 [22272/50000]\tLoss: 4.7191\tLR: 8.344246\n",
      "Training Epoch: 84 [22400/50000]\tLoss: 4.7342\tLR: 8.344501\n",
      "Training Epoch: 84 [22528/50000]\tLoss: 4.7450\tLR: 8.344757\n",
      "Training Epoch: 84 [22656/50000]\tLoss: 4.8108\tLR: 8.345013\n",
      "Training Epoch: 84 [22784/50000]\tLoss: 4.8360\tLR: 8.345269\n",
      "Training Epoch: 84 [22912/50000]\tLoss: 4.8396\tLR: 8.345524\n",
      "Training Epoch: 84 [23040/50000]\tLoss: 4.7328\tLR: 8.345780\n",
      "Training Epoch: 84 [23168/50000]\tLoss: 4.7976\tLR: 8.346036\n",
      "Training Epoch: 84 [23296/50000]\tLoss: 4.7346\tLR: 8.346292\n",
      "Training Epoch: 84 [23424/50000]\tLoss: 4.6845\tLR: 8.346547\n",
      "Training Epoch: 84 [23552/50000]\tLoss: 4.7374\tLR: 8.346803\n",
      "Training Epoch: 84 [23680/50000]\tLoss: 4.7504\tLR: 8.347059\n",
      "Training Epoch: 84 [23808/50000]\tLoss: 4.8072\tLR: 8.347315\n",
      "Training Epoch: 84 [23936/50000]\tLoss: 4.8762\tLR: 8.347570\n",
      "Training Epoch: 84 [24064/50000]\tLoss: 4.7653\tLR: 8.347826\n",
      "Training Epoch: 84 [24192/50000]\tLoss: 4.8289\tLR: 8.348082\n",
      "Training Epoch: 84 [24320/50000]\tLoss: 4.7141\tLR: 8.348338\n",
      "Training Epoch: 84 [24448/50000]\tLoss: 4.7839\tLR: 8.348593\n",
      "Training Epoch: 84 [24576/50000]\tLoss: 4.6805\tLR: 8.348849\n",
      "Training Epoch: 84 [24704/50000]\tLoss: 4.8460\tLR: 8.349105\n",
      "Training Epoch: 84 [24832/50000]\tLoss: 4.7641\tLR: 8.349361\n",
      "Training Epoch: 84 [24960/50000]\tLoss: 4.7889\tLR: 8.349616\n",
      "Training Epoch: 84 [25088/50000]\tLoss: 4.7294\tLR: 8.349872\n",
      "Training Epoch: 84 [25216/50000]\tLoss: 4.7779\tLR: 8.350128\n",
      "Training Epoch: 84 [25344/50000]\tLoss: 4.8078\tLR: 8.350384\n",
      "Training Epoch: 84 [25472/50000]\tLoss: 4.7909\tLR: 8.350639\n",
      "Training Epoch: 84 [25600/50000]\tLoss: 4.8345\tLR: 8.350895\n",
      "Training Epoch: 84 [25728/50000]\tLoss: 4.6961\tLR: 8.351151\n",
      "Training Epoch: 84 [25856/50000]\tLoss: 4.8387\tLR: 8.351407\n",
      "Training Epoch: 84 [25984/50000]\tLoss: 4.6944\tLR: 8.351662\n",
      "Training Epoch: 84 [26112/50000]\tLoss: 4.7454\tLR: 8.351918\n",
      "Training Epoch: 84 [26240/50000]\tLoss: 4.7751\tLR: 8.352174\n",
      "Training Epoch: 84 [26368/50000]\tLoss: 4.8390\tLR: 8.352430\n",
      "Training Epoch: 84 [26496/50000]\tLoss: 4.8391\tLR: 8.352685\n",
      "Training Epoch: 84 [26624/50000]\tLoss: 4.8124\tLR: 8.352941\n",
      "Training Epoch: 84 [26752/50000]\tLoss: 4.8547\tLR: 8.353197\n",
      "Training Epoch: 84 [26880/50000]\tLoss: 4.7502\tLR: 8.353453\n",
      "Training Epoch: 84 [27008/50000]\tLoss: 4.7121\tLR: 8.353708\n",
      "Training Epoch: 84 [27136/50000]\tLoss: 4.8085\tLR: 8.353964\n",
      "Training Epoch: 84 [27264/50000]\tLoss: 4.6951\tLR: 8.354220\n",
      "Training Epoch: 84 [27392/50000]\tLoss: 4.7530\tLR: 8.354476\n",
      "Training Epoch: 84 [27520/50000]\tLoss: 4.8110\tLR: 8.354731\n",
      "Training Epoch: 84 [27648/50000]\tLoss: 4.7953\tLR: 8.354987\n",
      "Training Epoch: 84 [27776/50000]\tLoss: 4.7429\tLR: 8.355243\n",
      "Training Epoch: 84 [27904/50000]\tLoss: 4.7101\tLR: 8.355499\n",
      "Training Epoch: 84 [28032/50000]\tLoss: 4.6841\tLR: 8.355754\n",
      "Training Epoch: 84 [28160/50000]\tLoss: 4.8018\tLR: 8.356010\n",
      "Training Epoch: 84 [28288/50000]\tLoss: 4.8714\tLR: 8.356266\n",
      "Training Epoch: 84 [28416/50000]\tLoss: 4.8974\tLR: 8.356522\n",
      "Training Epoch: 84 [28544/50000]\tLoss: 4.7342\tLR: 8.356777\n",
      "Training Epoch: 84 [28672/50000]\tLoss: 4.7216\tLR: 8.357033\n",
      "Training Epoch: 84 [28800/50000]\tLoss: 4.7473\tLR: 8.357289\n",
      "Training Epoch: 84 [28928/50000]\tLoss: 4.6995\tLR: 8.357545\n",
      "Training Epoch: 84 [29056/50000]\tLoss: 4.7673\tLR: 8.357801\n",
      "Training Epoch: 84 [29184/50000]\tLoss: 4.8893\tLR: 8.358056\n",
      "Training Epoch: 84 [29312/50000]\tLoss: 4.7293\tLR: 8.358312\n",
      "Training Epoch: 84 [29440/50000]\tLoss: 4.8575\tLR: 8.358568\n",
      "Training Epoch: 84 [29568/50000]\tLoss: 4.7340\tLR: 8.358824\n",
      "Training Epoch: 84 [29696/50000]\tLoss: 4.7992\tLR: 8.359079\n",
      "Training Epoch: 84 [29824/50000]\tLoss: 4.6905\tLR: 8.359335\n",
      "Training Epoch: 84 [29952/50000]\tLoss: 4.7917\tLR: 8.359591\n",
      "Training Epoch: 84 [30080/50000]\tLoss: 4.7784\tLR: 8.359847\n",
      "Training Epoch: 84 [30208/50000]\tLoss: 4.8026\tLR: 8.360102\n",
      "Training Epoch: 84 [30336/50000]\tLoss: 4.8021\tLR: 8.360358\n",
      "Training Epoch: 84 [30464/50000]\tLoss: 4.9328\tLR: 8.360614\n",
      "Training Epoch: 84 [30592/50000]\tLoss: 4.9160\tLR: 8.360870\n",
      "Training Epoch: 84 [30720/50000]\tLoss: 4.7306\tLR: 8.361125\n",
      "Training Epoch: 84 [30848/50000]\tLoss: 4.8108\tLR: 8.361381\n",
      "Training Epoch: 84 [30976/50000]\tLoss: 4.7409\tLR: 8.361637\n",
      "Training Epoch: 84 [31104/50000]\tLoss: 4.8109\tLR: 8.361893\n",
      "Training Epoch: 84 [31232/50000]\tLoss: 4.7569\tLR: 8.362148\n",
      "Training Epoch: 84 [31360/50000]\tLoss: 4.7161\tLR: 8.362404\n",
      "Training Epoch: 84 [31488/50000]\tLoss: 4.8026\tLR: 8.362660\n",
      "Training Epoch: 84 [31616/50000]\tLoss: 4.9228\tLR: 8.362916\n",
      "Training Epoch: 84 [31744/50000]\tLoss: 4.7628\tLR: 8.363171\n",
      "Training Epoch: 84 [31872/50000]\tLoss: 4.7123\tLR: 8.363427\n",
      "Training Epoch: 84 [32000/50000]\tLoss: 4.7537\tLR: 8.363683\n",
      "Training Epoch: 84 [32128/50000]\tLoss: 4.6452\tLR: 8.363939\n",
      "Training Epoch: 84 [32256/50000]\tLoss: 4.7832\tLR: 8.364194\n",
      "Training Epoch: 84 [32384/50000]\tLoss: 4.7472\tLR: 8.364450\n",
      "Training Epoch: 84 [32512/50000]\tLoss: 4.8120\tLR: 8.364706\n",
      "Training Epoch: 84 [32640/50000]\tLoss: 4.7192\tLR: 8.364962\n",
      "Training Epoch: 84 [32768/50000]\tLoss: 4.6748\tLR: 8.365217\n",
      "Training Epoch: 84 [32896/50000]\tLoss: 4.7704\tLR: 8.365473\n",
      "Training Epoch: 84 [33024/50000]\tLoss: 4.6843\tLR: 8.365729\n",
      "Training Epoch: 84 [33152/50000]\tLoss: 4.6821\tLR: 8.365985\n",
      "Training Epoch: 84 [33280/50000]\tLoss: 4.7348\tLR: 8.366240\n",
      "Training Epoch: 84 [33408/50000]\tLoss: 4.7585\tLR: 8.366496\n",
      "Training Epoch: 84 [33536/50000]\tLoss: 4.7482\tLR: 8.366752\n",
      "Training Epoch: 84 [33664/50000]\tLoss: 4.7855\tLR: 8.367008\n",
      "Training Epoch: 84 [33792/50000]\tLoss: 4.7816\tLR: 8.367263\n",
      "Training Epoch: 84 [33920/50000]\tLoss: 4.7884\tLR: 8.367519\n",
      "Training Epoch: 84 [34048/50000]\tLoss: 4.7094\tLR: 8.367775\n",
      "Training Epoch: 84 [34176/50000]\tLoss: 4.7736\tLR: 8.368031\n",
      "Training Epoch: 84 [34304/50000]\tLoss: 4.7828\tLR: 8.368286\n",
      "Training Epoch: 84 [34432/50000]\tLoss: 4.8390\tLR: 8.368542\n",
      "Training Epoch: 84 [34560/50000]\tLoss: 4.7507\tLR: 8.368798\n",
      "Training Epoch: 84 [34688/50000]\tLoss: 4.8750\tLR: 8.369054\n",
      "Training Epoch: 84 [34816/50000]\tLoss: 4.7765\tLR: 8.369309\n",
      "Training Epoch: 84 [34944/50000]\tLoss: 4.7015\tLR: 8.369565\n",
      "Training Epoch: 84 [35072/50000]\tLoss: 4.7405\tLR: 8.369821\n",
      "Training Epoch: 84 [35200/50000]\tLoss: 4.8626\tLR: 8.370077\n",
      "Training Epoch: 84 [35328/50000]\tLoss: 4.8211\tLR: 8.370332\n",
      "Training Epoch: 84 [35456/50000]\tLoss: 4.7323\tLR: 8.370588\n",
      "Training Epoch: 84 [35584/50000]\tLoss: 4.6796\tLR: 8.370844\n",
      "Training Epoch: 84 [35712/50000]\tLoss: 4.7836\tLR: 8.371100\n",
      "Training Epoch: 84 [35840/50000]\tLoss: 4.8446\tLR: 8.371355\n",
      "Training Epoch: 84 [35968/50000]\tLoss: 4.8428\tLR: 8.371611\n",
      "Training Epoch: 84 [36096/50000]\tLoss: 4.7552\tLR: 8.371867\n",
      "Training Epoch: 84 [36224/50000]\tLoss: 4.6897\tLR: 8.372123\n",
      "Training Epoch: 84 [36352/50000]\tLoss: 4.7588\tLR: 8.372379\n",
      "Training Epoch: 84 [36480/50000]\tLoss: 4.6376\tLR: 8.372634\n",
      "Training Epoch: 84 [36608/50000]\tLoss: 4.7002\tLR: 8.372890\n",
      "Training Epoch: 84 [36736/50000]\tLoss: 4.6525\tLR: 8.373146\n",
      "Training Epoch: 84 [36864/50000]\tLoss: 4.8621\tLR: 8.373402\n",
      "Training Epoch: 84 [36992/50000]\tLoss: 4.8705\tLR: 8.373657\n",
      "Training Epoch: 84 [37120/50000]\tLoss: 4.8117\tLR: 8.373913\n",
      "Training Epoch: 84 [37248/50000]\tLoss: 4.8608\tLR: 8.374169\n",
      "Training Epoch: 84 [37376/50000]\tLoss: 4.7924\tLR: 8.374425\n",
      "Training Epoch: 84 [37504/50000]\tLoss: 4.7378\tLR: 8.374680\n",
      "Training Epoch: 84 [37632/50000]\tLoss: 4.7621\tLR: 8.374936\n",
      "Training Epoch: 84 [37760/50000]\tLoss: 4.8108\tLR: 8.375192\n",
      "Training Epoch: 84 [37888/50000]\tLoss: 4.7462\tLR: 8.375448\n",
      "Training Epoch: 84 [38016/50000]\tLoss: 4.8057\tLR: 8.375703\n",
      "Training Epoch: 84 [38144/50000]\tLoss: 4.8776\tLR: 8.375959\n",
      "Training Epoch: 84 [38272/50000]\tLoss: 4.8715\tLR: 8.376215\n",
      "Training Epoch: 84 [38400/50000]\tLoss: 4.8210\tLR: 8.376471\n",
      "Training Epoch: 84 [38528/50000]\tLoss: 4.7025\tLR: 8.376726\n",
      "Training Epoch: 84 [38656/50000]\tLoss: 4.8146\tLR: 8.376982\n",
      "Training Epoch: 84 [38784/50000]\tLoss: 4.8612\tLR: 8.377238\n",
      "Training Epoch: 84 [38912/50000]\tLoss: 4.7105\tLR: 8.377494\n",
      "Training Epoch: 84 [39040/50000]\tLoss: 4.8371\tLR: 8.377749\n",
      "Training Epoch: 84 [39168/50000]\tLoss: 4.7617\tLR: 8.378005\n",
      "Training Epoch: 84 [39296/50000]\tLoss: 4.7924\tLR: 8.378261\n",
      "Training Epoch: 84 [39424/50000]\tLoss: 4.8470\tLR: 8.378517\n",
      "Training Epoch: 84 [39552/50000]\tLoss: 4.9040\tLR: 8.378772\n",
      "Training Epoch: 84 [39680/50000]\tLoss: 4.7995\tLR: 8.379028\n",
      "Training Epoch: 84 [39808/50000]\tLoss: 4.7693\tLR: 8.379284\n",
      "Training Epoch: 84 [39936/50000]\tLoss: 4.7492\tLR: 8.379540\n",
      "Training Epoch: 84 [40064/50000]\tLoss: 4.7868\tLR: 8.379795\n",
      "Training Epoch: 84 [40192/50000]\tLoss: 4.7374\tLR: 8.380051\n",
      "Training Epoch: 84 [40320/50000]\tLoss: 4.7560\tLR: 8.380307\n",
      "Training Epoch: 84 [40448/50000]\tLoss: 4.7572\tLR: 8.380563\n",
      "Training Epoch: 84 [40576/50000]\tLoss: 4.7457\tLR: 8.380818\n",
      "Training Epoch: 84 [40704/50000]\tLoss: 4.7727\tLR: 8.381074\n",
      "Training Epoch: 84 [40832/50000]\tLoss: 4.7604\tLR: 8.381330\n",
      "Training Epoch: 84 [40960/50000]\tLoss: 4.6970\tLR: 8.381586\n",
      "Training Epoch: 84 [41088/50000]\tLoss: 4.7990\tLR: 8.381841\n",
      "Training Epoch: 84 [41216/50000]\tLoss: 4.7429\tLR: 8.382097\n",
      "Training Epoch: 84 [41344/50000]\tLoss: 4.7316\tLR: 8.382353\n",
      "Training Epoch: 84 [41472/50000]\tLoss: 4.8329\tLR: 8.382609\n",
      "Training Epoch: 84 [41600/50000]\tLoss: 4.7707\tLR: 8.382864\n",
      "Training Epoch: 84 [41728/50000]\tLoss: 4.6866\tLR: 8.383120\n",
      "Training Epoch: 84 [41856/50000]\tLoss: 4.7307\tLR: 8.383376\n",
      "Training Epoch: 84 [41984/50000]\tLoss: 4.6883\tLR: 8.383632\n",
      "Training Epoch: 84 [42112/50000]\tLoss: 4.7861\tLR: 8.383887\n",
      "Training Epoch: 84 [42240/50000]\tLoss: 4.5836\tLR: 8.384143\n",
      "Training Epoch: 84 [42368/50000]\tLoss: 4.8726\tLR: 8.384399\n",
      "Training Epoch: 84 [42496/50000]\tLoss: 4.7921\tLR: 8.384655\n",
      "Training Epoch: 84 [42624/50000]\tLoss: 4.7496\tLR: 8.384910\n",
      "Training Epoch: 84 [42752/50000]\tLoss: 4.8345\tLR: 8.385166\n",
      "Training Epoch: 84 [42880/50000]\tLoss: 4.8014\tLR: 8.385422\n",
      "Training Epoch: 84 [43008/50000]\tLoss: 4.7276\tLR: 8.385678\n",
      "Training Epoch: 84 [43136/50000]\tLoss: 4.7329\tLR: 8.385934\n",
      "Training Epoch: 84 [43264/50000]\tLoss: 4.7532\tLR: 8.386189\n",
      "Training Epoch: 84 [43392/50000]\tLoss: 4.7853\tLR: 8.386445\n",
      "Training Epoch: 84 [43520/50000]\tLoss: 4.7420\tLR: 8.386701\n",
      "Training Epoch: 84 [43648/50000]\tLoss: 4.7734\tLR: 8.386957\n",
      "Training Epoch: 84 [43776/50000]\tLoss: 4.6864\tLR: 8.387212\n",
      "Training Epoch: 84 [43904/50000]\tLoss: 4.8001\tLR: 8.387468\n",
      "Training Epoch: 84 [44032/50000]\tLoss: 4.7594\tLR: 8.387724\n",
      "Training Epoch: 84 [44160/50000]\tLoss: 4.6798\tLR: 8.387980\n",
      "Training Epoch: 84 [44288/50000]\tLoss: 4.7015\tLR: 8.388235\n",
      "Training Epoch: 84 [44416/50000]\tLoss: 4.8354\tLR: 8.388491\n",
      "Training Epoch: 84 [44544/50000]\tLoss: 4.8261\tLR: 8.388747\n",
      "Training Epoch: 84 [44672/50000]\tLoss: 4.8137\tLR: 8.389003\n",
      "Training Epoch: 84 [44800/50000]\tLoss: 4.7099\tLR: 8.389258\n",
      "Training Epoch: 84 [44928/50000]\tLoss: 4.7793\tLR: 8.389514\n",
      "Training Epoch: 84 [45056/50000]\tLoss: 4.7422\tLR: 8.389770\n",
      "Training Epoch: 84 [45184/50000]\tLoss: 4.7282\tLR: 8.390026\n",
      "Training Epoch: 84 [45312/50000]\tLoss: 4.7639\tLR: 8.390281\n",
      "Training Epoch: 84 [45440/50000]\tLoss: 4.7024\tLR: 8.390537\n",
      "Training Epoch: 84 [45568/50000]\tLoss: 4.8195\tLR: 8.390793\n",
      "Training Epoch: 84 [45696/50000]\tLoss: 4.8238\tLR: 8.391049\n",
      "Training Epoch: 84 [45824/50000]\tLoss: 4.7595\tLR: 8.391304\n",
      "Training Epoch: 84 [45952/50000]\tLoss: 4.7445\tLR: 8.391560\n",
      "Training Epoch: 84 [46080/50000]\tLoss: 4.7342\tLR: 8.391816\n",
      "Training Epoch: 84 [46208/50000]\tLoss: 4.7796\tLR: 8.392072\n",
      "Training Epoch: 84 [46336/50000]\tLoss: 4.7135\tLR: 8.392327\n",
      "Training Epoch: 84 [46464/50000]\tLoss: 4.8471\tLR: 8.392583\n",
      "Training Epoch: 84 [46592/50000]\tLoss: 4.6997\tLR: 8.392839\n",
      "Training Epoch: 84 [46720/50000]\tLoss: 4.7655\tLR: 8.393095\n",
      "Training Epoch: 84 [46848/50000]\tLoss: 4.8752\tLR: 8.393350\n",
      "Training Epoch: 84 [46976/50000]\tLoss: 4.8110\tLR: 8.393606\n",
      "Training Epoch: 84 [47104/50000]\tLoss: 4.7345\tLR: 8.393862\n",
      "Training Epoch: 84 [47232/50000]\tLoss: 4.8606\tLR: 8.394118\n",
      "Training Epoch: 84 [47360/50000]\tLoss: 4.7670\tLR: 8.394373\n",
      "Training Epoch: 84 [47488/50000]\tLoss: 4.7224\tLR: 8.394629\n",
      "Training Epoch: 84 [47616/50000]\tLoss: 4.7235\tLR: 8.394885\n",
      "Training Epoch: 84 [47744/50000]\tLoss: 4.8146\tLR: 8.395141\n",
      "Training Epoch: 84 [47872/50000]\tLoss: 4.7816\tLR: 8.395396\n",
      "Training Epoch: 84 [48000/50000]\tLoss: 4.7771\tLR: 8.395652\n",
      "Training Epoch: 84 [48128/50000]\tLoss: 4.7593\tLR: 8.395908\n",
      "Training Epoch: 84 [48256/50000]\tLoss: 4.7861\tLR: 8.396164\n",
      "Training Epoch: 84 [48384/50000]\tLoss: 4.7185\tLR: 8.396419\n",
      "Training Epoch: 84 [48512/50000]\tLoss: 4.7333\tLR: 8.396675\n",
      "Training Epoch: 84 [48640/50000]\tLoss: 4.7815\tLR: 8.396931\n",
      "Training Epoch: 84 [48768/50000]\tLoss: 4.7544\tLR: 8.397187\n",
      "Training Epoch: 84 [48896/50000]\tLoss: 4.7272\tLR: 8.397442\n",
      "Training Epoch: 84 [49024/50000]\tLoss: 4.7916\tLR: 8.397698\n",
      "Training Epoch: 84 [49152/50000]\tLoss: 4.8135\tLR: 8.397954\n",
      "Training Epoch: 84 [49280/50000]\tLoss: 4.6933\tLR: 8.398210\n",
      "Training Epoch: 84 [49408/50000]\tLoss: 4.8541\tLR: 8.398465\n",
      "Training Epoch: 84 [49536/50000]\tLoss: 4.7845\tLR: 8.398721\n",
      "Training Epoch: 84 [49664/50000]\tLoss: 4.8963\tLR: 8.398977\n",
      "Training Epoch: 84 [49792/50000]\tLoss: 4.9652\tLR: 8.399233\n",
      "Training Epoch: 84 [49920/50000]\tLoss: 4.8372\tLR: 8.399488\n",
      "Training Epoch: 84 [50000/50000]\tLoss: 4.8191\tLR: 8.399744\n",
      "epoch 84 training time consumed: 489.35s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  117762 GB |  117762 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  117400 GB |  117400 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     361 GB |     361 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  117762 GB |  117762 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  117400 GB |  117400 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     361 GB |     361 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  116107 GB |  116107 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  115745 GB |  115745 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     361 GB |     361 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   12486 K  |   12486 K  |\n",
      "|       from large pool |      24    |      65    |    5323 K  |    5323 K  |\n",
      "|       from small pool |     231    |     274    |    7163 K  |    7163 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   12486 K  |   12486 K  |\n",
      "|       from large pool |      24    |      65    |    5323 K  |    5323 K  |\n",
      "|       from small pool |     231    |     274    |    7163 K  |    7163 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      47    |    7237 K  |    7237 K  |\n",
      "|       from large pool |      10    |      23    |    2558 K  |    2558 K  |\n",
      "|       from small pool |      26    |      35    |    4679 K  |    4679 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 84, Average loss: 0.0375, Accuracy: 0.0100, Time consumed:31.31s\n",
      "\n",
      "Training Epoch: 85 [128/50000]\tLoss: 4.6853\tLR: 0.020000\n",
      "Training Epoch: 85 [256/50000]\tLoss: 4.7450\tLR: 8.400256\n",
      "Training Epoch: 85 [384/50000]\tLoss: 4.7078\tLR: 8.400512\n",
      "Training Epoch: 85 [512/50000]\tLoss: 4.7015\tLR: 8.400767\n",
      "Training Epoch: 85 [640/50000]\tLoss: 4.7864\tLR: 8.401023\n",
      "Training Epoch: 85 [768/50000]\tLoss: 4.7691\tLR: 8.401279\n",
      "Training Epoch: 85 [896/50000]\tLoss: 4.7070\tLR: 8.401535\n",
      "Training Epoch: 85 [1024/50000]\tLoss: 4.7382\tLR: 8.401790\n",
      "Training Epoch: 85 [1152/50000]\tLoss: 4.7695\tLR: 8.402046\n",
      "Training Epoch: 85 [1280/50000]\tLoss: 4.6967\tLR: 8.402302\n",
      "Training Epoch: 85 [1408/50000]\tLoss: 4.7139\tLR: 8.402558\n",
      "Training Epoch: 85 [1536/50000]\tLoss: 4.8323\tLR: 8.402813\n",
      "Training Epoch: 85 [1664/50000]\tLoss: 4.7998\tLR: 8.403069\n",
      "Training Epoch: 85 [1792/50000]\tLoss: 4.7997\tLR: 8.403325\n",
      "Training Epoch: 85 [1920/50000]\tLoss: 4.7512\tLR: 8.403581\n",
      "Training Epoch: 85 [2048/50000]\tLoss: 4.6823\tLR: 8.403836\n",
      "Training Epoch: 85 [2176/50000]\tLoss: 4.8028\tLR: 8.404092\n",
      "Training Epoch: 85 [2304/50000]\tLoss: 4.7788\tLR: 8.404348\n",
      "Training Epoch: 85 [2432/50000]\tLoss: 4.6451\tLR: 8.404604\n",
      "Training Epoch: 85 [2560/50000]\tLoss: 4.8541\tLR: 8.404859\n",
      "Training Epoch: 85 [2688/50000]\tLoss: 4.8882\tLR: 8.405115\n",
      "Training Epoch: 85 [2816/50000]\tLoss: 4.7672\tLR: 8.405371\n",
      "Training Epoch: 85 [2944/50000]\tLoss: 4.7195\tLR: 8.405627\n",
      "Training Epoch: 85 [3072/50000]\tLoss: 4.7877\tLR: 8.405882\n",
      "Training Epoch: 85 [3200/50000]\tLoss: 4.6734\tLR: 8.406138\n",
      "Training Epoch: 85 [3328/50000]\tLoss: 4.9132\tLR: 8.406394\n",
      "Training Epoch: 85 [3456/50000]\tLoss: 4.7010\tLR: 8.406650\n",
      "Training Epoch: 85 [3584/50000]\tLoss: 4.6898\tLR: 8.406905\n",
      "Training Epoch: 85 [3712/50000]\tLoss: 4.7712\tLR: 8.407161\n",
      "Training Epoch: 85 [3840/50000]\tLoss: 4.7589\tLR: 8.407417\n",
      "Training Epoch: 85 [3968/50000]\tLoss: 4.7622\tLR: 8.407673\n",
      "Training Epoch: 85 [4096/50000]\tLoss: 4.7713\tLR: 8.407928\n",
      "Training Epoch: 85 [4224/50000]\tLoss: 4.7765\tLR: 8.408184\n",
      "Training Epoch: 85 [4352/50000]\tLoss: 4.7692\tLR: 8.408440\n",
      "Training Epoch: 85 [4480/50000]\tLoss: 4.8308\tLR: 8.408696\n",
      "Training Epoch: 85 [4608/50000]\tLoss: 4.8595\tLR: 8.408951\n",
      "Training Epoch: 85 [4736/50000]\tLoss: 4.7534\tLR: 8.409207\n",
      "Training Epoch: 85 [4864/50000]\tLoss: 4.7332\tLR: 8.409463\n",
      "Training Epoch: 85 [4992/50000]\tLoss: 4.7527\tLR: 8.409719\n",
      "Training Epoch: 85 [5120/50000]\tLoss: 4.7064\tLR: 8.409974\n",
      "Training Epoch: 85 [5248/50000]\tLoss: 4.7026\tLR: 8.410230\n",
      "Training Epoch: 85 [5376/50000]\tLoss: 4.7035\tLR: 8.410486\n",
      "Training Epoch: 85 [5504/50000]\tLoss: 4.7777\tLR: 8.410742\n",
      "Training Epoch: 85 [5632/50000]\tLoss: 4.7518\tLR: 8.410997\n",
      "Training Epoch: 85 [5760/50000]\tLoss: 4.7488\tLR: 8.411253\n",
      "Training Epoch: 85 [5888/50000]\tLoss: 4.6974\tLR: 8.411509\n",
      "Training Epoch: 85 [6016/50000]\tLoss: 4.7964\tLR: 8.411765\n",
      "Training Epoch: 85 [6144/50000]\tLoss: 4.7206\tLR: 8.412020\n",
      "Training Epoch: 85 [6272/50000]\tLoss: 4.7540\tLR: 8.412276\n",
      "Training Epoch: 85 [6400/50000]\tLoss: 4.8255\tLR: 8.412532\n",
      "Training Epoch: 85 [6528/50000]\tLoss: 4.7569\tLR: 8.412788\n",
      "Training Epoch: 85 [6656/50000]\tLoss: 4.7297\tLR: 8.413043\n",
      "Training Epoch: 85 [6784/50000]\tLoss: 4.7334\tLR: 8.413299\n",
      "Training Epoch: 85 [6912/50000]\tLoss: 4.7221\tLR: 8.413555\n",
      "Training Epoch: 85 [7040/50000]\tLoss: 4.7198\tLR: 8.413811\n",
      "Training Epoch: 85 [7168/50000]\tLoss: 4.7497\tLR: 8.414066\n",
      "Training Epoch: 85 [7296/50000]\tLoss: 4.7947\tLR: 8.414322\n",
      "Training Epoch: 85 [7424/50000]\tLoss: 4.7678\tLR: 8.414578\n",
      "Training Epoch: 85 [7552/50000]\tLoss: 4.7205\tLR: 8.414834\n",
      "Training Epoch: 85 [7680/50000]\tLoss: 4.7146\tLR: 8.415090\n",
      "Training Epoch: 85 [7808/50000]\tLoss: 4.7462\tLR: 8.415345\n",
      "Training Epoch: 85 [7936/50000]\tLoss: 4.6557\tLR: 8.415601\n",
      "Training Epoch: 85 [8064/50000]\tLoss: 4.6296\tLR: 8.415857\n",
      "Training Epoch: 85 [8192/50000]\tLoss: 4.7535\tLR: 8.416113\n",
      "Training Epoch: 85 [8320/50000]\tLoss: 4.7061\tLR: 8.416368\n",
      "Training Epoch: 85 [8448/50000]\tLoss: 4.7316\tLR: 8.416624\n",
      "Training Epoch: 85 [8576/50000]\tLoss: 4.7895\tLR: 8.416880\n",
      "Training Epoch: 85 [8704/50000]\tLoss: 4.7496\tLR: 8.417136\n",
      "Training Epoch: 85 [8832/50000]\tLoss: 4.8607\tLR: 8.417391\n",
      "Training Epoch: 85 [8960/50000]\tLoss: 4.7591\tLR: 8.417647\n",
      "Training Epoch: 85 [9088/50000]\tLoss: 4.7509\tLR: 8.417903\n",
      "Training Epoch: 85 [9216/50000]\tLoss: 4.7542\tLR: 8.418159\n",
      "Training Epoch: 85 [9344/50000]\tLoss: 4.7949\tLR: 8.418414\n",
      "Training Epoch: 85 [9472/50000]\tLoss: 4.8304\tLR: 8.418670\n",
      "Training Epoch: 85 [9600/50000]\tLoss: 4.7366\tLR: 8.418926\n",
      "Training Epoch: 85 [9728/50000]\tLoss: 4.8061\tLR: 8.419182\n",
      "Training Epoch: 85 [9856/50000]\tLoss: 4.8905\tLR: 8.419437\n",
      "Training Epoch: 85 [9984/50000]\tLoss: 4.8133\tLR: 8.419693\n",
      "Training Epoch: 85 [10112/50000]\tLoss: 4.6990\tLR: 8.419949\n",
      "Training Epoch: 85 [10240/50000]\tLoss: 4.5743\tLR: 8.420205\n",
      "Training Epoch: 85 [10368/50000]\tLoss: 4.7465\tLR: 8.420460\n",
      "Training Epoch: 85 [10496/50000]\tLoss: 4.8726\tLR: 8.420716\n",
      "Training Epoch: 85 [10624/50000]\tLoss: 4.7479\tLR: 8.420972\n",
      "Training Epoch: 85 [10752/50000]\tLoss: 4.7045\tLR: 8.421228\n",
      "Training Epoch: 85 [10880/50000]\tLoss: 4.8048\tLR: 8.421483\n",
      "Training Epoch: 85 [11008/50000]\tLoss: 4.7034\tLR: 8.421739\n",
      "Training Epoch: 85 [11136/50000]\tLoss: 4.7955\tLR: 8.421995\n",
      "Training Epoch: 85 [11264/50000]\tLoss: 4.7874\tLR: 8.422251\n",
      "Training Epoch: 85 [11392/50000]\tLoss: 4.7454\tLR: 8.422506\n",
      "Training Epoch: 85 [11520/50000]\tLoss: 4.7547\tLR: 8.422762\n",
      "Training Epoch: 85 [11648/50000]\tLoss: 4.7891\tLR: 8.423018\n",
      "Training Epoch: 85 [11776/50000]\tLoss: 4.8497\tLR: 8.423274\n",
      "Training Epoch: 85 [11904/50000]\tLoss: 4.8247\tLR: 8.423529\n",
      "Training Epoch: 85 [12032/50000]\tLoss: 4.7881\tLR: 8.423785\n",
      "Training Epoch: 85 [12160/50000]\tLoss: 4.8043\tLR: 8.424041\n",
      "Training Epoch: 85 [12288/50000]\tLoss: 4.7395\tLR: 8.424297\n",
      "Training Epoch: 85 [12416/50000]\tLoss: 4.8047\tLR: 8.424552\n",
      "Training Epoch: 85 [12544/50000]\tLoss: 4.6783\tLR: 8.424808\n",
      "Training Epoch: 85 [12672/50000]\tLoss: 4.7611\tLR: 8.425064\n",
      "Training Epoch: 85 [12800/50000]\tLoss: 4.7860\tLR: 8.425320\n",
      "Training Epoch: 85 [12928/50000]\tLoss: 4.6891\tLR: 8.425575\n",
      "Training Epoch: 85 [13056/50000]\tLoss: 4.7923\tLR: 8.425831\n",
      "Training Epoch: 85 [13184/50000]\tLoss: 4.7465\tLR: 8.426087\n",
      "Training Epoch: 85 [13312/50000]\tLoss: 4.7913\tLR: 8.426343\n",
      "Training Epoch: 85 [13440/50000]\tLoss: 4.7118\tLR: 8.426598\n",
      "Training Epoch: 85 [13568/50000]\tLoss: 4.7464\tLR: 8.426854\n",
      "Training Epoch: 85 [13696/50000]\tLoss: 4.7060\tLR: 8.427110\n",
      "Training Epoch: 85 [13824/50000]\tLoss: 4.6952\tLR: 8.427366\n",
      "Training Epoch: 85 [13952/50000]\tLoss: 4.7144\tLR: 8.427621\n",
      "Training Epoch: 85 [14080/50000]\tLoss: 4.7567\tLR: 8.427877\n",
      "Training Epoch: 85 [14208/50000]\tLoss: 4.6856\tLR: 8.428133\n",
      "Training Epoch: 85 [14336/50000]\tLoss: 4.7687\tLR: 8.428389\n",
      "Training Epoch: 85 [14464/50000]\tLoss: 4.8032\tLR: 8.428645\n",
      "Training Epoch: 85 [14592/50000]\tLoss: 4.6791\tLR: 8.428900\n",
      "Training Epoch: 85 [14720/50000]\tLoss: 4.7050\tLR: 8.429156\n",
      "Training Epoch: 85 [14848/50000]\tLoss: 4.7797\tLR: 8.429412\n",
      "Training Epoch: 85 [14976/50000]\tLoss: 4.7453\tLR: 8.429668\n",
      "Training Epoch: 85 [15104/50000]\tLoss: 4.7520\tLR: 8.429923\n",
      "Training Epoch: 85 [15232/50000]\tLoss: 4.7437\tLR: 8.430179\n",
      "Training Epoch: 85 [15360/50000]\tLoss: 4.7838\tLR: 8.430435\n",
      "Training Epoch: 85 [15488/50000]\tLoss: 4.7786\tLR: 8.430691\n",
      "Training Epoch: 85 [15616/50000]\tLoss: 4.7632\tLR: 8.430946\n",
      "Training Epoch: 85 [15744/50000]\tLoss: 4.7739\tLR: 8.431202\n",
      "Training Epoch: 85 [15872/50000]\tLoss: 4.7158\tLR: 8.431458\n",
      "Training Epoch: 85 [16000/50000]\tLoss: 4.7850\tLR: 8.431714\n",
      "Training Epoch: 85 [16128/50000]\tLoss: 4.6709\tLR: 8.431969\n",
      "Training Epoch: 85 [16256/50000]\tLoss: 4.6845\tLR: 8.432225\n",
      "Training Epoch: 85 [16384/50000]\tLoss: 4.7087\tLR: 8.432481\n",
      "Training Epoch: 85 [16512/50000]\tLoss: 4.9907\tLR: 8.432737\n",
      "Training Epoch: 85 [16640/50000]\tLoss: 4.8453\tLR: 8.432992\n",
      "Training Epoch: 85 [16768/50000]\tLoss: 4.7800\tLR: 8.433248\n",
      "Training Epoch: 85 [16896/50000]\tLoss: 4.7563\tLR: 8.433504\n",
      "Training Epoch: 85 [17024/50000]\tLoss: 4.6953\tLR: 8.433760\n",
      "Training Epoch: 85 [17152/50000]\tLoss: 4.7342\tLR: 8.434015\n",
      "Training Epoch: 85 [17280/50000]\tLoss: 4.7979\tLR: 8.434271\n",
      "Training Epoch: 85 [17408/50000]\tLoss: 4.8265\tLR: 8.434527\n",
      "Training Epoch: 85 [17536/50000]\tLoss: 4.7307\tLR: 8.434783\n",
      "Training Epoch: 85 [17664/50000]\tLoss: 4.7667\tLR: 8.435038\n",
      "Training Epoch: 85 [17792/50000]\tLoss: 4.8317\tLR: 8.435294\n",
      "Training Epoch: 85 [17920/50000]\tLoss: 4.7913\tLR: 8.435550\n",
      "Training Epoch: 85 [18048/50000]\tLoss: 4.7253\tLR: 8.435806\n",
      "Training Epoch: 85 [18176/50000]\tLoss: 4.9647\tLR: 8.436061\n",
      "Training Epoch: 85 [18304/50000]\tLoss: 4.8268\tLR: 8.436317\n",
      "Training Epoch: 85 [18432/50000]\tLoss: 4.7496\tLR: 8.436573\n",
      "Training Epoch: 85 [18560/50000]\tLoss: 4.6773\tLR: 8.436829\n",
      "Training Epoch: 85 [18688/50000]\tLoss: 4.7353\tLR: 8.437084\n",
      "Training Epoch: 85 [18816/50000]\tLoss: 4.7099\tLR: 8.437340\n",
      "Training Epoch: 85 [18944/50000]\tLoss: 4.7075\tLR: 8.437596\n",
      "Training Epoch: 85 [19072/50000]\tLoss: 4.7147\tLR: 8.437852\n",
      "Training Epoch: 85 [19200/50000]\tLoss: 4.7221\tLR: 8.438107\n",
      "Training Epoch: 85 [19328/50000]\tLoss: 4.7479\tLR: 8.438363\n",
      "Training Epoch: 85 [19456/50000]\tLoss: 4.7896\tLR: 8.438619\n",
      "Training Epoch: 85 [19584/50000]\tLoss: 4.7940\tLR: 8.438875\n",
      "Training Epoch: 85 [19712/50000]\tLoss: 4.8225\tLR: 8.439130\n",
      "Training Epoch: 85 [19840/50000]\tLoss: 4.8534\tLR: 8.439386\n",
      "Training Epoch: 85 [19968/50000]\tLoss: 4.9022\tLR: 8.439642\n",
      "Training Epoch: 85 [20096/50000]\tLoss: 4.6858\tLR: 8.439898\n",
      "Training Epoch: 85 [20224/50000]\tLoss: 4.7530\tLR: 8.440153\n",
      "Training Epoch: 85 [20352/50000]\tLoss: 4.7486\tLR: 8.440409\n",
      "Training Epoch: 85 [20480/50000]\tLoss: 4.7362\tLR: 8.440665\n",
      "Training Epoch: 85 [20608/50000]\tLoss: 4.9087\tLR: 8.440921\n",
      "Training Epoch: 85 [20736/50000]\tLoss: 4.7418\tLR: 8.441176\n",
      "Training Epoch: 85 [20864/50000]\tLoss: 4.7087\tLR: 8.441432\n",
      "Training Epoch: 85 [20992/50000]\tLoss: 4.7580\tLR: 8.441688\n",
      "Training Epoch: 85 [21120/50000]\tLoss: 4.8131\tLR: 8.441944\n",
      "Training Epoch: 85 [21248/50000]\tLoss: 4.7766\tLR: 8.442199\n",
      "Training Epoch: 85 [21376/50000]\tLoss: 4.7762\tLR: 8.442455\n",
      "Training Epoch: 85 [21504/50000]\tLoss: 4.6978\tLR: 8.442711\n",
      "Training Epoch: 85 [21632/50000]\tLoss: 4.7228\tLR: 8.442967\n",
      "Training Epoch: 85 [21760/50000]\tLoss: 4.7192\tLR: 8.443223\n",
      "Training Epoch: 85 [21888/50000]\tLoss: 4.7925\tLR: 8.443478\n",
      "Training Epoch: 85 [22016/50000]\tLoss: 4.8434\tLR: 8.443734\n",
      "Training Epoch: 85 [22144/50000]\tLoss: 4.7611\tLR: 8.443990\n",
      "Training Epoch: 85 [22272/50000]\tLoss: 4.8170\tLR: 8.444246\n",
      "Training Epoch: 85 [22400/50000]\tLoss: 4.7812\tLR: 8.444501\n",
      "Training Epoch: 85 [22528/50000]\tLoss: 4.6809\tLR: 8.444757\n",
      "Training Epoch: 85 [22656/50000]\tLoss: 4.8767\tLR: 8.445013\n",
      "Training Epoch: 85 [22784/50000]\tLoss: 4.7804\tLR: 8.445269\n",
      "Training Epoch: 85 [22912/50000]\tLoss: 4.6720\tLR: 8.445524\n",
      "Training Epoch: 85 [23040/50000]\tLoss: 4.8065\tLR: 8.445780\n",
      "Training Epoch: 85 [23168/50000]\tLoss: 4.7590\tLR: 8.446036\n",
      "Training Epoch: 85 [23296/50000]\tLoss: 4.7679\tLR: 8.446292\n",
      "Training Epoch: 85 [23424/50000]\tLoss: 4.6961\tLR: 8.446547\n",
      "Training Epoch: 85 [23552/50000]\tLoss: 4.7739\tLR: 8.446803\n",
      "Training Epoch: 85 [23680/50000]\tLoss: 4.7929\tLR: 8.447059\n",
      "Training Epoch: 85 [23808/50000]\tLoss: 4.8492\tLR: 8.447315\n",
      "Training Epoch: 85 [23936/50000]\tLoss: 4.7691\tLR: 8.447570\n",
      "Training Epoch: 85 [24064/50000]\tLoss: 4.7005\tLR: 8.447826\n",
      "Training Epoch: 85 [24192/50000]\tLoss: 4.8129\tLR: 8.448082\n",
      "Training Epoch: 85 [24320/50000]\tLoss: 4.7059\tLR: 8.448338\n",
      "Training Epoch: 85 [24448/50000]\tLoss: 4.8348\tLR: 8.448593\n",
      "Training Epoch: 85 [24576/50000]\tLoss: 4.8120\tLR: 8.448849\n",
      "Training Epoch: 85 [24704/50000]\tLoss: 4.8110\tLR: 8.449105\n",
      "Training Epoch: 85 [24832/50000]\tLoss: 4.7021\tLR: 8.449361\n",
      "Training Epoch: 85 [24960/50000]\tLoss: 4.8011\tLR: 8.449616\n",
      "Training Epoch: 85 [25088/50000]\tLoss: 4.6832\tLR: 8.449872\n",
      "Training Epoch: 85 [25216/50000]\tLoss: 4.7800\tLR: 8.450128\n",
      "Training Epoch: 85 [25344/50000]\tLoss: 4.6736\tLR: 8.450384\n",
      "Training Epoch: 85 [25472/50000]\tLoss: 4.8164\tLR: 8.450639\n",
      "Training Epoch: 85 [25600/50000]\tLoss: 4.8224\tLR: 8.450895\n",
      "Training Epoch: 85 [25728/50000]\tLoss: 4.7161\tLR: 8.451151\n",
      "Training Epoch: 85 [25856/50000]\tLoss: 4.7012\tLR: 8.451407\n",
      "Training Epoch: 85 [25984/50000]\tLoss: 4.7311\tLR: 8.451662\n",
      "Training Epoch: 85 [26112/50000]\tLoss: 4.7860\tLR: 8.451918\n",
      "Training Epoch: 85 [26240/50000]\tLoss: 4.7461\tLR: 8.452174\n",
      "Training Epoch: 85 [26368/50000]\tLoss: 4.6662\tLR: 8.452430\n",
      "Training Epoch: 85 [26496/50000]\tLoss: 4.7590\tLR: 8.452685\n",
      "Training Epoch: 85 [26624/50000]\tLoss: 4.7492\tLR: 8.452941\n",
      "Training Epoch: 85 [26752/50000]\tLoss: 4.7838\tLR: 8.453197\n",
      "Training Epoch: 85 [26880/50000]\tLoss: 4.6860\tLR: 8.453453\n",
      "Training Epoch: 85 [27008/50000]\tLoss: 4.8352\tLR: 8.453708\n",
      "Training Epoch: 85 [27136/50000]\tLoss: 4.8347\tLR: 8.453964\n",
      "Training Epoch: 85 [27264/50000]\tLoss: 4.9135\tLR: 8.454220\n",
      "Training Epoch: 85 [27392/50000]\tLoss: 4.7619\tLR: 8.454476\n",
      "Training Epoch: 85 [27520/50000]\tLoss: 4.7968\tLR: 8.454731\n",
      "Training Epoch: 85 [27648/50000]\tLoss: 4.7065\tLR: 8.454987\n",
      "Training Epoch: 85 [27776/50000]\tLoss: 4.6954\tLR: 8.455243\n",
      "Training Epoch: 85 [27904/50000]\tLoss: 4.7676\tLR: 8.455499\n",
      "Training Epoch: 85 [28032/50000]\tLoss: 4.8231\tLR: 8.455754\n",
      "Training Epoch: 85 [28160/50000]\tLoss: 4.8581\tLR: 8.456010\n",
      "Training Epoch: 85 [28288/50000]\tLoss: 4.8023\tLR: 8.456266\n",
      "Training Epoch: 85 [28416/50000]\tLoss: 4.7658\tLR: 8.456522\n",
      "Training Epoch: 85 [28544/50000]\tLoss: 4.6827\tLR: 8.456777\n",
      "Training Epoch: 85 [28672/50000]\tLoss: 4.7357\tLR: 8.457033\n",
      "Training Epoch: 85 [28800/50000]\tLoss: 4.7623\tLR: 8.457289\n",
      "Training Epoch: 85 [28928/50000]\tLoss: 4.8117\tLR: 8.457545\n",
      "Training Epoch: 85 [29056/50000]\tLoss: 4.7849\tLR: 8.457801\n",
      "Training Epoch: 85 [29184/50000]\tLoss: 4.8084\tLR: 8.458056\n",
      "Training Epoch: 85 [29312/50000]\tLoss: 4.7490\tLR: 8.458312\n",
      "Training Epoch: 85 [29440/50000]\tLoss: 4.7965\tLR: 8.458568\n",
      "Training Epoch: 85 [29568/50000]\tLoss: 4.7045\tLR: 8.458824\n",
      "Training Epoch: 85 [29696/50000]\tLoss: 4.7431\tLR: 8.459079\n",
      "Training Epoch: 85 [29824/50000]\tLoss: 4.7581\tLR: 8.459335\n",
      "Training Epoch: 85 [29952/50000]\tLoss: 4.8678\tLR: 8.459591\n",
      "Training Epoch: 85 [30080/50000]\tLoss: 4.7527\tLR: 8.459847\n",
      "Training Epoch: 85 [30208/50000]\tLoss: 4.8242\tLR: 8.460102\n",
      "Training Epoch: 85 [30336/50000]\tLoss: 4.7247\tLR: 8.460358\n",
      "Training Epoch: 85 [30464/50000]\tLoss: 4.7537\tLR: 8.460614\n",
      "Training Epoch: 85 [30592/50000]\tLoss: 4.7271\tLR: 8.460870\n",
      "Training Epoch: 85 [30720/50000]\tLoss: 4.7112\tLR: 8.461125\n",
      "Training Epoch: 85 [30848/50000]\tLoss: 4.7130\tLR: 8.461381\n",
      "Training Epoch: 85 [30976/50000]\tLoss: 4.7647\tLR: 8.461637\n",
      "Training Epoch: 85 [31104/50000]\tLoss: 4.7739\tLR: 8.461893\n",
      "Training Epoch: 85 [31232/50000]\tLoss: 4.7346\tLR: 8.462148\n",
      "Training Epoch: 85 [31360/50000]\tLoss: 4.7720\tLR: 8.462404\n",
      "Training Epoch: 85 [31488/50000]\tLoss: 4.6996\tLR: 8.462660\n",
      "Training Epoch: 85 [31616/50000]\tLoss: 4.7884\tLR: 8.462916\n",
      "Training Epoch: 85 [31744/50000]\tLoss: 4.8169\tLR: 8.463171\n",
      "Training Epoch: 85 [31872/50000]\tLoss: 4.7478\tLR: 8.463427\n",
      "Training Epoch: 85 [32000/50000]\tLoss: 4.8339\tLR: 8.463683\n",
      "Training Epoch: 85 [32128/50000]\tLoss: 4.7324\tLR: 8.463939\n",
      "Training Epoch: 85 [32256/50000]\tLoss: 4.7452\tLR: 8.464194\n",
      "Training Epoch: 85 [32384/50000]\tLoss: 4.7989\tLR: 8.464450\n",
      "Training Epoch: 85 [32512/50000]\tLoss: 4.7531\tLR: 8.464706\n",
      "Training Epoch: 85 [32640/50000]\tLoss: 4.7708\tLR: 8.464962\n",
      "Training Epoch: 85 [32768/50000]\tLoss: 4.7226\tLR: 8.465217\n",
      "Training Epoch: 85 [32896/50000]\tLoss: 4.8330\tLR: 8.465473\n",
      "Training Epoch: 85 [33024/50000]\tLoss: 4.7281\tLR: 8.465729\n",
      "Training Epoch: 85 [33152/50000]\tLoss: 4.7984\tLR: 8.465985\n",
      "Training Epoch: 85 [33280/50000]\tLoss: 4.7595\tLR: 8.466240\n",
      "Training Epoch: 85 [33408/50000]\tLoss: 4.7652\tLR: 8.466496\n",
      "Training Epoch: 85 [33536/50000]\tLoss: 4.7177\tLR: 8.466752\n",
      "Training Epoch: 85 [33664/50000]\tLoss: 4.7487\tLR: 8.467008\n",
      "Training Epoch: 85 [33792/50000]\tLoss: 4.7425\tLR: 8.467263\n",
      "Training Epoch: 85 [33920/50000]\tLoss: 4.7778\tLR: 8.467519\n",
      "Training Epoch: 85 [34048/50000]\tLoss: 4.7578\tLR: 8.467775\n",
      "Training Epoch: 85 [34176/50000]\tLoss: 4.8490\tLR: 8.468031\n",
      "Training Epoch: 85 [34304/50000]\tLoss: 4.6962\tLR: 8.468286\n",
      "Training Epoch: 85 [34432/50000]\tLoss: 4.7049\tLR: 8.468542\n",
      "Training Epoch: 85 [34560/50000]\tLoss: 4.8208\tLR: 8.468798\n",
      "Training Epoch: 85 [34688/50000]\tLoss: 4.8174\tLR: 8.469054\n",
      "Training Epoch: 85 [34816/50000]\tLoss: 4.7651\tLR: 8.469309\n",
      "Training Epoch: 85 [34944/50000]\tLoss: 4.7571\tLR: 8.469565\n",
      "Training Epoch: 85 [35072/50000]\tLoss: 4.7670\tLR: 8.469821\n",
      "Training Epoch: 85 [35200/50000]\tLoss: 4.7452\tLR: 8.470077\n",
      "Training Epoch: 85 [35328/50000]\tLoss: 4.7799\tLR: 8.470332\n",
      "Training Epoch: 85 [35456/50000]\tLoss: 4.6965\tLR: 8.470588\n",
      "Training Epoch: 85 [35584/50000]\tLoss: 4.8556\tLR: 8.470844\n",
      "Training Epoch: 85 [35712/50000]\tLoss: 4.7544\tLR: 8.471100\n",
      "Training Epoch: 85 [35840/50000]\tLoss: 4.8030\tLR: 8.471355\n",
      "Training Epoch: 85 [35968/50000]\tLoss: 4.7499\tLR: 8.471611\n",
      "Training Epoch: 85 [36096/50000]\tLoss: 4.7585\tLR: 8.471867\n",
      "Training Epoch: 85 [36224/50000]\tLoss: 4.7706\tLR: 8.472123\n",
      "Training Epoch: 85 [36352/50000]\tLoss: 4.6898\tLR: 8.472379\n",
      "Training Epoch: 85 [36480/50000]\tLoss: 4.8009\tLR: 8.472634\n",
      "Training Epoch: 85 [36608/50000]\tLoss: 4.6846\tLR: 8.472890\n",
      "Training Epoch: 85 [36736/50000]\tLoss: 4.7829\tLR: 8.473146\n",
      "Training Epoch: 85 [36864/50000]\tLoss: 4.7219\tLR: 8.473402\n",
      "Training Epoch: 85 [36992/50000]\tLoss: 4.6617\tLR: 8.473657\n",
      "Training Epoch: 85 [37120/50000]\tLoss: 4.7164\tLR: 8.473913\n",
      "Training Epoch: 85 [37248/50000]\tLoss: 4.8093\tLR: 8.474169\n",
      "Training Epoch: 85 [37376/50000]\tLoss: 4.7297\tLR: 8.474425\n",
      "Training Epoch: 85 [37504/50000]\tLoss: 4.7573\tLR: 8.474680\n",
      "Training Epoch: 85 [37632/50000]\tLoss: 4.8411\tLR: 8.474936\n",
      "Training Epoch: 85 [37760/50000]\tLoss: 4.8153\tLR: 8.475192\n",
      "Training Epoch: 85 [37888/50000]\tLoss: 4.7397\tLR: 8.475448\n",
      "Training Epoch: 85 [38016/50000]\tLoss: 4.7811\tLR: 8.475703\n",
      "Training Epoch: 85 [38144/50000]\tLoss: 4.7487\tLR: 8.475959\n",
      "Training Epoch: 85 [38272/50000]\tLoss: 4.7649\tLR: 8.476215\n",
      "Training Epoch: 85 [38400/50000]\tLoss: 4.8136\tLR: 8.476471\n",
      "Training Epoch: 85 [38528/50000]\tLoss: 4.7191\tLR: 8.476726\n",
      "Training Epoch: 85 [38656/50000]\tLoss: 4.8413\tLR: 8.476982\n",
      "Training Epoch: 85 [38784/50000]\tLoss: 4.8206\tLR: 8.477238\n",
      "Training Epoch: 85 [38912/50000]\tLoss: 4.7696\tLR: 8.477494\n",
      "Training Epoch: 85 [39040/50000]\tLoss: 4.6973\tLR: 8.477749\n",
      "Training Epoch: 85 [39168/50000]\tLoss: 4.7668\tLR: 8.478005\n",
      "Training Epoch: 85 [39296/50000]\tLoss: 4.8065\tLR: 8.478261\n",
      "Training Epoch: 85 [39424/50000]\tLoss: 4.7352\tLR: 8.478517\n",
      "Training Epoch: 85 [39552/50000]\tLoss: 4.8230\tLR: 8.478772\n",
      "Training Epoch: 85 [39680/50000]\tLoss: 4.7189\tLR: 8.479028\n",
      "Training Epoch: 85 [39808/50000]\tLoss: 4.7017\tLR: 8.479284\n",
      "Training Epoch: 85 [39936/50000]\tLoss: 4.8098\tLR: 8.479540\n",
      "Training Epoch: 85 [40064/50000]\tLoss: 4.8140\tLR: 8.479795\n",
      "Training Epoch: 85 [40192/50000]\tLoss: 4.8095\tLR: 8.480051\n",
      "Training Epoch: 85 [40320/50000]\tLoss: 4.7457\tLR: 8.480307\n",
      "Training Epoch: 85 [40448/50000]\tLoss: 4.7774\tLR: 8.480563\n",
      "Training Epoch: 85 [40576/50000]\tLoss: 4.7570\tLR: 8.480818\n",
      "Training Epoch: 85 [40704/50000]\tLoss: 4.6435\tLR: 8.481074\n",
      "Training Epoch: 85 [40832/50000]\tLoss: 4.7412\tLR: 8.481330\n",
      "Training Epoch: 85 [40960/50000]\tLoss: 4.7368\tLR: 8.481586\n",
      "Training Epoch: 85 [41088/50000]\tLoss: 4.7753\tLR: 8.481841\n",
      "Training Epoch: 85 [41216/50000]\tLoss: 4.8492\tLR: 8.482097\n",
      "Training Epoch: 85 [41344/50000]\tLoss: 4.7374\tLR: 8.482353\n",
      "Training Epoch: 85 [41472/50000]\tLoss: 4.7474\tLR: 8.482609\n",
      "Training Epoch: 85 [41600/50000]\tLoss: 4.7404\tLR: 8.482864\n",
      "Training Epoch: 85 [41728/50000]\tLoss: 4.7493\tLR: 8.483120\n",
      "Training Epoch: 85 [41856/50000]\tLoss: 4.7797\tLR: 8.483376\n",
      "Training Epoch: 85 [41984/50000]\tLoss: 4.7218\tLR: 8.483632\n",
      "Training Epoch: 85 [42112/50000]\tLoss: 4.7163\tLR: 8.483887\n",
      "Training Epoch: 85 [42240/50000]\tLoss: 4.7935\tLR: 8.484143\n",
      "Training Epoch: 85 [42368/50000]\tLoss: 4.8860\tLR: 8.484399\n",
      "Training Epoch: 85 [42496/50000]\tLoss: 4.7772\tLR: 8.484655\n",
      "Training Epoch: 85 [42624/50000]\tLoss: 4.7983\tLR: 8.484910\n",
      "Training Epoch: 85 [42752/50000]\tLoss: 4.7368\tLR: 8.485166\n",
      "Training Epoch: 85 [42880/50000]\tLoss: 4.7671\tLR: 8.485422\n",
      "Training Epoch: 85 [43008/50000]\tLoss: 4.7650\tLR: 8.485678\n",
      "Training Epoch: 85 [43136/50000]\tLoss: 4.7606\tLR: 8.485934\n",
      "Training Epoch: 85 [43264/50000]\tLoss: 4.8324\tLR: 8.486189\n",
      "Training Epoch: 85 [43392/50000]\tLoss: 4.7324\tLR: 8.486445\n",
      "Training Epoch: 85 [43520/50000]\tLoss: 4.7410\tLR: 8.486701\n",
      "Training Epoch: 85 [43648/50000]\tLoss: 4.8049\tLR: 8.486957\n",
      "Training Epoch: 85 [43776/50000]\tLoss: 4.7141\tLR: 8.487212\n",
      "Training Epoch: 85 [43904/50000]\tLoss: 4.7951\tLR: 8.487468\n",
      "Training Epoch: 85 [44032/50000]\tLoss: 4.8048\tLR: 8.487724\n",
      "Training Epoch: 85 [44160/50000]\tLoss: 4.8082\tLR: 8.487980\n",
      "Training Epoch: 85 [44288/50000]\tLoss: 4.8377\tLR: 8.488235\n",
      "Training Epoch: 85 [44416/50000]\tLoss: 4.7868\tLR: 8.488491\n",
      "Training Epoch: 85 [44544/50000]\tLoss: 4.6968\tLR: 8.488747\n",
      "Training Epoch: 85 [44672/50000]\tLoss: 4.7466\tLR: 8.489003\n",
      "Training Epoch: 85 [44800/50000]\tLoss: 4.7435\tLR: 8.489258\n",
      "Training Epoch: 85 [44928/50000]\tLoss: 4.7378\tLR: 8.489514\n",
      "Training Epoch: 85 [45056/50000]\tLoss: 4.8666\tLR: 8.489770\n",
      "Training Epoch: 85 [45184/50000]\tLoss: 4.7620\tLR: 8.490026\n",
      "Training Epoch: 85 [45312/50000]\tLoss: 4.7925\tLR: 8.490281\n",
      "Training Epoch: 85 [45440/50000]\tLoss: 4.7180\tLR: 8.490537\n",
      "Training Epoch: 85 [45568/50000]\tLoss: 4.8386\tLR: 8.490793\n",
      "Training Epoch: 85 [45696/50000]\tLoss: 4.7382\tLR: 8.491049\n",
      "Training Epoch: 85 [45824/50000]\tLoss: 4.7771\tLR: 8.491304\n",
      "Training Epoch: 85 [45952/50000]\tLoss: 4.8150\tLR: 8.491560\n",
      "Training Epoch: 85 [46080/50000]\tLoss: 4.7236\tLR: 8.491816\n",
      "Training Epoch: 85 [46208/50000]\tLoss: 4.8485\tLR: 8.492072\n",
      "Training Epoch: 85 [46336/50000]\tLoss: 4.7888\tLR: 8.492327\n",
      "Training Epoch: 85 [46464/50000]\tLoss: 4.8320\tLR: 8.492583\n",
      "Training Epoch: 85 [46592/50000]\tLoss: 4.7186\tLR: 8.492839\n",
      "Training Epoch: 85 [46720/50000]\tLoss: 4.7751\tLR: 8.493095\n",
      "Training Epoch: 85 [46848/50000]\tLoss: 4.8282\tLR: 8.493350\n",
      "Training Epoch: 85 [46976/50000]\tLoss: 4.7720\tLR: 8.493606\n",
      "Training Epoch: 85 [47104/50000]\tLoss: 4.7868\tLR: 8.493862\n",
      "Training Epoch: 85 [47232/50000]\tLoss: 4.7245\tLR: 8.494118\n",
      "Training Epoch: 85 [47360/50000]\tLoss: 4.6736\tLR: 8.494373\n",
      "Training Epoch: 85 [47488/50000]\tLoss: 4.8188\tLR: 8.494629\n",
      "Training Epoch: 85 [47616/50000]\tLoss: 4.8851\tLR: 8.494885\n",
      "Training Epoch: 85 [47744/50000]\tLoss: 4.7987\tLR: 8.495141\n",
      "Training Epoch: 85 [47872/50000]\tLoss: 4.6872\tLR: 8.495396\n",
      "Training Epoch: 85 [48000/50000]\tLoss: 4.8418\tLR: 8.495652\n",
      "Training Epoch: 85 [48128/50000]\tLoss: 4.8808\tLR: 8.495908\n",
      "Training Epoch: 85 [48256/50000]\tLoss: 4.8276\tLR: 8.496164\n",
      "Training Epoch: 85 [48384/50000]\tLoss: 4.7998\tLR: 8.496419\n",
      "Training Epoch: 85 [48512/50000]\tLoss: 4.8144\tLR: 8.496675\n",
      "Training Epoch: 85 [48640/50000]\tLoss: 4.7983\tLR: 8.496931\n",
      "Training Epoch: 85 [48768/50000]\tLoss: 4.7319\tLR: 8.497187\n",
      "Training Epoch: 85 [48896/50000]\tLoss: 4.7317\tLR: 8.497442\n",
      "Training Epoch: 85 [49024/50000]\tLoss: 4.8371\tLR: 8.497698\n",
      "Training Epoch: 85 [49152/50000]\tLoss: 4.7935\tLR: 8.497954\n",
      "Training Epoch: 85 [49280/50000]\tLoss: 4.8479\tLR: 8.498210\n",
      "Training Epoch: 85 [49408/50000]\tLoss: 4.7346\tLR: 8.498465\n",
      "Training Epoch: 85 [49536/50000]\tLoss: 4.8116\tLR: 8.498721\n",
      "Training Epoch: 85 [49664/50000]\tLoss: 4.7082\tLR: 8.498977\n",
      "Training Epoch: 85 [49792/50000]\tLoss: 4.6938\tLR: 8.499233\n",
      "Training Epoch: 85 [49920/50000]\tLoss: 4.8393\tLR: 8.499488\n",
      "Training Epoch: 85 [50000/50000]\tLoss: 4.7883\tLR: 8.499744\n",
      "epoch 85 training time consumed: 489.47s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  119164 GB |  119164 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  118798 GB |  118798 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     365 GB |     365 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  119164 GB |  119164 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  118798 GB |  118798 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     365 GB |     365 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  117489 GB |  117489 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  117123 GB |  117123 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     365 GB |     365 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   12635 K  |   12635 K  |\n",
      "|       from large pool |      24    |      65    |    5386 K  |    5386 K  |\n",
      "|       from small pool |     231    |     274    |    7249 K  |    7248 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   12635 K  |   12635 K  |\n",
      "|       from large pool |      24    |      65    |    5386 K  |    5386 K  |\n",
      "|       from small pool |     231    |     274    |    7249 K  |    7248 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      47    |    7323 K  |    7323 K  |\n",
      "|       from large pool |      10    |      23    |    2589 K  |    2589 K  |\n",
      "|       from small pool |      26    |      35    |    4734 K  |    4734 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 85, Average loss: 0.0377, Accuracy: 0.0100, Time consumed:31.31s\n",
      "\n",
      "Training Epoch: 86 [128/50000]\tLoss: 4.8024\tLR: 0.020000\n",
      "Training Epoch: 86 [256/50000]\tLoss: 4.8863\tLR: 8.500256\n",
      "Training Epoch: 86 [384/50000]\tLoss: 4.7975\tLR: 8.500512\n",
      "Training Epoch: 86 [512/50000]\tLoss: 4.7111\tLR: 8.500767\n",
      "Training Epoch: 86 [640/50000]\tLoss: 4.7687\tLR: 8.501023\n",
      "Training Epoch: 86 [768/50000]\tLoss: 4.7499\tLR: 8.501279\n",
      "Training Epoch: 86 [896/50000]\tLoss: 4.6533\tLR: 8.501535\n",
      "Training Epoch: 86 [1024/50000]\tLoss: 4.7619\tLR: 8.501790\n",
      "Training Epoch: 86 [1152/50000]\tLoss: 4.8592\tLR: 8.502046\n",
      "Training Epoch: 86 [1280/50000]\tLoss: 4.8203\tLR: 8.502302\n",
      "Training Epoch: 86 [1408/50000]\tLoss: 4.8212\tLR: 8.502558\n",
      "Training Epoch: 86 [1536/50000]\tLoss: 4.7677\tLR: 8.502813\n",
      "Training Epoch: 86 [1664/50000]\tLoss: 4.8255\tLR: 8.503069\n",
      "Training Epoch: 86 [1792/50000]\tLoss: 4.6578\tLR: 8.503325\n",
      "Training Epoch: 86 [1920/50000]\tLoss: 4.7010\tLR: 8.503581\n",
      "Training Epoch: 86 [2048/50000]\tLoss: 4.7738\tLR: 8.503836\n",
      "Training Epoch: 86 [2176/50000]\tLoss: 4.7938\tLR: 8.504092\n",
      "Training Epoch: 86 [2304/50000]\tLoss: 4.6949\tLR: 8.504348\n",
      "Training Epoch: 86 [2432/50000]\tLoss: 4.7898\tLR: 8.504604\n",
      "Training Epoch: 86 [2560/50000]\tLoss: 4.7192\tLR: 8.504859\n",
      "Training Epoch: 86 [2688/50000]\tLoss: 4.8259\tLR: 8.505115\n",
      "Training Epoch: 86 [2816/50000]\tLoss: 4.7171\tLR: 8.505371\n",
      "Training Epoch: 86 [2944/50000]\tLoss: 4.7338\tLR: 8.505627\n",
      "Training Epoch: 86 [3072/50000]\tLoss: 4.8063\tLR: 8.505882\n",
      "Training Epoch: 86 [3200/50000]\tLoss: 4.7522\tLR: 8.506138\n",
      "Training Epoch: 86 [3328/50000]\tLoss: 4.7445\tLR: 8.506394\n",
      "Training Epoch: 86 [3456/50000]\tLoss: 4.6410\tLR: 8.506650\n",
      "Training Epoch: 86 [3584/50000]\tLoss: 4.7729\tLR: 8.506905\n",
      "Training Epoch: 86 [3712/50000]\tLoss: 4.7860\tLR: 8.507161\n",
      "Training Epoch: 86 [3840/50000]\tLoss: 4.8573\tLR: 8.507417\n",
      "Training Epoch: 86 [3968/50000]\tLoss: 4.7623\tLR: 8.507673\n",
      "Training Epoch: 86 [4096/50000]\tLoss: 4.7018\tLR: 8.507928\n",
      "Training Epoch: 86 [4224/50000]\tLoss: 4.8141\tLR: 8.508184\n",
      "Training Epoch: 86 [4352/50000]\tLoss: 4.7295\tLR: 8.508440\n",
      "Training Epoch: 86 [4480/50000]\tLoss: 4.7578\tLR: 8.508696\n",
      "Training Epoch: 86 [4608/50000]\tLoss: 4.7482\tLR: 8.508951\n",
      "Training Epoch: 86 [4736/50000]\tLoss: 4.7779\tLR: 8.509207\n",
      "Training Epoch: 86 [4864/50000]\tLoss: 4.7517\tLR: 8.509463\n",
      "Training Epoch: 86 [4992/50000]\tLoss: 4.7367\tLR: 8.509719\n",
      "Training Epoch: 86 [5120/50000]\tLoss: 4.7236\tLR: 8.509974\n",
      "Training Epoch: 86 [5248/50000]\tLoss: 4.7038\tLR: 8.510230\n",
      "Training Epoch: 86 [5376/50000]\tLoss: 4.6378\tLR: 8.510486\n",
      "Training Epoch: 86 [5504/50000]\tLoss: 4.7464\tLR: 8.510742\n",
      "Training Epoch: 86 [5632/50000]\tLoss: 4.7736\tLR: 8.510997\n",
      "Training Epoch: 86 [5760/50000]\tLoss: 4.8282\tLR: 8.511253\n",
      "Training Epoch: 86 [5888/50000]\tLoss: 4.7657\tLR: 8.511509\n",
      "Training Epoch: 86 [6016/50000]\tLoss: 4.7416\tLR: 8.511765\n",
      "Training Epoch: 86 [6144/50000]\tLoss: 4.7790\tLR: 8.512020\n",
      "Training Epoch: 86 [6272/50000]\tLoss: 4.8361\tLR: 8.512276\n",
      "Training Epoch: 86 [6400/50000]\tLoss: 4.7574\tLR: 8.512532\n",
      "Training Epoch: 86 [6528/50000]\tLoss: 4.7201\tLR: 8.512788\n",
      "Training Epoch: 86 [6656/50000]\tLoss: 4.8145\tLR: 8.513043\n",
      "Training Epoch: 86 [6784/50000]\tLoss: 4.7370\tLR: 8.513299\n",
      "Training Epoch: 86 [6912/50000]\tLoss: 4.7363\tLR: 8.513555\n",
      "Training Epoch: 86 [7040/50000]\tLoss: 4.7747\tLR: 8.513811\n",
      "Training Epoch: 86 [7168/50000]\tLoss: 4.7481\tLR: 8.514066\n",
      "Training Epoch: 86 [7296/50000]\tLoss: 4.7985\tLR: 8.514322\n",
      "Training Epoch: 86 [7424/50000]\tLoss: 4.7990\tLR: 8.514578\n",
      "Training Epoch: 86 [7552/50000]\tLoss: 4.8056\tLR: 8.514834\n",
      "Training Epoch: 86 [7680/50000]\tLoss: 4.7637\tLR: 8.515090\n",
      "Training Epoch: 86 [7808/50000]\tLoss: 4.8003\tLR: 8.515345\n",
      "Training Epoch: 86 [7936/50000]\tLoss: 4.8011\tLR: 8.515601\n",
      "Training Epoch: 86 [8064/50000]\tLoss: 4.7357\tLR: 8.515857\n",
      "Training Epoch: 86 [8192/50000]\tLoss: 4.7792\tLR: 8.516113\n",
      "Training Epoch: 86 [8320/50000]\tLoss: 4.7406\tLR: 8.516368\n",
      "Training Epoch: 86 [8448/50000]\tLoss: 4.7763\tLR: 8.516624\n",
      "Training Epoch: 86 [8576/50000]\tLoss: 4.7422\tLR: 8.516880\n",
      "Training Epoch: 86 [8704/50000]\tLoss: 4.7566\tLR: 8.517136\n",
      "Training Epoch: 86 [8832/50000]\tLoss: 4.7931\tLR: 8.517391\n",
      "Training Epoch: 86 [8960/50000]\tLoss: 4.7451\tLR: 8.517647\n",
      "Training Epoch: 86 [9088/50000]\tLoss: 4.7558\tLR: 8.517903\n",
      "Training Epoch: 86 [9216/50000]\tLoss: 4.7321\tLR: 8.518159\n",
      "Training Epoch: 86 [9344/50000]\tLoss: 4.8215\tLR: 8.518414\n",
      "Training Epoch: 86 [9472/50000]\tLoss: 4.8010\tLR: 8.518670\n",
      "Training Epoch: 86 [9600/50000]\tLoss: 4.8318\tLR: 8.518926\n",
      "Training Epoch: 86 [9728/50000]\tLoss: 4.7840\tLR: 8.519182\n",
      "Training Epoch: 86 [9856/50000]\tLoss: 4.7152\tLR: 8.519437\n",
      "Training Epoch: 86 [9984/50000]\tLoss: 4.7119\tLR: 8.519693\n",
      "Training Epoch: 86 [10112/50000]\tLoss: 4.8105\tLR: 8.519949\n",
      "Training Epoch: 86 [10240/50000]\tLoss: 4.8188\tLR: 8.520205\n",
      "Training Epoch: 86 [10368/50000]\tLoss: 4.7621\tLR: 8.520460\n",
      "Training Epoch: 86 [10496/50000]\tLoss: 4.6752\tLR: 8.520716\n",
      "Training Epoch: 86 [10624/50000]\tLoss: 4.8558\tLR: 8.520972\n",
      "Training Epoch: 86 [10752/50000]\tLoss: 4.7556\tLR: 8.521228\n",
      "Training Epoch: 86 [10880/50000]\tLoss: 4.7599\tLR: 8.521483\n",
      "Training Epoch: 86 [11008/50000]\tLoss: 4.7633\tLR: 8.521739\n",
      "Training Epoch: 86 [11136/50000]\tLoss: 4.7137\tLR: 8.521995\n",
      "Training Epoch: 86 [11264/50000]\tLoss: 4.7293\tLR: 8.522251\n",
      "Training Epoch: 86 [11392/50000]\tLoss: 4.7606\tLR: 8.522506\n",
      "Training Epoch: 86 [11520/50000]\tLoss: 4.8198\tLR: 8.522762\n",
      "Training Epoch: 86 [11648/50000]\tLoss: 4.7111\tLR: 8.523018\n",
      "Training Epoch: 86 [11776/50000]\tLoss: 4.9088\tLR: 8.523274\n",
      "Training Epoch: 86 [11904/50000]\tLoss: 4.8235\tLR: 8.523529\n",
      "Training Epoch: 86 [12032/50000]\tLoss: 4.6384\tLR: 8.523785\n",
      "Training Epoch: 86 [12160/50000]\tLoss: 4.7452\tLR: 8.524041\n",
      "Training Epoch: 86 [12288/50000]\tLoss: 4.7368\tLR: 8.524297\n",
      "Training Epoch: 86 [12416/50000]\tLoss: 4.7919\tLR: 8.524552\n",
      "Training Epoch: 86 [12544/50000]\tLoss: 4.7352\tLR: 8.524808\n",
      "Training Epoch: 86 [12672/50000]\tLoss: 4.6947\tLR: 8.525064\n",
      "Training Epoch: 86 [12800/50000]\tLoss: 4.7328\tLR: 8.525320\n",
      "Training Epoch: 86 [12928/50000]\tLoss: 4.6338\tLR: 8.525575\n",
      "Training Epoch: 86 [13056/50000]\tLoss: 4.8023\tLR: 8.525831\n",
      "Training Epoch: 86 [13184/50000]\tLoss: 4.7150\tLR: 8.526087\n",
      "Training Epoch: 86 [13312/50000]\tLoss: 4.7288\tLR: 8.526343\n",
      "Training Epoch: 86 [13440/50000]\tLoss: 4.7363\tLR: 8.526598\n",
      "Training Epoch: 86 [13568/50000]\tLoss: 4.7739\tLR: 8.526854\n",
      "Training Epoch: 86 [13696/50000]\tLoss: 4.7240\tLR: 8.527110\n",
      "Training Epoch: 86 [13824/50000]\tLoss: 4.7870\tLR: 8.527366\n",
      "Training Epoch: 86 [13952/50000]\tLoss: 4.7946\tLR: 8.527621\n",
      "Training Epoch: 86 [14080/50000]\tLoss: 4.7727\tLR: 8.527877\n",
      "Training Epoch: 86 [14208/50000]\tLoss: 4.8059\tLR: 8.528133\n",
      "Training Epoch: 86 [14336/50000]\tLoss: 4.8823\tLR: 8.528389\n",
      "Training Epoch: 86 [14464/50000]\tLoss: 4.7522\tLR: 8.528645\n",
      "Training Epoch: 86 [14592/50000]\tLoss: 4.8361\tLR: 8.528900\n",
      "Training Epoch: 86 [14720/50000]\tLoss: 4.7811\tLR: 8.529156\n",
      "Training Epoch: 86 [14848/50000]\tLoss: 4.8265\tLR: 8.529412\n",
      "Training Epoch: 86 [14976/50000]\tLoss: 4.8189\tLR: 8.529668\n",
      "Training Epoch: 86 [15104/50000]\tLoss: 4.8020\tLR: 8.529923\n",
      "Training Epoch: 86 [15232/50000]\tLoss: 4.7823\tLR: 8.530179\n",
      "Training Epoch: 86 [15360/50000]\tLoss: 4.7830\tLR: 8.530435\n",
      "Training Epoch: 86 [15488/50000]\tLoss: 4.7243\tLR: 8.530691\n",
      "Training Epoch: 86 [15616/50000]\tLoss: 4.7075\tLR: 8.530946\n",
      "Training Epoch: 86 [15744/50000]\tLoss: 4.7336\tLR: 8.531202\n",
      "Training Epoch: 86 [15872/50000]\tLoss: 4.8733\tLR: 8.531458\n",
      "Training Epoch: 86 [16000/50000]\tLoss: 4.8161\tLR: 8.531714\n",
      "Training Epoch: 86 [16128/50000]\tLoss: 4.8071\tLR: 8.531969\n",
      "Training Epoch: 86 [16256/50000]\tLoss: 4.7660\tLR: 8.532225\n",
      "Training Epoch: 86 [16384/50000]\tLoss: 4.7864\tLR: 8.532481\n",
      "Training Epoch: 86 [16512/50000]\tLoss: 4.6415\tLR: 8.532737\n",
      "Training Epoch: 86 [16640/50000]\tLoss: 4.7812\tLR: 8.532992\n",
      "Training Epoch: 86 [16768/50000]\tLoss: 4.6856\tLR: 8.533248\n",
      "Training Epoch: 86 [16896/50000]\tLoss: 4.7363\tLR: 8.533504\n",
      "Training Epoch: 86 [17024/50000]\tLoss: 4.9529\tLR: 8.533760\n",
      "Training Epoch: 86 [17152/50000]\tLoss: 4.8382\tLR: 8.534015\n",
      "Training Epoch: 86 [17280/50000]\tLoss: 4.6813\tLR: 8.534271\n",
      "Training Epoch: 86 [17408/50000]\tLoss: 4.7444\tLR: 8.534527\n",
      "Training Epoch: 86 [17536/50000]\tLoss: 4.6904\tLR: 8.534783\n",
      "Training Epoch: 86 [17664/50000]\tLoss: 4.7469\tLR: 8.535038\n",
      "Training Epoch: 86 [17792/50000]\tLoss: 4.7354\tLR: 8.535294\n",
      "Training Epoch: 86 [17920/50000]\tLoss: 4.7821\tLR: 8.535550\n",
      "Training Epoch: 86 [18048/50000]\tLoss: 4.8007\tLR: 8.535806\n",
      "Training Epoch: 86 [18176/50000]\tLoss: 4.7216\tLR: 8.536061\n",
      "Training Epoch: 86 [18304/50000]\tLoss: 4.9090\tLR: 8.536317\n",
      "Training Epoch: 86 [18432/50000]\tLoss: 4.7348\tLR: 8.536573\n",
      "Training Epoch: 86 [18560/50000]\tLoss: 4.6945\tLR: 8.536829\n",
      "Training Epoch: 86 [18688/50000]\tLoss: 4.7570\tLR: 8.537084\n",
      "Training Epoch: 86 [18816/50000]\tLoss: 4.7662\tLR: 8.537340\n",
      "Training Epoch: 86 [18944/50000]\tLoss: 4.9215\tLR: 8.537596\n",
      "Training Epoch: 86 [19072/50000]\tLoss: 4.7492\tLR: 8.537852\n",
      "Training Epoch: 86 [19200/50000]\tLoss: 4.7656\tLR: 8.538107\n",
      "Training Epoch: 86 [19328/50000]\tLoss: 4.6897\tLR: 8.538363\n",
      "Training Epoch: 86 [19456/50000]\tLoss: 4.7794\tLR: 8.538619\n",
      "Training Epoch: 86 [19584/50000]\tLoss: 4.7482\tLR: 8.538875\n",
      "Training Epoch: 86 [19712/50000]\tLoss: 4.7996\tLR: 8.539130\n",
      "Training Epoch: 86 [19840/50000]\tLoss: 4.7115\tLR: 8.539386\n",
      "Training Epoch: 86 [19968/50000]\tLoss: 4.7834\tLR: 8.539642\n",
      "Training Epoch: 86 [20096/50000]\tLoss: 4.6992\tLR: 8.539898\n",
      "Training Epoch: 86 [20224/50000]\tLoss: 4.6859\tLR: 8.540153\n",
      "Training Epoch: 86 [20352/50000]\tLoss: 4.8013\tLR: 8.540409\n",
      "Training Epoch: 86 [20480/50000]\tLoss: 4.7699\tLR: 8.540665\n",
      "Training Epoch: 86 [20608/50000]\tLoss: 4.7496\tLR: 8.540921\n",
      "Training Epoch: 86 [20736/50000]\tLoss: 4.6704\tLR: 8.541176\n",
      "Training Epoch: 86 [20864/50000]\tLoss: 4.7322\tLR: 8.541432\n",
      "Training Epoch: 86 [20992/50000]\tLoss: 4.7578\tLR: 8.541688\n",
      "Training Epoch: 86 [21120/50000]\tLoss: 4.7251\tLR: 8.541944\n",
      "Training Epoch: 86 [21248/50000]\tLoss: 4.7829\tLR: 8.542199\n",
      "Training Epoch: 86 [21376/50000]\tLoss: 4.7028\tLR: 8.542455\n",
      "Training Epoch: 86 [21504/50000]\tLoss: 4.7747\tLR: 8.542711\n",
      "Training Epoch: 86 [21632/50000]\tLoss: 4.6689\tLR: 8.542967\n",
      "Training Epoch: 86 [21760/50000]\tLoss: 4.7414\tLR: 8.543223\n",
      "Training Epoch: 86 [21888/50000]\tLoss: 4.8127\tLR: 8.543478\n",
      "Training Epoch: 86 [22016/50000]\tLoss: 4.6945\tLR: 8.543734\n",
      "Training Epoch: 86 [22144/50000]\tLoss: 4.7811\tLR: 8.543990\n",
      "Training Epoch: 86 [22272/50000]\tLoss: 4.7666\tLR: 8.544246\n",
      "Training Epoch: 86 [22400/50000]\tLoss: 4.9300\tLR: 8.544501\n",
      "Training Epoch: 86 [22528/50000]\tLoss: 4.8471\tLR: 8.544757\n",
      "Training Epoch: 86 [22656/50000]\tLoss: 4.7509\tLR: 8.545013\n",
      "Training Epoch: 86 [22784/50000]\tLoss: 4.6205\tLR: 8.545269\n",
      "Training Epoch: 86 [22912/50000]\tLoss: 4.7182\tLR: 8.545524\n",
      "Training Epoch: 86 [23040/50000]\tLoss: 4.7293\tLR: 8.545780\n",
      "Training Epoch: 86 [23168/50000]\tLoss: 4.7924\tLR: 8.546036\n",
      "Training Epoch: 86 [23296/50000]\tLoss: 4.7130\tLR: 8.546292\n",
      "Training Epoch: 86 [23424/50000]\tLoss: 4.7996\tLR: 8.546547\n",
      "Training Epoch: 86 [23552/50000]\tLoss: 4.8713\tLR: 8.546803\n",
      "Training Epoch: 86 [23680/50000]\tLoss: 4.8614\tLR: 8.547059\n",
      "Training Epoch: 86 [23808/50000]\tLoss: 4.7571\tLR: 8.547315\n",
      "Training Epoch: 86 [23936/50000]\tLoss: 4.7486\tLR: 8.547570\n",
      "Training Epoch: 86 [24064/50000]\tLoss: 4.7139\tLR: 8.547826\n",
      "Training Epoch: 86 [24192/50000]\tLoss: 4.7201\tLR: 8.548082\n",
      "Training Epoch: 86 [24320/50000]\tLoss: 4.7995\tLR: 8.548338\n",
      "Training Epoch: 86 [24448/50000]\tLoss: 4.8238\tLR: 8.548593\n",
      "Training Epoch: 86 [24576/50000]\tLoss: 4.7897\tLR: 8.548849\n",
      "Training Epoch: 86 [24704/50000]\tLoss: 4.7260\tLR: 8.549105\n",
      "Training Epoch: 86 [24832/50000]\tLoss: 4.7266\tLR: 8.549361\n",
      "Training Epoch: 86 [24960/50000]\tLoss: 4.7411\tLR: 8.549616\n",
      "Training Epoch: 86 [25088/50000]\tLoss: 4.7347\tLR: 8.549872\n",
      "Training Epoch: 86 [25216/50000]\tLoss: 4.7756\tLR: 8.550128\n",
      "Training Epoch: 86 [25344/50000]\tLoss: 4.7443\tLR: 8.550384\n",
      "Training Epoch: 86 [25472/50000]\tLoss: 4.7347\tLR: 8.550639\n",
      "Training Epoch: 86 [25600/50000]\tLoss: 4.7699\tLR: 8.550895\n",
      "Training Epoch: 86 [25728/50000]\tLoss: 4.7814\tLR: 8.551151\n",
      "Training Epoch: 86 [25856/50000]\tLoss: 4.7814\tLR: 8.551407\n",
      "Training Epoch: 86 [25984/50000]\tLoss: 4.8126\tLR: 8.551662\n",
      "Training Epoch: 86 [26112/50000]\tLoss: 4.8153\tLR: 8.551918\n",
      "Training Epoch: 86 [26240/50000]\tLoss: 4.8226\tLR: 8.552174\n",
      "Training Epoch: 86 [26368/50000]\tLoss: 4.8047\tLR: 8.552430\n",
      "Training Epoch: 86 [26496/50000]\tLoss: 4.8652\tLR: 8.552685\n",
      "Training Epoch: 86 [26624/50000]\tLoss: 4.7837\tLR: 8.552941\n",
      "Training Epoch: 86 [26752/50000]\tLoss: 4.8304\tLR: 8.553197\n",
      "Training Epoch: 86 [26880/50000]\tLoss: 4.7461\tLR: 8.553453\n",
      "Training Epoch: 86 [27008/50000]\tLoss: 4.7520\tLR: 8.553708\n",
      "Training Epoch: 86 [27136/50000]\tLoss: 4.7229\tLR: 8.553964\n",
      "Training Epoch: 86 [27264/50000]\tLoss: 4.7533\tLR: 8.554220\n",
      "Training Epoch: 86 [27392/50000]\tLoss: 4.8530\tLR: 8.554476\n",
      "Training Epoch: 86 [27520/50000]\tLoss: 4.8135\tLR: 8.554731\n",
      "Training Epoch: 86 [27648/50000]\tLoss: 4.9133\tLR: 8.554987\n",
      "Training Epoch: 86 [27776/50000]\tLoss: 4.8776\tLR: 8.555243\n",
      "Training Epoch: 86 [27904/50000]\tLoss: 4.7781\tLR: 8.555499\n",
      "Training Epoch: 86 [28032/50000]\tLoss: 4.7266\tLR: 8.555754\n",
      "Training Epoch: 86 [28160/50000]\tLoss: 4.8194\tLR: 8.556010\n",
      "Training Epoch: 86 [28288/50000]\tLoss: 4.7823\tLR: 8.556266\n",
      "Training Epoch: 86 [28416/50000]\tLoss: 4.7817\tLR: 8.556522\n",
      "Training Epoch: 86 [28544/50000]\tLoss: 4.8686\tLR: 8.556777\n",
      "Training Epoch: 86 [28672/50000]\tLoss: 4.8296\tLR: 8.557033\n",
      "Training Epoch: 86 [28800/50000]\tLoss: 4.8045\tLR: 8.557289\n",
      "Training Epoch: 86 [28928/50000]\tLoss: 4.7274\tLR: 8.557545\n",
      "Training Epoch: 86 [29056/50000]\tLoss: 4.9011\tLR: 8.557801\n",
      "Training Epoch: 86 [29184/50000]\tLoss: 4.7961\tLR: 8.558056\n",
      "Training Epoch: 86 [29312/50000]\tLoss: 4.7658\tLR: 8.558312\n",
      "Training Epoch: 86 [29440/50000]\tLoss: 4.7545\tLR: 8.558568\n",
      "Training Epoch: 86 [29568/50000]\tLoss: 4.8406\tLR: 8.558824\n",
      "Training Epoch: 86 [29696/50000]\tLoss: 4.7067\tLR: 8.559079\n",
      "Training Epoch: 86 [29824/50000]\tLoss: 4.8129\tLR: 8.559335\n",
      "Training Epoch: 86 [29952/50000]\tLoss: 4.8002\tLR: 8.559591\n",
      "Training Epoch: 86 [30080/50000]\tLoss: 4.7335\tLR: 8.559847\n",
      "Training Epoch: 86 [30208/50000]\tLoss: 4.6866\tLR: 8.560102\n",
      "Training Epoch: 86 [30336/50000]\tLoss: 4.6997\tLR: 8.560358\n",
      "Training Epoch: 86 [30464/50000]\tLoss: 4.9351\tLR: 8.560614\n",
      "Training Epoch: 86 [30592/50000]\tLoss: 4.7246\tLR: 8.560870\n",
      "Training Epoch: 86 [30720/50000]\tLoss: 4.7429\tLR: 8.561125\n",
      "Training Epoch: 86 [30848/50000]\tLoss: 4.7496\tLR: 8.561381\n",
      "Training Epoch: 86 [30976/50000]\tLoss: 4.7655\tLR: 8.561637\n",
      "Training Epoch: 86 [31104/50000]\tLoss: 4.7369\tLR: 8.561893\n",
      "Training Epoch: 86 [31232/50000]\tLoss: 4.6897\tLR: 8.562148\n",
      "Training Epoch: 86 [31360/50000]\tLoss: 4.7787\tLR: 8.562404\n",
      "Training Epoch: 86 [31488/50000]\tLoss: 4.8078\tLR: 8.562660\n",
      "Training Epoch: 86 [31616/50000]\tLoss: 4.7291\tLR: 8.562916\n",
      "Training Epoch: 86 [31744/50000]\tLoss: 4.8023\tLR: 8.563171\n",
      "Training Epoch: 86 [31872/50000]\tLoss: 4.7262\tLR: 8.563427\n",
      "Training Epoch: 86 [32000/50000]\tLoss: 4.7725\tLR: 8.563683\n",
      "Training Epoch: 86 [32128/50000]\tLoss: 4.7911\tLR: 8.563939\n",
      "Training Epoch: 86 [32256/50000]\tLoss: 4.7995\tLR: 8.564194\n",
      "Training Epoch: 86 [32384/50000]\tLoss: 4.6822\tLR: 8.564450\n",
      "Training Epoch: 86 [32512/50000]\tLoss: 4.7719\tLR: 8.564706\n",
      "Training Epoch: 86 [32640/50000]\tLoss: 4.7181\tLR: 8.564962\n",
      "Training Epoch: 86 [32768/50000]\tLoss: 4.8862\tLR: 8.565217\n",
      "Training Epoch: 86 [32896/50000]\tLoss: 4.7960\tLR: 8.565473\n",
      "Training Epoch: 86 [33024/50000]\tLoss: 4.8420\tLR: 8.565729\n",
      "Training Epoch: 86 [33152/50000]\tLoss: 4.8661\tLR: 8.565985\n",
      "Training Epoch: 86 [33280/50000]\tLoss: 4.7345\tLR: 8.566240\n",
      "Training Epoch: 86 [33408/50000]\tLoss: 4.7233\tLR: 8.566496\n",
      "Training Epoch: 86 [33536/50000]\tLoss: 4.8039\tLR: 8.566752\n",
      "Training Epoch: 86 [33664/50000]\tLoss: 4.7516\tLR: 8.567008\n",
      "Training Epoch: 86 [33792/50000]\tLoss: 4.6972\tLR: 8.567263\n",
      "Training Epoch: 86 [33920/50000]\tLoss: 4.6562\tLR: 8.567519\n",
      "Training Epoch: 86 [34048/50000]\tLoss: 4.7251\tLR: 8.567775\n",
      "Training Epoch: 86 [34176/50000]\tLoss: 4.6964\tLR: 8.568031\n",
      "Training Epoch: 86 [34304/50000]\tLoss: 4.7494\tLR: 8.568286\n",
      "Training Epoch: 86 [34432/50000]\tLoss: 4.7341\tLR: 8.568542\n",
      "Training Epoch: 86 [34560/50000]\tLoss: 4.7703\tLR: 8.568798\n",
      "Training Epoch: 86 [34688/50000]\tLoss: 4.7634\tLR: 8.569054\n",
      "Training Epoch: 86 [34816/50000]\tLoss: 4.8254\tLR: 8.569309\n",
      "Training Epoch: 86 [34944/50000]\tLoss: 4.7035\tLR: 8.569565\n",
      "Training Epoch: 86 [35072/50000]\tLoss: 4.7873\tLR: 8.569821\n",
      "Training Epoch: 86 [35200/50000]\tLoss: 4.7651\tLR: 8.570077\n",
      "Training Epoch: 86 [35328/50000]\tLoss: 4.6973\tLR: 8.570332\n",
      "Training Epoch: 86 [35456/50000]\tLoss: 4.7726\tLR: 8.570588\n",
      "Training Epoch: 86 [35584/50000]\tLoss: 4.7833\tLR: 8.570844\n",
      "Training Epoch: 86 [35712/50000]\tLoss: 4.7665\tLR: 8.571100\n",
      "Training Epoch: 86 [35840/50000]\tLoss: 4.7689\tLR: 8.571355\n",
      "Training Epoch: 86 [35968/50000]\tLoss: 4.6962\tLR: 8.571611\n",
      "Training Epoch: 86 [36096/50000]\tLoss: 4.6824\tLR: 8.571867\n",
      "Training Epoch: 86 [36224/50000]\tLoss: 4.8169\tLR: 8.572123\n",
      "Training Epoch: 86 [36352/50000]\tLoss: 4.9320\tLR: 8.572379\n",
      "Training Epoch: 86 [36480/50000]\tLoss: 4.8399\tLR: 8.572634\n",
      "Training Epoch: 86 [36608/50000]\tLoss: 4.7461\tLR: 8.572890\n",
      "Training Epoch: 86 [36736/50000]\tLoss: 4.6687\tLR: 8.573146\n",
      "Training Epoch: 86 [36864/50000]\tLoss: 4.8064\tLR: 8.573402\n",
      "Training Epoch: 86 [36992/50000]\tLoss: 4.7661\tLR: 8.573657\n",
      "Training Epoch: 86 [37120/50000]\tLoss: 4.8692\tLR: 8.573913\n",
      "Training Epoch: 86 [37248/50000]\tLoss: 4.7452\tLR: 8.574169\n",
      "Training Epoch: 86 [37376/50000]\tLoss: 4.8092\tLR: 8.574425\n",
      "Training Epoch: 86 [37504/50000]\tLoss: 4.7190\tLR: 8.574680\n",
      "Training Epoch: 86 [37632/50000]\tLoss: 4.7967\tLR: 8.574936\n",
      "Training Epoch: 86 [37760/50000]\tLoss: 4.7851\tLR: 8.575192\n",
      "Training Epoch: 86 [37888/50000]\tLoss: 4.7521\tLR: 8.575448\n",
      "Training Epoch: 86 [38016/50000]\tLoss: 4.7193\tLR: 8.575703\n",
      "Training Epoch: 86 [38144/50000]\tLoss: 4.7634\tLR: 8.575959\n",
      "Training Epoch: 86 [38272/50000]\tLoss: 4.7214\tLR: 8.576215\n",
      "Training Epoch: 86 [38400/50000]\tLoss: 4.6936\tLR: 8.576471\n",
      "Training Epoch: 86 [38528/50000]\tLoss: 4.8322\tLR: 8.576726\n",
      "Training Epoch: 86 [38656/50000]\tLoss: 4.7538\tLR: 8.576982\n",
      "Training Epoch: 86 [38784/50000]\tLoss: 4.7288\tLR: 8.577238\n",
      "Training Epoch: 86 [38912/50000]\tLoss: 4.7235\tLR: 8.577494\n",
      "Training Epoch: 86 [39040/50000]\tLoss: 4.8390\tLR: 8.577749\n",
      "Training Epoch: 86 [39168/50000]\tLoss: 4.8251\tLR: 8.578005\n",
      "Training Epoch: 86 [39296/50000]\tLoss: 4.6564\tLR: 8.578261\n",
      "Training Epoch: 86 [39424/50000]\tLoss: 4.7401\tLR: 8.578517\n",
      "Training Epoch: 86 [39552/50000]\tLoss: 4.6850\tLR: 8.578772\n",
      "Training Epoch: 86 [39680/50000]\tLoss: 4.7028\tLR: 8.579028\n",
      "Training Epoch: 86 [39808/50000]\tLoss: 4.7310\tLR: 8.579284\n",
      "Training Epoch: 86 [39936/50000]\tLoss: 4.8235\tLR: 8.579540\n",
      "Training Epoch: 86 [40064/50000]\tLoss: 4.8888\tLR: 8.579795\n",
      "Training Epoch: 86 [40192/50000]\tLoss: 4.7978\tLR: 8.580051\n",
      "Training Epoch: 86 [40320/50000]\tLoss: 4.8610\tLR: 8.580307\n",
      "Training Epoch: 86 [40448/50000]\tLoss: 4.7166\tLR: 8.580563\n",
      "Training Epoch: 86 [40576/50000]\tLoss: 4.7377\tLR: 8.580818\n",
      "Training Epoch: 86 [40704/50000]\tLoss: 4.7131\tLR: 8.581074\n",
      "Training Epoch: 86 [40832/50000]\tLoss: 4.7656\tLR: 8.581330\n",
      "Training Epoch: 86 [40960/50000]\tLoss: 4.7487\tLR: 8.581586\n",
      "Training Epoch: 86 [41088/50000]\tLoss: 4.8326\tLR: 8.581841\n",
      "Training Epoch: 86 [41216/50000]\tLoss: 4.7833\tLR: 8.582097\n",
      "Training Epoch: 86 [41344/50000]\tLoss: 4.7265\tLR: 8.582353\n",
      "Training Epoch: 86 [41472/50000]\tLoss: 4.8184\tLR: 8.582609\n",
      "Training Epoch: 86 [41600/50000]\tLoss: 4.7791\tLR: 8.582864\n",
      "Training Epoch: 86 [41728/50000]\tLoss: 4.7907\tLR: 8.583120\n",
      "Training Epoch: 86 [41856/50000]\tLoss: 4.6708\tLR: 8.583376\n",
      "Training Epoch: 86 [41984/50000]\tLoss: 4.7158\tLR: 8.583632\n",
      "Training Epoch: 86 [42112/50000]\tLoss: 4.7979\tLR: 8.583887\n",
      "Training Epoch: 86 [42240/50000]\tLoss: 4.6934\tLR: 8.584143\n",
      "Training Epoch: 86 [42368/50000]\tLoss: 4.7502\tLR: 8.584399\n",
      "Training Epoch: 86 [42496/50000]\tLoss: 4.8117\tLR: 8.584655\n",
      "Training Epoch: 86 [42624/50000]\tLoss: 4.7759\tLR: 8.584910\n",
      "Training Epoch: 86 [42752/50000]\tLoss: 4.7697\tLR: 8.585166\n",
      "Training Epoch: 86 [42880/50000]\tLoss: 4.6851\tLR: 8.585422\n",
      "Training Epoch: 86 [43008/50000]\tLoss: 4.8402\tLR: 8.585678\n",
      "Training Epoch: 86 [43136/50000]\tLoss: 4.7731\tLR: 8.585934\n",
      "Training Epoch: 86 [43264/50000]\tLoss: 4.7346\tLR: 8.586189\n",
      "Training Epoch: 86 [43392/50000]\tLoss: 4.7960\tLR: 8.586445\n",
      "Training Epoch: 86 [43520/50000]\tLoss: 4.7079\tLR: 8.586701\n",
      "Training Epoch: 86 [43648/50000]\tLoss: 4.8105\tLR: 8.586957\n",
      "Training Epoch: 86 [43776/50000]\tLoss: 4.6980\tLR: 8.587212\n",
      "Training Epoch: 86 [43904/50000]\tLoss: 4.7352\tLR: 8.587468\n",
      "Training Epoch: 86 [44032/50000]\tLoss: 4.7752\tLR: 8.587724\n",
      "Training Epoch: 86 [44160/50000]\tLoss: 4.7226\tLR: 8.587980\n",
      "Training Epoch: 86 [44288/50000]\tLoss: 4.8108\tLR: 8.588235\n",
      "Training Epoch: 86 [44416/50000]\tLoss: 4.7829\tLR: 8.588491\n",
      "Training Epoch: 86 [44544/50000]\tLoss: 4.6936\tLR: 8.588747\n",
      "Training Epoch: 86 [44672/50000]\tLoss: 4.7920\tLR: 8.589003\n",
      "Training Epoch: 86 [44800/50000]\tLoss: 4.7887\tLR: 8.589258\n",
      "Training Epoch: 86 [44928/50000]\tLoss: 4.6888\tLR: 8.589514\n",
      "Training Epoch: 86 [45056/50000]\tLoss: 4.8142\tLR: 8.589770\n",
      "Training Epoch: 86 [45184/50000]\tLoss: 4.8180\tLR: 8.590026\n",
      "Training Epoch: 86 [45312/50000]\tLoss: 4.9313\tLR: 8.590281\n",
      "Training Epoch: 86 [45440/50000]\tLoss: 4.8071\tLR: 8.590537\n",
      "Training Epoch: 86 [45568/50000]\tLoss: 4.7870\tLR: 8.590793\n",
      "Training Epoch: 86 [45696/50000]\tLoss: 4.8725\tLR: 8.591049\n",
      "Training Epoch: 86 [45824/50000]\tLoss: 4.8051\tLR: 8.591304\n",
      "Training Epoch: 86 [45952/50000]\tLoss: 4.7001\tLR: 8.591560\n",
      "Training Epoch: 86 [46080/50000]\tLoss: 4.8362\tLR: 8.591816\n",
      "Training Epoch: 86 [46208/50000]\tLoss: 4.6989\tLR: 8.592072\n",
      "Training Epoch: 86 [46336/50000]\tLoss: 4.7553\tLR: 8.592327\n",
      "Training Epoch: 86 [46464/50000]\tLoss: 4.7734\tLR: 8.592583\n",
      "Training Epoch: 86 [46592/50000]\tLoss: 4.6940\tLR: 8.592839\n",
      "Training Epoch: 86 [46720/50000]\tLoss: 4.7385\tLR: 8.593095\n",
      "Training Epoch: 86 [46848/50000]\tLoss: 4.8704\tLR: 8.593350\n",
      "Training Epoch: 86 [46976/50000]\tLoss: 4.8760\tLR: 8.593606\n",
      "Training Epoch: 86 [47104/50000]\tLoss: 4.8456\tLR: 8.593862\n",
      "Training Epoch: 86 [47232/50000]\tLoss: 4.7938\tLR: 8.594118\n",
      "Training Epoch: 86 [47360/50000]\tLoss: 4.8100\tLR: 8.594373\n",
      "Training Epoch: 86 [47488/50000]\tLoss: 4.7914\tLR: 8.594629\n",
      "Training Epoch: 86 [47616/50000]\tLoss: 4.8689\tLR: 8.594885\n",
      "Training Epoch: 86 [47744/50000]\tLoss: 4.8073\tLR: 8.595141\n",
      "Training Epoch: 86 [47872/50000]\tLoss: 4.7828\tLR: 8.595396\n",
      "Training Epoch: 86 [48000/50000]\tLoss: 4.7993\tLR: 8.595652\n",
      "Training Epoch: 86 [48128/50000]\tLoss: 4.7914\tLR: 8.595908\n",
      "Training Epoch: 86 [48256/50000]\tLoss: 4.8145\tLR: 8.596164\n",
      "Training Epoch: 86 [48384/50000]\tLoss: 4.8637\tLR: 8.596419\n",
      "Training Epoch: 86 [48512/50000]\tLoss: 4.6352\tLR: 8.596675\n",
      "Training Epoch: 86 [48640/50000]\tLoss: 4.8038\tLR: 8.596931\n",
      "Training Epoch: 86 [48768/50000]\tLoss: 4.8250\tLR: 8.597187\n",
      "Training Epoch: 86 [48896/50000]\tLoss: 4.8530\tLR: 8.597442\n",
      "Training Epoch: 86 [49024/50000]\tLoss: 4.8088\tLR: 8.597698\n",
      "Training Epoch: 86 [49152/50000]\tLoss: 4.8162\tLR: 8.597954\n",
      "Training Epoch: 86 [49280/50000]\tLoss: 4.7388\tLR: 8.598210\n",
      "Training Epoch: 86 [49408/50000]\tLoss: 4.7928\tLR: 8.598465\n",
      "Training Epoch: 86 [49536/50000]\tLoss: 4.7217\tLR: 8.598721\n",
      "Training Epoch: 86 [49664/50000]\tLoss: 4.7619\tLR: 8.598977\n",
      "Training Epoch: 86 [49792/50000]\tLoss: 4.7937\tLR: 8.599233\n",
      "Training Epoch: 86 [49920/50000]\tLoss: 4.7087\tLR: 8.599488\n",
      "Training Epoch: 86 [50000/50000]\tLoss: 4.7850\tLR: 8.599744\n",
      "epoch 86 training time consumed: 489.34s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  120566 GB |  120566 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  120196 GB |  120196 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     370 GB |     370 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  120566 GB |  120566 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  120196 GB |  120196 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     370 GB |     370 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  118872 GB |  118872 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  118501 GB |  118501 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     370 GB |     370 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   12784 K  |   12783 K  |\n",
      "|       from large pool |      24    |      65    |    5449 K  |    5449 K  |\n",
      "|       from small pool |     231    |     274    |    7334 K  |    7334 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   12784 K  |   12783 K  |\n",
      "|       from large pool |      24    |      65    |    5449 K  |    5449 K  |\n",
      "|       from small pool |     231    |     274    |    7334 K  |    7334 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      38    |      47    |    7410 K  |    7410 K  |\n",
      "|       from large pool |      10    |      23    |    2619 K  |    2619 K  |\n",
      "|       from small pool |      28    |      35    |    4790 K  |    4790 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 86, Average loss: 0.0379, Accuracy: 0.0100, Time consumed:31.44s\n",
      "\n",
      "Training Epoch: 87 [128/50000]\tLoss: 4.7094\tLR: 0.020000\n",
      "Training Epoch: 87 [256/50000]\tLoss: 4.8079\tLR: 8.600256\n",
      "Training Epoch: 87 [384/50000]\tLoss: 4.8155\tLR: 8.600512\n",
      "Training Epoch: 87 [512/50000]\tLoss: 4.8012\tLR: 8.600767\n",
      "Training Epoch: 87 [640/50000]\tLoss: 4.8576\tLR: 8.601023\n",
      "Training Epoch: 87 [768/50000]\tLoss: 4.8380\tLR: 8.601279\n",
      "Training Epoch: 87 [896/50000]\tLoss: 4.8443\tLR: 8.601535\n",
      "Training Epoch: 87 [1024/50000]\tLoss: 4.8338\tLR: 8.601790\n",
      "Training Epoch: 87 [1152/50000]\tLoss: 4.7578\tLR: 8.602046\n",
      "Training Epoch: 87 [1280/50000]\tLoss: 4.7882\tLR: 8.602302\n",
      "Training Epoch: 87 [1408/50000]\tLoss: 4.6892\tLR: 8.602558\n",
      "Training Epoch: 87 [1536/50000]\tLoss: 4.7700\tLR: 8.602813\n",
      "Training Epoch: 87 [1664/50000]\tLoss: 4.7711\tLR: 8.603069\n",
      "Training Epoch: 87 [1792/50000]\tLoss: 4.8317\tLR: 8.603325\n",
      "Training Epoch: 87 [1920/50000]\tLoss: 4.8421\tLR: 8.603581\n",
      "Training Epoch: 87 [2048/50000]\tLoss: 4.6990\tLR: 8.603836\n",
      "Training Epoch: 87 [2176/50000]\tLoss: 4.7841\tLR: 8.604092\n",
      "Training Epoch: 87 [2304/50000]\tLoss: 4.6871\tLR: 8.604348\n",
      "Training Epoch: 87 [2432/50000]\tLoss: 4.5926\tLR: 8.604604\n",
      "Training Epoch: 87 [2560/50000]\tLoss: 4.7952\tLR: 8.604859\n",
      "Training Epoch: 87 [2688/50000]\tLoss: 4.8184\tLR: 8.605115\n",
      "Training Epoch: 87 [2816/50000]\tLoss: 4.6060\tLR: 8.605371\n",
      "Training Epoch: 87 [2944/50000]\tLoss: 4.8099\tLR: 8.605627\n",
      "Training Epoch: 87 [3072/50000]\tLoss: 4.8734\tLR: 8.605882\n",
      "Training Epoch: 87 [3200/50000]\tLoss: 4.7015\tLR: 8.606138\n",
      "Training Epoch: 87 [3328/50000]\tLoss: 4.7852\tLR: 8.606394\n",
      "Training Epoch: 87 [3456/50000]\tLoss: 4.7832\tLR: 8.606650\n",
      "Training Epoch: 87 [3584/50000]\tLoss: 4.7587\tLR: 8.606905\n",
      "Training Epoch: 87 [3712/50000]\tLoss: 4.7332\tLR: 8.607161\n",
      "Training Epoch: 87 [3840/50000]\tLoss: 4.7935\tLR: 8.607417\n",
      "Training Epoch: 87 [3968/50000]\tLoss: 4.7175\tLR: 8.607673\n",
      "Training Epoch: 87 [4096/50000]\tLoss: 4.7071\tLR: 8.607928\n",
      "Training Epoch: 87 [4224/50000]\tLoss: 4.7836\tLR: 8.608184\n",
      "Training Epoch: 87 [4352/50000]\tLoss: 4.7678\tLR: 8.608440\n",
      "Training Epoch: 87 [4480/50000]\tLoss: 4.8218\tLR: 8.608696\n",
      "Training Epoch: 87 [4608/50000]\tLoss: 4.7332\tLR: 8.608951\n",
      "Training Epoch: 87 [4736/50000]\tLoss: 4.7245\tLR: 8.609207\n",
      "Training Epoch: 87 [4864/50000]\tLoss: 4.8211\tLR: 8.609463\n",
      "Training Epoch: 87 [4992/50000]\tLoss: 4.7216\tLR: 8.609719\n",
      "Training Epoch: 87 [5120/50000]\tLoss: 4.7026\tLR: 8.609974\n",
      "Training Epoch: 87 [5248/50000]\tLoss: 4.8059\tLR: 8.610230\n",
      "Training Epoch: 87 [5376/50000]\tLoss: 4.8870\tLR: 8.610486\n",
      "Training Epoch: 87 [5504/50000]\tLoss: 4.7479\tLR: 8.610742\n",
      "Training Epoch: 87 [5632/50000]\tLoss: 4.6989\tLR: 8.610997\n",
      "Training Epoch: 87 [5760/50000]\tLoss: 4.7258\tLR: 8.611253\n",
      "Training Epoch: 87 [5888/50000]\tLoss: 4.7394\tLR: 8.611509\n",
      "Training Epoch: 87 [6016/50000]\tLoss: 4.8019\tLR: 8.611765\n",
      "Training Epoch: 87 [6144/50000]\tLoss: 4.6751\tLR: 8.612020\n",
      "Training Epoch: 87 [6272/50000]\tLoss: 4.8122\tLR: 8.612276\n",
      "Training Epoch: 87 [6400/50000]\tLoss: 4.6736\tLR: 8.612532\n",
      "Training Epoch: 87 [6528/50000]\tLoss: 4.8714\tLR: 8.612788\n",
      "Training Epoch: 87 [6656/50000]\tLoss: 4.8282\tLR: 8.613043\n",
      "Training Epoch: 87 [6784/50000]\tLoss: 4.7835\tLR: 8.613299\n",
      "Training Epoch: 87 [6912/50000]\tLoss: 4.7470\tLR: 8.613555\n",
      "Training Epoch: 87 [7040/50000]\tLoss: 4.8039\tLR: 8.613811\n",
      "Training Epoch: 87 [7168/50000]\tLoss: 4.7759\tLR: 8.614066\n",
      "Training Epoch: 87 [7296/50000]\tLoss: 4.7577\tLR: 8.614322\n",
      "Training Epoch: 87 [7424/50000]\tLoss: 4.7021\tLR: 8.614578\n",
      "Training Epoch: 87 [7552/50000]\tLoss: 4.7415\tLR: 8.614834\n",
      "Training Epoch: 87 [7680/50000]\tLoss: 4.8153\tLR: 8.615090\n",
      "Training Epoch: 87 [7808/50000]\tLoss: 4.7657\tLR: 8.615345\n",
      "Training Epoch: 87 [7936/50000]\tLoss: 4.7558\tLR: 8.615601\n",
      "Training Epoch: 87 [8064/50000]\tLoss: 4.7136\tLR: 8.615857\n",
      "Training Epoch: 87 [8192/50000]\tLoss: 4.6598\tLR: 8.616113\n",
      "Training Epoch: 87 [8320/50000]\tLoss: 4.7458\tLR: 8.616368\n",
      "Training Epoch: 87 [8448/50000]\tLoss: 4.6462\tLR: 8.616624\n",
      "Training Epoch: 87 [8576/50000]\tLoss: 4.7464\tLR: 8.616880\n",
      "Training Epoch: 87 [8704/50000]\tLoss: 4.8164\tLR: 8.617136\n",
      "Training Epoch: 87 [8832/50000]\tLoss: 4.7617\tLR: 8.617391\n",
      "Training Epoch: 87 [8960/50000]\tLoss: 4.8198\tLR: 8.617647\n",
      "Training Epoch: 87 [9088/50000]\tLoss: 4.8794\tLR: 8.617903\n",
      "Training Epoch: 87 [9216/50000]\tLoss: 4.7986\tLR: 8.618159\n",
      "Training Epoch: 87 [9344/50000]\tLoss: 4.8475\tLR: 8.618414\n",
      "Training Epoch: 87 [9472/50000]\tLoss: 4.7595\tLR: 8.618670\n",
      "Training Epoch: 87 [9600/50000]\tLoss: 4.6358\tLR: 8.618926\n",
      "Training Epoch: 87 [9728/50000]\tLoss: 4.6854\tLR: 8.619182\n",
      "Training Epoch: 87 [9856/50000]\tLoss: 4.7313\tLR: 8.619437\n",
      "Training Epoch: 87 [9984/50000]\tLoss: 4.7471\tLR: 8.619693\n",
      "Training Epoch: 87 [10112/50000]\tLoss: 4.7654\tLR: 8.619949\n",
      "Training Epoch: 87 [10240/50000]\tLoss: 4.8715\tLR: 8.620205\n",
      "Training Epoch: 87 [10368/50000]\tLoss: 4.8311\tLR: 8.620460\n",
      "Training Epoch: 87 [10496/50000]\tLoss: 4.7925\tLR: 8.620716\n",
      "Training Epoch: 87 [10624/50000]\tLoss: 4.8413\tLR: 8.620972\n",
      "Training Epoch: 87 [10752/50000]\tLoss: 4.7746\tLR: 8.621228\n",
      "Training Epoch: 87 [10880/50000]\tLoss: 4.8128\tLR: 8.621483\n",
      "Training Epoch: 87 [11008/50000]\tLoss: 4.7251\tLR: 8.621739\n",
      "Training Epoch: 87 [11136/50000]\tLoss: 4.8673\tLR: 8.621995\n",
      "Training Epoch: 87 [11264/50000]\tLoss: 4.8209\tLR: 8.622251\n",
      "Training Epoch: 87 [11392/50000]\tLoss: 4.8214\tLR: 8.622506\n",
      "Training Epoch: 87 [11520/50000]\tLoss: 4.7346\tLR: 8.622762\n",
      "Training Epoch: 87 [11648/50000]\tLoss: 4.7949\tLR: 8.623018\n",
      "Training Epoch: 87 [11776/50000]\tLoss: 4.8554\tLR: 8.623274\n",
      "Training Epoch: 87 [11904/50000]\tLoss: 4.8075\tLR: 8.623529\n",
      "Training Epoch: 87 [12032/50000]\tLoss: 4.7591\tLR: 8.623785\n",
      "Training Epoch: 87 [12160/50000]\tLoss: 4.7877\tLR: 8.624041\n",
      "Training Epoch: 87 [12288/50000]\tLoss: 4.8038\tLR: 8.624297\n",
      "Training Epoch: 87 [12416/50000]\tLoss: 4.7911\tLR: 8.624552\n",
      "Training Epoch: 87 [12544/50000]\tLoss: 4.7515\tLR: 8.624808\n",
      "Training Epoch: 87 [12672/50000]\tLoss: 4.7465\tLR: 8.625064\n",
      "Training Epoch: 87 [12800/50000]\tLoss: 4.8014\tLR: 8.625320\n",
      "Training Epoch: 87 [12928/50000]\tLoss: 4.5935\tLR: 8.625575\n",
      "Training Epoch: 87 [13056/50000]\tLoss: 4.6965\tLR: 8.625831\n",
      "Training Epoch: 87 [13184/50000]\tLoss: 4.8005\tLR: 8.626087\n",
      "Training Epoch: 87 [13312/50000]\tLoss: 4.7957\tLR: 8.626343\n",
      "Training Epoch: 87 [13440/50000]\tLoss: 4.7390\tLR: 8.626598\n",
      "Training Epoch: 87 [13568/50000]\tLoss: 4.7004\tLR: 8.626854\n",
      "Training Epoch: 87 [13696/50000]\tLoss: 4.7941\tLR: 8.627110\n",
      "Training Epoch: 87 [13824/50000]\tLoss: 4.7742\tLR: 8.627366\n",
      "Training Epoch: 87 [13952/50000]\tLoss: 4.7582\tLR: 8.627621\n",
      "Training Epoch: 87 [14080/50000]\tLoss: 4.6467\tLR: 8.627877\n",
      "Training Epoch: 87 [14208/50000]\tLoss: 4.7867\tLR: 8.628133\n",
      "Training Epoch: 87 [14336/50000]\tLoss: 4.8208\tLR: 8.628389\n",
      "Training Epoch: 87 [14464/50000]\tLoss: 4.7620\tLR: 8.628645\n",
      "Training Epoch: 87 [14592/50000]\tLoss: 4.8051\tLR: 8.628900\n",
      "Training Epoch: 87 [14720/50000]\tLoss: 4.7423\tLR: 8.629156\n",
      "Training Epoch: 87 [14848/50000]\tLoss: 4.7793\tLR: 8.629412\n",
      "Training Epoch: 87 [14976/50000]\tLoss: 4.7409\tLR: 8.629668\n",
      "Training Epoch: 87 [15104/50000]\tLoss: 4.7668\tLR: 8.629923\n",
      "Training Epoch: 87 [15232/50000]\tLoss: 4.7958\tLR: 8.630179\n",
      "Training Epoch: 87 [15360/50000]\tLoss: 4.7160\tLR: 8.630435\n",
      "Training Epoch: 87 [15488/50000]\tLoss: 4.9027\tLR: 8.630691\n",
      "Training Epoch: 87 [15616/50000]\tLoss: 4.7464\tLR: 8.630946\n",
      "Training Epoch: 87 [15744/50000]\tLoss: 4.7154\tLR: 8.631202\n",
      "Training Epoch: 87 [15872/50000]\tLoss: 4.7757\tLR: 8.631458\n",
      "Training Epoch: 87 [16000/50000]\tLoss: 4.8327\tLR: 8.631714\n",
      "Training Epoch: 87 [16128/50000]\tLoss: 4.7639\tLR: 8.631969\n",
      "Training Epoch: 87 [16256/50000]\tLoss: 4.7563\tLR: 8.632225\n",
      "Training Epoch: 87 [16384/50000]\tLoss: 4.7534\tLR: 8.632481\n",
      "Training Epoch: 87 [16512/50000]\tLoss: 4.7444\tLR: 8.632737\n",
      "Training Epoch: 87 [16640/50000]\tLoss: 4.6189\tLR: 8.632992\n",
      "Training Epoch: 87 [16768/50000]\tLoss: 4.6745\tLR: 8.633248\n",
      "Training Epoch: 87 [16896/50000]\tLoss: 4.8039\tLR: 8.633504\n",
      "Training Epoch: 87 [17024/50000]\tLoss: 4.6464\tLR: 8.633760\n",
      "Training Epoch: 87 [17152/50000]\tLoss: 4.8161\tLR: 8.634015\n",
      "Training Epoch: 87 [17280/50000]\tLoss: 4.7750\tLR: 8.634271\n",
      "Training Epoch: 87 [17408/50000]\tLoss: 4.7825\tLR: 8.634527\n",
      "Training Epoch: 87 [17536/50000]\tLoss: 4.6897\tLR: 8.634783\n",
      "Training Epoch: 87 [17664/50000]\tLoss: 4.7359\tLR: 8.635038\n",
      "Training Epoch: 87 [17792/50000]\tLoss: 4.7068\tLR: 8.635294\n",
      "Training Epoch: 87 [17920/50000]\tLoss: 4.8245\tLR: 8.635550\n",
      "Training Epoch: 87 [18048/50000]\tLoss: 4.7906\tLR: 8.635806\n",
      "Training Epoch: 87 [18176/50000]\tLoss: 4.8089\tLR: 8.636061\n",
      "Training Epoch: 87 [18304/50000]\tLoss: 4.8223\tLR: 8.636317\n",
      "Training Epoch: 87 [18432/50000]\tLoss: 4.7854\tLR: 8.636573\n",
      "Training Epoch: 87 [18560/50000]\tLoss: 4.6982\tLR: 8.636829\n",
      "Training Epoch: 87 [18688/50000]\tLoss: 4.6796\tLR: 8.637084\n",
      "Training Epoch: 87 [18816/50000]\tLoss: 4.7695\tLR: 8.637340\n",
      "Training Epoch: 87 [18944/50000]\tLoss: 4.7230\tLR: 8.637596\n",
      "Training Epoch: 87 [19072/50000]\tLoss: 4.7984\tLR: 8.637852\n",
      "Training Epoch: 87 [19200/50000]\tLoss: 4.6858\tLR: 8.638107\n",
      "Training Epoch: 87 [19328/50000]\tLoss: 4.7714\tLR: 8.638363\n",
      "Training Epoch: 87 [19456/50000]\tLoss: 4.8634\tLR: 8.638619\n",
      "Training Epoch: 87 [19584/50000]\tLoss: 4.8110\tLR: 8.638875\n",
      "Training Epoch: 87 [19712/50000]\tLoss: 4.8385\tLR: 8.639130\n",
      "Training Epoch: 87 [19840/50000]\tLoss: 4.7950\tLR: 8.639386\n",
      "Training Epoch: 87 [19968/50000]\tLoss: 4.8075\tLR: 8.639642\n",
      "Training Epoch: 87 [20096/50000]\tLoss: 4.7678\tLR: 8.639898\n",
      "Training Epoch: 87 [20224/50000]\tLoss: 4.7470\tLR: 8.640153\n",
      "Training Epoch: 87 [20352/50000]\tLoss: 4.8022\tLR: 8.640409\n",
      "Training Epoch: 87 [20480/50000]\tLoss: 4.7230\tLR: 8.640665\n",
      "Training Epoch: 87 [20608/50000]\tLoss: 4.7653\tLR: 8.640921\n",
      "Training Epoch: 87 [20736/50000]\tLoss: 4.6486\tLR: 8.641176\n",
      "Training Epoch: 87 [20864/50000]\tLoss: 4.7328\tLR: 8.641432\n",
      "Training Epoch: 87 [20992/50000]\tLoss: 4.7199\tLR: 8.641688\n",
      "Training Epoch: 87 [21120/50000]\tLoss: 4.6657\tLR: 8.641944\n",
      "Training Epoch: 87 [21248/50000]\tLoss: 4.7731\tLR: 8.642199\n",
      "Training Epoch: 87 [21376/50000]\tLoss: 4.7368\tLR: 8.642455\n",
      "Training Epoch: 87 [21504/50000]\tLoss: 4.7988\tLR: 8.642711\n",
      "Training Epoch: 87 [21632/50000]\tLoss: 4.7228\tLR: 8.642967\n",
      "Training Epoch: 87 [21760/50000]\tLoss: 4.7460\tLR: 8.643223\n",
      "Training Epoch: 87 [21888/50000]\tLoss: 4.7681\tLR: 8.643478\n",
      "Training Epoch: 87 [22016/50000]\tLoss: 4.7809\tLR: 8.643734\n",
      "Training Epoch: 87 [22144/50000]\tLoss: 4.7148\tLR: 8.643990\n",
      "Training Epoch: 87 [22272/50000]\tLoss: 4.7843\tLR: 8.644246\n",
      "Training Epoch: 87 [22400/50000]\tLoss: 4.7402\tLR: 8.644501\n",
      "Training Epoch: 87 [22528/50000]\tLoss: 4.7804\tLR: 8.644757\n",
      "Training Epoch: 87 [22656/50000]\tLoss: 4.7663\tLR: 8.645013\n",
      "Training Epoch: 87 [22784/50000]\tLoss: 4.8412\tLR: 8.645269\n",
      "Training Epoch: 87 [22912/50000]\tLoss: 4.8230\tLR: 8.645524\n",
      "Training Epoch: 87 [23040/50000]\tLoss: 4.7109\tLR: 8.645780\n",
      "Training Epoch: 87 [23168/50000]\tLoss: 4.9203\tLR: 8.646036\n",
      "Training Epoch: 87 [23296/50000]\tLoss: 4.7927\tLR: 8.646292\n",
      "Training Epoch: 87 [23424/50000]\tLoss: 4.7076\tLR: 8.646547\n",
      "Training Epoch: 87 [23552/50000]\tLoss: 4.7372\tLR: 8.646803\n",
      "Training Epoch: 87 [23680/50000]\tLoss: 4.7410\tLR: 8.647059\n",
      "Training Epoch: 87 [23808/50000]\tLoss: 4.6503\tLR: 8.647315\n",
      "Training Epoch: 87 [23936/50000]\tLoss: 4.7381\tLR: 8.647570\n",
      "Training Epoch: 87 [24064/50000]\tLoss: 4.8081\tLR: 8.647826\n",
      "Training Epoch: 87 [24192/50000]\tLoss: 4.8384\tLR: 8.648082\n",
      "Training Epoch: 87 [24320/50000]\tLoss: 4.6210\tLR: 8.648338\n",
      "Training Epoch: 87 [24448/50000]\tLoss: 4.7807\tLR: 8.648593\n",
      "Training Epoch: 87 [24576/50000]\tLoss: 4.7616\tLR: 8.648849\n",
      "Training Epoch: 87 [24704/50000]\tLoss: 4.8476\tLR: 8.649105\n",
      "Training Epoch: 87 [24832/50000]\tLoss: 4.7083\tLR: 8.649361\n",
      "Training Epoch: 87 [24960/50000]\tLoss: 4.7561\tLR: 8.649616\n",
      "Training Epoch: 87 [25088/50000]\tLoss: 4.8306\tLR: 8.649872\n",
      "Training Epoch: 87 [25216/50000]\tLoss: 4.8365\tLR: 8.650128\n",
      "Training Epoch: 87 [25344/50000]\tLoss: 4.6714\tLR: 8.650384\n",
      "Training Epoch: 87 [25472/50000]\tLoss: 4.7245\tLR: 8.650639\n",
      "Training Epoch: 87 [25600/50000]\tLoss: 4.7285\tLR: 8.650895\n",
      "Training Epoch: 87 [25728/50000]\tLoss: 4.7283\tLR: 8.651151\n",
      "Training Epoch: 87 [25856/50000]\tLoss: 4.8756\tLR: 8.651407\n",
      "Training Epoch: 87 [25984/50000]\tLoss: 4.8743\tLR: 8.651662\n",
      "Training Epoch: 87 [26112/50000]\tLoss: 4.8615\tLR: 8.651918\n",
      "Training Epoch: 87 [26240/50000]\tLoss: 4.7341\tLR: 8.652174\n",
      "Training Epoch: 87 [26368/50000]\tLoss: 4.6897\tLR: 8.652430\n",
      "Training Epoch: 87 [26496/50000]\tLoss: 4.8135\tLR: 8.652685\n",
      "Training Epoch: 87 [26624/50000]\tLoss: 4.7255\tLR: 8.652941\n",
      "Training Epoch: 87 [26752/50000]\tLoss: 4.6922\tLR: 8.653197\n",
      "Training Epoch: 87 [26880/50000]\tLoss: 4.7513\tLR: 8.653453\n",
      "Training Epoch: 87 [27008/50000]\tLoss: 4.7736\tLR: 8.653708\n",
      "Training Epoch: 87 [27136/50000]\tLoss: 4.7170\tLR: 8.653964\n",
      "Training Epoch: 87 [27264/50000]\tLoss: 4.8286\tLR: 8.654220\n",
      "Training Epoch: 87 [27392/50000]\tLoss: 4.7358\tLR: 8.654476\n",
      "Training Epoch: 87 [27520/50000]\tLoss: 4.6785\tLR: 8.654731\n",
      "Training Epoch: 87 [27648/50000]\tLoss: 4.8376\tLR: 8.654987\n",
      "Training Epoch: 87 [27776/50000]\tLoss: 4.7636\tLR: 8.655243\n",
      "Training Epoch: 87 [27904/50000]\tLoss: 4.8123\tLR: 8.655499\n",
      "Training Epoch: 87 [28032/50000]\tLoss: 4.7175\tLR: 8.655754\n",
      "Training Epoch: 87 [28160/50000]\tLoss: 4.7261\tLR: 8.656010\n",
      "Training Epoch: 87 [28288/50000]\tLoss: 4.8377\tLR: 8.656266\n",
      "Training Epoch: 87 [28416/50000]\tLoss: 4.7156\tLR: 8.656522\n",
      "Training Epoch: 87 [28544/50000]\tLoss: 4.6889\tLR: 8.656777\n",
      "Training Epoch: 87 [28672/50000]\tLoss: 4.7927\tLR: 8.657033\n",
      "Training Epoch: 87 [28800/50000]\tLoss: 4.8326\tLR: 8.657289\n",
      "Training Epoch: 87 [28928/50000]\tLoss: 4.8118\tLR: 8.657545\n",
      "Training Epoch: 87 [29056/50000]\tLoss: 4.8715\tLR: 8.657801\n",
      "Training Epoch: 87 [29184/50000]\tLoss: 4.7615\tLR: 8.658056\n",
      "Training Epoch: 87 [29312/50000]\tLoss: 4.7120\tLR: 8.658312\n",
      "Training Epoch: 87 [29440/50000]\tLoss: 4.8210\tLR: 8.658568\n",
      "Training Epoch: 87 [29568/50000]\tLoss: 4.6512\tLR: 8.658824\n",
      "Training Epoch: 87 [29696/50000]\tLoss: 4.7659\tLR: 8.659079\n",
      "Training Epoch: 87 [29824/50000]\tLoss: 4.8017\tLR: 8.659335\n",
      "Training Epoch: 87 [29952/50000]\tLoss: 4.7023\tLR: 8.659591\n",
      "Training Epoch: 87 [30080/50000]\tLoss: 4.8862\tLR: 8.659847\n",
      "Training Epoch: 87 [30208/50000]\tLoss: 4.7195\tLR: 8.660102\n",
      "Training Epoch: 87 [30336/50000]\tLoss: 4.8873\tLR: 8.660358\n",
      "Training Epoch: 87 [30464/50000]\tLoss: 4.7462\tLR: 8.660614\n",
      "Training Epoch: 87 [30592/50000]\tLoss: 4.6558\tLR: 8.660870\n",
      "Training Epoch: 87 [30720/50000]\tLoss: 4.7764\tLR: 8.661125\n",
      "Training Epoch: 87 [30848/50000]\tLoss: 4.7563\tLR: 8.661381\n",
      "Training Epoch: 87 [30976/50000]\tLoss: 4.7727\tLR: 8.661637\n",
      "Training Epoch: 87 [31104/50000]\tLoss: 4.7300\tLR: 8.661893\n",
      "Training Epoch: 87 [31232/50000]\tLoss: 4.6536\tLR: 8.662148\n",
      "Training Epoch: 87 [31360/50000]\tLoss: 4.7675\tLR: 8.662404\n",
      "Training Epoch: 87 [31488/50000]\tLoss: 4.7771\tLR: 8.662660\n",
      "Training Epoch: 87 [31616/50000]\tLoss: 4.7764\tLR: 8.662916\n",
      "Training Epoch: 87 [31744/50000]\tLoss: 4.7749\tLR: 8.663171\n",
      "Training Epoch: 87 [31872/50000]\tLoss: 4.6776\tLR: 8.663427\n",
      "Training Epoch: 87 [32000/50000]\tLoss: 4.7136\tLR: 8.663683\n",
      "Training Epoch: 87 [32128/50000]\tLoss: 4.7367\tLR: 8.663939\n",
      "Training Epoch: 87 [32256/50000]\tLoss: 4.6852\tLR: 8.664194\n",
      "Training Epoch: 87 [32384/50000]\tLoss: 4.8494\tLR: 8.664450\n",
      "Training Epoch: 87 [32512/50000]\tLoss: 4.7261\tLR: 8.664706\n",
      "Training Epoch: 87 [32640/50000]\tLoss: 4.7073\tLR: 8.664962\n",
      "Training Epoch: 87 [32768/50000]\tLoss: 4.7738\tLR: 8.665217\n",
      "Training Epoch: 87 [32896/50000]\tLoss: 4.7708\tLR: 8.665473\n",
      "Training Epoch: 87 [33024/50000]\tLoss: 4.7696\tLR: 8.665729\n",
      "Training Epoch: 87 [33152/50000]\tLoss: 4.7994\tLR: 8.665985\n",
      "Training Epoch: 87 [33280/50000]\tLoss: 4.8549\tLR: 8.666240\n",
      "Training Epoch: 87 [33408/50000]\tLoss: 4.8495\tLR: 8.666496\n",
      "Training Epoch: 87 [33536/50000]\tLoss: 4.6893\tLR: 8.666752\n",
      "Training Epoch: 87 [33664/50000]\tLoss: 4.6976\tLR: 8.667008\n",
      "Training Epoch: 87 [33792/50000]\tLoss: 4.8192\tLR: 8.667263\n",
      "Training Epoch: 87 [33920/50000]\tLoss: 4.6855\tLR: 8.667519\n",
      "Training Epoch: 87 [34048/50000]\tLoss: 4.7842\tLR: 8.667775\n",
      "Training Epoch: 87 [34176/50000]\tLoss: 4.6582\tLR: 8.668031\n",
      "Training Epoch: 87 [34304/50000]\tLoss: 4.7224\tLR: 8.668286\n",
      "Training Epoch: 87 [34432/50000]\tLoss: 4.6853\tLR: 8.668542\n",
      "Training Epoch: 87 [34560/50000]\tLoss: 4.6802\tLR: 8.668798\n",
      "Training Epoch: 87 [34688/50000]\tLoss: 4.7727\tLR: 8.669054\n",
      "Training Epoch: 87 [34816/50000]\tLoss: 4.8070\tLR: 8.669309\n",
      "Training Epoch: 87 [34944/50000]\tLoss: 4.8658\tLR: 8.669565\n",
      "Training Epoch: 87 [35072/50000]\tLoss: 4.7019\tLR: 8.669821\n",
      "Training Epoch: 87 [35200/50000]\tLoss: 4.7494\tLR: 8.670077\n",
      "Training Epoch: 87 [35328/50000]\tLoss: 4.7860\tLR: 8.670332\n",
      "Training Epoch: 87 [35456/50000]\tLoss: 4.7420\tLR: 8.670588\n",
      "Training Epoch: 87 [35584/50000]\tLoss: 4.6886\tLR: 8.670844\n",
      "Training Epoch: 87 [35712/50000]\tLoss: 4.7560\tLR: 8.671100\n",
      "Training Epoch: 87 [35840/50000]\tLoss: 4.7189\tLR: 8.671355\n",
      "Training Epoch: 87 [35968/50000]\tLoss: 4.7088\tLR: 8.671611\n",
      "Training Epoch: 87 [36096/50000]\tLoss: 4.7749\tLR: 8.671867\n",
      "Training Epoch: 87 [36224/50000]\tLoss: 4.7374\tLR: 8.672123\n",
      "Training Epoch: 87 [36352/50000]\tLoss: 4.7169\tLR: 8.672379\n",
      "Training Epoch: 87 [36480/50000]\tLoss: 4.7290\tLR: 8.672634\n",
      "Training Epoch: 87 [36608/50000]\tLoss: 4.7855\tLR: 8.672890\n",
      "Training Epoch: 87 [36736/50000]\tLoss: 4.7517\tLR: 8.673146\n",
      "Training Epoch: 87 [36864/50000]\tLoss: 4.7670\tLR: 8.673402\n",
      "Training Epoch: 87 [36992/50000]\tLoss: 4.7395\tLR: 8.673657\n",
      "Training Epoch: 87 [37120/50000]\tLoss: 4.7778\tLR: 8.673913\n",
      "Training Epoch: 87 [37248/50000]\tLoss: 4.7166\tLR: 8.674169\n",
      "Training Epoch: 87 [37376/50000]\tLoss: 4.7627\tLR: 8.674425\n",
      "Training Epoch: 87 [37504/50000]\tLoss: 4.7404\tLR: 8.674680\n",
      "Training Epoch: 87 [37632/50000]\tLoss: 4.7178\tLR: 8.674936\n",
      "Training Epoch: 87 [37760/50000]\tLoss: 4.7429\tLR: 8.675192\n",
      "Training Epoch: 87 [37888/50000]\tLoss: 4.6179\tLR: 8.675448\n",
      "Training Epoch: 87 [38016/50000]\tLoss: 4.7477\tLR: 8.675703\n",
      "Training Epoch: 87 [38144/50000]\tLoss: 4.7124\tLR: 8.675959\n",
      "Training Epoch: 87 [38272/50000]\tLoss: 4.8091\tLR: 8.676215\n",
      "Training Epoch: 87 [38400/50000]\tLoss: 4.7477\tLR: 8.676471\n",
      "Training Epoch: 87 [38528/50000]\tLoss: 4.7787\tLR: 8.676726\n",
      "Training Epoch: 87 [38656/50000]\tLoss: 4.7726\tLR: 8.676982\n",
      "Training Epoch: 87 [38784/50000]\tLoss: 4.7527\tLR: 8.677238\n",
      "Training Epoch: 87 [38912/50000]\tLoss: 4.7112\tLR: 8.677494\n",
      "Training Epoch: 87 [39040/50000]\tLoss: 4.7131\tLR: 8.677749\n",
      "Training Epoch: 87 [39168/50000]\tLoss: 4.6881\tLR: 8.678005\n",
      "Training Epoch: 87 [39296/50000]\tLoss: 4.7916\tLR: 8.678261\n",
      "Training Epoch: 87 [39424/50000]\tLoss: 4.7671\tLR: 8.678517\n",
      "Training Epoch: 87 [39552/50000]\tLoss: 4.8053\tLR: 8.678772\n",
      "Training Epoch: 87 [39680/50000]\tLoss: 4.8007\tLR: 8.679028\n",
      "Training Epoch: 87 [39808/50000]\tLoss: 4.7303\tLR: 8.679284\n",
      "Training Epoch: 87 [39936/50000]\tLoss: 4.7486\tLR: 8.679540\n",
      "Training Epoch: 87 [40064/50000]\tLoss: 4.8030\tLR: 8.679795\n",
      "Training Epoch: 87 [40192/50000]\tLoss: 4.8056\tLR: 8.680051\n",
      "Training Epoch: 87 [40320/50000]\tLoss: 4.8248\tLR: 8.680307\n",
      "Training Epoch: 87 [40448/50000]\tLoss: 4.7659\tLR: 8.680563\n",
      "Training Epoch: 87 [40576/50000]\tLoss: 4.8493\tLR: 8.680818\n",
      "Training Epoch: 87 [40704/50000]\tLoss: 4.7488\tLR: 8.681074\n",
      "Training Epoch: 87 [40832/50000]\tLoss: 4.7154\tLR: 8.681330\n",
      "Training Epoch: 87 [40960/50000]\tLoss: 4.7035\tLR: 8.681586\n",
      "Training Epoch: 87 [41088/50000]\tLoss: 4.8102\tLR: 8.681841\n",
      "Training Epoch: 87 [41216/50000]\tLoss: 4.9672\tLR: 8.682097\n",
      "Training Epoch: 87 [41344/50000]\tLoss: 4.7892\tLR: 8.682353\n",
      "Training Epoch: 87 [41472/50000]\tLoss: 4.8242\tLR: 8.682609\n",
      "Training Epoch: 87 [41600/50000]\tLoss: 4.9107\tLR: 8.682864\n",
      "Training Epoch: 87 [41728/50000]\tLoss: 4.9447\tLR: 8.683120\n",
      "Training Epoch: 87 [41856/50000]\tLoss: 4.8646\tLR: 8.683376\n",
      "Training Epoch: 87 [41984/50000]\tLoss: 4.7734\tLR: 8.683632\n",
      "Training Epoch: 87 [42112/50000]\tLoss: 4.6974\tLR: 8.683887\n",
      "Training Epoch: 87 [42240/50000]\tLoss: 4.7845\tLR: 8.684143\n",
      "Training Epoch: 87 [42368/50000]\tLoss: 4.8261\tLR: 8.684399\n",
      "Training Epoch: 87 [42496/50000]\tLoss: 4.7714\tLR: 8.684655\n",
      "Training Epoch: 87 [42624/50000]\tLoss: 4.8570\tLR: 8.684910\n",
      "Training Epoch: 87 [42752/50000]\tLoss: 4.7839\tLR: 8.685166\n",
      "Training Epoch: 87 [42880/50000]\tLoss: 4.6771\tLR: 8.685422\n",
      "Training Epoch: 87 [43008/50000]\tLoss: 4.7610\tLR: 8.685678\n",
      "Training Epoch: 87 [43136/50000]\tLoss: 4.7716\tLR: 8.685934\n",
      "Training Epoch: 87 [43264/50000]\tLoss: 4.6224\tLR: 8.686189\n",
      "Training Epoch: 87 [43392/50000]\tLoss: 4.8739\tLR: 8.686445\n",
      "Training Epoch: 87 [43520/50000]\tLoss: 4.8130\tLR: 8.686701\n",
      "Training Epoch: 87 [43648/50000]\tLoss: 4.7435\tLR: 8.686957\n",
      "Training Epoch: 87 [43776/50000]\tLoss: 4.7334\tLR: 8.687212\n",
      "Training Epoch: 87 [43904/50000]\tLoss: 4.8200\tLR: 8.687468\n",
      "Training Epoch: 87 [44032/50000]\tLoss: 4.8330\tLR: 8.687724\n",
      "Training Epoch: 87 [44160/50000]\tLoss: 4.7341\tLR: 8.687980\n",
      "Training Epoch: 87 [44288/50000]\tLoss: 4.7970\tLR: 8.688235\n",
      "Training Epoch: 87 [44416/50000]\tLoss: 4.7123\tLR: 8.688491\n",
      "Training Epoch: 87 [44544/50000]\tLoss: 4.7054\tLR: 8.688747\n",
      "Training Epoch: 87 [44672/50000]\tLoss: 4.7960\tLR: 8.689003\n",
      "Training Epoch: 87 [44800/50000]\tLoss: 4.8085\tLR: 8.689258\n",
      "Training Epoch: 87 [44928/50000]\tLoss: 4.8459\tLR: 8.689514\n",
      "Training Epoch: 87 [45056/50000]\tLoss: 4.7532\tLR: 8.689770\n",
      "Training Epoch: 87 [45184/50000]\tLoss: 4.7189\tLR: 8.690026\n",
      "Training Epoch: 87 [45312/50000]\tLoss: 4.7798\tLR: 8.690281\n",
      "Training Epoch: 87 [45440/50000]\tLoss: 4.8204\tLR: 8.690537\n",
      "Training Epoch: 87 [45568/50000]\tLoss: 4.8209\tLR: 8.690793\n",
      "Training Epoch: 87 [45696/50000]\tLoss: 4.7957\tLR: 8.691049\n",
      "Training Epoch: 87 [45824/50000]\tLoss: 4.8389\tLR: 8.691304\n",
      "Training Epoch: 87 [45952/50000]\tLoss: 4.7620\tLR: 8.691560\n",
      "Training Epoch: 87 [46080/50000]\tLoss: 4.8342\tLR: 8.691816\n",
      "Training Epoch: 87 [46208/50000]\tLoss: 4.7658\tLR: 8.692072\n",
      "Training Epoch: 87 [46336/50000]\tLoss: 4.7664\tLR: 8.692327\n",
      "Training Epoch: 87 [46464/50000]\tLoss: 4.8386\tLR: 8.692583\n",
      "Training Epoch: 87 [46592/50000]\tLoss: 4.7184\tLR: 8.692839\n",
      "Training Epoch: 87 [46720/50000]\tLoss: 4.8508\tLR: 8.693095\n",
      "Training Epoch: 87 [46848/50000]\tLoss: 4.8723\tLR: 8.693350\n",
      "Training Epoch: 87 [46976/50000]\tLoss: 4.8540\tLR: 8.693606\n",
      "Training Epoch: 87 [47104/50000]\tLoss: 4.8105\tLR: 8.693862\n",
      "Training Epoch: 87 [47232/50000]\tLoss: 4.7861\tLR: 8.694118\n",
      "Training Epoch: 87 [47360/50000]\tLoss: 4.7347\tLR: 8.694373\n",
      "Training Epoch: 87 [47488/50000]\tLoss: 4.7678\tLR: 8.694629\n",
      "Training Epoch: 87 [47616/50000]\tLoss: 4.7498\tLR: 8.694885\n",
      "Training Epoch: 87 [47744/50000]\tLoss: 4.7818\tLR: 8.695141\n",
      "Training Epoch: 87 [47872/50000]\tLoss: 4.8030\tLR: 8.695396\n",
      "Training Epoch: 87 [48000/50000]\tLoss: 4.7840\tLR: 8.695652\n",
      "Training Epoch: 87 [48128/50000]\tLoss: 4.7526\tLR: 8.695908\n",
      "Training Epoch: 87 [48256/50000]\tLoss: 4.7237\tLR: 8.696164\n",
      "Training Epoch: 87 [48384/50000]\tLoss: 4.7249\tLR: 8.696419\n",
      "Training Epoch: 87 [48512/50000]\tLoss: 4.7498\tLR: 8.696675\n",
      "Training Epoch: 87 [48640/50000]\tLoss: 4.8347\tLR: 8.696931\n",
      "Training Epoch: 87 [48768/50000]\tLoss: 4.8111\tLR: 8.697187\n",
      "Training Epoch: 87 [48896/50000]\tLoss: 4.7440\tLR: 8.697442\n",
      "Training Epoch: 87 [49024/50000]\tLoss: 4.8450\tLR: 8.697698\n",
      "Training Epoch: 87 [49152/50000]\tLoss: 4.7013\tLR: 8.697954\n",
      "Training Epoch: 87 [49280/50000]\tLoss: 4.8193\tLR: 8.698210\n",
      "Training Epoch: 87 [49408/50000]\tLoss: 4.8251\tLR: 8.698465\n",
      "Training Epoch: 87 [49536/50000]\tLoss: 4.8203\tLR: 8.698721\n",
      "Training Epoch: 87 [49664/50000]\tLoss: 4.8983\tLR: 8.698977\n",
      "Training Epoch: 87 [49792/50000]\tLoss: 4.8042\tLR: 8.699233\n",
      "Training Epoch: 87 [49920/50000]\tLoss: 4.8170\tLR: 8.699488\n",
      "Training Epoch: 87 [50000/50000]\tLoss: 4.9093\tLR: 8.699744\n",
      "epoch 87 training time consumed: 491.06s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  121968 GB |  121968 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  121593 GB |  121593 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     374 GB |     374 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  121968 GB |  121968 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  121593 GB |  121593 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     374 GB |     374 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  120254 GB |  120254 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  119879 GB |  119879 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     374 GB |     374 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   12932 K  |   12932 K  |\n",
      "|       from large pool |      24    |      65    |    5513 K  |    5513 K  |\n",
      "|       from small pool |     231    |     274    |    7419 K  |    7419 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   12932 K  |   12932 K  |\n",
      "|       from large pool |      24    |      65    |    5513 K  |    5513 K  |\n",
      "|       from small pool |     231    |     274    |    7419 K  |    7419 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      47    |    7496 K  |    7496 K  |\n",
      "|       from large pool |      10    |      23    |    2650 K  |    2650 K  |\n",
      "|       from small pool |      25    |      35    |    4846 K  |    4846 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 87, Average loss: 0.0377, Accuracy: 0.0100, Time consumed:31.35s\n",
      "\n",
      "Training Epoch: 88 [128/50000]\tLoss: 4.7983\tLR: 0.020000\n",
      "Training Epoch: 88 [256/50000]\tLoss: 4.7615\tLR: 8.700256\n",
      "Training Epoch: 88 [384/50000]\tLoss: 4.7655\tLR: 8.700512\n",
      "Training Epoch: 88 [512/50000]\tLoss: 4.8184\tLR: 8.700767\n",
      "Training Epoch: 88 [640/50000]\tLoss: 4.8089\tLR: 8.701023\n",
      "Training Epoch: 88 [768/50000]\tLoss: 4.7396\tLR: 8.701279\n",
      "Training Epoch: 88 [896/50000]\tLoss: 4.7555\tLR: 8.701535\n",
      "Training Epoch: 88 [1024/50000]\tLoss: 4.7462\tLR: 8.701790\n",
      "Training Epoch: 88 [1152/50000]\tLoss: 4.7645\tLR: 8.702046\n",
      "Training Epoch: 88 [1280/50000]\tLoss: 4.8790\tLR: 8.702302\n",
      "Training Epoch: 88 [1408/50000]\tLoss: 4.7202\tLR: 8.702558\n",
      "Training Epoch: 88 [1536/50000]\tLoss: 4.7213\tLR: 8.702813\n",
      "Training Epoch: 88 [1664/50000]\tLoss: 4.8014\tLR: 8.703069\n",
      "Training Epoch: 88 [1792/50000]\tLoss: 4.7482\tLR: 8.703325\n",
      "Training Epoch: 88 [1920/50000]\tLoss: 4.7771\tLR: 8.703581\n",
      "Training Epoch: 88 [2048/50000]\tLoss: 4.8151\tLR: 8.703836\n",
      "Training Epoch: 88 [2176/50000]\tLoss: 4.6805\tLR: 8.704092\n",
      "Training Epoch: 88 [2304/50000]\tLoss: 4.8622\tLR: 8.704348\n",
      "Training Epoch: 88 [2432/50000]\tLoss: 4.7677\tLR: 8.704604\n",
      "Training Epoch: 88 [2560/50000]\tLoss: 4.7970\tLR: 8.704859\n",
      "Training Epoch: 88 [2688/50000]\tLoss: 4.6873\tLR: 8.705115\n",
      "Training Epoch: 88 [2816/50000]\tLoss: 4.6860\tLR: 8.705371\n",
      "Training Epoch: 88 [2944/50000]\tLoss: 4.7063\tLR: 8.705627\n",
      "Training Epoch: 88 [3072/50000]\tLoss: 4.7908\tLR: 8.705882\n",
      "Training Epoch: 88 [3200/50000]\tLoss: 4.8296\tLR: 8.706138\n",
      "Training Epoch: 88 [3328/50000]\tLoss: 4.7590\tLR: 8.706394\n",
      "Training Epoch: 88 [3456/50000]\tLoss: 4.7731\tLR: 8.706650\n",
      "Training Epoch: 88 [3584/50000]\tLoss: 4.8058\tLR: 8.706905\n",
      "Training Epoch: 88 [3712/50000]\tLoss: 4.8116\tLR: 8.707161\n",
      "Training Epoch: 88 [3840/50000]\tLoss: 4.7146\tLR: 8.707417\n",
      "Training Epoch: 88 [3968/50000]\tLoss: 4.7203\tLR: 8.707673\n",
      "Training Epoch: 88 [4096/50000]\tLoss: 4.9134\tLR: 8.707928\n",
      "Training Epoch: 88 [4224/50000]\tLoss: 4.7765\tLR: 8.708184\n",
      "Training Epoch: 88 [4352/50000]\tLoss: 4.8634\tLR: 8.708440\n",
      "Training Epoch: 88 [4480/50000]\tLoss: 4.7227\tLR: 8.708696\n",
      "Training Epoch: 88 [4608/50000]\tLoss: 4.6782\tLR: 8.708951\n",
      "Training Epoch: 88 [4736/50000]\tLoss: 4.7773\tLR: 8.709207\n",
      "Training Epoch: 88 [4864/50000]\tLoss: 4.7635\tLR: 8.709463\n",
      "Training Epoch: 88 [4992/50000]\tLoss: 4.7822\tLR: 8.709719\n",
      "Training Epoch: 88 [5120/50000]\tLoss: 4.7899\tLR: 8.709974\n",
      "Training Epoch: 88 [5248/50000]\tLoss: 4.7346\tLR: 8.710230\n",
      "Training Epoch: 88 [5376/50000]\tLoss: 4.7701\tLR: 8.710486\n",
      "Training Epoch: 88 [5504/50000]\tLoss: 4.7557\tLR: 8.710742\n",
      "Training Epoch: 88 [5632/50000]\tLoss: 4.6853\tLR: 8.710997\n",
      "Training Epoch: 88 [5760/50000]\tLoss: 4.8219\tLR: 8.711253\n",
      "Training Epoch: 88 [5888/50000]\tLoss: 4.8278\tLR: 8.711509\n",
      "Training Epoch: 88 [6016/50000]\tLoss: 4.8798\tLR: 8.711765\n",
      "Training Epoch: 88 [6144/50000]\tLoss: 4.8382\tLR: 8.712020\n",
      "Training Epoch: 88 [6272/50000]\tLoss: 4.7872\tLR: 8.712276\n",
      "Training Epoch: 88 [6400/50000]\tLoss: 4.7657\tLR: 8.712532\n",
      "Training Epoch: 88 [6528/50000]\tLoss: 4.7368\tLR: 8.712788\n",
      "Training Epoch: 88 [6656/50000]\tLoss: 4.7206\tLR: 8.713043\n",
      "Training Epoch: 88 [6784/50000]\tLoss: 4.7969\tLR: 8.713299\n",
      "Training Epoch: 88 [6912/50000]\tLoss: 4.8086\tLR: 8.713555\n",
      "Training Epoch: 88 [7040/50000]\tLoss: 4.7743\tLR: 8.713811\n",
      "Training Epoch: 88 [7168/50000]\tLoss: 4.7598\tLR: 8.714066\n",
      "Training Epoch: 88 [7296/50000]\tLoss: 4.7508\tLR: 8.714322\n",
      "Training Epoch: 88 [7424/50000]\tLoss: 4.7039\tLR: 8.714578\n",
      "Training Epoch: 88 [7552/50000]\tLoss: 4.7025\tLR: 8.714834\n",
      "Training Epoch: 88 [7680/50000]\tLoss: 4.8179\tLR: 8.715090\n",
      "Training Epoch: 88 [7808/50000]\tLoss: 4.7865\tLR: 8.715345\n",
      "Training Epoch: 88 [7936/50000]\tLoss: 4.7920\tLR: 8.715601\n",
      "Training Epoch: 88 [8064/50000]\tLoss: 4.8089\tLR: 8.715857\n",
      "Training Epoch: 88 [8192/50000]\tLoss: 4.8029\tLR: 8.716113\n",
      "Training Epoch: 88 [8320/50000]\tLoss: 4.7290\tLR: 8.716368\n",
      "Training Epoch: 88 [8448/50000]\tLoss: 4.7353\tLR: 8.716624\n",
      "Training Epoch: 88 [8576/50000]\tLoss: 4.7875\tLR: 8.716880\n",
      "Training Epoch: 88 [8704/50000]\tLoss: 4.7854\tLR: 8.717136\n",
      "Training Epoch: 88 [8832/50000]\tLoss: 4.7865\tLR: 8.717391\n",
      "Training Epoch: 88 [8960/50000]\tLoss: 4.7433\tLR: 8.717647\n",
      "Training Epoch: 88 [9088/50000]\tLoss: 4.7688\tLR: 8.717903\n",
      "Training Epoch: 88 [9216/50000]\tLoss: 4.8152\tLR: 8.718159\n",
      "Training Epoch: 88 [9344/50000]\tLoss: 4.8189\tLR: 8.718414\n",
      "Training Epoch: 88 [9472/50000]\tLoss: 4.7390\tLR: 8.718670\n",
      "Training Epoch: 88 [9600/50000]\tLoss: 4.7424\tLR: 8.718926\n",
      "Training Epoch: 88 [9728/50000]\tLoss: 4.7414\tLR: 8.719182\n",
      "Training Epoch: 88 [9856/50000]\tLoss: 4.6762\tLR: 8.719437\n",
      "Training Epoch: 88 [9984/50000]\tLoss: 4.7802\tLR: 8.719693\n",
      "Training Epoch: 88 [10112/50000]\tLoss: 4.6954\tLR: 8.719949\n",
      "Training Epoch: 88 [10240/50000]\tLoss: 4.8276\tLR: 8.720205\n",
      "Training Epoch: 88 [10368/50000]\tLoss: 4.7791\tLR: 8.720460\n",
      "Training Epoch: 88 [10496/50000]\tLoss: 4.7825\tLR: 8.720716\n",
      "Training Epoch: 88 [10624/50000]\tLoss: 4.6925\tLR: 8.720972\n",
      "Training Epoch: 88 [10752/50000]\tLoss: 4.8851\tLR: 8.721228\n",
      "Training Epoch: 88 [10880/50000]\tLoss: 4.8149\tLR: 8.721483\n",
      "Training Epoch: 88 [11008/50000]\tLoss: 4.7847\tLR: 8.721739\n",
      "Training Epoch: 88 [11136/50000]\tLoss: 4.6975\tLR: 8.721995\n",
      "Training Epoch: 88 [11264/50000]\tLoss: 4.7169\tLR: 8.722251\n",
      "Training Epoch: 88 [11392/50000]\tLoss: 4.7794\tLR: 8.722506\n",
      "Training Epoch: 88 [11520/50000]\tLoss: 4.7538\tLR: 8.722762\n",
      "Training Epoch: 88 [11648/50000]\tLoss: 4.6825\tLR: 8.723018\n",
      "Training Epoch: 88 [11776/50000]\tLoss: 4.7779\tLR: 8.723274\n",
      "Training Epoch: 88 [11904/50000]\tLoss: 4.7339\tLR: 8.723529\n",
      "Training Epoch: 88 [12032/50000]\tLoss: 4.7453\tLR: 8.723785\n",
      "Training Epoch: 88 [12160/50000]\tLoss: 4.7925\tLR: 8.724041\n",
      "Training Epoch: 88 [12288/50000]\tLoss: 4.8618\tLR: 8.724297\n",
      "Training Epoch: 88 [12416/50000]\tLoss: 4.7803\tLR: 8.724552\n",
      "Training Epoch: 88 [12544/50000]\tLoss: 4.6743\tLR: 8.724808\n",
      "Training Epoch: 88 [12672/50000]\tLoss: 4.7609\tLR: 8.725064\n",
      "Training Epoch: 88 [12800/50000]\tLoss: 4.7747\tLR: 8.725320\n",
      "Training Epoch: 88 [12928/50000]\tLoss: 4.6940\tLR: 8.725575\n",
      "Training Epoch: 88 [13056/50000]\tLoss: 4.8573\tLR: 8.725831\n",
      "Training Epoch: 88 [13184/50000]\tLoss: 4.7997\tLR: 8.726087\n",
      "Training Epoch: 88 [13312/50000]\tLoss: 4.7833\tLR: 8.726343\n",
      "Training Epoch: 88 [13440/50000]\tLoss: 4.7808\tLR: 8.726598\n",
      "Training Epoch: 88 [13568/50000]\tLoss: 4.8067\tLR: 8.726854\n",
      "Training Epoch: 88 [13696/50000]\tLoss: 4.7613\tLR: 8.727110\n",
      "Training Epoch: 88 [13824/50000]\tLoss: 4.7839\tLR: 8.727366\n",
      "Training Epoch: 88 [13952/50000]\tLoss: 4.7560\tLR: 8.727621\n",
      "Training Epoch: 88 [14080/50000]\tLoss: 4.6962\tLR: 8.727877\n",
      "Training Epoch: 88 [14208/50000]\tLoss: 4.7380\tLR: 8.728133\n",
      "Training Epoch: 88 [14336/50000]\tLoss: 4.7544\tLR: 8.728389\n",
      "Training Epoch: 88 [14464/50000]\tLoss: 4.6853\tLR: 8.728645\n",
      "Training Epoch: 88 [14592/50000]\tLoss: 4.7620\tLR: 8.728900\n",
      "Training Epoch: 88 [14720/50000]\tLoss: 4.6312\tLR: 8.729156\n",
      "Training Epoch: 88 [14848/50000]\tLoss: 4.8363\tLR: 8.729412\n",
      "Training Epoch: 88 [14976/50000]\tLoss: 4.8874\tLR: 8.729668\n",
      "Training Epoch: 88 [15104/50000]\tLoss: 4.7993\tLR: 8.729923\n",
      "Training Epoch: 88 [15232/50000]\tLoss: 4.8011\tLR: 8.730179\n",
      "Training Epoch: 88 [15360/50000]\tLoss: 4.7940\tLR: 8.730435\n",
      "Training Epoch: 88 [15488/50000]\tLoss: 4.8352\tLR: 8.730691\n",
      "Training Epoch: 88 [15616/50000]\tLoss: 4.7585\tLR: 8.730946\n",
      "Training Epoch: 88 [15744/50000]\tLoss: 4.7081\tLR: 8.731202\n",
      "Training Epoch: 88 [15872/50000]\tLoss: 4.7250\tLR: 8.731458\n",
      "Training Epoch: 88 [16000/50000]\tLoss: 4.7511\tLR: 8.731714\n",
      "Training Epoch: 88 [16128/50000]\tLoss: 4.7269\tLR: 8.731969\n",
      "Training Epoch: 88 [16256/50000]\tLoss: 4.7096\tLR: 8.732225\n",
      "Training Epoch: 88 [16384/50000]\tLoss: 4.8076\tLR: 8.732481\n",
      "Training Epoch: 88 [16512/50000]\tLoss: 4.7318\tLR: 8.732737\n",
      "Training Epoch: 88 [16640/50000]\tLoss: 4.7729\tLR: 8.732992\n",
      "Training Epoch: 88 [16768/50000]\tLoss: 4.7809\tLR: 8.733248\n",
      "Training Epoch: 88 [16896/50000]\tLoss: 4.7815\tLR: 8.733504\n",
      "Training Epoch: 88 [17024/50000]\tLoss: 4.7542\tLR: 8.733760\n",
      "Training Epoch: 88 [17152/50000]\tLoss: 4.7213\tLR: 8.734015\n",
      "Training Epoch: 88 [17280/50000]\tLoss: 4.7373\tLR: 8.734271\n",
      "Training Epoch: 88 [17408/50000]\tLoss: 4.7011\tLR: 8.734527\n",
      "Training Epoch: 88 [17536/50000]\tLoss: 4.8264\tLR: 8.734783\n",
      "Training Epoch: 88 [17664/50000]\tLoss: 4.7789\tLR: 8.735038\n",
      "Training Epoch: 88 [17792/50000]\tLoss: 4.8737\tLR: 8.735294\n",
      "Training Epoch: 88 [17920/50000]\tLoss: 4.7523\tLR: 8.735550\n",
      "Training Epoch: 88 [18048/50000]\tLoss: 4.7785\tLR: 8.735806\n",
      "Training Epoch: 88 [18176/50000]\tLoss: 4.7679\tLR: 8.736061\n",
      "Training Epoch: 88 [18304/50000]\tLoss: 4.7472\tLR: 8.736317\n",
      "Training Epoch: 88 [18432/50000]\tLoss: 4.7424\tLR: 8.736573\n",
      "Training Epoch: 88 [18560/50000]\tLoss: 4.7445\tLR: 8.736829\n",
      "Training Epoch: 88 [18688/50000]\tLoss: 4.7653\tLR: 8.737084\n",
      "Training Epoch: 88 [18816/50000]\tLoss: 4.8508\tLR: 8.737340\n",
      "Training Epoch: 88 [18944/50000]\tLoss: 4.6424\tLR: 8.737596\n",
      "Training Epoch: 88 [19072/50000]\tLoss: 4.6884\tLR: 8.737852\n",
      "Training Epoch: 88 [19200/50000]\tLoss: 4.7535\tLR: 8.738107\n",
      "Training Epoch: 88 [19328/50000]\tLoss: 4.6784\tLR: 8.738363\n",
      "Training Epoch: 88 [19456/50000]\tLoss: 4.6853\tLR: 8.738619\n",
      "Training Epoch: 88 [19584/50000]\tLoss: 4.6857\tLR: 8.738875\n",
      "Training Epoch: 88 [19712/50000]\tLoss: 4.7478\tLR: 8.739130\n",
      "Training Epoch: 88 [19840/50000]\tLoss: 4.6851\tLR: 8.739386\n",
      "Training Epoch: 88 [19968/50000]\tLoss: 4.7351\tLR: 8.739642\n",
      "Training Epoch: 88 [20096/50000]\tLoss: 4.8070\tLR: 8.739898\n",
      "Training Epoch: 88 [20224/50000]\tLoss: 4.7255\tLR: 8.740153\n",
      "Training Epoch: 88 [20352/50000]\tLoss: 4.8114\tLR: 8.740409\n",
      "Training Epoch: 88 [20480/50000]\tLoss: 4.7972\tLR: 8.740665\n",
      "Training Epoch: 88 [20608/50000]\tLoss: 4.8002\tLR: 8.740921\n",
      "Training Epoch: 88 [20736/50000]\tLoss: 4.7653\tLR: 8.741176\n",
      "Training Epoch: 88 [20864/50000]\tLoss: 4.7709\tLR: 8.741432\n",
      "Training Epoch: 88 [20992/50000]\tLoss: 4.7556\tLR: 8.741688\n",
      "Training Epoch: 88 [21120/50000]\tLoss: 4.7097\tLR: 8.741944\n",
      "Training Epoch: 88 [21248/50000]\tLoss: 4.7550\tLR: 8.742199\n",
      "Training Epoch: 88 [21376/50000]\tLoss: 4.7751\tLR: 8.742455\n",
      "Training Epoch: 88 [21504/50000]\tLoss: 4.7495\tLR: 8.742711\n",
      "Training Epoch: 88 [21632/50000]\tLoss: 4.8631\tLR: 8.742967\n",
      "Training Epoch: 88 [21760/50000]\tLoss: 4.6293\tLR: 8.743223\n",
      "Training Epoch: 88 [21888/50000]\tLoss: 4.7815\tLR: 8.743478\n",
      "Training Epoch: 88 [22016/50000]\tLoss: 4.7465\tLR: 8.743734\n",
      "Training Epoch: 88 [22144/50000]\tLoss: 4.7080\tLR: 8.743990\n",
      "Training Epoch: 88 [22272/50000]\tLoss: 4.7794\tLR: 8.744246\n",
      "Training Epoch: 88 [22400/50000]\tLoss: 4.6632\tLR: 8.744501\n",
      "Training Epoch: 88 [22528/50000]\tLoss: 4.6765\tLR: 8.744757\n",
      "Training Epoch: 88 [22656/50000]\tLoss: 4.7721\tLR: 8.745013\n",
      "Training Epoch: 88 [22784/50000]\tLoss: 4.7013\tLR: 8.745269\n",
      "Training Epoch: 88 [22912/50000]\tLoss: 4.8626\tLR: 8.745524\n",
      "Training Epoch: 88 [23040/50000]\tLoss: 4.7795\tLR: 8.745780\n",
      "Training Epoch: 88 [23168/50000]\tLoss: 4.7852\tLR: 8.746036\n",
      "Training Epoch: 88 [23296/50000]\tLoss: 4.7804\tLR: 8.746292\n",
      "Training Epoch: 88 [23424/50000]\tLoss: 4.7640\tLR: 8.746547\n",
      "Training Epoch: 88 [23552/50000]\tLoss: 4.7330\tLR: 8.746803\n",
      "Training Epoch: 88 [23680/50000]\tLoss: 4.6857\tLR: 8.747059\n",
      "Training Epoch: 88 [23808/50000]\tLoss: 4.7621\tLR: 8.747315\n",
      "Training Epoch: 88 [23936/50000]\tLoss: 4.7435\tLR: 8.747570\n",
      "Training Epoch: 88 [24064/50000]\tLoss: 4.6703\tLR: 8.747826\n",
      "Training Epoch: 88 [24192/50000]\tLoss: 4.7413\tLR: 8.748082\n",
      "Training Epoch: 88 [24320/50000]\tLoss: 4.8998\tLR: 8.748338\n",
      "Training Epoch: 88 [24448/50000]\tLoss: 4.8947\tLR: 8.748593\n",
      "Training Epoch: 88 [24576/50000]\tLoss: 4.7754\tLR: 8.748849\n",
      "Training Epoch: 88 [24704/50000]\tLoss: 4.7951\tLR: 8.749105\n",
      "Training Epoch: 88 [24832/50000]\tLoss: 4.8361\tLR: 8.749361\n",
      "Training Epoch: 88 [24960/50000]\tLoss: 4.8559\tLR: 8.749616\n",
      "Training Epoch: 88 [25088/50000]\tLoss: 4.7865\tLR: 8.749872\n",
      "Training Epoch: 88 [25216/50000]\tLoss: 4.8270\tLR: 8.750128\n",
      "Training Epoch: 88 [25344/50000]\tLoss: 4.7403\tLR: 8.750384\n",
      "Training Epoch: 88 [25472/50000]\tLoss: 4.7431\tLR: 8.750639\n",
      "Training Epoch: 88 [25600/50000]\tLoss: 4.8354\tLR: 8.750895\n",
      "Training Epoch: 88 [25728/50000]\tLoss: 4.8298\tLR: 8.751151\n",
      "Training Epoch: 88 [25856/50000]\tLoss: 4.7272\tLR: 8.751407\n",
      "Training Epoch: 88 [25984/50000]\tLoss: 4.8843\tLR: 8.751662\n",
      "Training Epoch: 88 [26112/50000]\tLoss: 4.7585\tLR: 8.751918\n",
      "Training Epoch: 88 [26240/50000]\tLoss: 4.7881\tLR: 8.752174\n",
      "Training Epoch: 88 [26368/50000]\tLoss: 4.7109\tLR: 8.752430\n",
      "Training Epoch: 88 [26496/50000]\tLoss: 4.7029\tLR: 8.752685\n",
      "Training Epoch: 88 [26624/50000]\tLoss: 4.7102\tLR: 8.752941\n",
      "Training Epoch: 88 [26752/50000]\tLoss: 4.7617\tLR: 8.753197\n",
      "Training Epoch: 88 [26880/50000]\tLoss: 4.8199\tLR: 8.753453\n",
      "Training Epoch: 88 [27008/50000]\tLoss: 4.7303\tLR: 8.753708\n",
      "Training Epoch: 88 [27136/50000]\tLoss: 4.7880\tLR: 8.753964\n",
      "Training Epoch: 88 [27264/50000]\tLoss: 4.7777\tLR: 8.754220\n",
      "Training Epoch: 88 [27392/50000]\tLoss: 4.7298\tLR: 8.754476\n",
      "Training Epoch: 88 [27520/50000]\tLoss: 4.8633\tLR: 8.754731\n",
      "Training Epoch: 88 [27648/50000]\tLoss: 4.8545\tLR: 8.754987\n",
      "Training Epoch: 88 [27776/50000]\tLoss: 4.8193\tLR: 8.755243\n",
      "Training Epoch: 88 [27904/50000]\tLoss: 4.6658\tLR: 8.755499\n",
      "Training Epoch: 88 [28032/50000]\tLoss: 4.7832\tLR: 8.755754\n",
      "Training Epoch: 88 [28160/50000]\tLoss: 4.7521\tLR: 8.756010\n",
      "Training Epoch: 88 [28288/50000]\tLoss: 4.7464\tLR: 8.756266\n",
      "Training Epoch: 88 [28416/50000]\tLoss: 4.8370\tLR: 8.756522\n",
      "Training Epoch: 88 [28544/50000]\tLoss: 4.7758\tLR: 8.756777\n",
      "Training Epoch: 88 [28672/50000]\tLoss: 4.7422\tLR: 8.757033\n",
      "Training Epoch: 88 [28800/50000]\tLoss: 4.7922\tLR: 8.757289\n",
      "Training Epoch: 88 [28928/50000]\tLoss: 4.6865\tLR: 8.757545\n",
      "Training Epoch: 88 [29056/50000]\tLoss: 4.7660\tLR: 8.757801\n",
      "Training Epoch: 88 [29184/50000]\tLoss: 4.8036\tLR: 8.758056\n",
      "Training Epoch: 88 [29312/50000]\tLoss: 4.7618\tLR: 8.758312\n",
      "Training Epoch: 88 [29440/50000]\tLoss: 4.6704\tLR: 8.758568\n",
      "Training Epoch: 88 [29568/50000]\tLoss: 4.6985\tLR: 8.758824\n",
      "Training Epoch: 88 [29696/50000]\tLoss: 4.7144\tLR: 8.759079\n",
      "Training Epoch: 88 [29824/50000]\tLoss: 4.7492\tLR: 8.759335\n",
      "Training Epoch: 88 [29952/50000]\tLoss: 4.7651\tLR: 8.759591\n",
      "Training Epoch: 88 [30080/50000]\tLoss: 4.6252\tLR: 8.759847\n",
      "Training Epoch: 88 [30208/50000]\tLoss: 4.6699\tLR: 8.760102\n",
      "Training Epoch: 88 [30336/50000]\tLoss: 4.7266\tLR: 8.760358\n",
      "Training Epoch: 88 [30464/50000]\tLoss: 4.7042\tLR: 8.760614\n",
      "Training Epoch: 88 [30592/50000]\tLoss: 4.8129\tLR: 8.760870\n",
      "Training Epoch: 88 [30720/50000]\tLoss: 4.7581\tLR: 8.761125\n",
      "Training Epoch: 88 [30848/50000]\tLoss: 4.7704\tLR: 8.761381\n",
      "Training Epoch: 88 [30976/50000]\tLoss: 4.6465\tLR: 8.761637\n",
      "Training Epoch: 88 [31104/50000]\tLoss: 4.7275\tLR: 8.761893\n",
      "Training Epoch: 88 [31232/50000]\tLoss: 4.7854\tLR: 8.762148\n",
      "Training Epoch: 88 [31360/50000]\tLoss: 4.7320\tLR: 8.762404\n",
      "Training Epoch: 88 [31488/50000]\tLoss: 4.7680\tLR: 8.762660\n",
      "Training Epoch: 88 [31616/50000]\tLoss: 4.6849\tLR: 8.762916\n",
      "Training Epoch: 88 [31744/50000]\tLoss: 4.7528\tLR: 8.763171\n",
      "Training Epoch: 88 [31872/50000]\tLoss: 4.8534\tLR: 8.763427\n",
      "Training Epoch: 88 [32000/50000]\tLoss: 4.8678\tLR: 8.763683\n",
      "Training Epoch: 88 [32128/50000]\tLoss: 4.7718\tLR: 8.763939\n",
      "Training Epoch: 88 [32256/50000]\tLoss: 4.7254\tLR: 8.764194\n",
      "Training Epoch: 88 [32384/50000]\tLoss: 4.7706\tLR: 8.764450\n",
      "Training Epoch: 88 [32512/50000]\tLoss: 4.7379\tLR: 8.764706\n",
      "Training Epoch: 88 [32640/50000]\tLoss: 4.7455\tLR: 8.764962\n",
      "Training Epoch: 88 [32768/50000]\tLoss: 4.7111\tLR: 8.765217\n",
      "Training Epoch: 88 [32896/50000]\tLoss: 4.7915\tLR: 8.765473\n",
      "Training Epoch: 88 [33024/50000]\tLoss: 4.7397\tLR: 8.765729\n",
      "Training Epoch: 88 [33152/50000]\tLoss: 4.8762\tLR: 8.765985\n",
      "Training Epoch: 88 [33280/50000]\tLoss: 4.7348\tLR: 8.766240\n",
      "Training Epoch: 88 [33408/50000]\tLoss: 4.7870\tLR: 8.766496\n",
      "Training Epoch: 88 [33536/50000]\tLoss: 4.6841\tLR: 8.766752\n",
      "Training Epoch: 88 [33664/50000]\tLoss: 4.8071\tLR: 8.767008\n",
      "Training Epoch: 88 [33792/50000]\tLoss: 4.7040\tLR: 8.767263\n",
      "Training Epoch: 88 [33920/50000]\tLoss: 4.7520\tLR: 8.767519\n",
      "Training Epoch: 88 [34048/50000]\tLoss: 4.7793\tLR: 8.767775\n",
      "Training Epoch: 88 [34176/50000]\tLoss: 4.7919\tLR: 8.768031\n",
      "Training Epoch: 88 [34304/50000]\tLoss: 4.8692\tLR: 8.768286\n",
      "Training Epoch: 88 [34432/50000]\tLoss: 4.6416\tLR: 8.768542\n",
      "Training Epoch: 88 [34560/50000]\tLoss: 4.7223\tLR: 8.768798\n",
      "Training Epoch: 88 [34688/50000]\tLoss: 4.7390\tLR: 8.769054\n",
      "Training Epoch: 88 [34816/50000]\tLoss: 4.8249\tLR: 8.769309\n",
      "Training Epoch: 88 [34944/50000]\tLoss: 4.6180\tLR: 8.769565\n",
      "Training Epoch: 88 [35072/50000]\tLoss: 4.7777\tLR: 8.769821\n",
      "Training Epoch: 88 [35200/50000]\tLoss: 4.7899\tLR: 8.770077\n",
      "Training Epoch: 88 [35328/50000]\tLoss: 4.7742\tLR: 8.770332\n",
      "Training Epoch: 88 [35456/50000]\tLoss: 4.7634\tLR: 8.770588\n",
      "Training Epoch: 88 [35584/50000]\tLoss: 4.8051\tLR: 8.770844\n",
      "Training Epoch: 88 [35712/50000]\tLoss: 4.7536\tLR: 8.771100\n",
      "Training Epoch: 88 [35840/50000]\tLoss: 4.7250\tLR: 8.771355\n",
      "Training Epoch: 88 [35968/50000]\tLoss: 4.6686\tLR: 8.771611\n",
      "Training Epoch: 88 [36096/50000]\tLoss: 4.6768\tLR: 8.771867\n",
      "Training Epoch: 88 [36224/50000]\tLoss: 4.8443\tLR: 8.772123\n",
      "Training Epoch: 88 [36352/50000]\tLoss: 4.7676\tLR: 8.772379\n",
      "Training Epoch: 88 [36480/50000]\tLoss: 4.8233\tLR: 8.772634\n",
      "Training Epoch: 88 [36608/50000]\tLoss: 4.8456\tLR: 8.772890\n",
      "Training Epoch: 88 [36736/50000]\tLoss: 4.7689\tLR: 8.773146\n",
      "Training Epoch: 88 [36864/50000]\tLoss: 4.7291\tLR: 8.773402\n",
      "Training Epoch: 88 [36992/50000]\tLoss: 4.7845\tLR: 8.773657\n",
      "Training Epoch: 88 [37120/50000]\tLoss: 4.7471\tLR: 8.773913\n",
      "Training Epoch: 88 [37248/50000]\tLoss: 4.7235\tLR: 8.774169\n",
      "Training Epoch: 88 [37376/50000]\tLoss: 4.7553\tLR: 8.774425\n",
      "Training Epoch: 88 [37504/50000]\tLoss: 4.6933\tLR: 8.774680\n",
      "Training Epoch: 88 [37632/50000]\tLoss: 4.7615\tLR: 8.774936\n",
      "Training Epoch: 88 [37760/50000]\tLoss: 4.8494\tLR: 8.775192\n",
      "Training Epoch: 88 [37888/50000]\tLoss: 4.7998\tLR: 8.775448\n",
      "Training Epoch: 88 [38016/50000]\tLoss: 4.7919\tLR: 8.775703\n",
      "Training Epoch: 88 [38144/50000]\tLoss: 4.7422\tLR: 8.775959\n",
      "Training Epoch: 88 [38272/50000]\tLoss: 4.8033\tLR: 8.776215\n",
      "Training Epoch: 88 [38400/50000]\tLoss: 4.7795\tLR: 8.776471\n",
      "Training Epoch: 88 [38528/50000]\tLoss: 4.7305\tLR: 8.776726\n",
      "Training Epoch: 88 [38656/50000]\tLoss: 4.6908\tLR: 8.776982\n",
      "Training Epoch: 88 [38784/50000]\tLoss: 4.7806\tLR: 8.777238\n",
      "Training Epoch: 88 [38912/50000]\tLoss: 4.7265\tLR: 8.777494\n",
      "Training Epoch: 88 [39040/50000]\tLoss: 4.8183\tLR: 8.777749\n",
      "Training Epoch: 88 [39168/50000]\tLoss: 4.7629\tLR: 8.778005\n",
      "Training Epoch: 88 [39296/50000]\tLoss: 4.7822\tLR: 8.778261\n",
      "Training Epoch: 88 [39424/50000]\tLoss: 4.7830\tLR: 8.778517\n",
      "Training Epoch: 88 [39552/50000]\tLoss: 4.7913\tLR: 8.778772\n",
      "Training Epoch: 88 [39680/50000]\tLoss: 4.8301\tLR: 8.779028\n",
      "Training Epoch: 88 [39808/50000]\tLoss: 4.7705\tLR: 8.779284\n",
      "Training Epoch: 88 [39936/50000]\tLoss: 4.8465\tLR: 8.779540\n",
      "Training Epoch: 88 [40064/50000]\tLoss: 4.7250\tLR: 8.779795\n",
      "Training Epoch: 88 [40192/50000]\tLoss: 4.8146\tLR: 8.780051\n",
      "Training Epoch: 88 [40320/50000]\tLoss: 4.7385\tLR: 8.780307\n",
      "Training Epoch: 88 [40448/50000]\tLoss: 4.7536\tLR: 8.780563\n",
      "Training Epoch: 88 [40576/50000]\tLoss: 4.7616\tLR: 8.780818\n",
      "Training Epoch: 88 [40704/50000]\tLoss: 4.7820\tLR: 8.781074\n",
      "Training Epoch: 88 [40832/50000]\tLoss: 4.7731\tLR: 8.781330\n",
      "Training Epoch: 88 [40960/50000]\tLoss: 4.8330\tLR: 8.781586\n",
      "Training Epoch: 88 [41088/50000]\tLoss: 4.7652\tLR: 8.781841\n",
      "Training Epoch: 88 [41216/50000]\tLoss: 4.8114\tLR: 8.782097\n",
      "Training Epoch: 88 [41344/50000]\tLoss: 4.6457\tLR: 8.782353\n",
      "Training Epoch: 88 [41472/50000]\tLoss: 4.7831\tLR: 8.782609\n",
      "Training Epoch: 88 [41600/50000]\tLoss: 4.8192\tLR: 8.782864\n",
      "Training Epoch: 88 [41728/50000]\tLoss: 4.7669\tLR: 8.783120\n",
      "Training Epoch: 88 [41856/50000]\tLoss: 4.8336\tLR: 8.783376\n",
      "Training Epoch: 88 [41984/50000]\tLoss: 4.9046\tLR: 8.783632\n",
      "Training Epoch: 88 [42112/50000]\tLoss: 4.8928\tLR: 8.783887\n",
      "Training Epoch: 88 [42240/50000]\tLoss: 4.7200\tLR: 8.784143\n",
      "Training Epoch: 88 [42368/50000]\tLoss: 4.8116\tLR: 8.784399\n",
      "Training Epoch: 88 [42496/50000]\tLoss: 4.7431\tLR: 8.784655\n",
      "Training Epoch: 88 [42624/50000]\tLoss: 4.7477\tLR: 8.784910\n",
      "Training Epoch: 88 [42752/50000]\tLoss: 4.8091\tLR: 8.785166\n",
      "Training Epoch: 88 [42880/50000]\tLoss: 4.7539\tLR: 8.785422\n",
      "Training Epoch: 88 [43008/50000]\tLoss: 4.8058\tLR: 8.785678\n",
      "Training Epoch: 88 [43136/50000]\tLoss: 4.8147\tLR: 8.785934\n",
      "Training Epoch: 88 [43264/50000]\tLoss: 4.8142\tLR: 8.786189\n",
      "Training Epoch: 88 [43392/50000]\tLoss: 4.8831\tLR: 8.786445\n",
      "Training Epoch: 88 [43520/50000]\tLoss: 4.7260\tLR: 8.786701\n",
      "Training Epoch: 88 [43648/50000]\tLoss: 4.8309\tLR: 8.786957\n",
      "Training Epoch: 88 [43776/50000]\tLoss: 4.7922\tLR: 8.787212\n",
      "Training Epoch: 88 [43904/50000]\tLoss: 4.6544\tLR: 8.787468\n",
      "Training Epoch: 88 [44032/50000]\tLoss: 4.7360\tLR: 8.787724\n",
      "Training Epoch: 88 [44160/50000]\tLoss: 4.7186\tLR: 8.787980\n",
      "Training Epoch: 88 [44288/50000]\tLoss: 4.8874\tLR: 8.788235\n",
      "Training Epoch: 88 [44416/50000]\tLoss: 4.7892\tLR: 8.788491\n",
      "Training Epoch: 88 [44544/50000]\tLoss: 4.7233\tLR: 8.788747\n",
      "Training Epoch: 88 [44672/50000]\tLoss: 4.8347\tLR: 8.789003\n",
      "Training Epoch: 88 [44800/50000]\tLoss: 4.8006\tLR: 8.789258\n",
      "Training Epoch: 88 [44928/50000]\tLoss: 4.8174\tLR: 8.789514\n",
      "Training Epoch: 88 [45056/50000]\tLoss: 4.7703\tLR: 8.789770\n",
      "Training Epoch: 88 [45184/50000]\tLoss: 4.8265\tLR: 8.790026\n",
      "Training Epoch: 88 [45312/50000]\tLoss: 4.7561\tLR: 8.790281\n",
      "Training Epoch: 88 [45440/50000]\tLoss: 4.8296\tLR: 8.790537\n",
      "Training Epoch: 88 [45568/50000]\tLoss: 4.6854\tLR: 8.790793\n",
      "Training Epoch: 88 [45696/50000]\tLoss: 4.8116\tLR: 8.791049\n",
      "Training Epoch: 88 [45824/50000]\tLoss: 4.9366\tLR: 8.791304\n",
      "Training Epoch: 88 [45952/50000]\tLoss: 4.8704\tLR: 8.791560\n",
      "Training Epoch: 88 [46080/50000]\tLoss: 4.8252\tLR: 8.791816\n",
      "Training Epoch: 88 [46208/50000]\tLoss: 4.7231\tLR: 8.792072\n",
      "Training Epoch: 88 [46336/50000]\tLoss: 4.7883\tLR: 8.792327\n",
      "Training Epoch: 88 [46464/50000]\tLoss: 4.8410\tLR: 8.792583\n",
      "Training Epoch: 88 [46592/50000]\tLoss: 4.7242\tLR: 8.792839\n",
      "Training Epoch: 88 [46720/50000]\tLoss: 4.8035\tLR: 8.793095\n",
      "Training Epoch: 88 [46848/50000]\tLoss: 4.7668\tLR: 8.793350\n",
      "Training Epoch: 88 [46976/50000]\tLoss: 4.8531\tLR: 8.793606\n",
      "Training Epoch: 88 [47104/50000]\tLoss: 4.8744\tLR: 8.793862\n",
      "Training Epoch: 88 [47232/50000]\tLoss: 4.7565\tLR: 8.794118\n",
      "Training Epoch: 88 [47360/50000]\tLoss: 4.7098\tLR: 8.794373\n",
      "Training Epoch: 88 [47488/50000]\tLoss: 4.7890\tLR: 8.794629\n",
      "Training Epoch: 88 [47616/50000]\tLoss: 4.7752\tLR: 8.794885\n",
      "Training Epoch: 88 [47744/50000]\tLoss: 4.7119\tLR: 8.795141\n",
      "Training Epoch: 88 [47872/50000]\tLoss: 4.7276\tLR: 8.795396\n",
      "Training Epoch: 88 [48000/50000]\tLoss: 4.9017\tLR: 8.795652\n",
      "Training Epoch: 88 [48128/50000]\tLoss: 4.7551\tLR: 8.795908\n",
      "Training Epoch: 88 [48256/50000]\tLoss: 4.7815\tLR: 8.796164\n",
      "Training Epoch: 88 [48384/50000]\tLoss: 4.8762\tLR: 8.796419\n",
      "Training Epoch: 88 [48512/50000]\tLoss: 4.7257\tLR: 8.796675\n",
      "Training Epoch: 88 [48640/50000]\tLoss: 4.7816\tLR: 8.796931\n",
      "Training Epoch: 88 [48768/50000]\tLoss: 4.8103\tLR: 8.797187\n",
      "Training Epoch: 88 [48896/50000]\tLoss: 4.8691\tLR: 8.797442\n",
      "Training Epoch: 88 [49024/50000]\tLoss: 4.7730\tLR: 8.797698\n",
      "Training Epoch: 88 [49152/50000]\tLoss: 4.8101\tLR: 8.797954\n",
      "Training Epoch: 88 [49280/50000]\tLoss: 4.8246\tLR: 8.798210\n",
      "Training Epoch: 88 [49408/50000]\tLoss: 4.6043\tLR: 8.798465\n",
      "Training Epoch: 88 [49536/50000]\tLoss: 4.7710\tLR: 8.798721\n",
      "Training Epoch: 88 [49664/50000]\tLoss: 4.7283\tLR: 8.798977\n",
      "Training Epoch: 88 [49792/50000]\tLoss: 4.7741\tLR: 8.799233\n",
      "Training Epoch: 88 [49920/50000]\tLoss: 4.8480\tLR: 8.799488\n",
      "Training Epoch: 88 [50000/50000]\tLoss: 4.7996\tLR: 8.799744\n",
      "epoch 88 training time consumed: 489.33s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  123370 GB |  123369 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  122991 GB |  122991 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     378 GB |     378 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  123370 GB |  123369 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  122991 GB |  122991 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     378 GB |     378 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  121636 GB |  121636 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  121257 GB |  121257 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     378 GB |     378 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   13081 K  |   13081 K  |\n",
      "|       from large pool |      24    |      65    |    5576 K  |    5576 K  |\n",
      "|       from small pool |     231    |     274    |    7504 K  |    7504 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   13081 K  |   13081 K  |\n",
      "|       from large pool |      24    |      65    |    5576 K  |    5576 K  |\n",
      "|       from small pool |     231    |     274    |    7504 K  |    7504 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      47    |    7582 K  |    7582 K  |\n",
      "|       from large pool |      10    |      23    |    2680 K  |    2680 K  |\n",
      "|       from small pool |      27    |      35    |    4902 K  |    4901 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 88, Average loss: 0.0378, Accuracy: 0.0100, Time consumed:31.52s\n",
      "\n",
      "Training Epoch: 89 [128/50000]\tLoss: 4.8582\tLR: 0.020000\n",
      "Training Epoch: 89 [256/50000]\tLoss: 4.7908\tLR: 8.800256\n",
      "Training Epoch: 89 [384/50000]\tLoss: 4.8039\tLR: 8.800512\n",
      "Training Epoch: 89 [512/50000]\tLoss: 4.6747\tLR: 8.800767\n",
      "Training Epoch: 89 [640/50000]\tLoss: 4.6968\tLR: 8.801023\n",
      "Training Epoch: 89 [768/50000]\tLoss: 4.7244\tLR: 8.801279\n",
      "Training Epoch: 89 [896/50000]\tLoss: 4.7107\tLR: 8.801535\n",
      "Training Epoch: 89 [1024/50000]\tLoss: 4.7468\tLR: 8.801790\n",
      "Training Epoch: 89 [1152/50000]\tLoss: 4.7325\tLR: 8.802046\n",
      "Training Epoch: 89 [1280/50000]\tLoss: 4.7763\tLR: 8.802302\n",
      "Training Epoch: 89 [1408/50000]\tLoss: 4.8026\tLR: 8.802558\n",
      "Training Epoch: 89 [1536/50000]\tLoss: 4.8416\tLR: 8.802813\n",
      "Training Epoch: 89 [1664/50000]\tLoss: 4.8167\tLR: 8.803069\n",
      "Training Epoch: 89 [1792/50000]\tLoss: 4.7484\tLR: 8.803325\n",
      "Training Epoch: 89 [1920/50000]\tLoss: 4.7224\tLR: 8.803581\n",
      "Training Epoch: 89 [2048/50000]\tLoss: 4.7457\tLR: 8.803836\n",
      "Training Epoch: 89 [2176/50000]\tLoss: 4.7370\tLR: 8.804092\n",
      "Training Epoch: 89 [2304/50000]\tLoss: 4.7940\tLR: 8.804348\n",
      "Training Epoch: 89 [2432/50000]\tLoss: 4.7017\tLR: 8.804604\n",
      "Training Epoch: 89 [2560/50000]\tLoss: 4.7217\tLR: 8.804859\n",
      "Training Epoch: 89 [2688/50000]\tLoss: 4.6492\tLR: 8.805115\n",
      "Training Epoch: 89 [2816/50000]\tLoss: 4.7522\tLR: 8.805371\n",
      "Training Epoch: 89 [2944/50000]\tLoss: 4.6599\tLR: 8.805627\n",
      "Training Epoch: 89 [3072/50000]\tLoss: 4.8088\tLR: 8.805882\n",
      "Training Epoch: 89 [3200/50000]\tLoss: 4.8140\tLR: 8.806138\n",
      "Training Epoch: 89 [3328/50000]\tLoss: 4.8035\tLR: 8.806394\n",
      "Training Epoch: 89 [3456/50000]\tLoss: 4.7810\tLR: 8.806650\n",
      "Training Epoch: 89 [3584/50000]\tLoss: 4.7049\tLR: 8.806905\n",
      "Training Epoch: 89 [3712/50000]\tLoss: 4.7166\tLR: 8.807161\n",
      "Training Epoch: 89 [3840/50000]\tLoss: 4.8195\tLR: 8.807417\n",
      "Training Epoch: 89 [3968/50000]\tLoss: 4.7733\tLR: 8.807673\n",
      "Training Epoch: 89 [4096/50000]\tLoss: 4.7656\tLR: 8.807928\n",
      "Training Epoch: 89 [4224/50000]\tLoss: 4.8781\tLR: 8.808184\n",
      "Training Epoch: 89 [4352/50000]\tLoss: 4.8310\tLR: 8.808440\n",
      "Training Epoch: 89 [4480/50000]\tLoss: 4.7573\tLR: 8.808696\n",
      "Training Epoch: 89 [4608/50000]\tLoss: 4.8543\tLR: 8.808951\n",
      "Training Epoch: 89 [4736/50000]\tLoss: 4.7345\tLR: 8.809207\n",
      "Training Epoch: 89 [4864/50000]\tLoss: 4.7949\tLR: 8.809463\n",
      "Training Epoch: 89 [4992/50000]\tLoss: 4.7057\tLR: 8.809719\n",
      "Training Epoch: 89 [5120/50000]\tLoss: 4.7265\tLR: 8.809974\n",
      "Training Epoch: 89 [5248/50000]\tLoss: 4.7585\tLR: 8.810230\n",
      "Training Epoch: 89 [5376/50000]\tLoss: 4.8076\tLR: 8.810486\n",
      "Training Epoch: 89 [5504/50000]\tLoss: 4.7982\tLR: 8.810742\n",
      "Training Epoch: 89 [5632/50000]\tLoss: 4.8941\tLR: 8.810997\n",
      "Training Epoch: 89 [5760/50000]\tLoss: 4.8834\tLR: 8.811253\n",
      "Training Epoch: 89 [5888/50000]\tLoss: 4.7056\tLR: 8.811509\n",
      "Training Epoch: 89 [6016/50000]\tLoss: 4.7576\tLR: 8.811765\n",
      "Training Epoch: 89 [6144/50000]\tLoss: 4.7016\tLR: 8.812020\n",
      "Training Epoch: 89 [6272/50000]\tLoss: 4.7278\tLR: 8.812276\n",
      "Training Epoch: 89 [6400/50000]\tLoss: 4.7823\tLR: 8.812532\n",
      "Training Epoch: 89 [6528/50000]\tLoss: 4.8152\tLR: 8.812788\n",
      "Training Epoch: 89 [6656/50000]\tLoss: 4.7867\tLR: 8.813043\n",
      "Training Epoch: 89 [6784/50000]\tLoss: 4.7714\tLR: 8.813299\n",
      "Training Epoch: 89 [6912/50000]\tLoss: 4.7956\tLR: 8.813555\n",
      "Training Epoch: 89 [7040/50000]\tLoss: 4.7914\tLR: 8.813811\n",
      "Training Epoch: 89 [7168/50000]\tLoss: 4.7769\tLR: 8.814066\n",
      "Training Epoch: 89 [7296/50000]\tLoss: 4.7946\tLR: 8.814322\n",
      "Training Epoch: 89 [7424/50000]\tLoss: 4.7990\tLR: 8.814578\n",
      "Training Epoch: 89 [7552/50000]\tLoss: 4.8035\tLR: 8.814834\n",
      "Training Epoch: 89 [7680/50000]\tLoss: 4.6589\tLR: 8.815090\n",
      "Training Epoch: 89 [7808/50000]\tLoss: 4.7049\tLR: 8.815345\n",
      "Training Epoch: 89 [7936/50000]\tLoss: 4.7623\tLR: 8.815601\n",
      "Training Epoch: 89 [8064/50000]\tLoss: 4.7581\tLR: 8.815857\n",
      "Training Epoch: 89 [8192/50000]\tLoss: 4.6933\tLR: 8.816113\n",
      "Training Epoch: 89 [8320/50000]\tLoss: 4.7183\tLR: 8.816368\n",
      "Training Epoch: 89 [8448/50000]\tLoss: 4.6587\tLR: 8.816624\n",
      "Training Epoch: 89 [8576/50000]\tLoss: 4.7825\tLR: 8.816880\n",
      "Training Epoch: 89 [8704/50000]\tLoss: 4.8044\tLR: 8.817136\n",
      "Training Epoch: 89 [8832/50000]\tLoss: 4.7699\tLR: 8.817391\n",
      "Training Epoch: 89 [8960/50000]\tLoss: 4.6618\tLR: 8.817647\n",
      "Training Epoch: 89 [9088/50000]\tLoss: 4.7550\tLR: 8.817903\n",
      "Training Epoch: 89 [9216/50000]\tLoss: 4.8286\tLR: 8.818159\n",
      "Training Epoch: 89 [9344/50000]\tLoss: 4.7204\tLR: 8.818414\n",
      "Training Epoch: 89 [9472/50000]\tLoss: 4.7199\tLR: 8.818670\n",
      "Training Epoch: 89 [9600/50000]\tLoss: 4.7140\tLR: 8.818926\n",
      "Training Epoch: 89 [9728/50000]\tLoss: 4.8101\tLR: 8.819182\n",
      "Training Epoch: 89 [9856/50000]\tLoss: 4.7183\tLR: 8.819437\n",
      "Training Epoch: 89 [9984/50000]\tLoss: 4.7346\tLR: 8.819693\n",
      "Training Epoch: 89 [10112/50000]\tLoss: 4.7670\tLR: 8.819949\n",
      "Training Epoch: 89 [10240/50000]\tLoss: 4.7901\tLR: 8.820205\n",
      "Training Epoch: 89 [10368/50000]\tLoss: 4.7642\tLR: 8.820460\n",
      "Training Epoch: 89 [10496/50000]\tLoss: 4.7232\tLR: 8.820716\n",
      "Training Epoch: 89 [10624/50000]\tLoss: 4.7856\tLR: 8.820972\n",
      "Training Epoch: 89 [10752/50000]\tLoss: 4.6734\tLR: 8.821228\n",
      "Training Epoch: 89 [10880/50000]\tLoss: 4.8253\tLR: 8.821483\n",
      "Training Epoch: 89 [11008/50000]\tLoss: 4.7814\tLR: 8.821739\n",
      "Training Epoch: 89 [11136/50000]\tLoss: 4.7507\tLR: 8.821995\n",
      "Training Epoch: 89 [11264/50000]\tLoss: 4.7505\tLR: 8.822251\n",
      "Training Epoch: 89 [11392/50000]\tLoss: 4.7428\tLR: 8.822506\n",
      "Training Epoch: 89 [11520/50000]\tLoss: 4.7535\tLR: 8.822762\n",
      "Training Epoch: 89 [11648/50000]\tLoss: 4.7415\tLR: 8.823018\n",
      "Training Epoch: 89 [11776/50000]\tLoss: 4.8407\tLR: 8.823274\n",
      "Training Epoch: 89 [11904/50000]\tLoss: 4.7568\tLR: 8.823529\n",
      "Training Epoch: 89 [12032/50000]\tLoss: 4.6221\tLR: 8.823785\n",
      "Training Epoch: 89 [12160/50000]\tLoss: 4.7423\tLR: 8.824041\n",
      "Training Epoch: 89 [12288/50000]\tLoss: 4.8289\tLR: 8.824297\n",
      "Training Epoch: 89 [12416/50000]\tLoss: 4.7406\tLR: 8.824552\n",
      "Training Epoch: 89 [12544/50000]\tLoss: 4.8259\tLR: 8.824808\n",
      "Training Epoch: 89 [12672/50000]\tLoss: 4.7857\tLR: 8.825064\n",
      "Training Epoch: 89 [12800/50000]\tLoss: 4.7188\tLR: 8.825320\n",
      "Training Epoch: 89 [12928/50000]\tLoss: 4.7561\tLR: 8.825575\n",
      "Training Epoch: 89 [13056/50000]\tLoss: 4.6928\tLR: 8.825831\n",
      "Training Epoch: 89 [13184/50000]\tLoss: 4.7409\tLR: 8.826087\n",
      "Training Epoch: 89 [13312/50000]\tLoss: 4.6890\tLR: 8.826343\n",
      "Training Epoch: 89 [13440/50000]\tLoss: 4.7195\tLR: 8.826598\n",
      "Training Epoch: 89 [13568/50000]\tLoss: 4.8574\tLR: 8.826854\n",
      "Training Epoch: 89 [13696/50000]\tLoss: 4.8106\tLR: 8.827110\n",
      "Training Epoch: 89 [13824/50000]\tLoss: 4.7905\tLR: 8.827366\n",
      "Training Epoch: 89 [13952/50000]\tLoss: 4.8137\tLR: 8.827621\n",
      "Training Epoch: 89 [14080/50000]\tLoss: 4.7570\tLR: 8.827877\n",
      "Training Epoch: 89 [14208/50000]\tLoss: 4.8584\tLR: 8.828133\n",
      "Training Epoch: 89 [14336/50000]\tLoss: 4.7747\tLR: 8.828389\n",
      "Training Epoch: 89 [14464/50000]\tLoss: 4.7997\tLR: 8.828645\n",
      "Training Epoch: 89 [14592/50000]\tLoss: 4.6407\tLR: 8.828900\n",
      "Training Epoch: 89 [14720/50000]\tLoss: 4.7865\tLR: 8.829156\n",
      "Training Epoch: 89 [14848/50000]\tLoss: 4.7526\tLR: 8.829412\n",
      "Training Epoch: 89 [14976/50000]\tLoss: 4.7815\tLR: 8.829668\n",
      "Training Epoch: 89 [15104/50000]\tLoss: 4.8508\tLR: 8.829923\n",
      "Training Epoch: 89 [15232/50000]\tLoss: 4.7258\tLR: 8.830179\n",
      "Training Epoch: 89 [15360/50000]\tLoss: 4.7624\tLR: 8.830435\n",
      "Training Epoch: 89 [15488/50000]\tLoss: 4.8601\tLR: 8.830691\n",
      "Training Epoch: 89 [15616/50000]\tLoss: 4.8020\tLR: 8.830946\n",
      "Training Epoch: 89 [15744/50000]\tLoss: 4.7747\tLR: 8.831202\n",
      "Training Epoch: 89 [15872/50000]\tLoss: 4.7772\tLR: 8.831458\n",
      "Training Epoch: 89 [16000/50000]\tLoss: 4.8124\tLR: 8.831714\n",
      "Training Epoch: 89 [16128/50000]\tLoss: 4.7487\tLR: 8.831969\n",
      "Training Epoch: 89 [16256/50000]\tLoss: 4.8262\tLR: 8.832225\n",
      "Training Epoch: 89 [16384/50000]\tLoss: 4.7901\tLR: 8.832481\n",
      "Training Epoch: 89 [16512/50000]\tLoss: 4.6232\tLR: 8.832737\n",
      "Training Epoch: 89 [16640/50000]\tLoss: 4.7683\tLR: 8.832992\n",
      "Training Epoch: 89 [16768/50000]\tLoss: 4.8829\tLR: 8.833248\n",
      "Training Epoch: 89 [16896/50000]\tLoss: 4.8051\tLR: 8.833504\n",
      "Training Epoch: 89 [17024/50000]\tLoss: 4.7203\tLR: 8.833760\n",
      "Training Epoch: 89 [17152/50000]\tLoss: 4.8516\tLR: 8.834015\n",
      "Training Epoch: 89 [17280/50000]\tLoss: 4.7999\tLR: 8.834271\n",
      "Training Epoch: 89 [17408/50000]\tLoss: 4.7565\tLR: 8.834527\n",
      "Training Epoch: 89 [17536/50000]\tLoss: 4.7802\tLR: 8.834783\n",
      "Training Epoch: 89 [17664/50000]\tLoss: 4.6988\tLR: 8.835038\n",
      "Training Epoch: 89 [17792/50000]\tLoss: 4.7811\tLR: 8.835294\n",
      "Training Epoch: 89 [17920/50000]\tLoss: 4.7265\tLR: 8.835550\n",
      "Training Epoch: 89 [18048/50000]\tLoss: 4.6686\tLR: 8.835806\n",
      "Training Epoch: 89 [18176/50000]\tLoss: 4.7790\tLR: 8.836061\n",
      "Training Epoch: 89 [18304/50000]\tLoss: 4.6785\tLR: 8.836317\n",
      "Training Epoch: 89 [18432/50000]\tLoss: 4.7671\tLR: 8.836573\n",
      "Training Epoch: 89 [18560/50000]\tLoss: 4.7528\tLR: 8.836829\n",
      "Training Epoch: 89 [18688/50000]\tLoss: 4.7416\tLR: 8.837084\n",
      "Training Epoch: 89 [18816/50000]\tLoss: 4.7688\tLR: 8.837340\n",
      "Training Epoch: 89 [18944/50000]\tLoss: 4.7657\tLR: 8.837596\n",
      "Training Epoch: 89 [19072/50000]\tLoss: 4.7517\tLR: 8.837852\n",
      "Training Epoch: 89 [19200/50000]\tLoss: 4.7706\tLR: 8.838107\n",
      "Training Epoch: 89 [19328/50000]\tLoss: 4.8030\tLR: 8.838363\n",
      "Training Epoch: 89 [19456/50000]\tLoss: 4.7196\tLR: 8.838619\n",
      "Training Epoch: 89 [19584/50000]\tLoss: 4.7728\tLR: 8.838875\n",
      "Training Epoch: 89 [19712/50000]\tLoss: 4.6899\tLR: 8.839130\n",
      "Training Epoch: 89 [19840/50000]\tLoss: 4.8724\tLR: 8.839386\n",
      "Training Epoch: 89 [19968/50000]\tLoss: 4.8019\tLR: 8.839642\n",
      "Training Epoch: 89 [20096/50000]\tLoss: 4.6779\tLR: 8.839898\n",
      "Training Epoch: 89 [20224/50000]\tLoss: 4.7207\tLR: 8.840153\n",
      "Training Epoch: 89 [20352/50000]\tLoss: 4.7248\tLR: 8.840409\n",
      "Training Epoch: 89 [20480/50000]\tLoss: 4.7314\tLR: 8.840665\n",
      "Training Epoch: 89 [20608/50000]\tLoss: 4.8183\tLR: 8.840921\n",
      "Training Epoch: 89 [20736/50000]\tLoss: 4.8526\tLR: 8.841176\n",
      "Training Epoch: 89 [20864/50000]\tLoss: 4.8282\tLR: 8.841432\n",
      "Training Epoch: 89 [20992/50000]\tLoss: 4.8763\tLR: 8.841688\n",
      "Training Epoch: 89 [21120/50000]\tLoss: 4.6531\tLR: 8.841944\n",
      "Training Epoch: 89 [21248/50000]\tLoss: 4.8169\tLR: 8.842199\n",
      "Training Epoch: 89 [21376/50000]\tLoss: 4.7633\tLR: 8.842455\n",
      "Training Epoch: 89 [21504/50000]\tLoss: 4.7708\tLR: 8.842711\n",
      "Training Epoch: 89 [21632/50000]\tLoss: 4.7966\tLR: 8.842967\n",
      "Training Epoch: 89 [21760/50000]\tLoss: 4.7424\tLR: 8.843223\n",
      "Training Epoch: 89 [21888/50000]\tLoss: 4.7943\tLR: 8.843478\n",
      "Training Epoch: 89 [22016/50000]\tLoss: 4.7204\tLR: 8.843734\n",
      "Training Epoch: 89 [22144/50000]\tLoss: 4.7525\tLR: 8.843990\n",
      "Training Epoch: 89 [22272/50000]\tLoss: 4.8196\tLR: 8.844246\n",
      "Training Epoch: 89 [22400/50000]\tLoss: 4.7453\tLR: 8.844501\n",
      "Training Epoch: 89 [22528/50000]\tLoss: 4.8377\tLR: 8.844757\n",
      "Training Epoch: 89 [22656/50000]\tLoss: 4.7559\tLR: 8.845013\n",
      "Training Epoch: 89 [22784/50000]\tLoss: 4.7941\tLR: 8.845269\n",
      "Training Epoch: 89 [22912/50000]\tLoss: 4.7571\tLR: 8.845524\n",
      "Training Epoch: 89 [23040/50000]\tLoss: 4.7181\tLR: 8.845780\n",
      "Training Epoch: 89 [23168/50000]\tLoss: 4.7759\tLR: 8.846036\n",
      "Training Epoch: 89 [23296/50000]\tLoss: 4.8316\tLR: 8.846292\n",
      "Training Epoch: 89 [23424/50000]\tLoss: 4.7118\tLR: 8.846547\n",
      "Training Epoch: 89 [23552/50000]\tLoss: 4.7806\tLR: 8.846803\n",
      "Training Epoch: 89 [23680/50000]\tLoss: 4.7052\tLR: 8.847059\n",
      "Training Epoch: 89 [23808/50000]\tLoss: 4.8463\tLR: 8.847315\n",
      "Training Epoch: 89 [23936/50000]\tLoss: 4.7832\tLR: 8.847570\n",
      "Training Epoch: 89 [24064/50000]\tLoss: 4.8317\tLR: 8.847826\n",
      "Training Epoch: 89 [24192/50000]\tLoss: 4.8296\tLR: 8.848082\n",
      "Training Epoch: 89 [24320/50000]\tLoss: 4.7391\tLR: 8.848338\n",
      "Training Epoch: 89 [24448/50000]\tLoss: 4.7578\tLR: 8.848593\n",
      "Training Epoch: 89 [24576/50000]\tLoss: 4.7339\tLR: 8.848849\n",
      "Training Epoch: 89 [24704/50000]\tLoss: 4.7383\tLR: 8.849105\n",
      "Training Epoch: 89 [24832/50000]\tLoss: 4.6870\tLR: 8.849361\n",
      "Training Epoch: 89 [24960/50000]\tLoss: 4.8533\tLR: 8.849616\n",
      "Training Epoch: 89 [25088/50000]\tLoss: 4.8886\tLR: 8.849872\n",
      "Training Epoch: 89 [25216/50000]\tLoss: 4.7811\tLR: 8.850128\n",
      "Training Epoch: 89 [25344/50000]\tLoss: 4.7280\tLR: 8.850384\n",
      "Training Epoch: 89 [25472/50000]\tLoss: 4.7861\tLR: 8.850639\n",
      "Training Epoch: 89 [25600/50000]\tLoss: 4.7496\tLR: 8.850895\n",
      "Training Epoch: 89 [25728/50000]\tLoss: 4.7419\tLR: 8.851151\n",
      "Training Epoch: 89 [25856/50000]\tLoss: 4.7256\tLR: 8.851407\n",
      "Training Epoch: 89 [25984/50000]\tLoss: 4.7408\tLR: 8.851662\n",
      "Training Epoch: 89 [26112/50000]\tLoss: 4.8321\tLR: 8.851918\n",
      "Training Epoch: 89 [26240/50000]\tLoss: 4.7926\tLR: 8.852174\n",
      "Training Epoch: 89 [26368/50000]\tLoss: 4.7889\tLR: 8.852430\n",
      "Training Epoch: 89 [26496/50000]\tLoss: 4.8340\tLR: 8.852685\n",
      "Training Epoch: 89 [26624/50000]\tLoss: 4.8058\tLR: 8.852941\n",
      "Training Epoch: 89 [26752/50000]\tLoss: 4.7852\tLR: 8.853197\n",
      "Training Epoch: 89 [26880/50000]\tLoss: 4.8179\tLR: 8.853453\n",
      "Training Epoch: 89 [27008/50000]\tLoss: 4.6945\tLR: 8.853708\n",
      "Training Epoch: 89 [27136/50000]\tLoss: 4.7130\tLR: 8.853964\n",
      "Training Epoch: 89 [27264/50000]\tLoss: 4.7908\tLR: 8.854220\n",
      "Training Epoch: 89 [27392/50000]\tLoss: 4.8239\tLR: 8.854476\n",
      "Training Epoch: 89 [27520/50000]\tLoss: 4.7087\tLR: 8.854731\n",
      "Training Epoch: 89 [27648/50000]\tLoss: 4.7361\tLR: 8.854987\n",
      "Training Epoch: 89 [27776/50000]\tLoss: 4.6838\tLR: 8.855243\n",
      "Training Epoch: 89 [27904/50000]\tLoss: 4.7532\tLR: 8.855499\n",
      "Training Epoch: 89 [28032/50000]\tLoss: 4.7387\tLR: 8.855754\n",
      "Training Epoch: 89 [28160/50000]\tLoss: 4.7254\tLR: 8.856010\n",
      "Training Epoch: 89 [28288/50000]\tLoss: 4.7263\tLR: 8.856266\n",
      "Training Epoch: 89 [28416/50000]\tLoss: 4.6781\tLR: 8.856522\n",
      "Training Epoch: 89 [28544/50000]\tLoss: 4.7317\tLR: 8.856777\n",
      "Training Epoch: 89 [28672/50000]\tLoss: 4.6882\tLR: 8.857033\n",
      "Training Epoch: 89 [28800/50000]\tLoss: 4.7795\tLR: 8.857289\n",
      "Training Epoch: 89 [28928/50000]\tLoss: 4.7624\tLR: 8.857545\n",
      "Training Epoch: 89 [29056/50000]\tLoss: 4.6771\tLR: 8.857801\n",
      "Training Epoch: 89 [29184/50000]\tLoss: 4.6667\tLR: 8.858056\n",
      "Training Epoch: 89 [29312/50000]\tLoss: 4.7374\tLR: 8.858312\n",
      "Training Epoch: 89 [29440/50000]\tLoss: 4.8106\tLR: 8.858568\n",
      "Training Epoch: 89 [29568/50000]\tLoss: 4.7970\tLR: 8.858824\n",
      "Training Epoch: 89 [29696/50000]\tLoss: 4.7636\tLR: 8.859079\n",
      "Training Epoch: 89 [29824/50000]\tLoss: 4.7035\tLR: 8.859335\n",
      "Training Epoch: 89 [29952/50000]\tLoss: 4.7979\tLR: 8.859591\n",
      "Training Epoch: 89 [30080/50000]\tLoss: 4.7759\tLR: 8.859847\n",
      "Training Epoch: 89 [30208/50000]\tLoss: 4.6094\tLR: 8.860102\n",
      "Training Epoch: 89 [30336/50000]\tLoss: 4.7382\tLR: 8.860358\n",
      "Training Epoch: 89 [30464/50000]\tLoss: 4.7208\tLR: 8.860614\n",
      "Training Epoch: 89 [30592/50000]\tLoss: 4.7255\tLR: 8.860870\n",
      "Training Epoch: 89 [30720/50000]\tLoss: 4.7188\tLR: 8.861125\n",
      "Training Epoch: 89 [30848/50000]\tLoss: 4.7907\tLR: 8.861381\n",
      "Training Epoch: 89 [30976/50000]\tLoss: 4.7470\tLR: 8.861637\n",
      "Training Epoch: 89 [31104/50000]\tLoss: 4.7581\tLR: 8.861893\n",
      "Training Epoch: 89 [31232/50000]\tLoss: 4.8413\tLR: 8.862148\n",
      "Training Epoch: 89 [31360/50000]\tLoss: 4.7800\tLR: 8.862404\n",
      "Training Epoch: 89 [31488/50000]\tLoss: 4.7504\tLR: 8.862660\n",
      "Training Epoch: 89 [31616/50000]\tLoss: 4.8010\tLR: 8.862916\n",
      "Training Epoch: 89 [31744/50000]\tLoss: 4.8150\tLR: 8.863171\n",
      "Training Epoch: 89 [31872/50000]\tLoss: 4.7492\tLR: 8.863427\n",
      "Training Epoch: 89 [32000/50000]\tLoss: 4.8045\tLR: 8.863683\n",
      "Training Epoch: 89 [32128/50000]\tLoss: 4.7174\tLR: 8.863939\n",
      "Training Epoch: 89 [32256/50000]\tLoss: 4.7622\tLR: 8.864194\n",
      "Training Epoch: 89 [32384/50000]\tLoss: 4.8171\tLR: 8.864450\n",
      "Training Epoch: 89 [32512/50000]\tLoss: 4.8094\tLR: 8.864706\n",
      "Training Epoch: 89 [32640/50000]\tLoss: 4.8383\tLR: 8.864962\n",
      "Training Epoch: 89 [32768/50000]\tLoss: 4.8377\tLR: 8.865217\n",
      "Training Epoch: 89 [32896/50000]\tLoss: 4.7409\tLR: 8.865473\n",
      "Training Epoch: 89 [33024/50000]\tLoss: 4.7806\tLR: 8.865729\n",
      "Training Epoch: 89 [33152/50000]\tLoss: 4.7988\tLR: 8.865985\n",
      "Training Epoch: 89 [33280/50000]\tLoss: 4.7714\tLR: 8.866240\n",
      "Training Epoch: 89 [33408/50000]\tLoss: 4.7728\tLR: 8.866496\n",
      "Training Epoch: 89 [33536/50000]\tLoss: 4.8702\tLR: 8.866752\n",
      "Training Epoch: 89 [33664/50000]\tLoss: 4.7708\tLR: 8.867008\n",
      "Training Epoch: 89 [33792/50000]\tLoss: 4.7621\tLR: 8.867263\n",
      "Training Epoch: 89 [33920/50000]\tLoss: 4.8128\tLR: 8.867519\n",
      "Training Epoch: 89 [34048/50000]\tLoss: 4.8149\tLR: 8.867775\n",
      "Training Epoch: 89 [34176/50000]\tLoss: 4.8590\tLR: 8.868031\n",
      "Training Epoch: 89 [34304/50000]\tLoss: 4.8907\tLR: 8.868286\n",
      "Training Epoch: 89 [34432/50000]\tLoss: 4.8439\tLR: 8.868542\n",
      "Training Epoch: 89 [34560/50000]\tLoss: 4.7661\tLR: 8.868798\n",
      "Training Epoch: 89 [34688/50000]\tLoss: 4.7149\tLR: 8.869054\n",
      "Training Epoch: 89 [34816/50000]\tLoss: 4.7526\tLR: 8.869309\n",
      "Training Epoch: 89 [34944/50000]\tLoss: 4.8294\tLR: 8.869565\n",
      "Training Epoch: 89 [35072/50000]\tLoss: 4.7192\tLR: 8.869821\n",
      "Training Epoch: 89 [35200/50000]\tLoss: 4.6987\tLR: 8.870077\n",
      "Training Epoch: 89 [35328/50000]\tLoss: 4.7178\tLR: 8.870332\n",
      "Training Epoch: 89 [35456/50000]\tLoss: 4.7987\tLR: 8.870588\n",
      "Training Epoch: 89 [35584/50000]\tLoss: 4.9401\tLR: 8.870844\n",
      "Training Epoch: 89 [35712/50000]\tLoss: 4.8056\tLR: 8.871100\n",
      "Training Epoch: 89 [35840/50000]\tLoss: 4.7566\tLR: 8.871355\n",
      "Training Epoch: 89 [35968/50000]\tLoss: 4.7792\tLR: 8.871611\n",
      "Training Epoch: 89 [36096/50000]\tLoss: 4.7461\tLR: 8.871867\n",
      "Training Epoch: 89 [36224/50000]\tLoss: 4.7748\tLR: 8.872123\n",
      "Training Epoch: 89 [36352/50000]\tLoss: 4.8540\tLR: 8.872379\n",
      "Training Epoch: 89 [36480/50000]\tLoss: 4.7868\tLR: 8.872634\n",
      "Training Epoch: 89 [36608/50000]\tLoss: 4.7715\tLR: 8.872890\n",
      "Training Epoch: 89 [36736/50000]\tLoss: 4.7553\tLR: 8.873146\n",
      "Training Epoch: 89 [36864/50000]\tLoss: 4.8260\tLR: 8.873402\n",
      "Training Epoch: 89 [36992/50000]\tLoss: 4.8017\tLR: 8.873657\n",
      "Training Epoch: 89 [37120/50000]\tLoss: 4.7202\tLR: 8.873913\n",
      "Training Epoch: 89 [37248/50000]\tLoss: 4.8056\tLR: 8.874169\n",
      "Training Epoch: 89 [37376/50000]\tLoss: 4.8282\tLR: 8.874425\n",
      "Training Epoch: 89 [37504/50000]\tLoss: 4.8041\tLR: 8.874680\n",
      "Training Epoch: 89 [37632/50000]\tLoss: 4.7568\tLR: 8.874936\n",
      "Training Epoch: 89 [37760/50000]\tLoss: 4.7936\tLR: 8.875192\n",
      "Training Epoch: 89 [37888/50000]\tLoss: 4.8109\tLR: 8.875448\n",
      "Training Epoch: 89 [38016/50000]\tLoss: 4.6874\tLR: 8.875703\n",
      "Training Epoch: 89 [38144/50000]\tLoss: 4.7784\tLR: 8.875959\n",
      "Training Epoch: 89 [38272/50000]\tLoss: 4.7342\tLR: 8.876215\n",
      "Training Epoch: 89 [38400/50000]\tLoss: 4.7555\tLR: 8.876471\n",
      "Training Epoch: 89 [38528/50000]\tLoss: 4.7999\tLR: 8.876726\n",
      "Training Epoch: 89 [38656/50000]\tLoss: 4.7845\tLR: 8.876982\n",
      "Training Epoch: 89 [38784/50000]\tLoss: 4.7494\tLR: 8.877238\n",
      "Training Epoch: 89 [38912/50000]\tLoss: 4.7253\tLR: 8.877494\n",
      "Training Epoch: 89 [39040/50000]\tLoss: 4.6992\tLR: 8.877749\n",
      "Training Epoch: 89 [39168/50000]\tLoss: 4.6710\tLR: 8.878005\n",
      "Training Epoch: 89 [39296/50000]\tLoss: 4.7867\tLR: 8.878261\n",
      "Training Epoch: 89 [39424/50000]\tLoss: 4.8284\tLR: 8.878517\n",
      "Training Epoch: 89 [39552/50000]\tLoss: 4.7295\tLR: 8.878772\n",
      "Training Epoch: 89 [39680/50000]\tLoss: 4.8754\tLR: 8.879028\n",
      "Training Epoch: 89 [39808/50000]\tLoss: 4.7431\tLR: 8.879284\n",
      "Training Epoch: 89 [39936/50000]\tLoss: 4.8165\tLR: 8.879540\n",
      "Training Epoch: 89 [40064/50000]\tLoss: 4.8455\tLR: 8.879795\n",
      "Training Epoch: 89 [40192/50000]\tLoss: 4.7805\tLR: 8.880051\n",
      "Training Epoch: 89 [40320/50000]\tLoss: 4.7437\tLR: 8.880307\n",
      "Training Epoch: 89 [40448/50000]\tLoss: 4.9356\tLR: 8.880563\n",
      "Training Epoch: 89 [40576/50000]\tLoss: 4.7366\tLR: 8.880818\n",
      "Training Epoch: 89 [40704/50000]\tLoss: 4.7734\tLR: 8.881074\n",
      "Training Epoch: 89 [40832/50000]\tLoss: 4.8447\tLR: 8.881330\n",
      "Training Epoch: 89 [40960/50000]\tLoss: 4.7337\tLR: 8.881586\n",
      "Training Epoch: 89 [41088/50000]\tLoss: 4.9256\tLR: 8.881841\n",
      "Training Epoch: 89 [41216/50000]\tLoss: 4.8385\tLR: 8.882097\n",
      "Training Epoch: 89 [41344/50000]\tLoss: 4.7970\tLR: 8.882353\n",
      "Training Epoch: 89 [41472/50000]\tLoss: 4.7187\tLR: 8.882609\n",
      "Training Epoch: 89 [41600/50000]\tLoss: 4.7734\tLR: 8.882864\n",
      "Training Epoch: 89 [41728/50000]\tLoss: 4.8180\tLR: 8.883120\n",
      "Training Epoch: 89 [41856/50000]\tLoss: 4.7692\tLR: 8.883376\n",
      "Training Epoch: 89 [41984/50000]\tLoss: 4.7224\tLR: 8.883632\n",
      "Training Epoch: 89 [42112/50000]\tLoss: 4.6995\tLR: 8.883887\n",
      "Training Epoch: 89 [42240/50000]\tLoss: 4.7591\tLR: 8.884143\n",
      "Training Epoch: 89 [42368/50000]\tLoss: 4.7605\tLR: 8.884399\n",
      "Training Epoch: 89 [42496/50000]\tLoss: 4.7988\tLR: 8.884655\n",
      "Training Epoch: 89 [42624/50000]\tLoss: 4.7459\tLR: 8.884910\n",
      "Training Epoch: 89 [42752/50000]\tLoss: 4.7903\tLR: 8.885166\n",
      "Training Epoch: 89 [42880/50000]\tLoss: 4.7802\tLR: 8.885422\n",
      "Training Epoch: 89 [43008/50000]\tLoss: 4.8050\tLR: 8.885678\n",
      "Training Epoch: 89 [43136/50000]\tLoss: 4.7098\tLR: 8.885934\n",
      "Training Epoch: 89 [43264/50000]\tLoss: 4.7554\tLR: 8.886189\n",
      "Training Epoch: 89 [43392/50000]\tLoss: 4.9271\tLR: 8.886445\n",
      "Training Epoch: 89 [43520/50000]\tLoss: 4.8542\tLR: 8.886701\n",
      "Training Epoch: 89 [43648/50000]\tLoss: 4.8500\tLR: 8.886957\n",
      "Training Epoch: 89 [43776/50000]\tLoss: 4.7552\tLR: 8.887212\n",
      "Training Epoch: 89 [43904/50000]\tLoss: 4.7584\tLR: 8.887468\n",
      "Training Epoch: 89 [44032/50000]\tLoss: 4.8165\tLR: 8.887724\n",
      "Training Epoch: 89 [44160/50000]\tLoss: 4.7516\tLR: 8.887980\n",
      "Training Epoch: 89 [44288/50000]\tLoss: 4.8216\tLR: 8.888235\n",
      "Training Epoch: 89 [44416/50000]\tLoss: 4.8056\tLR: 8.888491\n",
      "Training Epoch: 89 [44544/50000]\tLoss: 4.7994\tLR: 8.888747\n",
      "Training Epoch: 89 [44672/50000]\tLoss: 4.8379\tLR: 8.889003\n",
      "Training Epoch: 89 [44800/50000]\tLoss: 4.8528\tLR: 8.889258\n",
      "Training Epoch: 89 [44928/50000]\tLoss: 4.7502\tLR: 8.889514\n",
      "Training Epoch: 89 [45056/50000]\tLoss: 4.7121\tLR: 8.889770\n",
      "Training Epoch: 89 [45184/50000]\tLoss: 4.6754\tLR: 8.890026\n",
      "Training Epoch: 89 [45312/50000]\tLoss: 4.7287\tLR: 8.890281\n",
      "Training Epoch: 89 [45440/50000]\tLoss: 4.7898\tLR: 8.890537\n",
      "Training Epoch: 89 [45568/50000]\tLoss: 4.8340\tLR: 8.890793\n",
      "Training Epoch: 89 [45696/50000]\tLoss: 4.7365\tLR: 8.891049\n",
      "Training Epoch: 89 [45824/50000]\tLoss: 4.7887\tLR: 8.891304\n",
      "Training Epoch: 89 [45952/50000]\tLoss: 4.7731\tLR: 8.891560\n",
      "Training Epoch: 89 [46080/50000]\tLoss: 4.8619\tLR: 8.891816\n",
      "Training Epoch: 89 [46208/50000]\tLoss: 4.7993\tLR: 8.892072\n",
      "Training Epoch: 89 [46336/50000]\tLoss: 4.8322\tLR: 8.892327\n",
      "Training Epoch: 89 [46464/50000]\tLoss: 4.7987\tLR: 8.892583\n",
      "Training Epoch: 89 [46592/50000]\tLoss: 4.6834\tLR: 8.892839\n",
      "Training Epoch: 89 [46720/50000]\tLoss: 4.8311\tLR: 8.893095\n",
      "Training Epoch: 89 [46848/50000]\tLoss: 4.7135\tLR: 8.893350\n",
      "Training Epoch: 89 [46976/50000]\tLoss: 4.6989\tLR: 8.893606\n",
      "Training Epoch: 89 [47104/50000]\tLoss: 4.8024\tLR: 8.893862\n",
      "Training Epoch: 89 [47232/50000]\tLoss: 4.9433\tLR: 8.894118\n",
      "Training Epoch: 89 [47360/50000]\tLoss: 4.7528\tLR: 8.894373\n",
      "Training Epoch: 89 [47488/50000]\tLoss: 4.6979\tLR: 8.894629\n",
      "Training Epoch: 89 [47616/50000]\tLoss: 4.8774\tLR: 8.894885\n",
      "Training Epoch: 89 [47744/50000]\tLoss: 4.8056\tLR: 8.895141\n",
      "Training Epoch: 89 [47872/50000]\tLoss: 4.8056\tLR: 8.895396\n",
      "Training Epoch: 89 [48000/50000]\tLoss: 4.7962\tLR: 8.895652\n",
      "Training Epoch: 89 [48128/50000]\tLoss: 4.8624\tLR: 8.895908\n",
      "Training Epoch: 89 [48256/50000]\tLoss: 4.7072\tLR: 8.896164\n",
      "Training Epoch: 89 [48384/50000]\tLoss: 4.6892\tLR: 8.896419\n",
      "Training Epoch: 89 [48512/50000]\tLoss: 4.8565\tLR: 8.896675\n",
      "Training Epoch: 89 [48640/50000]\tLoss: 4.7814\tLR: 8.896931\n",
      "Training Epoch: 89 [48768/50000]\tLoss: 4.7038\tLR: 8.897187\n",
      "Training Epoch: 89 [48896/50000]\tLoss: 4.7295\tLR: 8.897442\n",
      "Training Epoch: 89 [49024/50000]\tLoss: 4.7951\tLR: 8.897698\n",
      "Training Epoch: 89 [49152/50000]\tLoss: 4.6950\tLR: 8.897954\n",
      "Training Epoch: 89 [49280/50000]\tLoss: 4.6693\tLR: 8.898210\n",
      "Training Epoch: 89 [49408/50000]\tLoss: 4.7031\tLR: 8.898465\n",
      "Training Epoch: 89 [49536/50000]\tLoss: 4.7913\tLR: 8.898721\n",
      "Training Epoch: 89 [49664/50000]\tLoss: 4.7376\tLR: 8.898977\n",
      "Training Epoch: 89 [49792/50000]\tLoss: 4.7097\tLR: 8.899233\n",
      "Training Epoch: 89 [49920/50000]\tLoss: 4.7468\tLR: 8.899488\n",
      "Training Epoch: 89 [50000/50000]\tLoss: 4.7486\tLR: 8.899744\n",
      "epoch 89 training time consumed: 489.17s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  124772 GB |  124771 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  124389 GB |  124388 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     383 GB |     383 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  124772 GB |  124771 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  124389 GB |  124388 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     383 GB |     383 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  123018 GB |  123018 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  122635 GB |  122635 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     383 GB |     383 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   13230 K  |   13229 K  |\n",
      "|       from large pool |      24    |      65    |    5639 K  |    5639 K  |\n",
      "|       from small pool |     231    |     274    |    7590 K  |    7590 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   13230 K  |   13229 K  |\n",
      "|       from large pool |      24    |      65    |    5639 K  |    5639 K  |\n",
      "|       from small pool |     231    |     274    |    7590 K  |    7590 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      47    |    7668 K  |    7668 K  |\n",
      "|       from large pool |      10    |      23    |    2710 K  |    2710 K  |\n",
      "|       from small pool |      27    |      35    |    4957 K  |    4957 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 89, Average loss: 0.0376, Accuracy: 0.0100, Time consumed:31.10s\n",
      "\n",
      "Training Epoch: 90 [128/50000]\tLoss: 4.6650\tLR: 0.020000\n",
      "Training Epoch: 90 [256/50000]\tLoss: 4.7258\tLR: 8.900256\n",
      "Training Epoch: 90 [384/50000]\tLoss: 4.6695\tLR: 8.900512\n",
      "Training Epoch: 90 [512/50000]\tLoss: 4.6621\tLR: 8.900767\n",
      "Training Epoch: 90 [640/50000]\tLoss: 4.8701\tLR: 8.901023\n",
      "Training Epoch: 90 [768/50000]\tLoss: 4.7813\tLR: 8.901279\n",
      "Training Epoch: 90 [896/50000]\tLoss: 4.8443\tLR: 8.901535\n",
      "Training Epoch: 90 [1024/50000]\tLoss: 4.8113\tLR: 8.901790\n",
      "Training Epoch: 90 [1152/50000]\tLoss: 4.7364\tLR: 8.902046\n",
      "Training Epoch: 90 [1280/50000]\tLoss: 4.7500\tLR: 8.902302\n",
      "Training Epoch: 90 [1408/50000]\tLoss: 4.8190\tLR: 8.902558\n",
      "Training Epoch: 90 [1536/50000]\tLoss: 4.7898\tLR: 8.902813\n",
      "Training Epoch: 90 [1664/50000]\tLoss: 4.8985\tLR: 8.903069\n",
      "Training Epoch: 90 [1792/50000]\tLoss: 4.8983\tLR: 8.903325\n",
      "Training Epoch: 90 [1920/50000]\tLoss: 4.7227\tLR: 8.903581\n",
      "Training Epoch: 90 [2048/50000]\tLoss: 4.7575\tLR: 8.903836\n",
      "Training Epoch: 90 [2176/50000]\tLoss: 4.8030\tLR: 8.904092\n",
      "Training Epoch: 90 [2304/50000]\tLoss: 4.7929\tLR: 8.904348\n",
      "Training Epoch: 90 [2432/50000]\tLoss: 4.7855\tLR: 8.904604\n",
      "Training Epoch: 90 [2560/50000]\tLoss: 4.7251\tLR: 8.904859\n",
      "Training Epoch: 90 [2688/50000]\tLoss: 4.6939\tLR: 8.905115\n",
      "Training Epoch: 90 [2816/50000]\tLoss: 4.8035\tLR: 8.905371\n",
      "Training Epoch: 90 [2944/50000]\tLoss: 4.7686\tLR: 8.905627\n",
      "Training Epoch: 90 [3072/50000]\tLoss: 4.7464\tLR: 8.905882\n",
      "Training Epoch: 90 [3200/50000]\tLoss: 4.6959\tLR: 8.906138\n",
      "Training Epoch: 90 [3328/50000]\tLoss: 4.8047\tLR: 8.906394\n",
      "Training Epoch: 90 [3456/50000]\tLoss: 4.6976\tLR: 8.906650\n",
      "Training Epoch: 90 [3584/50000]\tLoss: 4.7938\tLR: 8.906905\n",
      "Training Epoch: 90 [3712/50000]\tLoss: 4.8379\tLR: 8.907161\n",
      "Training Epoch: 90 [3840/50000]\tLoss: 4.8632\tLR: 8.907417\n",
      "Training Epoch: 90 [3968/50000]\tLoss: 4.7571\tLR: 8.907673\n",
      "Training Epoch: 90 [4096/50000]\tLoss: 4.7779\tLR: 8.907928\n",
      "Training Epoch: 90 [4224/50000]\tLoss: 4.6975\tLR: 8.908184\n",
      "Training Epoch: 90 [4352/50000]\tLoss: 4.7247\tLR: 8.908440\n",
      "Training Epoch: 90 [4480/50000]\tLoss: 4.7456\tLR: 8.908696\n",
      "Training Epoch: 90 [4608/50000]\tLoss: 4.7601\tLR: 8.908951\n",
      "Training Epoch: 90 [4736/50000]\tLoss: 4.7543\tLR: 8.909207\n",
      "Training Epoch: 90 [4864/50000]\tLoss: 4.8436\tLR: 8.909463\n",
      "Training Epoch: 90 [4992/50000]\tLoss: 4.8182\tLR: 8.909719\n",
      "Training Epoch: 90 [5120/50000]\tLoss: 4.8780\tLR: 8.909974\n",
      "Training Epoch: 90 [5248/50000]\tLoss: 4.8089\tLR: 8.910230\n",
      "Training Epoch: 90 [5376/50000]\tLoss: 4.9033\tLR: 8.910486\n",
      "Training Epoch: 90 [5504/50000]\tLoss: 4.7854\tLR: 8.910742\n",
      "Training Epoch: 90 [5632/50000]\tLoss: 4.6696\tLR: 8.910997\n",
      "Training Epoch: 90 [5760/50000]\tLoss: 4.7626\tLR: 8.911253\n",
      "Training Epoch: 90 [5888/50000]\tLoss: 4.8336\tLR: 8.911509\n",
      "Training Epoch: 90 [6016/50000]\tLoss: 4.6731\tLR: 8.911765\n",
      "Training Epoch: 90 [6144/50000]\tLoss: 4.6932\tLR: 8.912020\n",
      "Training Epoch: 90 [6272/50000]\tLoss: 4.7275\tLR: 8.912276\n",
      "Training Epoch: 90 [6400/50000]\tLoss: 4.7872\tLR: 8.912532\n",
      "Training Epoch: 90 [6528/50000]\tLoss: 4.7594\tLR: 8.912788\n",
      "Training Epoch: 90 [6656/50000]\tLoss: 4.7328\tLR: 8.913043\n",
      "Training Epoch: 90 [6784/50000]\tLoss: 4.7737\tLR: 8.913299\n",
      "Training Epoch: 90 [6912/50000]\tLoss: 4.7704\tLR: 8.913555\n",
      "Training Epoch: 90 [7040/50000]\tLoss: 4.7958\tLR: 8.913811\n",
      "Training Epoch: 90 [7168/50000]\tLoss: 4.8211\tLR: 8.914066\n",
      "Training Epoch: 90 [7296/50000]\tLoss: 4.7055\tLR: 8.914322\n",
      "Training Epoch: 90 [7424/50000]\tLoss: 4.8053\tLR: 8.914578\n",
      "Training Epoch: 90 [7552/50000]\tLoss: 4.7690\tLR: 8.914834\n",
      "Training Epoch: 90 [7680/50000]\tLoss: 4.7550\tLR: 8.915090\n",
      "Training Epoch: 90 [7808/50000]\tLoss: 4.8282\tLR: 8.915345\n",
      "Training Epoch: 90 [7936/50000]\tLoss: 4.7389\tLR: 8.915601\n",
      "Training Epoch: 90 [8064/50000]\tLoss: 4.7934\tLR: 8.915857\n",
      "Training Epoch: 90 [8192/50000]\tLoss: 4.7454\tLR: 8.916113\n",
      "Training Epoch: 90 [8320/50000]\tLoss: 4.7387\tLR: 8.916368\n",
      "Training Epoch: 90 [8448/50000]\tLoss: 4.7968\tLR: 8.916624\n",
      "Training Epoch: 90 [8576/50000]\tLoss: 4.8149\tLR: 8.916880\n",
      "Training Epoch: 90 [8704/50000]\tLoss: 4.7629\tLR: 8.917136\n",
      "Training Epoch: 90 [8832/50000]\tLoss: 4.8291\tLR: 8.917391\n",
      "Training Epoch: 90 [8960/50000]\tLoss: 4.7249\tLR: 8.917647\n",
      "Training Epoch: 90 [9088/50000]\tLoss: 4.6883\tLR: 8.917903\n",
      "Training Epoch: 90 [9216/50000]\tLoss: 4.8878\tLR: 8.918159\n",
      "Training Epoch: 90 [9344/50000]\tLoss: 4.7426\tLR: 8.918414\n",
      "Training Epoch: 90 [9472/50000]\tLoss: 4.7025\tLR: 8.918670\n",
      "Training Epoch: 90 [9600/50000]\tLoss: 4.6695\tLR: 8.918926\n",
      "Training Epoch: 90 [9728/50000]\tLoss: 4.6799\tLR: 8.919182\n",
      "Training Epoch: 90 [9856/50000]\tLoss: 4.6767\tLR: 8.919437\n",
      "Training Epoch: 90 [9984/50000]\tLoss: 4.7300\tLR: 8.919693\n",
      "Training Epoch: 90 [10112/50000]\tLoss: 4.7665\tLR: 8.919949\n",
      "Training Epoch: 90 [10240/50000]\tLoss: 4.7406\tLR: 8.920205\n",
      "Training Epoch: 90 [10368/50000]\tLoss: 4.7578\tLR: 8.920460\n",
      "Training Epoch: 90 [10496/50000]\tLoss: 4.7722\tLR: 8.920716\n",
      "Training Epoch: 90 [10624/50000]\tLoss: 4.7336\tLR: 8.920972\n",
      "Training Epoch: 90 [10752/50000]\tLoss: 4.7147\tLR: 8.921228\n",
      "Training Epoch: 90 [10880/50000]\tLoss: 4.7790\tLR: 8.921483\n",
      "Training Epoch: 90 [11008/50000]\tLoss: 4.7092\tLR: 8.921739\n",
      "Training Epoch: 90 [11136/50000]\tLoss: 4.7255\tLR: 8.921995\n",
      "Training Epoch: 90 [11264/50000]\tLoss: 4.7907\tLR: 8.922251\n",
      "Training Epoch: 90 [11392/50000]\tLoss: 4.7818\tLR: 8.922506\n",
      "Training Epoch: 90 [11520/50000]\tLoss: 4.7603\tLR: 8.922762\n",
      "Training Epoch: 90 [11648/50000]\tLoss: 4.7410\tLR: 8.923018\n",
      "Training Epoch: 90 [11776/50000]\tLoss: 4.8017\tLR: 8.923274\n",
      "Training Epoch: 90 [11904/50000]\tLoss: 4.7554\tLR: 8.923529\n",
      "Training Epoch: 90 [12032/50000]\tLoss: 4.7327\tLR: 8.923785\n",
      "Training Epoch: 90 [12160/50000]\tLoss: 4.7332\tLR: 8.924041\n",
      "Training Epoch: 90 [12288/50000]\tLoss: 4.8019\tLR: 8.924297\n",
      "Training Epoch: 90 [12416/50000]\tLoss: 4.7283\tLR: 8.924552\n",
      "Training Epoch: 90 [12544/50000]\tLoss: 4.7752\tLR: 8.924808\n",
      "Training Epoch: 90 [12672/50000]\tLoss: 4.7367\tLR: 8.925064\n",
      "Training Epoch: 90 [12800/50000]\tLoss: 4.7515\tLR: 8.925320\n",
      "Training Epoch: 90 [12928/50000]\tLoss: 4.7483\tLR: 8.925575\n",
      "Training Epoch: 90 [13056/50000]\tLoss: 4.7539\tLR: 8.925831\n",
      "Training Epoch: 90 [13184/50000]\tLoss: 4.6754\tLR: 8.926087\n",
      "Training Epoch: 90 [13312/50000]\tLoss: 4.7924\tLR: 8.926343\n",
      "Training Epoch: 90 [13440/50000]\tLoss: 4.7879\tLR: 8.926598\n",
      "Training Epoch: 90 [13568/50000]\tLoss: 4.7096\tLR: 8.926854\n",
      "Training Epoch: 90 [13696/50000]\tLoss: 4.8145\tLR: 8.927110\n",
      "Training Epoch: 90 [13824/50000]\tLoss: 4.7169\tLR: 8.927366\n",
      "Training Epoch: 90 [13952/50000]\tLoss: 4.7917\tLR: 8.927621\n",
      "Training Epoch: 90 [14080/50000]\tLoss: 4.7176\tLR: 8.927877\n",
      "Training Epoch: 90 [14208/50000]\tLoss: 4.7146\tLR: 8.928133\n",
      "Training Epoch: 90 [14336/50000]\tLoss: 4.7350\tLR: 8.928389\n",
      "Training Epoch: 90 [14464/50000]\tLoss: 4.7452\tLR: 8.928645\n",
      "Training Epoch: 90 [14592/50000]\tLoss: 4.7599\tLR: 8.928900\n",
      "Training Epoch: 90 [14720/50000]\tLoss: 4.7895\tLR: 8.929156\n",
      "Training Epoch: 90 [14848/50000]\tLoss: 4.8619\tLR: 8.929412\n",
      "Training Epoch: 90 [14976/50000]\tLoss: 4.8153\tLR: 8.929668\n",
      "Training Epoch: 90 [15104/50000]\tLoss: 4.8855\tLR: 8.929923\n",
      "Training Epoch: 90 [15232/50000]\tLoss: 4.8467\tLR: 8.930179\n",
      "Training Epoch: 90 [15360/50000]\tLoss: 4.7592\tLR: 8.930435\n",
      "Training Epoch: 90 [15488/50000]\tLoss: 4.7938\tLR: 8.930691\n",
      "Training Epoch: 90 [15616/50000]\tLoss: 4.8035\tLR: 8.930946\n",
      "Training Epoch: 90 [15744/50000]\tLoss: 4.7801\tLR: 8.931202\n",
      "Training Epoch: 90 [15872/50000]\tLoss: 4.8234\tLR: 8.931458\n",
      "Training Epoch: 90 [16000/50000]\tLoss: 4.8469\tLR: 8.931714\n",
      "Training Epoch: 90 [16128/50000]\tLoss: 4.7587\tLR: 8.931969\n",
      "Training Epoch: 90 [16256/50000]\tLoss: 4.8049\tLR: 8.932225\n",
      "Training Epoch: 90 [16384/50000]\tLoss: 4.8108\tLR: 8.932481\n",
      "Training Epoch: 90 [16512/50000]\tLoss: 4.8288\tLR: 8.932737\n",
      "Training Epoch: 90 [16640/50000]\tLoss: 4.7071\tLR: 8.932992\n",
      "Training Epoch: 90 [16768/50000]\tLoss: 4.7291\tLR: 8.933248\n",
      "Training Epoch: 90 [16896/50000]\tLoss: 4.7998\tLR: 8.933504\n",
      "Training Epoch: 90 [17024/50000]\tLoss: 4.7217\tLR: 8.933760\n",
      "Training Epoch: 90 [17152/50000]\tLoss: 4.7364\tLR: 8.934015\n",
      "Training Epoch: 90 [17280/50000]\tLoss: 4.7329\tLR: 8.934271\n",
      "Training Epoch: 90 [17408/50000]\tLoss: 4.7566\tLR: 8.934527\n",
      "Training Epoch: 90 [17536/50000]\tLoss: 4.7797\tLR: 8.934783\n",
      "Training Epoch: 90 [17664/50000]\tLoss: 4.7926\tLR: 8.935038\n",
      "Training Epoch: 90 [17792/50000]\tLoss: 4.9293\tLR: 8.935294\n",
      "Training Epoch: 90 [17920/50000]\tLoss: 4.7955\tLR: 8.935550\n",
      "Training Epoch: 90 [18048/50000]\tLoss: 4.7688\tLR: 8.935806\n",
      "Training Epoch: 90 [18176/50000]\tLoss: 4.7854\tLR: 8.936061\n",
      "Training Epoch: 90 [18304/50000]\tLoss: 4.8188\tLR: 8.936317\n",
      "Training Epoch: 90 [18432/50000]\tLoss: 4.7530\tLR: 8.936573\n",
      "Training Epoch: 90 [18560/50000]\tLoss: 4.7172\tLR: 8.936829\n",
      "Training Epoch: 90 [18688/50000]\tLoss: 4.8997\tLR: 8.937084\n",
      "Training Epoch: 90 [18816/50000]\tLoss: 4.8355\tLR: 8.937340\n",
      "Training Epoch: 90 [18944/50000]\tLoss: 4.8419\tLR: 8.937596\n",
      "Training Epoch: 90 [19072/50000]\tLoss: 4.7000\tLR: 8.937852\n",
      "Training Epoch: 90 [19200/50000]\tLoss: 4.8550\tLR: 8.938107\n",
      "Training Epoch: 90 [19328/50000]\tLoss: 4.6805\tLR: 8.938363\n",
      "Training Epoch: 90 [19456/50000]\tLoss: 4.7649\tLR: 8.938619\n",
      "Training Epoch: 90 [19584/50000]\tLoss: 4.7741\tLR: 8.938875\n",
      "Training Epoch: 90 [19712/50000]\tLoss: 4.7623\tLR: 8.939130\n",
      "Training Epoch: 90 [19840/50000]\tLoss: 4.6746\tLR: 8.939386\n",
      "Training Epoch: 90 [19968/50000]\tLoss: 4.7388\tLR: 8.939642\n",
      "Training Epoch: 90 [20096/50000]\tLoss: 4.7800\tLR: 8.939898\n",
      "Training Epoch: 90 [20224/50000]\tLoss: 4.7234\tLR: 8.940153\n",
      "Training Epoch: 90 [20352/50000]\tLoss: 4.7125\tLR: 8.940409\n",
      "Training Epoch: 90 [20480/50000]\tLoss: 4.7805\tLR: 8.940665\n",
      "Training Epoch: 90 [20608/50000]\tLoss: 4.8301\tLR: 8.940921\n",
      "Training Epoch: 90 [20736/50000]\tLoss: 4.7543\tLR: 8.941176\n",
      "Training Epoch: 90 [20864/50000]\tLoss: 4.7381\tLR: 8.941432\n",
      "Training Epoch: 90 [20992/50000]\tLoss: 4.6942\tLR: 8.941688\n",
      "Training Epoch: 90 [21120/50000]\tLoss: 4.6867\tLR: 8.941944\n",
      "Training Epoch: 90 [21248/50000]\tLoss: 4.7007\tLR: 8.942199\n",
      "Training Epoch: 90 [21376/50000]\tLoss: 4.8022\tLR: 8.942455\n",
      "Training Epoch: 90 [21504/50000]\tLoss: 4.8074\tLR: 8.942711\n",
      "Training Epoch: 90 [21632/50000]\tLoss: 4.6659\tLR: 8.942967\n",
      "Training Epoch: 90 [21760/50000]\tLoss: 4.8832\tLR: 8.943223\n",
      "Training Epoch: 90 [21888/50000]\tLoss: 4.8463\tLR: 8.943478\n",
      "Training Epoch: 90 [22016/50000]\tLoss: 4.8239\tLR: 8.943734\n",
      "Training Epoch: 90 [22144/50000]\tLoss: 4.7349\tLR: 8.943990\n",
      "Training Epoch: 90 [22272/50000]\tLoss: 4.7567\tLR: 8.944246\n",
      "Training Epoch: 90 [22400/50000]\tLoss: 4.7241\tLR: 8.944501\n",
      "Training Epoch: 90 [22528/50000]\tLoss: 4.8466\tLR: 8.944757\n",
      "Training Epoch: 90 [22656/50000]\tLoss: 4.7927\tLR: 8.945013\n",
      "Training Epoch: 90 [22784/50000]\tLoss: 4.7386\tLR: 8.945269\n",
      "Training Epoch: 90 [22912/50000]\tLoss: 4.8074\tLR: 8.945524\n",
      "Training Epoch: 90 [23040/50000]\tLoss: 4.7461\tLR: 8.945780\n",
      "Training Epoch: 90 [23168/50000]\tLoss: 4.7899\tLR: 8.946036\n",
      "Training Epoch: 90 [23296/50000]\tLoss: 4.8022\tLR: 8.946292\n",
      "Training Epoch: 90 [23424/50000]\tLoss: 4.8202\tLR: 8.946547\n",
      "Training Epoch: 90 [23552/50000]\tLoss: 4.7371\tLR: 8.946803\n",
      "Training Epoch: 90 [23680/50000]\tLoss: 4.7064\tLR: 8.947059\n",
      "Training Epoch: 90 [23808/50000]\tLoss: 4.7573\tLR: 8.947315\n",
      "Training Epoch: 90 [23936/50000]\tLoss: 4.7281\tLR: 8.947570\n",
      "Training Epoch: 90 [24064/50000]\tLoss: 4.8750\tLR: 8.947826\n",
      "Training Epoch: 90 [24192/50000]\tLoss: 4.7709\tLR: 8.948082\n",
      "Training Epoch: 90 [24320/50000]\tLoss: 4.7416\tLR: 8.948338\n",
      "Training Epoch: 90 [24448/50000]\tLoss: 4.7811\tLR: 8.948593\n",
      "Training Epoch: 90 [24576/50000]\tLoss: 4.7373\tLR: 8.948849\n",
      "Training Epoch: 90 [24704/50000]\tLoss: 4.7534\tLR: 8.949105\n",
      "Training Epoch: 90 [24832/50000]\tLoss: 4.8969\tLR: 8.949361\n",
      "Training Epoch: 90 [24960/50000]\tLoss: 4.8647\tLR: 8.949616\n",
      "Training Epoch: 90 [25088/50000]\tLoss: 4.8510\tLR: 8.949872\n",
      "Training Epoch: 90 [25216/50000]\tLoss: 4.7779\tLR: 8.950128\n",
      "Training Epoch: 90 [25344/50000]\tLoss: 4.7673\tLR: 8.950384\n",
      "Training Epoch: 90 [25472/50000]\tLoss: 4.7334\tLR: 8.950639\n",
      "Training Epoch: 90 [25600/50000]\tLoss: 4.7257\tLR: 8.950895\n",
      "Training Epoch: 90 [25728/50000]\tLoss: 4.7500\tLR: 8.951151\n",
      "Training Epoch: 90 [25856/50000]\tLoss: 4.7648\tLR: 8.951407\n",
      "Training Epoch: 90 [25984/50000]\tLoss: 4.7171\tLR: 8.951662\n",
      "Training Epoch: 90 [26112/50000]\tLoss: 4.7890\tLR: 8.951918\n",
      "Training Epoch: 90 [26240/50000]\tLoss: 4.8291\tLR: 8.952174\n",
      "Training Epoch: 90 [26368/50000]\tLoss: 4.7447\tLR: 8.952430\n",
      "Training Epoch: 90 [26496/50000]\tLoss: 4.8505\tLR: 8.952685\n",
      "Training Epoch: 90 [26624/50000]\tLoss: 4.8338\tLR: 8.952941\n",
      "Training Epoch: 90 [26752/50000]\tLoss: 4.8422\tLR: 8.953197\n",
      "Training Epoch: 90 [26880/50000]\tLoss: 4.7832\tLR: 8.953453\n",
      "Training Epoch: 90 [27008/50000]\tLoss: 4.7681\tLR: 8.953708\n",
      "Training Epoch: 90 [27136/50000]\tLoss: 4.7748\tLR: 8.953964\n",
      "Training Epoch: 90 [27264/50000]\tLoss: 4.7669\tLR: 8.954220\n",
      "Training Epoch: 90 [27392/50000]\tLoss: 4.7910\tLR: 8.954476\n",
      "Training Epoch: 90 [27520/50000]\tLoss: 4.8101\tLR: 8.954731\n",
      "Training Epoch: 90 [27648/50000]\tLoss: 4.7449\tLR: 8.954987\n",
      "Training Epoch: 90 [27776/50000]\tLoss: 4.6940\tLR: 8.955243\n",
      "Training Epoch: 90 [27904/50000]\tLoss: 4.8693\tLR: 8.955499\n",
      "Training Epoch: 90 [28032/50000]\tLoss: 4.7261\tLR: 8.955754\n",
      "Training Epoch: 90 [28160/50000]\tLoss: 4.7474\tLR: 8.956010\n",
      "Training Epoch: 90 [28288/50000]\tLoss: 4.7422\tLR: 8.956266\n",
      "Training Epoch: 90 [28416/50000]\tLoss: 4.8688\tLR: 8.956522\n",
      "Training Epoch: 90 [28544/50000]\tLoss: 4.8525\tLR: 8.956777\n",
      "Training Epoch: 90 [28672/50000]\tLoss: 4.8116\tLR: 8.957033\n",
      "Training Epoch: 90 [28800/50000]\tLoss: 4.6811\tLR: 8.957289\n",
      "Training Epoch: 90 [28928/50000]\tLoss: 4.7560\tLR: 8.957545\n",
      "Training Epoch: 90 [29056/50000]\tLoss: 4.6912\tLR: 8.957801\n",
      "Training Epoch: 90 [29184/50000]\tLoss: 4.7703\tLR: 8.958056\n",
      "Training Epoch: 90 [29312/50000]\tLoss: 4.7387\tLR: 8.958312\n",
      "Training Epoch: 90 [29440/50000]\tLoss: 4.6951\tLR: 8.958568\n",
      "Training Epoch: 90 [29568/50000]\tLoss: 4.7890\tLR: 8.958824\n",
      "Training Epoch: 90 [29696/50000]\tLoss: 4.8371\tLR: 8.959079\n",
      "Training Epoch: 90 [29824/50000]\tLoss: 4.7862\tLR: 8.959335\n",
      "Training Epoch: 90 [29952/50000]\tLoss: 4.8049\tLR: 8.959591\n",
      "Training Epoch: 90 [30080/50000]\tLoss: 4.7565\tLR: 8.959847\n",
      "Training Epoch: 90 [30208/50000]\tLoss: 4.7264\tLR: 8.960102\n",
      "Training Epoch: 90 [30336/50000]\tLoss: 4.7146\tLR: 8.960358\n",
      "Training Epoch: 90 [30464/50000]\tLoss: 4.8531\tLR: 8.960614\n",
      "Training Epoch: 90 [30592/50000]\tLoss: 4.7900\tLR: 8.960870\n",
      "Training Epoch: 90 [30720/50000]\tLoss: 4.7816\tLR: 8.961125\n",
      "Training Epoch: 90 [30848/50000]\tLoss: 4.7147\tLR: 8.961381\n",
      "Training Epoch: 90 [30976/50000]\tLoss: 4.8935\tLR: 8.961637\n",
      "Training Epoch: 90 [31104/50000]\tLoss: 4.7504\tLR: 8.961893\n",
      "Training Epoch: 90 [31232/50000]\tLoss: 4.8724\tLR: 8.962148\n",
      "Training Epoch: 90 [31360/50000]\tLoss: 4.7773\tLR: 8.962404\n",
      "Training Epoch: 90 [31488/50000]\tLoss: 4.8328\tLR: 8.962660\n",
      "Training Epoch: 90 [31616/50000]\tLoss: 4.8245\tLR: 8.962916\n",
      "Training Epoch: 90 [31744/50000]\tLoss: 4.8221\tLR: 8.963171\n",
      "Training Epoch: 90 [31872/50000]\tLoss: 4.7934\tLR: 8.963427\n",
      "Training Epoch: 90 [32000/50000]\tLoss: 4.7688\tLR: 8.963683\n",
      "Training Epoch: 90 [32128/50000]\tLoss: 4.7539\tLR: 8.963939\n",
      "Training Epoch: 90 [32256/50000]\tLoss: 4.8072\tLR: 8.964194\n",
      "Training Epoch: 90 [32384/50000]\tLoss: 4.6839\tLR: 8.964450\n",
      "Training Epoch: 90 [32512/50000]\tLoss: 4.8375\tLR: 8.964706\n",
      "Training Epoch: 90 [32640/50000]\tLoss: 4.8648\tLR: 8.964962\n",
      "Training Epoch: 90 [32768/50000]\tLoss: 4.8694\tLR: 8.965217\n",
      "Training Epoch: 90 [32896/50000]\tLoss: 4.9794\tLR: 8.965473\n",
      "Training Epoch: 90 [33024/50000]\tLoss: 4.7572\tLR: 8.965729\n",
      "Training Epoch: 90 [33152/50000]\tLoss: 4.7653\tLR: 8.965985\n",
      "Training Epoch: 90 [33280/50000]\tLoss: 4.7037\tLR: 8.966240\n",
      "Training Epoch: 90 [33408/50000]\tLoss: 4.7229\tLR: 8.966496\n",
      "Training Epoch: 90 [33536/50000]\tLoss: 4.7087\tLR: 8.966752\n",
      "Training Epoch: 90 [33664/50000]\tLoss: 4.7809\tLR: 8.967008\n",
      "Training Epoch: 90 [33792/50000]\tLoss: 4.8027\tLR: 8.967263\n",
      "Training Epoch: 90 [33920/50000]\tLoss: 4.7997\tLR: 8.967519\n",
      "Training Epoch: 90 [34048/50000]\tLoss: 4.7980\tLR: 8.967775\n",
      "Training Epoch: 90 [34176/50000]\tLoss: 4.7894\tLR: 8.968031\n",
      "Training Epoch: 90 [34304/50000]\tLoss: 4.7458\tLR: 8.968286\n",
      "Training Epoch: 90 [34432/50000]\tLoss: 4.7713\tLR: 8.968542\n",
      "Training Epoch: 90 [34560/50000]\tLoss: 4.8286\tLR: 8.968798\n",
      "Training Epoch: 90 [34688/50000]\tLoss: 4.7065\tLR: 8.969054\n",
      "Training Epoch: 90 [34816/50000]\tLoss: 4.7086\tLR: 8.969309\n",
      "Training Epoch: 90 [34944/50000]\tLoss: 4.7409\tLR: 8.969565\n",
      "Training Epoch: 90 [35072/50000]\tLoss: 4.7512\tLR: 8.969821\n",
      "Training Epoch: 90 [35200/50000]\tLoss: 4.7933\tLR: 8.970077\n",
      "Training Epoch: 90 [35328/50000]\tLoss: 4.6477\tLR: 8.970332\n",
      "Training Epoch: 90 [35456/50000]\tLoss: 4.6994\tLR: 8.970588\n",
      "Training Epoch: 90 [35584/50000]\tLoss: 4.7006\tLR: 8.970844\n",
      "Training Epoch: 90 [35712/50000]\tLoss: 4.7474\tLR: 8.971100\n",
      "Training Epoch: 90 [35840/50000]\tLoss: 4.8639\tLR: 8.971355\n",
      "Training Epoch: 90 [35968/50000]\tLoss: 4.7947\tLR: 8.971611\n",
      "Training Epoch: 90 [36096/50000]\tLoss: 4.7300\tLR: 8.971867\n",
      "Training Epoch: 90 [36224/50000]\tLoss: 4.8322\tLR: 8.972123\n",
      "Training Epoch: 90 [36352/50000]\tLoss: 4.7158\tLR: 8.972379\n",
      "Training Epoch: 90 [36480/50000]\tLoss: 4.7263\tLR: 8.972634\n",
      "Training Epoch: 90 [36608/50000]\tLoss: 4.7680\tLR: 8.972890\n",
      "Training Epoch: 90 [36736/50000]\tLoss: 4.7350\tLR: 8.973146\n",
      "Training Epoch: 90 [36864/50000]\tLoss: 4.7046\tLR: 8.973402\n",
      "Training Epoch: 90 [36992/50000]\tLoss: 4.7105\tLR: 8.973657\n",
      "Training Epoch: 90 [37120/50000]\tLoss: 4.7481\tLR: 8.973913\n",
      "Training Epoch: 90 [37248/50000]\tLoss: 4.7173\tLR: 8.974169\n",
      "Training Epoch: 90 [37376/50000]\tLoss: 4.8554\tLR: 8.974425\n",
      "Training Epoch: 90 [37504/50000]\tLoss: 4.7633\tLR: 8.974680\n",
      "Training Epoch: 90 [37632/50000]\tLoss: 4.7858\tLR: 8.974936\n",
      "Training Epoch: 90 [37760/50000]\tLoss: 4.8200\tLR: 8.975192\n",
      "Training Epoch: 90 [37888/50000]\tLoss: 4.8089\tLR: 8.975448\n",
      "Training Epoch: 90 [38016/50000]\tLoss: 4.7481\tLR: 8.975703\n",
      "Training Epoch: 90 [38144/50000]\tLoss: 4.7442\tLR: 8.975959\n",
      "Training Epoch: 90 [38272/50000]\tLoss: 4.7916\tLR: 8.976215\n",
      "Training Epoch: 90 [38400/50000]\tLoss: 4.8012\tLR: 8.976471\n",
      "Training Epoch: 90 [38528/50000]\tLoss: 4.7594\tLR: 8.976726\n",
      "Training Epoch: 90 [38656/50000]\tLoss: 4.7694\tLR: 8.976982\n",
      "Training Epoch: 90 [38784/50000]\tLoss: 4.8443\tLR: 8.977238\n",
      "Training Epoch: 90 [38912/50000]\tLoss: 4.7230\tLR: 8.977494\n",
      "Training Epoch: 90 [39040/50000]\tLoss: 4.8300\tLR: 8.977749\n",
      "Training Epoch: 90 [39168/50000]\tLoss: 4.8084\tLR: 8.978005\n",
      "Training Epoch: 90 [39296/50000]\tLoss: 4.7740\tLR: 8.978261\n",
      "Training Epoch: 90 [39424/50000]\tLoss: 4.7434\tLR: 8.978517\n",
      "Training Epoch: 90 [39552/50000]\tLoss: 4.8768\tLR: 8.978772\n",
      "Training Epoch: 90 [39680/50000]\tLoss: 4.7538\tLR: 8.979028\n",
      "Training Epoch: 90 [39808/50000]\tLoss: 4.8205\tLR: 8.979284\n",
      "Training Epoch: 90 [39936/50000]\tLoss: 4.7475\tLR: 8.979540\n",
      "Training Epoch: 90 [40064/50000]\tLoss: 4.7524\tLR: 8.979795\n",
      "Training Epoch: 90 [40192/50000]\tLoss: 4.8208\tLR: 8.980051\n",
      "Training Epoch: 90 [40320/50000]\tLoss: 4.7673\tLR: 8.980307\n",
      "Training Epoch: 90 [40448/50000]\tLoss: 4.7982\tLR: 8.980563\n",
      "Training Epoch: 90 [40576/50000]\tLoss: 4.7461\tLR: 8.980818\n",
      "Training Epoch: 90 [40704/50000]\tLoss: 4.7265\tLR: 8.981074\n",
      "Training Epoch: 90 [40832/50000]\tLoss: 4.5933\tLR: 8.981330\n",
      "Training Epoch: 90 [40960/50000]\tLoss: 4.7255\tLR: 8.981586\n",
      "Training Epoch: 90 [41088/50000]\tLoss: 4.7639\tLR: 8.981841\n",
      "Training Epoch: 90 [41216/50000]\tLoss: 4.7404\tLR: 8.982097\n",
      "Training Epoch: 90 [41344/50000]\tLoss: 4.7710\tLR: 8.982353\n",
      "Training Epoch: 90 [41472/50000]\tLoss: 4.7516\tLR: 8.982609\n",
      "Training Epoch: 90 [41600/50000]\tLoss: 4.7365\tLR: 8.982864\n",
      "Training Epoch: 90 [41728/50000]\tLoss: 4.7506\tLR: 8.983120\n",
      "Training Epoch: 90 [41856/50000]\tLoss: 4.7906\tLR: 8.983376\n",
      "Training Epoch: 90 [41984/50000]\tLoss: 4.7451\tLR: 8.983632\n",
      "Training Epoch: 90 [42112/50000]\tLoss: 4.7405\tLR: 8.983887\n",
      "Training Epoch: 90 [42240/50000]\tLoss: 4.8265\tLR: 8.984143\n",
      "Training Epoch: 90 [42368/50000]\tLoss: 4.7827\tLR: 8.984399\n",
      "Training Epoch: 90 [42496/50000]\tLoss: 4.7756\tLR: 8.984655\n",
      "Training Epoch: 90 [42624/50000]\tLoss: 4.8530\tLR: 8.984910\n",
      "Training Epoch: 90 [42752/50000]\tLoss: 4.6928\tLR: 8.985166\n",
      "Training Epoch: 90 [42880/50000]\tLoss: 4.6736\tLR: 8.985422\n",
      "Training Epoch: 90 [43008/50000]\tLoss: 4.7660\tLR: 8.985678\n",
      "Training Epoch: 90 [43136/50000]\tLoss: 4.7939\tLR: 8.985934\n",
      "Training Epoch: 90 [43264/50000]\tLoss: 4.6759\tLR: 8.986189\n",
      "Training Epoch: 90 [43392/50000]\tLoss: 4.7198\tLR: 8.986445\n",
      "Training Epoch: 90 [43520/50000]\tLoss: 4.6754\tLR: 8.986701\n",
      "Training Epoch: 90 [43648/50000]\tLoss: 4.7655\tLR: 8.986957\n",
      "Training Epoch: 90 [43776/50000]\tLoss: 4.7036\tLR: 8.987212\n",
      "Training Epoch: 90 [43904/50000]\tLoss: 4.8360\tLR: 8.987468\n",
      "Training Epoch: 90 [44032/50000]\tLoss: 4.7780\tLR: 8.987724\n",
      "Training Epoch: 90 [44160/50000]\tLoss: 4.8510\tLR: 8.987980\n",
      "Training Epoch: 90 [44288/50000]\tLoss: 4.7002\tLR: 8.988235\n",
      "Training Epoch: 90 [44416/50000]\tLoss: 4.7624\tLR: 8.988491\n",
      "Training Epoch: 90 [44544/50000]\tLoss: 4.7530\tLR: 8.988747\n",
      "Training Epoch: 90 [44672/50000]\tLoss: 4.7274\tLR: 8.989003\n",
      "Training Epoch: 90 [44800/50000]\tLoss: 4.7027\tLR: 8.989258\n",
      "Training Epoch: 90 [44928/50000]\tLoss: 4.8257\tLR: 8.989514\n",
      "Training Epoch: 90 [45056/50000]\tLoss: 4.8023\tLR: 8.989770\n",
      "Training Epoch: 90 [45184/50000]\tLoss: 4.7691\tLR: 8.990026\n",
      "Training Epoch: 90 [45312/50000]\tLoss: 4.8296\tLR: 8.990281\n",
      "Training Epoch: 90 [45440/50000]\tLoss: 4.7167\tLR: 8.990537\n",
      "Training Epoch: 90 [45568/50000]\tLoss: 4.7858\tLR: 8.990793\n",
      "Training Epoch: 90 [45696/50000]\tLoss: 4.6377\tLR: 8.991049\n",
      "Training Epoch: 90 [45824/50000]\tLoss: 4.8317\tLR: 8.991304\n",
      "Training Epoch: 90 [45952/50000]\tLoss: 4.7569\tLR: 8.991560\n",
      "Training Epoch: 90 [46080/50000]\tLoss: 4.8315\tLR: 8.991816\n",
      "Training Epoch: 90 [46208/50000]\tLoss: 4.8634\tLR: 8.992072\n",
      "Training Epoch: 90 [46336/50000]\tLoss: 4.8074\tLR: 8.992327\n",
      "Training Epoch: 90 [46464/50000]\tLoss: 4.7522\tLR: 8.992583\n",
      "Training Epoch: 90 [46592/50000]\tLoss: 4.7234\tLR: 8.992839\n",
      "Training Epoch: 90 [46720/50000]\tLoss: 4.7667\tLR: 8.993095\n",
      "Training Epoch: 90 [46848/50000]\tLoss: 4.6871\tLR: 8.993350\n",
      "Training Epoch: 90 [46976/50000]\tLoss: 4.8321\tLR: 8.993606\n",
      "Training Epoch: 90 [47104/50000]\tLoss: 4.9038\tLR: 8.993862\n",
      "Training Epoch: 90 [47232/50000]\tLoss: 4.6696\tLR: 8.994118\n",
      "Training Epoch: 90 [47360/50000]\tLoss: 4.7779\tLR: 8.994373\n",
      "Training Epoch: 90 [47488/50000]\tLoss: 4.7055\tLR: 8.994629\n",
      "Training Epoch: 90 [47616/50000]\tLoss: 4.8251\tLR: 8.994885\n",
      "Training Epoch: 90 [47744/50000]\tLoss: 4.6498\tLR: 8.995141\n",
      "Training Epoch: 90 [47872/50000]\tLoss: 4.7466\tLR: 8.995396\n",
      "Training Epoch: 90 [48000/50000]\tLoss: 4.7563\tLR: 8.995652\n",
      "Training Epoch: 90 [48128/50000]\tLoss: 4.7185\tLR: 8.995908\n",
      "Training Epoch: 90 [48256/50000]\tLoss: 4.7900\tLR: 8.996164\n",
      "Training Epoch: 90 [48384/50000]\tLoss: 4.7971\tLR: 8.996419\n",
      "Training Epoch: 90 [48512/50000]\tLoss: 4.8609\tLR: 8.996675\n",
      "Training Epoch: 90 [48640/50000]\tLoss: 4.7761\tLR: 8.996931\n",
      "Training Epoch: 90 [48768/50000]\tLoss: 4.7599\tLR: 8.997187\n",
      "Training Epoch: 90 [48896/50000]\tLoss: 4.7665\tLR: 8.997442\n",
      "Training Epoch: 90 [49024/50000]\tLoss: 4.7228\tLR: 8.997698\n",
      "Training Epoch: 90 [49152/50000]\tLoss: 4.7608\tLR: 8.997954\n",
      "Training Epoch: 90 [49280/50000]\tLoss: 4.7495\tLR: 8.998210\n",
      "Training Epoch: 90 [49408/50000]\tLoss: 4.7426\tLR: 8.998465\n",
      "Training Epoch: 90 [49536/50000]\tLoss: 4.8367\tLR: 8.998721\n",
      "Training Epoch: 90 [49664/50000]\tLoss: 4.7681\tLR: 8.998977\n",
      "Training Epoch: 90 [49792/50000]\tLoss: 4.7433\tLR: 8.999233\n",
      "Training Epoch: 90 [49920/50000]\tLoss: 4.8412\tLR: 8.999488\n",
      "Training Epoch: 90 [50000/50000]\tLoss: 4.7647\tLR: 8.999744\n",
      "epoch 90 training time consumed: 489.04s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  126173 GB |  126173 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  125786 GB |  125786 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     387 GB |     387 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  126173 GB |  126173 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  125786 GB |  125786 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     387 GB |     387 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  124401 GB |  124401 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  124013 GB |  124013 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     387 GB |     387 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   13378 K  |   13378 K  |\n",
      "|       from large pool |      24    |      65    |    5703 K  |    5703 K  |\n",
      "|       from small pool |     231    |     274    |    7675 K  |    7675 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   13378 K  |   13378 K  |\n",
      "|       from large pool |      24    |      65    |    5703 K  |    5703 K  |\n",
      "|       from small pool |     231    |     274    |    7675 K  |    7675 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      34    |      47    |    7755 K  |    7754 K  |\n",
      "|       from large pool |      10    |      23    |    2741 K  |    2741 K  |\n",
      "|       from small pool |      24    |      35    |    5013 K  |    5013 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 90, Average loss: 0.0379, Accuracy: 0.0100, Time consumed:31.16s\n",
      "\n",
      "saving weights file to checkpoint\\resnet18\\Monday_25_July_2022_16h_56m_47s\\resnet18-90-regular.pth\n",
      "Training Epoch: 91 [128/50000]\tLoss: 4.7084\tLR: 0.020000\n",
      "Training Epoch: 91 [256/50000]\tLoss: 4.8104\tLR: 9.000256\n",
      "Training Epoch: 91 [384/50000]\tLoss: 4.7472\tLR: 9.000512\n",
      "Training Epoch: 91 [512/50000]\tLoss: 4.7965\tLR: 9.000767\n",
      "Training Epoch: 91 [640/50000]\tLoss: 4.7214\tLR: 9.001023\n",
      "Training Epoch: 91 [768/50000]\tLoss: 4.8104\tLR: 9.001279\n",
      "Training Epoch: 91 [896/50000]\tLoss: 4.7364\tLR: 9.001535\n",
      "Training Epoch: 91 [1024/50000]\tLoss: 4.7149\tLR: 9.001790\n",
      "Training Epoch: 91 [1152/50000]\tLoss: 4.8348\tLR: 9.002046\n",
      "Training Epoch: 91 [1280/50000]\tLoss: 4.8894\tLR: 9.002302\n",
      "Training Epoch: 91 [1408/50000]\tLoss: 4.6696\tLR: 9.002558\n",
      "Training Epoch: 91 [1536/50000]\tLoss: 4.7383\tLR: 9.002813\n",
      "Training Epoch: 91 [1664/50000]\tLoss: 4.6568\tLR: 9.003069\n",
      "Training Epoch: 91 [1792/50000]\tLoss: 4.8248\tLR: 9.003325\n",
      "Training Epoch: 91 [1920/50000]\tLoss: 4.7304\tLR: 9.003581\n",
      "Training Epoch: 91 [2048/50000]\tLoss: 4.7551\tLR: 9.003836\n",
      "Training Epoch: 91 [2176/50000]\tLoss: 4.7605\tLR: 9.004092\n",
      "Training Epoch: 91 [2304/50000]\tLoss: 4.7824\tLR: 9.004348\n",
      "Training Epoch: 91 [2432/50000]\tLoss: 4.7100\tLR: 9.004604\n",
      "Training Epoch: 91 [2560/50000]\tLoss: 4.8307\tLR: 9.004859\n",
      "Training Epoch: 91 [2688/50000]\tLoss: 4.7445\tLR: 9.005115\n",
      "Training Epoch: 91 [2816/50000]\tLoss: 4.7221\tLR: 9.005371\n",
      "Training Epoch: 91 [2944/50000]\tLoss: 4.7486\tLR: 9.005627\n",
      "Training Epoch: 91 [3072/50000]\tLoss: 4.7410\tLR: 9.005882\n",
      "Training Epoch: 91 [3200/50000]\tLoss: 4.7901\tLR: 9.006138\n",
      "Training Epoch: 91 [3328/50000]\tLoss: 4.7351\tLR: 9.006394\n",
      "Training Epoch: 91 [3456/50000]\tLoss: 4.7245\tLR: 9.006650\n",
      "Training Epoch: 91 [3584/50000]\tLoss: 4.7528\tLR: 9.006905\n",
      "Training Epoch: 91 [3712/50000]\tLoss: 4.6645\tLR: 9.007161\n",
      "Training Epoch: 91 [3840/50000]\tLoss: 4.7472\tLR: 9.007417\n",
      "Training Epoch: 91 [3968/50000]\tLoss: 4.7286\tLR: 9.007673\n",
      "Training Epoch: 91 [4096/50000]\tLoss: 4.7831\tLR: 9.007928\n",
      "Training Epoch: 91 [4224/50000]\tLoss: 4.7848\tLR: 9.008184\n",
      "Training Epoch: 91 [4352/50000]\tLoss: 4.9225\tLR: 9.008440\n",
      "Training Epoch: 91 [4480/50000]\tLoss: 4.8155\tLR: 9.008696\n",
      "Training Epoch: 91 [4608/50000]\tLoss: 4.7875\tLR: 9.008951\n",
      "Training Epoch: 91 [4736/50000]\tLoss: 4.8144\tLR: 9.009207\n",
      "Training Epoch: 91 [4864/50000]\tLoss: 4.7573\tLR: 9.009463\n",
      "Training Epoch: 91 [4992/50000]\tLoss: 4.7751\tLR: 9.009719\n",
      "Training Epoch: 91 [5120/50000]\tLoss: 4.9003\tLR: 9.009974\n",
      "Training Epoch: 91 [5248/50000]\tLoss: 4.7023\tLR: 9.010230\n",
      "Training Epoch: 91 [5376/50000]\tLoss: 4.7428\tLR: 9.010486\n",
      "Training Epoch: 91 [5504/50000]\tLoss: 4.8283\tLR: 9.010742\n",
      "Training Epoch: 91 [5632/50000]\tLoss: 4.7366\tLR: 9.010997\n",
      "Training Epoch: 91 [5760/50000]\tLoss: 4.8052\tLR: 9.011253\n",
      "Training Epoch: 91 [5888/50000]\tLoss: 4.8300\tLR: 9.011509\n",
      "Training Epoch: 91 [6016/50000]\tLoss: 4.8267\tLR: 9.011765\n",
      "Training Epoch: 91 [6144/50000]\tLoss: 4.6915\tLR: 9.012020\n",
      "Training Epoch: 91 [6272/50000]\tLoss: 4.7084\tLR: 9.012276\n",
      "Training Epoch: 91 [6400/50000]\tLoss: 4.7420\tLR: 9.012532\n",
      "Training Epoch: 91 [6528/50000]\tLoss: 4.8238\tLR: 9.012788\n",
      "Training Epoch: 91 [6656/50000]\tLoss: 4.6964\tLR: 9.013043\n",
      "Training Epoch: 91 [6784/50000]\tLoss: 4.8040\tLR: 9.013299\n",
      "Training Epoch: 91 [6912/50000]\tLoss: 4.7926\tLR: 9.013555\n",
      "Training Epoch: 91 [7040/50000]\tLoss: 4.7736\tLR: 9.013811\n",
      "Training Epoch: 91 [7168/50000]\tLoss: 4.7823\tLR: 9.014066\n",
      "Training Epoch: 91 [7296/50000]\tLoss: 4.7381\tLR: 9.014322\n",
      "Training Epoch: 91 [7424/50000]\tLoss: 4.7943\tLR: 9.014578\n",
      "Training Epoch: 91 [7552/50000]\tLoss: 4.7570\tLR: 9.014834\n",
      "Training Epoch: 91 [7680/50000]\tLoss: 4.7201\tLR: 9.015090\n",
      "Training Epoch: 91 [7808/50000]\tLoss: 4.8123\tLR: 9.015345\n",
      "Training Epoch: 91 [7936/50000]\tLoss: 4.8853\tLR: 9.015601\n",
      "Training Epoch: 91 [8064/50000]\tLoss: 4.7948\tLR: 9.015857\n",
      "Training Epoch: 91 [8192/50000]\tLoss: 4.7748\tLR: 9.016113\n",
      "Training Epoch: 91 [8320/50000]\tLoss: 4.7437\tLR: 9.016368\n",
      "Training Epoch: 91 [8448/50000]\tLoss: 4.7691\tLR: 9.016624\n",
      "Training Epoch: 91 [8576/50000]\tLoss: 4.8926\tLR: 9.016880\n",
      "Training Epoch: 91 [8704/50000]\tLoss: 4.7697\tLR: 9.017136\n",
      "Training Epoch: 91 [8832/50000]\tLoss: 4.8459\tLR: 9.017391\n",
      "Training Epoch: 91 [8960/50000]\tLoss: 4.8834\tLR: 9.017647\n",
      "Training Epoch: 91 [9088/50000]\tLoss: 4.7047\tLR: 9.017903\n",
      "Training Epoch: 91 [9216/50000]\tLoss: 4.8464\tLR: 9.018159\n",
      "Training Epoch: 91 [9344/50000]\tLoss: 4.6821\tLR: 9.018414\n",
      "Training Epoch: 91 [9472/50000]\tLoss: 4.6820\tLR: 9.018670\n",
      "Training Epoch: 91 [9600/50000]\tLoss: 4.7624\tLR: 9.018926\n",
      "Training Epoch: 91 [9728/50000]\tLoss: 4.7075\tLR: 9.019182\n",
      "Training Epoch: 91 [9856/50000]\tLoss: 4.6974\tLR: 9.019437\n",
      "Training Epoch: 91 [9984/50000]\tLoss: 4.8628\tLR: 9.019693\n",
      "Training Epoch: 91 [10112/50000]\tLoss: 4.8172\tLR: 9.019949\n",
      "Training Epoch: 91 [10240/50000]\tLoss: 4.7009\tLR: 9.020205\n",
      "Training Epoch: 91 [10368/50000]\tLoss: 4.7757\tLR: 9.020460\n",
      "Training Epoch: 91 [10496/50000]\tLoss: 4.6848\tLR: 9.020716\n",
      "Training Epoch: 91 [10624/50000]\tLoss: 4.6913\tLR: 9.020972\n",
      "Training Epoch: 91 [10752/50000]\tLoss: 4.8164\tLR: 9.021228\n",
      "Training Epoch: 91 [10880/50000]\tLoss: 4.7914\tLR: 9.021483\n",
      "Training Epoch: 91 [11008/50000]\tLoss: 4.7781\tLR: 9.021739\n",
      "Training Epoch: 91 [11136/50000]\tLoss: 4.8330\tLR: 9.021995\n",
      "Training Epoch: 91 [11264/50000]\tLoss: 4.8336\tLR: 9.022251\n",
      "Training Epoch: 91 [11392/50000]\tLoss: 4.8310\tLR: 9.022506\n",
      "Training Epoch: 91 [11520/50000]\tLoss: 4.8760\tLR: 9.022762\n",
      "Training Epoch: 91 [11648/50000]\tLoss: 4.7378\tLR: 9.023018\n",
      "Training Epoch: 91 [11776/50000]\tLoss: 4.7542\tLR: 9.023274\n",
      "Training Epoch: 91 [11904/50000]\tLoss: 4.7767\tLR: 9.023529\n",
      "Training Epoch: 91 [12032/50000]\tLoss: 4.8615\tLR: 9.023785\n",
      "Training Epoch: 91 [12160/50000]\tLoss: 4.7032\tLR: 9.024041\n",
      "Training Epoch: 91 [12288/50000]\tLoss: 4.7018\tLR: 9.024297\n",
      "Training Epoch: 91 [12416/50000]\tLoss: 4.6349\tLR: 9.024552\n",
      "Training Epoch: 91 [12544/50000]\tLoss: 4.8625\tLR: 9.024808\n",
      "Training Epoch: 91 [12672/50000]\tLoss: 4.7872\tLR: 9.025064\n",
      "Training Epoch: 91 [12800/50000]\tLoss: 4.8329\tLR: 9.025320\n",
      "Training Epoch: 91 [12928/50000]\tLoss: 4.7630\tLR: 9.025575\n",
      "Training Epoch: 91 [13056/50000]\tLoss: 4.7223\tLR: 9.025831\n",
      "Training Epoch: 91 [13184/50000]\tLoss: 4.8104\tLR: 9.026087\n",
      "Training Epoch: 91 [13312/50000]\tLoss: 4.7312\tLR: 9.026343\n",
      "Training Epoch: 91 [13440/50000]\tLoss: 4.6780\tLR: 9.026598\n",
      "Training Epoch: 91 [13568/50000]\tLoss: 4.7988\tLR: 9.026854\n",
      "Training Epoch: 91 [13696/50000]\tLoss: 4.8183\tLR: 9.027110\n",
      "Training Epoch: 91 [13824/50000]\tLoss: 4.7701\tLR: 9.027366\n",
      "Training Epoch: 91 [13952/50000]\tLoss: 4.7392\tLR: 9.027621\n",
      "Training Epoch: 91 [14080/50000]\tLoss: 4.7105\tLR: 9.027877\n",
      "Training Epoch: 91 [14208/50000]\tLoss: 4.7736\tLR: 9.028133\n",
      "Training Epoch: 91 [14336/50000]\tLoss: 4.7704\tLR: 9.028389\n",
      "Training Epoch: 91 [14464/50000]\tLoss: 4.7800\tLR: 9.028645\n",
      "Training Epoch: 91 [14592/50000]\tLoss: 4.8632\tLR: 9.028900\n",
      "Training Epoch: 91 [14720/50000]\tLoss: 4.7371\tLR: 9.029156\n",
      "Training Epoch: 91 [14848/50000]\tLoss: 4.7284\tLR: 9.029412\n",
      "Training Epoch: 91 [14976/50000]\tLoss: 4.7711\tLR: 9.029668\n",
      "Training Epoch: 91 [15104/50000]\tLoss: 4.7477\tLR: 9.029923\n",
      "Training Epoch: 91 [15232/50000]\tLoss: 4.8701\tLR: 9.030179\n",
      "Training Epoch: 91 [15360/50000]\tLoss: 4.8199\tLR: 9.030435\n",
      "Training Epoch: 91 [15488/50000]\tLoss: 4.7761\tLR: 9.030691\n",
      "Training Epoch: 91 [15616/50000]\tLoss: 4.8056\tLR: 9.030946\n",
      "Training Epoch: 91 [15744/50000]\tLoss: 4.6841\tLR: 9.031202\n",
      "Training Epoch: 91 [15872/50000]\tLoss: 4.7006\tLR: 9.031458\n",
      "Training Epoch: 91 [16000/50000]\tLoss: 4.7589\tLR: 9.031714\n",
      "Training Epoch: 91 [16128/50000]\tLoss: 4.8356\tLR: 9.031969\n",
      "Training Epoch: 91 [16256/50000]\tLoss: 4.8053\tLR: 9.032225\n",
      "Training Epoch: 91 [16384/50000]\tLoss: 4.7795\tLR: 9.032481\n",
      "Training Epoch: 91 [16512/50000]\tLoss: 4.8223\tLR: 9.032737\n",
      "Training Epoch: 91 [16640/50000]\tLoss: 4.8486\tLR: 9.032992\n",
      "Training Epoch: 91 [16768/50000]\tLoss: 4.8078\tLR: 9.033248\n",
      "Training Epoch: 91 [16896/50000]\tLoss: 4.6933\tLR: 9.033504\n",
      "Training Epoch: 91 [17024/50000]\tLoss: 4.8257\tLR: 9.033760\n",
      "Training Epoch: 91 [17152/50000]\tLoss: 4.7978\tLR: 9.034015\n",
      "Training Epoch: 91 [17280/50000]\tLoss: 4.7047\tLR: 9.034271\n",
      "Training Epoch: 91 [17408/50000]\tLoss: 4.7864\tLR: 9.034527\n",
      "Training Epoch: 91 [17536/50000]\tLoss: 4.7949\tLR: 9.034783\n",
      "Training Epoch: 91 [17664/50000]\tLoss: 4.7623\tLR: 9.035038\n",
      "Training Epoch: 91 [17792/50000]\tLoss: 4.7746\tLR: 9.035294\n",
      "Training Epoch: 91 [17920/50000]\tLoss: 4.6511\tLR: 9.035550\n",
      "Training Epoch: 91 [18048/50000]\tLoss: 4.7937\tLR: 9.035806\n",
      "Training Epoch: 91 [18176/50000]\tLoss: 4.6868\tLR: 9.036061\n",
      "Training Epoch: 91 [18304/50000]\tLoss: 4.7127\tLR: 9.036317\n",
      "Training Epoch: 91 [18432/50000]\tLoss: 4.6799\tLR: 9.036573\n",
      "Training Epoch: 91 [18560/50000]\tLoss: 4.7739\tLR: 9.036829\n",
      "Training Epoch: 91 [18688/50000]\tLoss: 4.9220\tLR: 9.037084\n",
      "Training Epoch: 91 [18816/50000]\tLoss: 4.7943\tLR: 9.037340\n",
      "Training Epoch: 91 [18944/50000]\tLoss: 4.9003\tLR: 9.037596\n",
      "Training Epoch: 91 [19072/50000]\tLoss: 4.8081\tLR: 9.037852\n",
      "Training Epoch: 91 [19200/50000]\tLoss: 4.7407\tLR: 9.038107\n",
      "Training Epoch: 91 [19328/50000]\tLoss: 4.8054\tLR: 9.038363\n",
      "Training Epoch: 91 [19456/50000]\tLoss: 4.7360\tLR: 9.038619\n",
      "Training Epoch: 91 [19584/50000]\tLoss: 4.7707\tLR: 9.038875\n",
      "Training Epoch: 91 [19712/50000]\tLoss: 4.7066\tLR: 9.039130\n",
      "Training Epoch: 91 [19840/50000]\tLoss: 4.7897\tLR: 9.039386\n",
      "Training Epoch: 91 [19968/50000]\tLoss: 4.7168\tLR: 9.039642\n",
      "Training Epoch: 91 [20096/50000]\tLoss: 4.7377\tLR: 9.039898\n",
      "Training Epoch: 91 [20224/50000]\tLoss: 4.6471\tLR: 9.040153\n",
      "Training Epoch: 91 [20352/50000]\tLoss: 4.8010\tLR: 9.040409\n",
      "Training Epoch: 91 [20480/50000]\tLoss: 4.8915\tLR: 9.040665\n",
      "Training Epoch: 91 [20608/50000]\tLoss: 4.8019\tLR: 9.040921\n",
      "Training Epoch: 91 [20736/50000]\tLoss: 4.7045\tLR: 9.041176\n",
      "Training Epoch: 91 [20864/50000]\tLoss: 4.8285\tLR: 9.041432\n",
      "Training Epoch: 91 [20992/50000]\tLoss: 4.7787\tLR: 9.041688\n",
      "Training Epoch: 91 [21120/50000]\tLoss: 4.6939\tLR: 9.041944\n",
      "Training Epoch: 91 [21248/50000]\tLoss: 4.8002\tLR: 9.042199\n",
      "Training Epoch: 91 [21376/50000]\tLoss: 4.7435\tLR: 9.042455\n",
      "Training Epoch: 91 [21504/50000]\tLoss: 4.6914\tLR: 9.042711\n",
      "Training Epoch: 91 [21632/50000]\tLoss: 4.8059\tLR: 9.042967\n",
      "Training Epoch: 91 [21760/50000]\tLoss: 4.8001\tLR: 9.043223\n",
      "Training Epoch: 91 [21888/50000]\tLoss: 4.8044\tLR: 9.043478\n",
      "Training Epoch: 91 [22016/50000]\tLoss: 4.6903\tLR: 9.043734\n",
      "Training Epoch: 91 [22144/50000]\tLoss: 4.7821\tLR: 9.043990\n",
      "Training Epoch: 91 [22272/50000]\tLoss: 4.7446\tLR: 9.044246\n",
      "Training Epoch: 91 [22400/50000]\tLoss: 4.7787\tLR: 9.044501\n",
      "Training Epoch: 91 [22528/50000]\tLoss: 4.7540\tLR: 9.044757\n",
      "Training Epoch: 91 [22656/50000]\tLoss: 4.7411\tLR: 9.045013\n",
      "Training Epoch: 91 [22784/50000]\tLoss: 4.7818\tLR: 9.045269\n",
      "Training Epoch: 91 [22912/50000]\tLoss: 4.9267\tLR: 9.045524\n",
      "Training Epoch: 91 [23040/50000]\tLoss: 4.8273\tLR: 9.045780\n",
      "Training Epoch: 91 [23168/50000]\tLoss: 4.8496\tLR: 9.046036\n",
      "Training Epoch: 91 [23296/50000]\tLoss: 4.7505\tLR: 9.046292\n",
      "Training Epoch: 91 [23424/50000]\tLoss: 4.8067\tLR: 9.046547\n",
      "Training Epoch: 91 [23552/50000]\tLoss: 4.7582\tLR: 9.046803\n",
      "Training Epoch: 91 [23680/50000]\tLoss: 4.7648\tLR: 9.047059\n",
      "Training Epoch: 91 [23808/50000]\tLoss: 4.7440\tLR: 9.047315\n",
      "Training Epoch: 91 [23936/50000]\tLoss: 4.7223\tLR: 9.047570\n",
      "Training Epoch: 91 [24064/50000]\tLoss: 4.7522\tLR: 9.047826\n",
      "Training Epoch: 91 [24192/50000]\tLoss: 4.8218\tLR: 9.048082\n",
      "Training Epoch: 91 [24320/50000]\tLoss: 4.8405\tLR: 9.048338\n",
      "Training Epoch: 91 [24448/50000]\tLoss: 4.8026\tLR: 9.048593\n",
      "Training Epoch: 91 [24576/50000]\tLoss: 4.7048\tLR: 9.048849\n",
      "Training Epoch: 91 [24704/50000]\tLoss: 4.7975\tLR: 9.049105\n",
      "Training Epoch: 91 [24832/50000]\tLoss: 4.6518\tLR: 9.049361\n",
      "Training Epoch: 91 [24960/50000]\tLoss: 4.7441\tLR: 9.049616\n",
      "Training Epoch: 91 [25088/50000]\tLoss: 4.7998\tLR: 9.049872\n",
      "Training Epoch: 91 [25216/50000]\tLoss: 4.6184\tLR: 9.050128\n",
      "Training Epoch: 91 [25344/50000]\tLoss: 4.8485\tLR: 9.050384\n",
      "Training Epoch: 91 [25472/50000]\tLoss: 4.7459\tLR: 9.050639\n",
      "Training Epoch: 91 [25600/50000]\tLoss: 4.8233\tLR: 9.050895\n",
      "Training Epoch: 91 [25728/50000]\tLoss: 4.6966\tLR: 9.051151\n",
      "Training Epoch: 91 [25856/50000]\tLoss: 4.8090\tLR: 9.051407\n",
      "Training Epoch: 91 [25984/50000]\tLoss: 4.7500\tLR: 9.051662\n",
      "Training Epoch: 91 [26112/50000]\tLoss: 4.7878\tLR: 9.051918\n",
      "Training Epoch: 91 [26240/50000]\tLoss: 4.6856\tLR: 9.052174\n",
      "Training Epoch: 91 [26368/50000]\tLoss: 4.7731\tLR: 9.052430\n",
      "Training Epoch: 91 [26496/50000]\tLoss: 4.8028\tLR: 9.052685\n",
      "Training Epoch: 91 [26624/50000]\tLoss: 4.7393\tLR: 9.052941\n",
      "Training Epoch: 91 [26752/50000]\tLoss: 4.6306\tLR: 9.053197\n",
      "Training Epoch: 91 [26880/50000]\tLoss: 4.8925\tLR: 9.053453\n",
      "Training Epoch: 91 [27008/50000]\tLoss: 4.8715\tLR: 9.053708\n",
      "Training Epoch: 91 [27136/50000]\tLoss: 4.7073\tLR: 9.053964\n",
      "Training Epoch: 91 [27264/50000]\tLoss: 4.9013\tLR: 9.054220\n",
      "Training Epoch: 91 [27392/50000]\tLoss: 4.7580\tLR: 9.054476\n",
      "Training Epoch: 91 [27520/50000]\tLoss: 4.8196\tLR: 9.054731\n",
      "Training Epoch: 91 [27648/50000]\tLoss: 4.8144\tLR: 9.054987\n",
      "Training Epoch: 91 [27776/50000]\tLoss: 4.7733\tLR: 9.055243\n",
      "Training Epoch: 91 [27904/50000]\tLoss: 4.7535\tLR: 9.055499\n",
      "Training Epoch: 91 [28032/50000]\tLoss: 4.8289\tLR: 9.055754\n",
      "Training Epoch: 91 [28160/50000]\tLoss: 4.8791\tLR: 9.056010\n",
      "Training Epoch: 91 [28288/50000]\tLoss: 4.7470\tLR: 9.056266\n",
      "Training Epoch: 91 [28416/50000]\tLoss: 4.7332\tLR: 9.056522\n",
      "Training Epoch: 91 [28544/50000]\tLoss: 4.8259\tLR: 9.056777\n",
      "Training Epoch: 91 [28672/50000]\tLoss: 4.7775\tLR: 9.057033\n",
      "Training Epoch: 91 [28800/50000]\tLoss: 4.6771\tLR: 9.057289\n",
      "Training Epoch: 91 [28928/50000]\tLoss: 4.7490\tLR: 9.057545\n",
      "Training Epoch: 91 [29056/50000]\tLoss: 4.7699\tLR: 9.057801\n",
      "Training Epoch: 91 [29184/50000]\tLoss: 4.7965\tLR: 9.058056\n",
      "Training Epoch: 91 [29312/50000]\tLoss: 4.6871\tLR: 9.058312\n",
      "Training Epoch: 91 [29440/50000]\tLoss: 4.8182\tLR: 9.058568\n",
      "Training Epoch: 91 [29568/50000]\tLoss: 4.7231\tLR: 9.058824\n",
      "Training Epoch: 91 [29696/50000]\tLoss: 4.8125\tLR: 9.059079\n",
      "Training Epoch: 91 [29824/50000]\tLoss: 4.8027\tLR: 9.059335\n",
      "Training Epoch: 91 [29952/50000]\tLoss: 4.6840\tLR: 9.059591\n",
      "Training Epoch: 91 [30080/50000]\tLoss: 4.6773\tLR: 9.059847\n",
      "Training Epoch: 91 [30208/50000]\tLoss: 4.7777\tLR: 9.060102\n",
      "Training Epoch: 91 [30336/50000]\tLoss: 4.7629\tLR: 9.060358\n",
      "Training Epoch: 91 [30464/50000]\tLoss: 4.7075\tLR: 9.060614\n",
      "Training Epoch: 91 [30592/50000]\tLoss: 4.7579\tLR: 9.060870\n",
      "Training Epoch: 91 [30720/50000]\tLoss: 4.7827\tLR: 9.061125\n",
      "Training Epoch: 91 [30848/50000]\tLoss: 4.7317\tLR: 9.061381\n",
      "Training Epoch: 91 [30976/50000]\tLoss: 4.7733\tLR: 9.061637\n",
      "Training Epoch: 91 [31104/50000]\tLoss: 4.6824\tLR: 9.061893\n",
      "Training Epoch: 91 [31232/50000]\tLoss: 4.8152\tLR: 9.062148\n",
      "Training Epoch: 91 [31360/50000]\tLoss: 4.7693\tLR: 9.062404\n",
      "Training Epoch: 91 [31488/50000]\tLoss: 4.8779\tLR: 9.062660\n",
      "Training Epoch: 91 [31616/50000]\tLoss: 4.8363\tLR: 9.062916\n",
      "Training Epoch: 91 [31744/50000]\tLoss: 4.8340\tLR: 9.063171\n",
      "Training Epoch: 91 [31872/50000]\tLoss: 4.7938\tLR: 9.063427\n",
      "Training Epoch: 91 [32000/50000]\tLoss: 4.7635\tLR: 9.063683\n",
      "Training Epoch: 91 [32128/50000]\tLoss: 4.8684\tLR: 9.063939\n",
      "Training Epoch: 91 [32256/50000]\tLoss: 4.9512\tLR: 9.064194\n",
      "Training Epoch: 91 [32384/50000]\tLoss: 4.8124\tLR: 9.064450\n",
      "Training Epoch: 91 [32512/50000]\tLoss: 4.7868\tLR: 9.064706\n",
      "Training Epoch: 91 [32640/50000]\tLoss: 4.7724\tLR: 9.064962\n",
      "Training Epoch: 91 [32768/50000]\tLoss: 4.7292\tLR: 9.065217\n",
      "Training Epoch: 91 [32896/50000]\tLoss: 4.8311\tLR: 9.065473\n",
      "Training Epoch: 91 [33024/50000]\tLoss: 4.7403\tLR: 9.065729\n",
      "Training Epoch: 91 [33152/50000]\tLoss: 4.8846\tLR: 9.065985\n",
      "Training Epoch: 91 [33280/50000]\tLoss: 4.8224\tLR: 9.066240\n",
      "Training Epoch: 91 [33408/50000]\tLoss: 4.8562\tLR: 9.066496\n",
      "Training Epoch: 91 [33536/50000]\tLoss: 4.8311\tLR: 9.066752\n",
      "Training Epoch: 91 [33664/50000]\tLoss: 4.7166\tLR: 9.067008\n",
      "Training Epoch: 91 [33792/50000]\tLoss: 4.6922\tLR: 9.067263\n",
      "Training Epoch: 91 [33920/50000]\tLoss: 4.6784\tLR: 9.067519\n",
      "Training Epoch: 91 [34048/50000]\tLoss: 4.6161\tLR: 9.067775\n",
      "Training Epoch: 91 [34176/50000]\tLoss: 4.8389\tLR: 9.068031\n",
      "Training Epoch: 91 [34304/50000]\tLoss: 4.8983\tLR: 9.068286\n",
      "Training Epoch: 91 [34432/50000]\tLoss: 4.8118\tLR: 9.068542\n",
      "Training Epoch: 91 [34560/50000]\tLoss: 4.7914\tLR: 9.068798\n",
      "Training Epoch: 91 [34688/50000]\tLoss: 4.8289\tLR: 9.069054\n",
      "Training Epoch: 91 [34816/50000]\tLoss: 4.8743\tLR: 9.069309\n",
      "Training Epoch: 91 [34944/50000]\tLoss: 4.7962\tLR: 9.069565\n",
      "Training Epoch: 91 [35072/50000]\tLoss: 4.7395\tLR: 9.069821\n",
      "Training Epoch: 91 [35200/50000]\tLoss: 4.7731\tLR: 9.070077\n",
      "Training Epoch: 91 [35328/50000]\tLoss: 4.6903\tLR: 9.070332\n",
      "Training Epoch: 91 [35456/50000]\tLoss: 4.7865\tLR: 9.070588\n",
      "Training Epoch: 91 [35584/50000]\tLoss: 4.8432\tLR: 9.070844\n",
      "Training Epoch: 91 [35712/50000]\tLoss: 4.7884\tLR: 9.071100\n",
      "Training Epoch: 91 [35840/50000]\tLoss: 4.7466\tLR: 9.071355\n",
      "Training Epoch: 91 [35968/50000]\tLoss: 4.6214\tLR: 9.071611\n",
      "Training Epoch: 91 [36096/50000]\tLoss: 4.7181\tLR: 9.071867\n",
      "Training Epoch: 91 [36224/50000]\tLoss: 4.6934\tLR: 9.072123\n",
      "Training Epoch: 91 [36352/50000]\tLoss: 4.8663\tLR: 9.072379\n",
      "Training Epoch: 91 [36480/50000]\tLoss: 4.9296\tLR: 9.072634\n",
      "Training Epoch: 91 [36608/50000]\tLoss: 4.7619\tLR: 9.072890\n",
      "Training Epoch: 91 [36736/50000]\tLoss: 4.6460\tLR: 9.073146\n",
      "Training Epoch: 91 [36864/50000]\tLoss: 4.8035\tLR: 9.073402\n",
      "Training Epoch: 91 [36992/50000]\tLoss: 4.8221\tLR: 9.073657\n",
      "Training Epoch: 91 [37120/50000]\tLoss: 4.7786\tLR: 9.073913\n",
      "Training Epoch: 91 [37248/50000]\tLoss: 4.8091\tLR: 9.074169\n",
      "Training Epoch: 91 [37376/50000]\tLoss: 4.8371\tLR: 9.074425\n",
      "Training Epoch: 91 [37504/50000]\tLoss: 4.8094\tLR: 9.074680\n",
      "Training Epoch: 91 [37632/50000]\tLoss: 4.7223\tLR: 9.074936\n",
      "Training Epoch: 91 [37760/50000]\tLoss: 4.7777\tLR: 9.075192\n",
      "Training Epoch: 91 [37888/50000]\tLoss: 4.7324\tLR: 9.075448\n",
      "Training Epoch: 91 [38016/50000]\tLoss: 4.7751\tLR: 9.075703\n",
      "Training Epoch: 91 [38144/50000]\tLoss: 4.7795\tLR: 9.075959\n",
      "Training Epoch: 91 [38272/50000]\tLoss: 4.8654\tLR: 9.076215\n",
      "Training Epoch: 91 [38400/50000]\tLoss: 4.9020\tLR: 9.076471\n",
      "Training Epoch: 91 [38528/50000]\tLoss: 4.7232\tLR: 9.076726\n",
      "Training Epoch: 91 [38656/50000]\tLoss: 4.6955\tLR: 9.076982\n",
      "Training Epoch: 91 [38784/50000]\tLoss: 4.7348\tLR: 9.077238\n",
      "Training Epoch: 91 [38912/50000]\tLoss: 4.7747\tLR: 9.077494\n",
      "Training Epoch: 91 [39040/50000]\tLoss: 4.7628\tLR: 9.077749\n",
      "Training Epoch: 91 [39168/50000]\tLoss: 4.7972\tLR: 9.078005\n",
      "Training Epoch: 91 [39296/50000]\tLoss: 4.8277\tLR: 9.078261\n",
      "Training Epoch: 91 [39424/50000]\tLoss: 4.8301\tLR: 9.078517\n",
      "Training Epoch: 91 [39552/50000]\tLoss: 4.7694\tLR: 9.078772\n",
      "Training Epoch: 91 [39680/50000]\tLoss: 4.7338\tLR: 9.079028\n",
      "Training Epoch: 91 [39808/50000]\tLoss: 4.7713\tLR: 9.079284\n",
      "Training Epoch: 91 [39936/50000]\tLoss: 4.6931\tLR: 9.079540\n",
      "Training Epoch: 91 [40064/50000]\tLoss: 4.6895\tLR: 9.079795\n",
      "Training Epoch: 91 [40192/50000]\tLoss: 4.8036\tLR: 9.080051\n",
      "Training Epoch: 91 [40320/50000]\tLoss: 4.8315\tLR: 9.080307\n",
      "Training Epoch: 91 [40448/50000]\tLoss: 4.7205\tLR: 9.080563\n",
      "Training Epoch: 91 [40576/50000]\tLoss: 4.7826\tLR: 9.080818\n",
      "Training Epoch: 91 [40704/50000]\tLoss: 4.7421\tLR: 9.081074\n",
      "Training Epoch: 91 [40832/50000]\tLoss: 4.7459\tLR: 9.081330\n",
      "Training Epoch: 91 [40960/50000]\tLoss: 4.7176\tLR: 9.081586\n",
      "Training Epoch: 91 [41088/50000]\tLoss: 4.8215\tLR: 9.081841\n",
      "Training Epoch: 91 [41216/50000]\tLoss: 4.7117\tLR: 9.082097\n",
      "Training Epoch: 91 [41344/50000]\tLoss: 4.7580\tLR: 9.082353\n",
      "Training Epoch: 91 [41472/50000]\tLoss: 4.8003\tLR: 9.082609\n",
      "Training Epoch: 91 [41600/50000]\tLoss: 4.8067\tLR: 9.082864\n",
      "Training Epoch: 91 [41728/50000]\tLoss: 4.7860\tLR: 9.083120\n",
      "Training Epoch: 91 [41856/50000]\tLoss: 4.8260\tLR: 9.083376\n",
      "Training Epoch: 91 [41984/50000]\tLoss: 4.7735\tLR: 9.083632\n",
      "Training Epoch: 91 [42112/50000]\tLoss: 4.7639\tLR: 9.083887\n",
      "Training Epoch: 91 [42240/50000]\tLoss: 4.8088\tLR: 9.084143\n",
      "Training Epoch: 91 [42368/50000]\tLoss: 4.8317\tLR: 9.084399\n",
      "Training Epoch: 91 [42496/50000]\tLoss: 4.6867\tLR: 9.084655\n",
      "Training Epoch: 91 [42624/50000]\tLoss: 4.7516\tLR: 9.084910\n",
      "Training Epoch: 91 [42752/50000]\tLoss: 4.8121\tLR: 9.085166\n",
      "Training Epoch: 91 [42880/50000]\tLoss: 4.7792\tLR: 9.085422\n",
      "Training Epoch: 91 [43008/50000]\tLoss: 4.8028\tLR: 9.085678\n",
      "Training Epoch: 91 [43136/50000]\tLoss: 4.8285\tLR: 9.085934\n",
      "Training Epoch: 91 [43264/50000]\tLoss: 4.8242\tLR: 9.086189\n",
      "Training Epoch: 91 [43392/50000]\tLoss: 4.7250\tLR: 9.086445\n",
      "Training Epoch: 91 [43520/50000]\tLoss: 4.8056\tLR: 9.086701\n",
      "Training Epoch: 91 [43648/50000]\tLoss: 4.7574\tLR: 9.086957\n",
      "Training Epoch: 91 [43776/50000]\tLoss: 4.6952\tLR: 9.087212\n",
      "Training Epoch: 91 [43904/50000]\tLoss: 4.6437\tLR: 9.087468\n",
      "Training Epoch: 91 [44032/50000]\tLoss: 4.7924\tLR: 9.087724\n",
      "Training Epoch: 91 [44160/50000]\tLoss: 4.7505\tLR: 9.087980\n",
      "Training Epoch: 91 [44288/50000]\tLoss: 4.7524\tLR: 9.088235\n",
      "Training Epoch: 91 [44416/50000]\tLoss: 4.8137\tLR: 9.088491\n",
      "Training Epoch: 91 [44544/50000]\tLoss: 4.7986\tLR: 9.088747\n",
      "Training Epoch: 91 [44672/50000]\tLoss: 4.7312\tLR: 9.089003\n",
      "Training Epoch: 91 [44800/50000]\tLoss: 4.7402\tLR: 9.089258\n",
      "Training Epoch: 91 [44928/50000]\tLoss: 4.7890\tLR: 9.089514\n",
      "Training Epoch: 91 [45056/50000]\tLoss: 4.8314\tLR: 9.089770\n",
      "Training Epoch: 91 [45184/50000]\tLoss: 4.8507\tLR: 9.090026\n",
      "Training Epoch: 91 [45312/50000]\tLoss: 4.7632\tLR: 9.090281\n",
      "Training Epoch: 91 [45440/50000]\tLoss: 4.7614\tLR: 9.090537\n",
      "Training Epoch: 91 [45568/50000]\tLoss: 4.7880\tLR: 9.090793\n",
      "Training Epoch: 91 [45696/50000]\tLoss: 4.8359\tLR: 9.091049\n",
      "Training Epoch: 91 [45824/50000]\tLoss: 4.8216\tLR: 9.091304\n",
      "Training Epoch: 91 [45952/50000]\tLoss: 4.7679\tLR: 9.091560\n",
      "Training Epoch: 91 [46080/50000]\tLoss: 4.7651\tLR: 9.091816\n",
      "Training Epoch: 91 [46208/50000]\tLoss: 4.8420\tLR: 9.092072\n",
      "Training Epoch: 91 [46336/50000]\tLoss: 4.6905\tLR: 9.092327\n",
      "Training Epoch: 91 [46464/50000]\tLoss: 4.7738\tLR: 9.092583\n",
      "Training Epoch: 91 [46592/50000]\tLoss: 4.7240\tLR: 9.092839\n",
      "Training Epoch: 91 [46720/50000]\tLoss: 4.8772\tLR: 9.093095\n",
      "Training Epoch: 91 [46848/50000]\tLoss: 4.7834\tLR: 9.093350\n",
      "Training Epoch: 91 [46976/50000]\tLoss: 4.8499\tLR: 9.093606\n",
      "Training Epoch: 91 [47104/50000]\tLoss: 4.6929\tLR: 9.093862\n",
      "Training Epoch: 91 [47232/50000]\tLoss: 4.7462\tLR: 9.094118\n",
      "Training Epoch: 91 [47360/50000]\tLoss: 4.7915\tLR: 9.094373\n",
      "Training Epoch: 91 [47488/50000]\tLoss: 4.8306\tLR: 9.094629\n",
      "Training Epoch: 91 [47616/50000]\tLoss: 4.7698\tLR: 9.094885\n",
      "Training Epoch: 91 [47744/50000]\tLoss: 4.7666\tLR: 9.095141\n",
      "Training Epoch: 91 [47872/50000]\tLoss: 4.9010\tLR: 9.095396\n",
      "Training Epoch: 91 [48000/50000]\tLoss: 4.7553\tLR: 9.095652\n",
      "Training Epoch: 91 [48128/50000]\tLoss: 4.7715\tLR: 9.095908\n",
      "Training Epoch: 91 [48256/50000]\tLoss: 4.7801\tLR: 9.096164\n",
      "Training Epoch: 91 [48384/50000]\tLoss: 4.8510\tLR: 9.096419\n",
      "Training Epoch: 91 [48512/50000]\tLoss: 4.7233\tLR: 9.096675\n",
      "Training Epoch: 91 [48640/50000]\tLoss: 4.8638\tLR: 9.096931\n",
      "Training Epoch: 91 [48768/50000]\tLoss: 4.8911\tLR: 9.097187\n",
      "Training Epoch: 91 [48896/50000]\tLoss: 4.8630\tLR: 9.097442\n",
      "Training Epoch: 91 [49024/50000]\tLoss: 4.7494\tLR: 9.097698\n",
      "Training Epoch: 91 [49152/50000]\tLoss: 4.8284\tLR: 9.097954\n",
      "Training Epoch: 91 [49280/50000]\tLoss: 4.8043\tLR: 9.098210\n",
      "Training Epoch: 91 [49408/50000]\tLoss: 4.8249\tLR: 9.098465\n",
      "Training Epoch: 91 [49536/50000]\tLoss: 4.7956\tLR: 9.098721\n",
      "Training Epoch: 91 [49664/50000]\tLoss: 4.7655\tLR: 9.098977\n",
      "Training Epoch: 91 [49792/50000]\tLoss: 4.7957\tLR: 9.099233\n",
      "Training Epoch: 91 [49920/50000]\tLoss: 4.7297\tLR: 9.099488\n",
      "Training Epoch: 91 [50000/50000]\tLoss: 4.6378\tLR: 9.099744\n",
      "epoch 91 training time consumed: 489.04s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  127575 GB |  127575 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  127184 GB |  127184 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     391 GB |     391 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  127575 GB |  127575 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  127184 GB |  127184 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     391 GB |     391 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  125783 GB |  125783 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  125391 GB |  125391 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     391 GB |     391 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   13527 K  |   13527 K  |\n",
      "|       from large pool |      24    |      65    |    5766 K  |    5766 K  |\n",
      "|       from small pool |     231    |     274    |    7760 K  |    7760 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   13527 K  |   13527 K  |\n",
      "|       from large pool |      24    |      65    |    5766 K  |    5766 K  |\n",
      "|       from small pool |     231    |     274    |    7760 K  |    7760 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      34    |      47    |    7841 K  |    7841 K  |\n",
      "|       from large pool |      10    |      23    |    2771 K  |    2771 K  |\n",
      "|       from small pool |      24    |      35    |    5069 K  |    5069 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 91, Average loss: 0.0378, Accuracy: 0.0100, Time consumed:31.22s\n",
      "\n",
      "Training Epoch: 92 [128/50000]\tLoss: 4.8552\tLR: 0.020000\n",
      "Training Epoch: 92 [256/50000]\tLoss: 4.8451\tLR: 9.100256\n",
      "Training Epoch: 92 [384/50000]\tLoss: 4.7928\tLR: 9.100512\n",
      "Training Epoch: 92 [512/50000]\tLoss: 4.7856\tLR: 9.100767\n",
      "Training Epoch: 92 [640/50000]\tLoss: 4.7817\tLR: 9.101023\n",
      "Training Epoch: 92 [768/50000]\tLoss: 4.7785\tLR: 9.101279\n",
      "Training Epoch: 92 [896/50000]\tLoss: 4.8225\tLR: 9.101535\n",
      "Training Epoch: 92 [1024/50000]\tLoss: 4.7245\tLR: 9.101790\n",
      "Training Epoch: 92 [1152/50000]\tLoss: 4.7384\tLR: 9.102046\n",
      "Training Epoch: 92 [1280/50000]\tLoss: 4.7913\tLR: 9.102302\n",
      "Training Epoch: 92 [1408/50000]\tLoss: 4.7679\tLR: 9.102558\n",
      "Training Epoch: 92 [1536/50000]\tLoss: 4.7581\tLR: 9.102813\n",
      "Training Epoch: 92 [1664/50000]\tLoss: 4.7094\tLR: 9.103069\n",
      "Training Epoch: 92 [1792/50000]\tLoss: 4.8205\tLR: 9.103325\n",
      "Training Epoch: 92 [1920/50000]\tLoss: 4.8397\tLR: 9.103581\n",
      "Training Epoch: 92 [2048/50000]\tLoss: 4.6966\tLR: 9.103836\n",
      "Training Epoch: 92 [2176/50000]\tLoss: 4.8023\tLR: 9.104092\n",
      "Training Epoch: 92 [2304/50000]\tLoss: 4.7305\tLR: 9.104348\n",
      "Training Epoch: 92 [2432/50000]\tLoss: 4.8150\tLR: 9.104604\n",
      "Training Epoch: 92 [2560/50000]\tLoss: 4.7638\tLR: 9.104859\n",
      "Training Epoch: 92 [2688/50000]\tLoss: 4.8162\tLR: 9.105115\n",
      "Training Epoch: 92 [2816/50000]\tLoss: 4.8976\tLR: 9.105371\n",
      "Training Epoch: 92 [2944/50000]\tLoss: 4.8521\tLR: 9.105627\n",
      "Training Epoch: 92 [3072/50000]\tLoss: 4.7431\tLR: 9.105882\n",
      "Training Epoch: 92 [3200/50000]\tLoss: 4.8381\tLR: 9.106138\n",
      "Training Epoch: 92 [3328/50000]\tLoss: 4.7397\tLR: 9.106394\n",
      "Training Epoch: 92 [3456/50000]\tLoss: 4.7903\tLR: 9.106650\n",
      "Training Epoch: 92 [3584/50000]\tLoss: 4.7741\tLR: 9.106905\n",
      "Training Epoch: 92 [3712/50000]\tLoss: 4.7455\tLR: 9.107161\n",
      "Training Epoch: 92 [3840/50000]\tLoss: 4.7484\tLR: 9.107417\n",
      "Training Epoch: 92 [3968/50000]\tLoss: 4.7720\tLR: 9.107673\n",
      "Training Epoch: 92 [4096/50000]\tLoss: 4.6753\tLR: 9.107928\n",
      "Training Epoch: 92 [4224/50000]\tLoss: 4.7101\tLR: 9.108184\n",
      "Training Epoch: 92 [4352/50000]\tLoss: 4.7611\tLR: 9.108440\n",
      "Training Epoch: 92 [4480/50000]\tLoss: 4.8238\tLR: 9.108696\n",
      "Training Epoch: 92 [4608/50000]\tLoss: 4.7485\tLR: 9.108951\n",
      "Training Epoch: 92 [4736/50000]\tLoss: 4.7879\tLR: 9.109207\n",
      "Training Epoch: 92 [4864/50000]\tLoss: 4.7199\tLR: 9.109463\n",
      "Training Epoch: 92 [4992/50000]\tLoss: 4.7641\tLR: 9.109719\n",
      "Training Epoch: 92 [5120/50000]\tLoss: 4.7146\tLR: 9.109974\n",
      "Training Epoch: 92 [5248/50000]\tLoss: 4.7214\tLR: 9.110230\n",
      "Training Epoch: 92 [5376/50000]\tLoss: 4.7743\tLR: 9.110486\n",
      "Training Epoch: 92 [5504/50000]\tLoss: 4.8065\tLR: 9.110742\n",
      "Training Epoch: 92 [5632/50000]\tLoss: 4.7664\tLR: 9.110997\n",
      "Training Epoch: 92 [5760/50000]\tLoss: 4.7487\tLR: 9.111253\n",
      "Training Epoch: 92 [5888/50000]\tLoss: 4.7125\tLR: 9.111509\n",
      "Training Epoch: 92 [6016/50000]\tLoss: 4.7434\tLR: 9.111765\n",
      "Training Epoch: 92 [6144/50000]\tLoss: 4.8411\tLR: 9.112020\n",
      "Training Epoch: 92 [6272/50000]\tLoss: 4.6876\tLR: 9.112276\n",
      "Training Epoch: 92 [6400/50000]\tLoss: 4.6925\tLR: 9.112532\n",
      "Training Epoch: 92 [6528/50000]\tLoss: 4.7889\tLR: 9.112788\n",
      "Training Epoch: 92 [6656/50000]\tLoss: 4.7600\tLR: 9.113043\n",
      "Training Epoch: 92 [6784/50000]\tLoss: 4.6950\tLR: 9.113299\n",
      "Training Epoch: 92 [6912/50000]\tLoss: 4.7597\tLR: 9.113555\n",
      "Training Epoch: 92 [7040/50000]\tLoss: 4.8894\tLR: 9.113811\n",
      "Training Epoch: 92 [7168/50000]\tLoss: 4.8414\tLR: 9.114066\n",
      "Training Epoch: 92 [7296/50000]\tLoss: 4.8353\tLR: 9.114322\n",
      "Training Epoch: 92 [7424/50000]\tLoss: 4.7374\tLR: 9.114578\n",
      "Training Epoch: 92 [7552/50000]\tLoss: 4.7864\tLR: 9.114834\n",
      "Training Epoch: 92 [7680/50000]\tLoss: 4.7791\tLR: 9.115090\n",
      "Training Epoch: 92 [7808/50000]\tLoss: 4.7508\tLR: 9.115345\n",
      "Training Epoch: 92 [7936/50000]\tLoss: 4.8330\tLR: 9.115601\n",
      "Training Epoch: 92 [8064/50000]\tLoss: 4.8649\tLR: 9.115857\n",
      "Training Epoch: 92 [8192/50000]\tLoss: 4.7471\tLR: 9.116113\n",
      "Training Epoch: 92 [8320/50000]\tLoss: 4.7237\tLR: 9.116368\n",
      "Training Epoch: 92 [8448/50000]\tLoss: 4.8416\tLR: 9.116624\n",
      "Training Epoch: 92 [8576/50000]\tLoss: 4.7296\tLR: 9.116880\n",
      "Training Epoch: 92 [8704/50000]\tLoss: 4.7113\tLR: 9.117136\n",
      "Training Epoch: 92 [8832/50000]\tLoss: 4.8633\tLR: 9.117391\n",
      "Training Epoch: 92 [8960/50000]\tLoss: 4.8517\tLR: 9.117647\n",
      "Training Epoch: 92 [9088/50000]\tLoss: 4.7922\tLR: 9.117903\n",
      "Training Epoch: 92 [9216/50000]\tLoss: 4.7111\tLR: 9.118159\n",
      "Training Epoch: 92 [9344/50000]\tLoss: 4.7696\tLR: 9.118414\n",
      "Training Epoch: 92 [9472/50000]\tLoss: 4.7655\tLR: 9.118670\n",
      "Training Epoch: 92 [9600/50000]\tLoss: 4.6817\tLR: 9.118926\n",
      "Training Epoch: 92 [9728/50000]\tLoss: 4.7280\tLR: 9.119182\n",
      "Training Epoch: 92 [9856/50000]\tLoss: 4.7285\tLR: 9.119437\n",
      "Training Epoch: 92 [9984/50000]\tLoss: 4.7106\tLR: 9.119693\n",
      "Training Epoch: 92 [10112/50000]\tLoss: 4.8526\tLR: 9.119949\n",
      "Training Epoch: 92 [10240/50000]\tLoss: 4.8368\tLR: 9.120205\n",
      "Training Epoch: 92 [10368/50000]\tLoss: 4.7360\tLR: 9.120460\n",
      "Training Epoch: 92 [10496/50000]\tLoss: 4.7845\tLR: 9.120716\n",
      "Training Epoch: 92 [10624/50000]\tLoss: 4.7313\tLR: 9.120972\n",
      "Training Epoch: 92 [10752/50000]\tLoss: 4.7580\tLR: 9.121228\n",
      "Training Epoch: 92 [10880/50000]\tLoss: 4.8174\tLR: 9.121483\n",
      "Training Epoch: 92 [11008/50000]\tLoss: 4.6811\tLR: 9.121739\n",
      "Training Epoch: 92 [11136/50000]\tLoss: 4.7162\tLR: 9.121995\n",
      "Training Epoch: 92 [11264/50000]\tLoss: 4.7426\tLR: 9.122251\n",
      "Training Epoch: 92 [11392/50000]\tLoss: 4.7440\tLR: 9.122506\n",
      "Training Epoch: 92 [11520/50000]\tLoss: 4.8166\tLR: 9.122762\n",
      "Training Epoch: 92 [11648/50000]\tLoss: 4.8276\tLR: 9.123018\n",
      "Training Epoch: 92 [11776/50000]\tLoss: 4.7387\tLR: 9.123274\n",
      "Training Epoch: 92 [11904/50000]\tLoss: 4.8781\tLR: 9.123529\n",
      "Training Epoch: 92 [12032/50000]\tLoss: 4.7188\tLR: 9.123785\n",
      "Training Epoch: 92 [12160/50000]\tLoss: 4.8543\tLR: 9.124041\n",
      "Training Epoch: 92 [12288/50000]\tLoss: 4.6471\tLR: 9.124297\n",
      "Training Epoch: 92 [12416/50000]\tLoss: 4.8237\tLR: 9.124552\n",
      "Training Epoch: 92 [12544/50000]\tLoss: 4.8064\tLR: 9.124808\n",
      "Training Epoch: 92 [12672/50000]\tLoss: 4.7335\tLR: 9.125064\n",
      "Training Epoch: 92 [12800/50000]\tLoss: 4.8636\tLR: 9.125320\n",
      "Training Epoch: 92 [12928/50000]\tLoss: 4.8084\tLR: 9.125575\n",
      "Training Epoch: 92 [13056/50000]\tLoss: 4.8291\tLR: 9.125831\n",
      "Training Epoch: 92 [13184/50000]\tLoss: 4.8310\tLR: 9.126087\n",
      "Training Epoch: 92 [13312/50000]\tLoss: 4.9241\tLR: 9.126343\n",
      "Training Epoch: 92 [13440/50000]\tLoss: 4.8922\tLR: 9.126598\n",
      "Training Epoch: 92 [13568/50000]\tLoss: 4.7386\tLR: 9.126854\n",
      "Training Epoch: 92 [13696/50000]\tLoss: 4.8044\tLR: 9.127110\n",
      "Training Epoch: 92 [13824/50000]\tLoss: 4.8931\tLR: 9.127366\n",
      "Training Epoch: 92 [13952/50000]\tLoss: 4.7646\tLR: 9.127621\n",
      "Training Epoch: 92 [14080/50000]\tLoss: 4.8237\tLR: 9.127877\n",
      "Training Epoch: 92 [14208/50000]\tLoss: 4.7872\tLR: 9.128133\n",
      "Training Epoch: 92 [14336/50000]\tLoss: 4.7128\tLR: 9.128389\n",
      "Training Epoch: 92 [14464/50000]\tLoss: 4.7737\tLR: 9.128645\n",
      "Training Epoch: 92 [14592/50000]\tLoss: 4.7735\tLR: 9.128900\n",
      "Training Epoch: 92 [14720/50000]\tLoss: 4.7114\tLR: 9.129156\n",
      "Training Epoch: 92 [14848/50000]\tLoss: 4.8391\tLR: 9.129412\n",
      "Training Epoch: 92 [14976/50000]\tLoss: 4.6882\tLR: 9.129668\n",
      "Training Epoch: 92 [15104/50000]\tLoss: 4.8575\tLR: 9.129923\n",
      "Training Epoch: 92 [15232/50000]\tLoss: 4.7643\tLR: 9.130179\n",
      "Training Epoch: 92 [15360/50000]\tLoss: 4.7670\tLR: 9.130435\n",
      "Training Epoch: 92 [15488/50000]\tLoss: 4.9133\tLR: 9.130691\n",
      "Training Epoch: 92 [15616/50000]\tLoss: 4.8183\tLR: 9.130946\n",
      "Training Epoch: 92 [15744/50000]\tLoss: 4.6755\tLR: 9.131202\n",
      "Training Epoch: 92 [15872/50000]\tLoss: 4.7627\tLR: 9.131458\n",
      "Training Epoch: 92 [16000/50000]\tLoss: 4.7912\tLR: 9.131714\n",
      "Training Epoch: 92 [16128/50000]\tLoss: 4.8852\tLR: 9.131969\n",
      "Training Epoch: 92 [16256/50000]\tLoss: 4.7540\tLR: 9.132225\n",
      "Training Epoch: 92 [16384/50000]\tLoss: 4.7570\tLR: 9.132481\n",
      "Training Epoch: 92 [16512/50000]\tLoss: 4.6512\tLR: 9.132737\n",
      "Training Epoch: 92 [16640/50000]\tLoss: 4.7318\tLR: 9.132992\n",
      "Training Epoch: 92 [16768/50000]\tLoss: 4.8412\tLR: 9.133248\n",
      "Training Epoch: 92 [16896/50000]\tLoss: 4.9201\tLR: 9.133504\n",
      "Training Epoch: 92 [17024/50000]\tLoss: 4.7793\tLR: 9.133760\n",
      "Training Epoch: 92 [17152/50000]\tLoss: 4.7678\tLR: 9.134015\n",
      "Training Epoch: 92 [17280/50000]\tLoss: 4.7948\tLR: 9.134271\n",
      "Training Epoch: 92 [17408/50000]\tLoss: 4.7847\tLR: 9.134527\n",
      "Training Epoch: 92 [17536/50000]\tLoss: 4.9080\tLR: 9.134783\n",
      "Training Epoch: 92 [17664/50000]\tLoss: 4.8890\tLR: 9.135038\n",
      "Training Epoch: 92 [17792/50000]\tLoss: 4.7605\tLR: 9.135294\n",
      "Training Epoch: 92 [17920/50000]\tLoss: 4.7582\tLR: 9.135550\n",
      "Training Epoch: 92 [18048/50000]\tLoss: 4.8045\tLR: 9.135806\n",
      "Training Epoch: 92 [18176/50000]\tLoss: 4.8463\tLR: 9.136061\n",
      "Training Epoch: 92 [18304/50000]\tLoss: 4.8648\tLR: 9.136317\n",
      "Training Epoch: 92 [18432/50000]\tLoss: 4.8305\tLR: 9.136573\n",
      "Training Epoch: 92 [18560/50000]\tLoss: 4.8710\tLR: 9.136829\n",
      "Training Epoch: 92 [18688/50000]\tLoss: 4.6808\tLR: 9.137084\n",
      "Training Epoch: 92 [18816/50000]\tLoss: 4.8831\tLR: 9.137340\n",
      "Training Epoch: 92 [18944/50000]\tLoss: 4.8263\tLR: 9.137596\n",
      "Training Epoch: 92 [19072/50000]\tLoss: 4.7261\tLR: 9.137852\n",
      "Training Epoch: 92 [19200/50000]\tLoss: 4.7414\tLR: 9.138107\n",
      "Training Epoch: 92 [19328/50000]\tLoss: 4.7558\tLR: 9.138363\n",
      "Training Epoch: 92 [19456/50000]\tLoss: 4.8066\tLR: 9.138619\n",
      "Training Epoch: 92 [19584/50000]\tLoss: 4.8684\tLR: 9.138875\n",
      "Training Epoch: 92 [19712/50000]\tLoss: 4.7893\tLR: 9.139130\n",
      "Training Epoch: 92 [19840/50000]\tLoss: 4.7986\tLR: 9.139386\n",
      "Training Epoch: 92 [19968/50000]\tLoss: 4.6702\tLR: 9.139642\n",
      "Training Epoch: 92 [20096/50000]\tLoss: 4.7693\tLR: 9.139898\n",
      "Training Epoch: 92 [20224/50000]\tLoss: 4.7957\tLR: 9.140153\n",
      "Training Epoch: 92 [20352/50000]\tLoss: 4.8415\tLR: 9.140409\n",
      "Training Epoch: 92 [20480/50000]\tLoss: 4.7239\tLR: 9.140665\n",
      "Training Epoch: 92 [20608/50000]\tLoss: 4.7994\tLR: 9.140921\n",
      "Training Epoch: 92 [20736/50000]\tLoss: 4.7959\tLR: 9.141176\n",
      "Training Epoch: 92 [20864/50000]\tLoss: 4.8138\tLR: 9.141432\n",
      "Training Epoch: 92 [20992/50000]\tLoss: 4.7970\tLR: 9.141688\n",
      "Training Epoch: 92 [21120/50000]\tLoss: 4.7420\tLR: 9.141944\n",
      "Training Epoch: 92 [21248/50000]\tLoss: 4.8457\tLR: 9.142199\n",
      "Training Epoch: 92 [21376/50000]\tLoss: 4.8074\tLR: 9.142455\n",
      "Training Epoch: 92 [21504/50000]\tLoss: 4.8569\tLR: 9.142711\n",
      "Training Epoch: 92 [21632/50000]\tLoss: 4.7392\tLR: 9.142967\n",
      "Training Epoch: 92 [21760/50000]\tLoss: 4.7340\tLR: 9.143223\n",
      "Training Epoch: 92 [21888/50000]\tLoss: 4.7443\tLR: 9.143478\n",
      "Training Epoch: 92 [22016/50000]\tLoss: 4.7708\tLR: 9.143734\n",
      "Training Epoch: 92 [22144/50000]\tLoss: 4.7576\tLR: 9.143990\n",
      "Training Epoch: 92 [22272/50000]\tLoss: 4.8359\tLR: 9.144246\n",
      "Training Epoch: 92 [22400/50000]\tLoss: 4.6868\tLR: 9.144501\n",
      "Training Epoch: 92 [22528/50000]\tLoss: 4.7781\tLR: 9.144757\n",
      "Training Epoch: 92 [22656/50000]\tLoss: 4.7852\tLR: 9.145013\n",
      "Training Epoch: 92 [22784/50000]\tLoss: 4.7197\tLR: 9.145269\n",
      "Training Epoch: 92 [22912/50000]\tLoss: 4.7844\tLR: 9.145524\n",
      "Training Epoch: 92 [23040/50000]\tLoss: 4.6731\tLR: 9.145780\n",
      "Training Epoch: 92 [23168/50000]\tLoss: 4.7568\tLR: 9.146036\n",
      "Training Epoch: 92 [23296/50000]\tLoss: 4.6983\tLR: 9.146292\n",
      "Training Epoch: 92 [23424/50000]\tLoss: 4.8051\tLR: 9.146547\n",
      "Training Epoch: 92 [23552/50000]\tLoss: 4.7944\tLR: 9.146803\n",
      "Training Epoch: 92 [23680/50000]\tLoss: 4.7783\tLR: 9.147059\n",
      "Training Epoch: 92 [23808/50000]\tLoss: 4.6867\tLR: 9.147315\n",
      "Training Epoch: 92 [23936/50000]\tLoss: 4.7863\tLR: 9.147570\n",
      "Training Epoch: 92 [24064/50000]\tLoss: 4.7049\tLR: 9.147826\n",
      "Training Epoch: 92 [24192/50000]\tLoss: 4.8020\tLR: 9.148082\n",
      "Training Epoch: 92 [24320/50000]\tLoss: 4.8320\tLR: 9.148338\n",
      "Training Epoch: 92 [24448/50000]\tLoss: 4.7750\tLR: 9.148593\n",
      "Training Epoch: 92 [24576/50000]\tLoss: 4.7923\tLR: 9.148849\n",
      "Training Epoch: 92 [24704/50000]\tLoss: 4.8400\tLR: 9.149105\n",
      "Training Epoch: 92 [24832/50000]\tLoss: 4.6242\tLR: 9.149361\n",
      "Training Epoch: 92 [24960/50000]\tLoss: 4.7480\tLR: 9.149616\n",
      "Training Epoch: 92 [25088/50000]\tLoss: 4.7957\tLR: 9.149872\n",
      "Training Epoch: 92 [25216/50000]\tLoss: 4.8138\tLR: 9.150128\n",
      "Training Epoch: 92 [25344/50000]\tLoss: 4.7741\tLR: 9.150384\n",
      "Training Epoch: 92 [25472/50000]\tLoss: 4.6596\tLR: 9.150639\n",
      "Training Epoch: 92 [25600/50000]\tLoss: 4.7179\tLR: 9.150895\n",
      "Training Epoch: 92 [25728/50000]\tLoss: 4.6939\tLR: 9.151151\n",
      "Training Epoch: 92 [25856/50000]\tLoss: 4.7848\tLR: 9.151407\n",
      "Training Epoch: 92 [25984/50000]\tLoss: 4.6888\tLR: 9.151662\n",
      "Training Epoch: 92 [26112/50000]\tLoss: 4.7366\tLR: 9.151918\n",
      "Training Epoch: 92 [26240/50000]\tLoss: 4.7288\tLR: 9.152174\n",
      "Training Epoch: 92 [26368/50000]\tLoss: 4.7675\tLR: 9.152430\n",
      "Training Epoch: 92 [26496/50000]\tLoss: 4.8502\tLR: 9.152685\n",
      "Training Epoch: 92 [26624/50000]\tLoss: 4.9064\tLR: 9.152941\n",
      "Training Epoch: 92 [26752/50000]\tLoss: 4.7585\tLR: 9.153197\n",
      "Training Epoch: 92 [26880/50000]\tLoss: 4.7455\tLR: 9.153453\n",
      "Training Epoch: 92 [27008/50000]\tLoss: 4.8356\tLR: 9.153708\n",
      "Training Epoch: 92 [27136/50000]\tLoss: 4.7750\tLR: 9.153964\n",
      "Training Epoch: 92 [27264/50000]\tLoss: 4.7133\tLR: 9.154220\n",
      "Training Epoch: 92 [27392/50000]\tLoss: 4.7469\tLR: 9.154476\n",
      "Training Epoch: 92 [27520/50000]\tLoss: 4.6873\tLR: 9.154731\n",
      "Training Epoch: 92 [27648/50000]\tLoss: 4.8781\tLR: 9.154987\n",
      "Training Epoch: 92 [27776/50000]\tLoss: 4.8488\tLR: 9.155243\n",
      "Training Epoch: 92 [27904/50000]\tLoss: 4.7481\tLR: 9.155499\n",
      "Training Epoch: 92 [28032/50000]\tLoss: 4.8158\tLR: 9.155754\n",
      "Training Epoch: 92 [28160/50000]\tLoss: 4.8456\tLR: 9.156010\n",
      "Training Epoch: 92 [28288/50000]\tLoss: 4.8032\tLR: 9.156266\n",
      "Training Epoch: 92 [28416/50000]\tLoss: 4.9172\tLR: 9.156522\n",
      "Training Epoch: 92 [28544/50000]\tLoss: 4.6988\tLR: 9.156777\n",
      "Training Epoch: 92 [28672/50000]\tLoss: 4.8571\tLR: 9.157033\n",
      "Training Epoch: 92 [28800/50000]\tLoss: 4.7179\tLR: 9.157289\n",
      "Training Epoch: 92 [28928/50000]\tLoss: 4.7263\tLR: 9.157545\n",
      "Training Epoch: 92 [29056/50000]\tLoss: 4.7415\tLR: 9.157801\n",
      "Training Epoch: 92 [29184/50000]\tLoss: 4.7883\tLR: 9.158056\n",
      "Training Epoch: 92 [29312/50000]\tLoss: 4.9037\tLR: 9.158312\n",
      "Training Epoch: 92 [29440/50000]\tLoss: 4.7803\tLR: 9.158568\n",
      "Training Epoch: 92 [29568/50000]\tLoss: 4.8417\tLR: 9.158824\n",
      "Training Epoch: 92 [29696/50000]\tLoss: 4.9483\tLR: 9.159079\n",
      "Training Epoch: 92 [29824/50000]\tLoss: 4.7796\tLR: 9.159335\n",
      "Training Epoch: 92 [29952/50000]\tLoss: 4.7655\tLR: 9.159591\n",
      "Training Epoch: 92 [30080/50000]\tLoss: 4.7273\tLR: 9.159847\n",
      "Training Epoch: 92 [30208/50000]\tLoss: 4.7595\tLR: 9.160102\n",
      "Training Epoch: 92 [30336/50000]\tLoss: 4.6415\tLR: 9.160358\n",
      "Training Epoch: 92 [30464/50000]\tLoss: 4.7184\tLR: 9.160614\n",
      "Training Epoch: 92 [30592/50000]\tLoss: 4.6490\tLR: 9.160870\n",
      "Training Epoch: 92 [30720/50000]\tLoss: 4.8521\tLR: 9.161125\n",
      "Training Epoch: 92 [30848/50000]\tLoss: 4.8084\tLR: 9.161381\n",
      "Training Epoch: 92 [30976/50000]\tLoss: 4.7250\tLR: 9.161637\n",
      "Training Epoch: 92 [31104/50000]\tLoss: 4.8240\tLR: 9.161893\n",
      "Training Epoch: 92 [31232/50000]\tLoss: 4.7662\tLR: 9.162148\n",
      "Training Epoch: 92 [31360/50000]\tLoss: 4.7336\tLR: 9.162404\n",
      "Training Epoch: 92 [31488/50000]\tLoss: 4.7695\tLR: 9.162660\n",
      "Training Epoch: 92 [31616/50000]\tLoss: 4.7691\tLR: 9.162916\n",
      "Training Epoch: 92 [31744/50000]\tLoss: 4.6453\tLR: 9.163171\n",
      "Training Epoch: 92 [31872/50000]\tLoss: 4.8654\tLR: 9.163427\n",
      "Training Epoch: 92 [32000/50000]\tLoss: 4.7682\tLR: 9.163683\n",
      "Training Epoch: 92 [32128/50000]\tLoss: 4.8050\tLR: 9.163939\n",
      "Training Epoch: 92 [32256/50000]\tLoss: 4.9155\tLR: 9.164194\n",
      "Training Epoch: 92 [32384/50000]\tLoss: 4.8333\tLR: 9.164450\n",
      "Training Epoch: 92 [32512/50000]\tLoss: 4.8022\tLR: 9.164706\n",
      "Training Epoch: 92 [32640/50000]\tLoss: 4.7722\tLR: 9.164962\n",
      "Training Epoch: 92 [32768/50000]\tLoss: 4.8075\tLR: 9.165217\n",
      "Training Epoch: 92 [32896/50000]\tLoss: 4.8190\tLR: 9.165473\n",
      "Training Epoch: 92 [33024/50000]\tLoss: 4.7819\tLR: 9.165729\n",
      "Training Epoch: 92 [33152/50000]\tLoss: 4.7637\tLR: 9.165985\n",
      "Training Epoch: 92 [33280/50000]\tLoss: 4.8383\tLR: 9.166240\n",
      "Training Epoch: 92 [33408/50000]\tLoss: 4.7885\tLR: 9.166496\n",
      "Training Epoch: 92 [33536/50000]\tLoss: 4.7327\tLR: 9.166752\n",
      "Training Epoch: 92 [33664/50000]\tLoss: 4.7482\tLR: 9.167008\n",
      "Training Epoch: 92 [33792/50000]\tLoss: 4.9037\tLR: 9.167263\n",
      "Training Epoch: 92 [33920/50000]\tLoss: 4.7418\tLR: 9.167519\n",
      "Training Epoch: 92 [34048/50000]\tLoss: 4.7690\tLR: 9.167775\n",
      "Training Epoch: 92 [34176/50000]\tLoss: 4.8021\tLR: 9.168031\n",
      "Training Epoch: 92 [34304/50000]\tLoss: 4.8351\tLR: 9.168286\n",
      "Training Epoch: 92 [34432/50000]\tLoss: 4.7496\tLR: 9.168542\n",
      "Training Epoch: 92 [34560/50000]\tLoss: 4.9081\tLR: 9.168798\n",
      "Training Epoch: 92 [34688/50000]\tLoss: 4.7916\tLR: 9.169054\n",
      "Training Epoch: 92 [34816/50000]\tLoss: 4.7870\tLR: 9.169309\n",
      "Training Epoch: 92 [34944/50000]\tLoss: 4.7707\tLR: 9.169565\n",
      "Training Epoch: 92 [35072/50000]\tLoss: 4.7188\tLR: 9.169821\n",
      "Training Epoch: 92 [35200/50000]\tLoss: 4.8880\tLR: 9.170077\n",
      "Training Epoch: 92 [35328/50000]\tLoss: 4.8042\tLR: 9.170332\n",
      "Training Epoch: 92 [35456/50000]\tLoss: 4.7221\tLR: 9.170588\n",
      "Training Epoch: 92 [35584/50000]\tLoss: 4.8309\tLR: 9.170844\n",
      "Training Epoch: 92 [35712/50000]\tLoss: 4.8672\tLR: 9.171100\n",
      "Training Epoch: 92 [35840/50000]\tLoss: 4.8603\tLR: 9.171355\n",
      "Training Epoch: 92 [35968/50000]\tLoss: 4.7836\tLR: 9.171611\n",
      "Training Epoch: 92 [36096/50000]\tLoss: 4.8678\tLR: 9.171867\n",
      "Training Epoch: 92 [36224/50000]\tLoss: 4.8091\tLR: 9.172123\n",
      "Training Epoch: 92 [36352/50000]\tLoss: 4.7435\tLR: 9.172379\n",
      "Training Epoch: 92 [36480/50000]\tLoss: 4.8565\tLR: 9.172634\n",
      "Training Epoch: 92 [36608/50000]\tLoss: 4.8165\tLR: 9.172890\n",
      "Training Epoch: 92 [36736/50000]\tLoss: 4.8269\tLR: 9.173146\n",
      "Training Epoch: 92 [36864/50000]\tLoss: 4.8363\tLR: 9.173402\n",
      "Training Epoch: 92 [36992/50000]\tLoss: 4.7880\tLR: 9.173657\n",
      "Training Epoch: 92 [37120/50000]\tLoss: 4.7227\tLR: 9.173913\n",
      "Training Epoch: 92 [37248/50000]\tLoss: 4.8111\tLR: 9.174169\n",
      "Training Epoch: 92 [37376/50000]\tLoss: 4.8404\tLR: 9.174425\n",
      "Training Epoch: 92 [37504/50000]\tLoss: 4.7950\tLR: 9.174680\n",
      "Training Epoch: 92 [37632/50000]\tLoss: 4.7983\tLR: 9.174936\n",
      "Training Epoch: 92 [37760/50000]\tLoss: 4.7067\tLR: 9.175192\n",
      "Training Epoch: 92 [37888/50000]\tLoss: 4.8299\tLR: 9.175448\n",
      "Training Epoch: 92 [38016/50000]\tLoss: 4.7531\tLR: 9.175703\n",
      "Training Epoch: 92 [38144/50000]\tLoss: 4.8208\tLR: 9.175959\n",
      "Training Epoch: 92 [38272/50000]\tLoss: 4.7799\tLR: 9.176215\n",
      "Training Epoch: 92 [38400/50000]\tLoss: 4.7936\tLR: 9.176471\n",
      "Training Epoch: 92 [38528/50000]\tLoss: 4.8092\tLR: 9.176726\n",
      "Training Epoch: 92 [38656/50000]\tLoss: 4.7503\tLR: 9.176982\n",
      "Training Epoch: 92 [38784/50000]\tLoss: 4.7212\tLR: 9.177238\n",
      "Training Epoch: 92 [38912/50000]\tLoss: 4.8513\tLR: 9.177494\n",
      "Training Epoch: 92 [39040/50000]\tLoss: 4.7924\tLR: 9.177749\n",
      "Training Epoch: 92 [39168/50000]\tLoss: 4.7662\tLR: 9.178005\n",
      "Training Epoch: 92 [39296/50000]\tLoss: 4.9740\tLR: 9.178261\n",
      "Training Epoch: 92 [39424/50000]\tLoss: 4.6369\tLR: 9.178517\n",
      "Training Epoch: 92 [39552/50000]\tLoss: 4.7360\tLR: 9.178772\n",
      "Training Epoch: 92 [39680/50000]\tLoss: 4.8698\tLR: 9.179028\n",
      "Training Epoch: 92 [39808/50000]\tLoss: 4.8155\tLR: 9.179284\n",
      "Training Epoch: 92 [39936/50000]\tLoss: 4.7254\tLR: 9.179540\n",
      "Training Epoch: 92 [40064/50000]\tLoss: 4.6831\tLR: 9.179795\n",
      "Training Epoch: 92 [40192/50000]\tLoss: 4.7049\tLR: 9.180051\n",
      "Training Epoch: 92 [40320/50000]\tLoss: 4.8585\tLR: 9.180307\n",
      "Training Epoch: 92 [40448/50000]\tLoss: 4.6985\tLR: 9.180563\n",
      "Training Epoch: 92 [40576/50000]\tLoss: 4.8310\tLR: 9.180818\n",
      "Training Epoch: 92 [40704/50000]\tLoss: 4.7062\tLR: 9.181074\n",
      "Training Epoch: 92 [40832/50000]\tLoss: 4.7789\tLR: 9.181330\n",
      "Training Epoch: 92 [40960/50000]\tLoss: 4.7688\tLR: 9.181586\n",
      "Training Epoch: 92 [41088/50000]\tLoss: 4.7696\tLR: 9.181841\n",
      "Training Epoch: 92 [41216/50000]\tLoss: 4.7441\tLR: 9.182097\n",
      "Training Epoch: 92 [41344/50000]\tLoss: 4.7439\tLR: 9.182353\n",
      "Training Epoch: 92 [41472/50000]\tLoss: 4.8254\tLR: 9.182609\n",
      "Training Epoch: 92 [41600/50000]\tLoss: 4.7471\tLR: 9.182864\n",
      "Training Epoch: 92 [41728/50000]\tLoss: 4.7489\tLR: 9.183120\n",
      "Training Epoch: 92 [41856/50000]\tLoss: 4.7958\tLR: 9.183376\n",
      "Training Epoch: 92 [41984/50000]\tLoss: 4.7777\tLR: 9.183632\n",
      "Training Epoch: 92 [42112/50000]\tLoss: 4.7997\tLR: 9.183887\n",
      "Training Epoch: 92 [42240/50000]\tLoss: 4.7971\tLR: 9.184143\n",
      "Training Epoch: 92 [42368/50000]\tLoss: 4.7594\tLR: 9.184399\n",
      "Training Epoch: 92 [42496/50000]\tLoss: 4.7002\tLR: 9.184655\n",
      "Training Epoch: 92 [42624/50000]\tLoss: 4.7343\tLR: 9.184910\n",
      "Training Epoch: 92 [42752/50000]\tLoss: 4.7311\tLR: 9.185166\n",
      "Training Epoch: 92 [42880/50000]\tLoss: 4.7260\tLR: 9.185422\n",
      "Training Epoch: 92 [43008/50000]\tLoss: 4.8123\tLR: 9.185678\n",
      "Training Epoch: 92 [43136/50000]\tLoss: 4.6903\tLR: 9.185934\n",
      "Training Epoch: 92 [43264/50000]\tLoss: 4.7259\tLR: 9.186189\n",
      "Training Epoch: 92 [43392/50000]\tLoss: 4.7609\tLR: 9.186445\n",
      "Training Epoch: 92 [43520/50000]\tLoss: 4.7205\tLR: 9.186701\n",
      "Training Epoch: 92 [43648/50000]\tLoss: 4.7698\tLR: 9.186957\n",
      "Training Epoch: 92 [43776/50000]\tLoss: 4.8314\tLR: 9.187212\n",
      "Training Epoch: 92 [43904/50000]\tLoss: 4.6495\tLR: 9.187468\n",
      "Training Epoch: 92 [44032/50000]\tLoss: 4.7373\tLR: 9.187724\n",
      "Training Epoch: 92 [44160/50000]\tLoss: 4.7436\tLR: 9.187980\n",
      "Training Epoch: 92 [44288/50000]\tLoss: 4.6818\tLR: 9.188235\n",
      "Training Epoch: 92 [44416/50000]\tLoss: 4.7894\tLR: 9.188491\n",
      "Training Epoch: 92 [44544/50000]\tLoss: 4.8148\tLR: 9.188747\n",
      "Training Epoch: 92 [44672/50000]\tLoss: 4.7300\tLR: 9.189003\n",
      "Training Epoch: 92 [44800/50000]\tLoss: 4.7373\tLR: 9.189258\n",
      "Training Epoch: 92 [44928/50000]\tLoss: 4.7762\tLR: 9.189514\n",
      "Training Epoch: 92 [45056/50000]\tLoss: 4.7393\tLR: 9.189770\n",
      "Training Epoch: 92 [45184/50000]\tLoss: 4.8465\tLR: 9.190026\n",
      "Training Epoch: 92 [45312/50000]\tLoss: 4.7095\tLR: 9.190281\n",
      "Training Epoch: 92 [45440/50000]\tLoss: 4.8195\tLR: 9.190537\n",
      "Training Epoch: 92 [45568/50000]\tLoss: 4.7256\tLR: 9.190793\n",
      "Training Epoch: 92 [45696/50000]\tLoss: 4.6656\tLR: 9.191049\n",
      "Training Epoch: 92 [45824/50000]\tLoss: 4.7819\tLR: 9.191304\n",
      "Training Epoch: 92 [45952/50000]\tLoss: 4.8120\tLR: 9.191560\n",
      "Training Epoch: 92 [46080/50000]\tLoss: 4.7866\tLR: 9.191816\n",
      "Training Epoch: 92 [46208/50000]\tLoss: 4.7523\tLR: 9.192072\n",
      "Training Epoch: 92 [46336/50000]\tLoss: 4.8373\tLR: 9.192327\n",
      "Training Epoch: 92 [46464/50000]\tLoss: 4.8726\tLR: 9.192583\n",
      "Training Epoch: 92 [46592/50000]\tLoss: 4.8259\tLR: 9.192839\n",
      "Training Epoch: 92 [46720/50000]\tLoss: 4.8403\tLR: 9.193095\n",
      "Training Epoch: 92 [46848/50000]\tLoss: 4.8765\tLR: 9.193350\n",
      "Training Epoch: 92 [46976/50000]\tLoss: 4.7353\tLR: 9.193606\n",
      "Training Epoch: 92 [47104/50000]\tLoss: 4.7107\tLR: 9.193862\n",
      "Training Epoch: 92 [47232/50000]\tLoss: 4.8112\tLR: 9.194118\n",
      "Training Epoch: 92 [47360/50000]\tLoss: 4.7734\tLR: 9.194373\n",
      "Training Epoch: 92 [47488/50000]\tLoss: 4.6883\tLR: 9.194629\n",
      "Training Epoch: 92 [47616/50000]\tLoss: 4.8404\tLR: 9.194885\n",
      "Training Epoch: 92 [47744/50000]\tLoss: 4.7030\tLR: 9.195141\n",
      "Training Epoch: 92 [47872/50000]\tLoss: 4.7124\tLR: 9.195396\n",
      "Training Epoch: 92 [48000/50000]\tLoss: 4.7108\tLR: 9.195652\n",
      "Training Epoch: 92 [48128/50000]\tLoss: 4.7286\tLR: 9.195908\n",
      "Training Epoch: 92 [48256/50000]\tLoss: 4.7854\tLR: 9.196164\n",
      "Training Epoch: 92 [48384/50000]\tLoss: 4.7612\tLR: 9.196419\n",
      "Training Epoch: 92 [48512/50000]\tLoss: 4.8338\tLR: 9.196675\n",
      "Training Epoch: 92 [48640/50000]\tLoss: 4.7445\tLR: 9.196931\n",
      "Training Epoch: 92 [48768/50000]\tLoss: 4.7591\tLR: 9.197187\n",
      "Training Epoch: 92 [48896/50000]\tLoss: 4.7968\tLR: 9.197442\n",
      "Training Epoch: 92 [49024/50000]\tLoss: 4.7303\tLR: 9.197698\n",
      "Training Epoch: 92 [49152/50000]\tLoss: 4.7486\tLR: 9.197954\n",
      "Training Epoch: 92 [49280/50000]\tLoss: 4.7174\tLR: 9.198210\n",
      "Training Epoch: 92 [49408/50000]\tLoss: 4.7695\tLR: 9.198465\n",
      "Training Epoch: 92 [49536/50000]\tLoss: 4.7059\tLR: 9.198721\n",
      "Training Epoch: 92 [49664/50000]\tLoss: 4.6953\tLR: 9.198977\n",
      "Training Epoch: 92 [49792/50000]\tLoss: 4.6846\tLR: 9.199233\n",
      "Training Epoch: 92 [49920/50000]\tLoss: 4.6334\tLR: 9.199488\n",
      "Training Epoch: 92 [50000/50000]\tLoss: 4.7343\tLR: 9.199744\n",
      "epoch 92 training time consumed: 489.14s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  128977 GB |  128977 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  128581 GB |  128581 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     395 GB |     395 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  128977 GB |  128977 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  128581 GB |  128581 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     395 GB |     395 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  127165 GB |  127165 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  126769 GB |  126769 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     395 GB |     395 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   13676 K  |   13675 K  |\n",
      "|       from large pool |      24    |      65    |    5830 K  |    5829 K  |\n",
      "|       from small pool |     231    |     274    |    7846 K  |    7845 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   13676 K  |   13675 K  |\n",
      "|       from large pool |      24    |      65    |    5830 K  |    5829 K  |\n",
      "|       from small pool |     231    |     274    |    7846 K  |    7845 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      47    |    7927 K  |    7927 K  |\n",
      "|       from large pool |      10    |      23    |    2802 K  |    2802 K  |\n",
      "|       from small pool |      27    |      35    |    5125 K  |    5125 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 92, Average loss: 0.0379, Accuracy: 0.0100, Time consumed:31.33s\n",
      "\n",
      "Training Epoch: 93 [128/50000]\tLoss: 4.8373\tLR: 0.020000\n",
      "Training Epoch: 93 [256/50000]\tLoss: 4.7668\tLR: 9.200256\n",
      "Training Epoch: 93 [384/50000]\tLoss: 4.7548\tLR: 9.200512\n",
      "Training Epoch: 93 [512/50000]\tLoss: 4.9203\tLR: 9.200767\n",
      "Training Epoch: 93 [640/50000]\tLoss: 4.7268\tLR: 9.201023\n",
      "Training Epoch: 93 [768/50000]\tLoss: 4.6357\tLR: 9.201279\n",
      "Training Epoch: 93 [896/50000]\tLoss: 4.7161\tLR: 9.201535\n",
      "Training Epoch: 93 [1024/50000]\tLoss: 4.7793\tLR: 9.201790\n",
      "Training Epoch: 93 [1152/50000]\tLoss: 4.8833\tLR: 9.202046\n",
      "Training Epoch: 93 [1280/50000]\tLoss: 4.7888\tLR: 9.202302\n",
      "Training Epoch: 93 [1408/50000]\tLoss: 4.8405\tLR: 9.202558\n",
      "Training Epoch: 93 [1536/50000]\tLoss: 4.8580\tLR: 9.202813\n",
      "Training Epoch: 93 [1664/50000]\tLoss: 4.8179\tLR: 9.203069\n",
      "Training Epoch: 93 [1792/50000]\tLoss: 4.7248\tLR: 9.203325\n",
      "Training Epoch: 93 [1920/50000]\tLoss: 4.7918\tLR: 9.203581\n",
      "Training Epoch: 93 [2048/50000]\tLoss: 4.8297\tLR: 9.203836\n",
      "Training Epoch: 93 [2176/50000]\tLoss: 4.8491\tLR: 9.204092\n",
      "Training Epoch: 93 [2304/50000]\tLoss: 4.8286\tLR: 9.204348\n",
      "Training Epoch: 93 [2432/50000]\tLoss: 4.7736\tLR: 9.204604\n",
      "Training Epoch: 93 [2560/50000]\tLoss: 4.6755\tLR: 9.204859\n",
      "Training Epoch: 93 [2688/50000]\tLoss: 4.7888\tLR: 9.205115\n",
      "Training Epoch: 93 [2816/50000]\tLoss: 4.7404\tLR: 9.205371\n",
      "Training Epoch: 93 [2944/50000]\tLoss: 4.7488\tLR: 9.205627\n",
      "Training Epoch: 93 [3072/50000]\tLoss: 4.9621\tLR: 9.205882\n",
      "Training Epoch: 93 [3200/50000]\tLoss: 4.8193\tLR: 9.206138\n",
      "Training Epoch: 93 [3328/50000]\tLoss: 4.9025\tLR: 9.206394\n",
      "Training Epoch: 93 [3456/50000]\tLoss: 4.7367\tLR: 9.206650\n",
      "Training Epoch: 93 [3584/50000]\tLoss: 4.7771\tLR: 9.206905\n",
      "Training Epoch: 93 [3712/50000]\tLoss: 4.8200\tLR: 9.207161\n",
      "Training Epoch: 93 [3840/50000]\tLoss: 4.9218\tLR: 9.207417\n",
      "Training Epoch: 93 [3968/50000]\tLoss: 4.8557\tLR: 9.207673\n",
      "Training Epoch: 93 [4096/50000]\tLoss: 4.7798\tLR: 9.207928\n",
      "Training Epoch: 93 [4224/50000]\tLoss: 4.6854\tLR: 9.208184\n",
      "Training Epoch: 93 [4352/50000]\tLoss: 4.7967\tLR: 9.208440\n",
      "Training Epoch: 93 [4480/50000]\tLoss: 4.7898\tLR: 9.208696\n",
      "Training Epoch: 93 [4608/50000]\tLoss: 4.7553\tLR: 9.208951\n",
      "Training Epoch: 93 [4736/50000]\tLoss: 4.7272\tLR: 9.209207\n",
      "Training Epoch: 93 [4864/50000]\tLoss: 4.7846\tLR: 9.209463\n",
      "Training Epoch: 93 [4992/50000]\tLoss: 4.8516\tLR: 9.209719\n",
      "Training Epoch: 93 [5120/50000]\tLoss: 4.8668\tLR: 9.209974\n",
      "Training Epoch: 93 [5248/50000]\tLoss: 4.8468\tLR: 9.210230\n",
      "Training Epoch: 93 [5376/50000]\tLoss: 4.8217\tLR: 9.210486\n",
      "Training Epoch: 93 [5504/50000]\tLoss: 4.7078\tLR: 9.210742\n",
      "Training Epoch: 93 [5632/50000]\tLoss: 4.7865\tLR: 9.210997\n",
      "Training Epoch: 93 [5760/50000]\tLoss: 4.7602\tLR: 9.211253\n",
      "Training Epoch: 93 [5888/50000]\tLoss: 4.8105\tLR: 9.211509\n",
      "Training Epoch: 93 [6016/50000]\tLoss: 4.7293\tLR: 9.211765\n",
      "Training Epoch: 93 [6144/50000]\tLoss: 4.7359\tLR: 9.212020\n",
      "Training Epoch: 93 [6272/50000]\tLoss: 4.7723\tLR: 9.212276\n",
      "Training Epoch: 93 [6400/50000]\tLoss: 4.8846\tLR: 9.212532\n",
      "Training Epoch: 93 [6528/50000]\tLoss: 4.8309\tLR: 9.212788\n",
      "Training Epoch: 93 [6656/50000]\tLoss: 4.7464\tLR: 9.213043\n",
      "Training Epoch: 93 [6784/50000]\tLoss: 4.7552\tLR: 9.213299\n",
      "Training Epoch: 93 [6912/50000]\tLoss: 4.9141\tLR: 9.213555\n",
      "Training Epoch: 93 [7040/50000]\tLoss: 4.8656\tLR: 9.213811\n",
      "Training Epoch: 93 [7168/50000]\tLoss: 4.8367\tLR: 9.214066\n",
      "Training Epoch: 93 [7296/50000]\tLoss: 4.7397\tLR: 9.214322\n",
      "Training Epoch: 93 [7424/50000]\tLoss: 4.7925\tLR: 9.214578\n",
      "Training Epoch: 93 [7552/50000]\tLoss: 4.7172\tLR: 9.214834\n",
      "Training Epoch: 93 [7680/50000]\tLoss: 4.8033\tLR: 9.215090\n",
      "Training Epoch: 93 [7808/50000]\tLoss: 4.8058\tLR: 9.215345\n",
      "Training Epoch: 93 [7936/50000]\tLoss: 4.8060\tLR: 9.215601\n",
      "Training Epoch: 93 [8064/50000]\tLoss: 4.8295\tLR: 9.215857\n",
      "Training Epoch: 93 [8192/50000]\tLoss: 4.9336\tLR: 9.216113\n",
      "Training Epoch: 93 [8320/50000]\tLoss: 4.9368\tLR: 9.216368\n",
      "Training Epoch: 93 [8448/50000]\tLoss: 4.8034\tLR: 9.216624\n",
      "Training Epoch: 93 [8576/50000]\tLoss: 4.9044\tLR: 9.216880\n",
      "Training Epoch: 93 [8704/50000]\tLoss: 4.7027\tLR: 9.217136\n",
      "Training Epoch: 93 [8832/50000]\tLoss: 4.7297\tLR: 9.217391\n",
      "Training Epoch: 93 [8960/50000]\tLoss: 4.7283\tLR: 9.217647\n",
      "Training Epoch: 93 [9088/50000]\tLoss: 4.8856\tLR: 9.217903\n",
      "Training Epoch: 93 [9216/50000]\tLoss: 4.7635\tLR: 9.218159\n",
      "Training Epoch: 93 [9344/50000]\tLoss: 4.7429\tLR: 9.218414\n",
      "Training Epoch: 93 [9472/50000]\tLoss: 4.7413\tLR: 9.218670\n",
      "Training Epoch: 93 [9600/50000]\tLoss: 4.7522\tLR: 9.218926\n",
      "Training Epoch: 93 [9728/50000]\tLoss: 4.7484\tLR: 9.219182\n",
      "Training Epoch: 93 [9856/50000]\tLoss: 4.8357\tLR: 9.219437\n",
      "Training Epoch: 93 [9984/50000]\tLoss: 4.7704\tLR: 9.219693\n",
      "Training Epoch: 93 [10112/50000]\tLoss: 4.8496\tLR: 9.219949\n",
      "Training Epoch: 93 [10240/50000]\tLoss: 4.7515\tLR: 9.220205\n",
      "Training Epoch: 93 [10368/50000]\tLoss: 4.7516\tLR: 9.220460\n",
      "Training Epoch: 93 [10496/50000]\tLoss: 4.7628\tLR: 9.220716\n",
      "Training Epoch: 93 [10624/50000]\tLoss: 4.7880\tLR: 9.220972\n",
      "Training Epoch: 93 [10752/50000]\tLoss: 4.7620\tLR: 9.221228\n",
      "Training Epoch: 93 [10880/50000]\tLoss: 4.8665\tLR: 9.221483\n",
      "Training Epoch: 93 [11008/50000]\tLoss: 4.8147\tLR: 9.221739\n",
      "Training Epoch: 93 [11136/50000]\tLoss: 4.7961\tLR: 9.221995\n",
      "Training Epoch: 93 [11264/50000]\tLoss: 4.7233\tLR: 9.222251\n",
      "Training Epoch: 93 [11392/50000]\tLoss: 4.9181\tLR: 9.222506\n",
      "Training Epoch: 93 [11520/50000]\tLoss: 4.7572\tLR: 9.222762\n",
      "Training Epoch: 93 [11648/50000]\tLoss: 4.7317\tLR: 9.223018\n",
      "Training Epoch: 93 [11776/50000]\tLoss: 4.8153\tLR: 9.223274\n",
      "Training Epoch: 93 [11904/50000]\tLoss: 4.7779\tLR: 9.223529\n",
      "Training Epoch: 93 [12032/50000]\tLoss: 4.6826\tLR: 9.223785\n",
      "Training Epoch: 93 [12160/50000]\tLoss: 4.9127\tLR: 9.224041\n",
      "Training Epoch: 93 [12288/50000]\tLoss: 4.7856\tLR: 9.224297\n",
      "Training Epoch: 93 [12416/50000]\tLoss: 4.7990\tLR: 9.224552\n",
      "Training Epoch: 93 [12544/50000]\tLoss: 4.7496\tLR: 9.224808\n",
      "Training Epoch: 93 [12672/50000]\tLoss: 4.8123\tLR: 9.225064\n",
      "Training Epoch: 93 [12800/50000]\tLoss: 4.8261\tLR: 9.225320\n",
      "Training Epoch: 93 [12928/50000]\tLoss: 4.8124\tLR: 9.225575\n",
      "Training Epoch: 93 [13056/50000]\tLoss: 4.8071\tLR: 9.225831\n",
      "Training Epoch: 93 [13184/50000]\tLoss: 4.7472\tLR: 9.226087\n",
      "Training Epoch: 93 [13312/50000]\tLoss: 4.7873\tLR: 9.226343\n",
      "Training Epoch: 93 [13440/50000]\tLoss: 4.8033\tLR: 9.226598\n",
      "Training Epoch: 93 [13568/50000]\tLoss: 4.8012\tLR: 9.226854\n",
      "Training Epoch: 93 [13696/50000]\tLoss: 4.9017\tLR: 9.227110\n",
      "Training Epoch: 93 [13824/50000]\tLoss: 4.6912\tLR: 9.227366\n",
      "Training Epoch: 93 [13952/50000]\tLoss: 4.8004\tLR: 9.227621\n",
      "Training Epoch: 93 [14080/50000]\tLoss: 4.8388\tLR: 9.227877\n",
      "Training Epoch: 93 [14208/50000]\tLoss: 4.6655\tLR: 9.228133\n",
      "Training Epoch: 93 [14336/50000]\tLoss: 4.7788\tLR: 9.228389\n",
      "Training Epoch: 93 [14464/50000]\tLoss: 4.7163\tLR: 9.228645\n",
      "Training Epoch: 93 [14592/50000]\tLoss: 4.7013\tLR: 9.228900\n",
      "Training Epoch: 93 [14720/50000]\tLoss: 4.6924\tLR: 9.229156\n",
      "Training Epoch: 93 [14848/50000]\tLoss: 4.7966\tLR: 9.229412\n",
      "Training Epoch: 93 [14976/50000]\tLoss: 4.7729\tLR: 9.229668\n",
      "Training Epoch: 93 [15104/50000]\tLoss: 4.8425\tLR: 9.229923\n",
      "Training Epoch: 93 [15232/50000]\tLoss: 4.7289\tLR: 9.230179\n",
      "Training Epoch: 93 [15360/50000]\tLoss: 4.6988\tLR: 9.230435\n",
      "Training Epoch: 93 [15488/50000]\tLoss: 4.7961\tLR: 9.230691\n",
      "Training Epoch: 93 [15616/50000]\tLoss: 4.7019\tLR: 9.230946\n",
      "Training Epoch: 93 [15744/50000]\tLoss: 4.7837\tLR: 9.231202\n",
      "Training Epoch: 93 [15872/50000]\tLoss: 4.8088\tLR: 9.231458\n",
      "Training Epoch: 93 [16000/50000]\tLoss: 4.8395\tLR: 9.231714\n",
      "Training Epoch: 93 [16128/50000]\tLoss: 4.7742\tLR: 9.231969\n",
      "Training Epoch: 93 [16256/50000]\tLoss: 4.7733\tLR: 9.232225\n",
      "Training Epoch: 93 [16384/50000]\tLoss: 4.7103\tLR: 9.232481\n",
      "Training Epoch: 93 [16512/50000]\tLoss: 4.7686\tLR: 9.232737\n",
      "Training Epoch: 93 [16640/50000]\tLoss: 4.7419\tLR: 9.232992\n",
      "Training Epoch: 93 [16768/50000]\tLoss: 4.7726\tLR: 9.233248\n",
      "Training Epoch: 93 [16896/50000]\tLoss: 4.8251\tLR: 9.233504\n",
      "Training Epoch: 93 [17024/50000]\tLoss: 4.8259\tLR: 9.233760\n",
      "Training Epoch: 93 [17152/50000]\tLoss: 4.7565\tLR: 9.234015\n",
      "Training Epoch: 93 [17280/50000]\tLoss: 4.8089\tLR: 9.234271\n",
      "Training Epoch: 93 [17408/50000]\tLoss: 4.8285\tLR: 9.234527\n",
      "Training Epoch: 93 [17536/50000]\tLoss: 4.7240\tLR: 9.234783\n",
      "Training Epoch: 93 [17664/50000]\tLoss: 4.8219\tLR: 9.235038\n",
      "Training Epoch: 93 [17792/50000]\tLoss: 4.8380\tLR: 9.235294\n",
      "Training Epoch: 93 [17920/50000]\tLoss: 4.7569\tLR: 9.235550\n",
      "Training Epoch: 93 [18048/50000]\tLoss: 4.7862\tLR: 9.235806\n",
      "Training Epoch: 93 [18176/50000]\tLoss: 4.6656\tLR: 9.236061\n",
      "Training Epoch: 93 [18304/50000]\tLoss: 4.8872\tLR: 9.236317\n",
      "Training Epoch: 93 [18432/50000]\tLoss: 4.8172\tLR: 9.236573\n",
      "Training Epoch: 93 [18560/50000]\tLoss: 4.7127\tLR: 9.236829\n",
      "Training Epoch: 93 [18688/50000]\tLoss: 4.7928\tLR: 9.237084\n",
      "Training Epoch: 93 [18816/50000]\tLoss: 4.7715\tLR: 9.237340\n",
      "Training Epoch: 93 [18944/50000]\tLoss: 4.7021\tLR: 9.237596\n",
      "Training Epoch: 93 [19072/50000]\tLoss: 4.6981\tLR: 9.237852\n",
      "Training Epoch: 93 [19200/50000]\tLoss: 4.7804\tLR: 9.238107\n",
      "Training Epoch: 93 [19328/50000]\tLoss: 4.6911\tLR: 9.238363\n",
      "Training Epoch: 93 [19456/50000]\tLoss: 4.7772\tLR: 9.238619\n",
      "Training Epoch: 93 [19584/50000]\tLoss: 4.7058\tLR: 9.238875\n",
      "Training Epoch: 93 [19712/50000]\tLoss: 4.6878\tLR: 9.239130\n",
      "Training Epoch: 93 [19840/50000]\tLoss: 4.7192\tLR: 9.239386\n",
      "Training Epoch: 93 [19968/50000]\tLoss: 4.7298\tLR: 9.239642\n",
      "Training Epoch: 93 [20096/50000]\tLoss: 4.7746\tLR: 9.239898\n",
      "Training Epoch: 93 [20224/50000]\tLoss: 4.6917\tLR: 9.240153\n",
      "Training Epoch: 93 [20352/50000]\tLoss: 4.7818\tLR: 9.240409\n",
      "Training Epoch: 93 [20480/50000]\tLoss: 4.7213\tLR: 9.240665\n",
      "Training Epoch: 93 [20608/50000]\tLoss: 4.7213\tLR: 9.240921\n",
      "Training Epoch: 93 [20736/50000]\tLoss: 4.7855\tLR: 9.241176\n",
      "Training Epoch: 93 [20864/50000]\tLoss: 4.8735\tLR: 9.241432\n",
      "Training Epoch: 93 [20992/50000]\tLoss: 4.8504\tLR: 9.241688\n",
      "Training Epoch: 93 [21120/50000]\tLoss: 4.8370\tLR: 9.241944\n",
      "Training Epoch: 93 [21248/50000]\tLoss: 4.7212\tLR: 9.242199\n",
      "Training Epoch: 93 [21376/50000]\tLoss: 4.6987\tLR: 9.242455\n",
      "Training Epoch: 93 [21504/50000]\tLoss: 4.7175\tLR: 9.242711\n",
      "Training Epoch: 93 [21632/50000]\tLoss: 4.7235\tLR: 9.242967\n",
      "Training Epoch: 93 [21760/50000]\tLoss: 4.8930\tLR: 9.243223\n",
      "Training Epoch: 93 [21888/50000]\tLoss: 4.8543\tLR: 9.243478\n",
      "Training Epoch: 93 [22016/50000]\tLoss: 4.8354\tLR: 9.243734\n",
      "Training Epoch: 93 [22144/50000]\tLoss: 4.8157\tLR: 9.243990\n",
      "Training Epoch: 93 [22272/50000]\tLoss: 4.7310\tLR: 9.244246\n",
      "Training Epoch: 93 [22400/50000]\tLoss: 4.7187\tLR: 9.244501\n",
      "Training Epoch: 93 [22528/50000]\tLoss: 4.7919\tLR: 9.244757\n",
      "Training Epoch: 93 [22656/50000]\tLoss: 4.7542\tLR: 9.245013\n",
      "Training Epoch: 93 [22784/50000]\tLoss: 4.6910\tLR: 9.245269\n",
      "Training Epoch: 93 [22912/50000]\tLoss: 4.8919\tLR: 9.245524\n",
      "Training Epoch: 93 [23040/50000]\tLoss: 4.8490\tLR: 9.245780\n",
      "Training Epoch: 93 [23168/50000]\tLoss: 4.8541\tLR: 9.246036\n",
      "Training Epoch: 93 [23296/50000]\tLoss: 4.8536\tLR: 9.246292\n",
      "Training Epoch: 93 [23424/50000]\tLoss: 4.8745\tLR: 9.246547\n",
      "Training Epoch: 93 [23552/50000]\tLoss: 4.6782\tLR: 9.246803\n",
      "Training Epoch: 93 [23680/50000]\tLoss: 4.6527\tLR: 9.247059\n",
      "Training Epoch: 93 [23808/50000]\tLoss: 4.6642\tLR: 9.247315\n",
      "Training Epoch: 93 [23936/50000]\tLoss: 4.7567\tLR: 9.247570\n",
      "Training Epoch: 93 [24064/50000]\tLoss: 4.8296\tLR: 9.247826\n",
      "Training Epoch: 93 [24192/50000]\tLoss: 4.7393\tLR: 9.248082\n",
      "Training Epoch: 93 [24320/50000]\tLoss: 4.7863\tLR: 9.248338\n",
      "Training Epoch: 93 [24448/50000]\tLoss: 4.7335\tLR: 9.248593\n",
      "Training Epoch: 93 [24576/50000]\tLoss: 4.8251\tLR: 9.248849\n",
      "Training Epoch: 93 [24704/50000]\tLoss: 4.7946\tLR: 9.249105\n",
      "Training Epoch: 93 [24832/50000]\tLoss: 4.7541\tLR: 9.249361\n",
      "Training Epoch: 93 [24960/50000]\tLoss: 4.8924\tLR: 9.249616\n",
      "Training Epoch: 93 [25088/50000]\tLoss: 4.9179\tLR: 9.249872\n",
      "Training Epoch: 93 [25216/50000]\tLoss: 4.7735\tLR: 9.250128\n",
      "Training Epoch: 93 [25344/50000]\tLoss: 4.8143\tLR: 9.250384\n",
      "Training Epoch: 93 [25472/50000]\tLoss: 4.7492\tLR: 9.250639\n",
      "Training Epoch: 93 [25600/50000]\tLoss: 4.7974\tLR: 9.250895\n",
      "Training Epoch: 93 [25728/50000]\tLoss: 4.8175\tLR: 9.251151\n",
      "Training Epoch: 93 [25856/50000]\tLoss: 4.8613\tLR: 9.251407\n",
      "Training Epoch: 93 [25984/50000]\tLoss: 4.8046\tLR: 9.251662\n",
      "Training Epoch: 93 [26112/50000]\tLoss: 5.0035\tLR: 9.251918\n",
      "Training Epoch: 93 [26240/50000]\tLoss: 4.8037\tLR: 9.252174\n",
      "Training Epoch: 93 [26368/50000]\tLoss: 4.7397\tLR: 9.252430\n",
      "Training Epoch: 93 [26496/50000]\tLoss: 4.8682\tLR: 9.252685\n",
      "Training Epoch: 93 [26624/50000]\tLoss: 4.7896\tLR: 9.252941\n",
      "Training Epoch: 93 [26752/50000]\tLoss: 4.9334\tLR: 9.253197\n",
      "Training Epoch: 93 [26880/50000]\tLoss: 4.8471\tLR: 9.253453\n",
      "Training Epoch: 93 [27008/50000]\tLoss: 4.8657\tLR: 9.253708\n",
      "Training Epoch: 93 [27136/50000]\tLoss: 4.7619\tLR: 9.253964\n",
      "Training Epoch: 93 [27264/50000]\tLoss: 4.7184\tLR: 9.254220\n",
      "Training Epoch: 93 [27392/50000]\tLoss: 4.8020\tLR: 9.254476\n",
      "Training Epoch: 93 [27520/50000]\tLoss: 4.8234\tLR: 9.254731\n",
      "Training Epoch: 93 [27648/50000]\tLoss: 4.8833\tLR: 9.254987\n",
      "Training Epoch: 93 [27776/50000]\tLoss: 4.8410\tLR: 9.255243\n",
      "Training Epoch: 93 [27904/50000]\tLoss: 4.7351\tLR: 9.255499\n",
      "Training Epoch: 93 [28032/50000]\tLoss: 4.7253\tLR: 9.255754\n",
      "Training Epoch: 93 [28160/50000]\tLoss: 4.7191\tLR: 9.256010\n",
      "Training Epoch: 93 [28288/50000]\tLoss: 4.6654\tLR: 9.256266\n",
      "Training Epoch: 93 [28416/50000]\tLoss: 4.7709\tLR: 9.256522\n",
      "Training Epoch: 93 [28544/50000]\tLoss: 4.7789\tLR: 9.256777\n",
      "Training Epoch: 93 [28672/50000]\tLoss: 4.7549\tLR: 9.257033\n",
      "Training Epoch: 93 [28800/50000]\tLoss: 4.7884\tLR: 9.257289\n",
      "Training Epoch: 93 [28928/50000]\tLoss: 4.7691\tLR: 9.257545\n",
      "Training Epoch: 93 [29056/50000]\tLoss: 4.7453\tLR: 9.257801\n",
      "Training Epoch: 93 [29184/50000]\tLoss: 4.6890\tLR: 9.258056\n",
      "Training Epoch: 93 [29312/50000]\tLoss: 4.7286\tLR: 9.258312\n",
      "Training Epoch: 93 [29440/50000]\tLoss: 4.7189\tLR: 9.258568\n",
      "Training Epoch: 93 [29568/50000]\tLoss: 4.7705\tLR: 9.258824\n",
      "Training Epoch: 93 [29696/50000]\tLoss: 4.7235\tLR: 9.259079\n",
      "Training Epoch: 93 [29824/50000]\tLoss: 4.8591\tLR: 9.259335\n",
      "Training Epoch: 93 [29952/50000]\tLoss: 4.7669\tLR: 9.259591\n",
      "Training Epoch: 93 [30080/50000]\tLoss: 4.7667\tLR: 9.259847\n",
      "Training Epoch: 93 [30208/50000]\tLoss: 4.7624\tLR: 9.260102\n",
      "Training Epoch: 93 [30336/50000]\tLoss: 4.7295\tLR: 9.260358\n",
      "Training Epoch: 93 [30464/50000]\tLoss: 4.8072\tLR: 9.260614\n",
      "Training Epoch: 93 [30592/50000]\tLoss: 4.8010\tLR: 9.260870\n",
      "Training Epoch: 93 [30720/50000]\tLoss: 4.8053\tLR: 9.261125\n",
      "Training Epoch: 93 [30848/50000]\tLoss: 4.7906\tLR: 9.261381\n",
      "Training Epoch: 93 [30976/50000]\tLoss: 4.7270\tLR: 9.261637\n",
      "Training Epoch: 93 [31104/50000]\tLoss: 4.7846\tLR: 9.261893\n",
      "Training Epoch: 93 [31232/50000]\tLoss: 4.7534\tLR: 9.262148\n",
      "Training Epoch: 93 [31360/50000]\tLoss: 4.7382\tLR: 9.262404\n",
      "Training Epoch: 93 [31488/50000]\tLoss: 4.7962\tLR: 9.262660\n",
      "Training Epoch: 93 [31616/50000]\tLoss: 4.8714\tLR: 9.262916\n",
      "Training Epoch: 93 [31744/50000]\tLoss: 4.8194\tLR: 9.263171\n",
      "Training Epoch: 93 [31872/50000]\tLoss: 4.8230\tLR: 9.263427\n",
      "Training Epoch: 93 [32000/50000]\tLoss: 4.8083\tLR: 9.263683\n",
      "Training Epoch: 93 [32128/50000]\tLoss: 4.7898\tLR: 9.263939\n",
      "Training Epoch: 93 [32256/50000]\tLoss: 4.7396\tLR: 9.264194\n",
      "Training Epoch: 93 [32384/50000]\tLoss: 4.6673\tLR: 9.264450\n",
      "Training Epoch: 93 [32512/50000]\tLoss: 4.8530\tLR: 9.264706\n",
      "Training Epoch: 93 [32640/50000]\tLoss: 4.6985\tLR: 9.264962\n",
      "Training Epoch: 93 [32768/50000]\tLoss: 4.8683\tLR: 9.265217\n",
      "Training Epoch: 93 [32896/50000]\tLoss: 4.8314\tLR: 9.265473\n",
      "Training Epoch: 93 [33024/50000]\tLoss: 4.8614\tLR: 9.265729\n",
      "Training Epoch: 93 [33152/50000]\tLoss: 4.8440\tLR: 9.265985\n",
      "Training Epoch: 93 [33280/50000]\tLoss: 4.7215\tLR: 9.266240\n",
      "Training Epoch: 93 [33408/50000]\tLoss: 4.7750\tLR: 9.266496\n",
      "Training Epoch: 93 [33536/50000]\tLoss: 4.8558\tLR: 9.266752\n",
      "Training Epoch: 93 [33664/50000]\tLoss: 4.8527\tLR: 9.267008\n",
      "Training Epoch: 93 [33792/50000]\tLoss: 4.7701\tLR: 9.267263\n",
      "Training Epoch: 93 [33920/50000]\tLoss: 4.7765\tLR: 9.267519\n",
      "Training Epoch: 93 [34048/50000]\tLoss: 4.7133\tLR: 9.267775\n",
      "Training Epoch: 93 [34176/50000]\tLoss: 4.7289\tLR: 9.268031\n",
      "Training Epoch: 93 [34304/50000]\tLoss: 4.6216\tLR: 9.268286\n",
      "Training Epoch: 93 [34432/50000]\tLoss: 4.7873\tLR: 9.268542\n",
      "Training Epoch: 93 [34560/50000]\tLoss: 4.7884\tLR: 9.268798\n",
      "Training Epoch: 93 [34688/50000]\tLoss: 4.8007\tLR: 9.269054\n",
      "Training Epoch: 93 [34816/50000]\tLoss: 4.7788\tLR: 9.269309\n",
      "Training Epoch: 93 [34944/50000]\tLoss: 4.8042\tLR: 9.269565\n",
      "Training Epoch: 93 [35072/50000]\tLoss: 4.7955\tLR: 9.269821\n",
      "Training Epoch: 93 [35200/50000]\tLoss: 4.7854\tLR: 9.270077\n",
      "Training Epoch: 93 [35328/50000]\tLoss: 4.7314\tLR: 9.270332\n",
      "Training Epoch: 93 [35456/50000]\tLoss: 4.7347\tLR: 9.270588\n",
      "Training Epoch: 93 [35584/50000]\tLoss: 4.7747\tLR: 9.270844\n",
      "Training Epoch: 93 [35712/50000]\tLoss: 4.7463\tLR: 9.271100\n",
      "Training Epoch: 93 [35840/50000]\tLoss: 4.6780\tLR: 9.271355\n",
      "Training Epoch: 93 [35968/50000]\tLoss: 4.7554\tLR: 9.271611\n",
      "Training Epoch: 93 [36096/50000]\tLoss: 4.7752\tLR: 9.271867\n",
      "Training Epoch: 93 [36224/50000]\tLoss: 4.8092\tLR: 9.272123\n",
      "Training Epoch: 93 [36352/50000]\tLoss: 4.6965\tLR: 9.272379\n",
      "Training Epoch: 93 [36480/50000]\tLoss: 4.7516\tLR: 9.272634\n",
      "Training Epoch: 93 [36608/50000]\tLoss: 4.6949\tLR: 9.272890\n",
      "Training Epoch: 93 [36736/50000]\tLoss: 4.7081\tLR: 9.273146\n",
      "Training Epoch: 93 [36864/50000]\tLoss: 4.7332\tLR: 9.273402\n",
      "Training Epoch: 93 [36992/50000]\tLoss: 4.8040\tLR: 9.273657\n",
      "Training Epoch: 93 [37120/50000]\tLoss: 4.6925\tLR: 9.273913\n",
      "Training Epoch: 93 [37248/50000]\tLoss: 4.7776\tLR: 9.274169\n",
      "Training Epoch: 93 [37376/50000]\tLoss: 4.8088\tLR: 9.274425\n",
      "Training Epoch: 93 [37504/50000]\tLoss: 4.7785\tLR: 9.274680\n",
      "Training Epoch: 93 [37632/50000]\tLoss: 4.6695\tLR: 9.274936\n",
      "Training Epoch: 93 [37760/50000]\tLoss: 4.7908\tLR: 9.275192\n",
      "Training Epoch: 93 [37888/50000]\tLoss: 4.9497\tLR: 9.275448\n",
      "Training Epoch: 93 [38016/50000]\tLoss: 4.8111\tLR: 9.275703\n",
      "Training Epoch: 93 [38144/50000]\tLoss: 4.6616\tLR: 9.275959\n",
      "Training Epoch: 93 [38272/50000]\tLoss: 4.7265\tLR: 9.276215\n",
      "Training Epoch: 93 [38400/50000]\tLoss: 4.6565\tLR: 9.276471\n",
      "Training Epoch: 93 [38528/50000]\tLoss: 4.7364\tLR: 9.276726\n",
      "Training Epoch: 93 [38656/50000]\tLoss: 4.7092\tLR: 9.276982\n",
      "Training Epoch: 93 [38784/50000]\tLoss: 4.7814\tLR: 9.277238\n",
      "Training Epoch: 93 [38912/50000]\tLoss: 4.7275\tLR: 9.277494\n",
      "Training Epoch: 93 [39040/50000]\tLoss: 4.7541\tLR: 9.277749\n",
      "Training Epoch: 93 [39168/50000]\tLoss: 4.7518\tLR: 9.278005\n",
      "Training Epoch: 93 [39296/50000]\tLoss: 4.7441\tLR: 9.278261\n",
      "Training Epoch: 93 [39424/50000]\tLoss: 4.8464\tLR: 9.278517\n",
      "Training Epoch: 93 [39552/50000]\tLoss: 4.7621\tLR: 9.278772\n",
      "Training Epoch: 93 [39680/50000]\tLoss: 4.8054\tLR: 9.279028\n",
      "Training Epoch: 93 [39808/50000]\tLoss: 4.7365\tLR: 9.279284\n",
      "Training Epoch: 93 [39936/50000]\tLoss: 4.7594\tLR: 9.279540\n",
      "Training Epoch: 93 [40064/50000]\tLoss: 4.7161\tLR: 9.279795\n",
      "Training Epoch: 93 [40192/50000]\tLoss: 4.8558\tLR: 9.280051\n",
      "Training Epoch: 93 [40320/50000]\tLoss: 4.8234\tLR: 9.280307\n",
      "Training Epoch: 93 [40448/50000]\tLoss: 4.8653\tLR: 9.280563\n",
      "Training Epoch: 93 [40576/50000]\tLoss: 4.7551\tLR: 9.280818\n",
      "Training Epoch: 93 [40704/50000]\tLoss: 4.7032\tLR: 9.281074\n",
      "Training Epoch: 93 [40832/50000]\tLoss: 4.6944\tLR: 9.281330\n",
      "Training Epoch: 93 [40960/50000]\tLoss: 4.7530\tLR: 9.281586\n",
      "Training Epoch: 93 [41088/50000]\tLoss: 4.8018\tLR: 9.281841\n",
      "Training Epoch: 93 [41216/50000]\tLoss: 4.7816\tLR: 9.282097\n",
      "Training Epoch: 93 [41344/50000]\tLoss: 4.8358\tLR: 9.282353\n",
      "Training Epoch: 93 [41472/50000]\tLoss: 4.7860\tLR: 9.282609\n",
      "Training Epoch: 93 [41600/50000]\tLoss: 4.7461\tLR: 9.282864\n",
      "Training Epoch: 93 [41728/50000]\tLoss: 4.8010\tLR: 9.283120\n",
      "Training Epoch: 93 [41856/50000]\tLoss: 4.7777\tLR: 9.283376\n",
      "Training Epoch: 93 [41984/50000]\tLoss: 4.8093\tLR: 9.283632\n",
      "Training Epoch: 93 [42112/50000]\tLoss: 4.7452\tLR: 9.283887\n",
      "Training Epoch: 93 [42240/50000]\tLoss: 4.8408\tLR: 9.284143\n",
      "Training Epoch: 93 [42368/50000]\tLoss: 4.7685\tLR: 9.284399\n",
      "Training Epoch: 93 [42496/50000]\tLoss: 4.8271\tLR: 9.284655\n",
      "Training Epoch: 93 [42624/50000]\tLoss: 4.8246\tLR: 9.284910\n",
      "Training Epoch: 93 [42752/50000]\tLoss: 4.8048\tLR: 9.285166\n",
      "Training Epoch: 93 [42880/50000]\tLoss: 4.7348\tLR: 9.285422\n",
      "Training Epoch: 93 [43008/50000]\tLoss: 4.7323\tLR: 9.285678\n",
      "Training Epoch: 93 [43136/50000]\tLoss: 4.8099\tLR: 9.285934\n",
      "Training Epoch: 93 [43264/50000]\tLoss: 4.7195\tLR: 9.286189\n",
      "Training Epoch: 93 [43392/50000]\tLoss: 4.7760\tLR: 9.286445\n",
      "Training Epoch: 93 [43520/50000]\tLoss: 4.7065\tLR: 9.286701\n",
      "Training Epoch: 93 [43648/50000]\tLoss: 4.6912\tLR: 9.286957\n",
      "Training Epoch: 93 [43776/50000]\tLoss: 4.6965\tLR: 9.287212\n",
      "Training Epoch: 93 [43904/50000]\tLoss: 4.8310\tLR: 9.287468\n",
      "Training Epoch: 93 [44032/50000]\tLoss: 4.7074\tLR: 9.287724\n",
      "Training Epoch: 93 [44160/50000]\tLoss: 4.6581\tLR: 9.287980\n",
      "Training Epoch: 93 [44288/50000]\tLoss: 4.7951\tLR: 9.288235\n",
      "Training Epoch: 93 [44416/50000]\tLoss: 4.7110\tLR: 9.288491\n",
      "Training Epoch: 93 [44544/50000]\tLoss: 4.7224\tLR: 9.288747\n",
      "Training Epoch: 93 [44672/50000]\tLoss: 4.7814\tLR: 9.289003\n",
      "Training Epoch: 93 [44800/50000]\tLoss: 4.7512\tLR: 9.289258\n",
      "Training Epoch: 93 [44928/50000]\tLoss: 4.6984\tLR: 9.289514\n",
      "Training Epoch: 93 [45056/50000]\tLoss: 4.7664\tLR: 9.289770\n",
      "Training Epoch: 93 [45184/50000]\tLoss: 4.7384\tLR: 9.290026\n",
      "Training Epoch: 93 [45312/50000]\tLoss: 4.7954\tLR: 9.290281\n",
      "Training Epoch: 93 [45440/50000]\tLoss: 4.7575\tLR: 9.290537\n",
      "Training Epoch: 93 [45568/50000]\tLoss: 4.7354\tLR: 9.290793\n",
      "Training Epoch: 93 [45696/50000]\tLoss: 4.8073\tLR: 9.291049\n",
      "Training Epoch: 93 [45824/50000]\tLoss: 4.7820\tLR: 9.291304\n",
      "Training Epoch: 93 [45952/50000]\tLoss: 4.8493\tLR: 9.291560\n",
      "Training Epoch: 93 [46080/50000]\tLoss: 4.6943\tLR: 9.291816\n",
      "Training Epoch: 93 [46208/50000]\tLoss: 4.7748\tLR: 9.292072\n",
      "Training Epoch: 93 [46336/50000]\tLoss: 4.7264\tLR: 9.292327\n",
      "Training Epoch: 93 [46464/50000]\tLoss: 4.8536\tLR: 9.292583\n",
      "Training Epoch: 93 [46592/50000]\tLoss: 4.7735\tLR: 9.292839\n",
      "Training Epoch: 93 [46720/50000]\tLoss: 4.7677\tLR: 9.293095\n",
      "Training Epoch: 93 [46848/50000]\tLoss: 4.7436\tLR: 9.293350\n",
      "Training Epoch: 93 [46976/50000]\tLoss: 4.7904\tLR: 9.293606\n",
      "Training Epoch: 93 [47104/50000]\tLoss: 4.8320\tLR: 9.293862\n",
      "Training Epoch: 93 [47232/50000]\tLoss: 4.7989\tLR: 9.294118\n",
      "Training Epoch: 93 [47360/50000]\tLoss: 4.8581\tLR: 9.294373\n",
      "Training Epoch: 93 [47488/50000]\tLoss: 4.8639\tLR: 9.294629\n",
      "Training Epoch: 93 [47616/50000]\tLoss: 4.8265\tLR: 9.294885\n",
      "Training Epoch: 93 [47744/50000]\tLoss: 4.6980\tLR: 9.295141\n",
      "Training Epoch: 93 [47872/50000]\tLoss: 4.7648\tLR: 9.295396\n",
      "Training Epoch: 93 [48000/50000]\tLoss: 4.7176\tLR: 9.295652\n",
      "Training Epoch: 93 [48128/50000]\tLoss: 4.7419\tLR: 9.295908\n",
      "Training Epoch: 93 [48256/50000]\tLoss: 4.7546\tLR: 9.296164\n",
      "Training Epoch: 93 [48384/50000]\tLoss: 4.7127\tLR: 9.296419\n",
      "Training Epoch: 93 [48512/50000]\tLoss: 4.8413\tLR: 9.296675\n",
      "Training Epoch: 93 [48640/50000]\tLoss: 4.6492\tLR: 9.296931\n",
      "Training Epoch: 93 [48768/50000]\tLoss: 4.8397\tLR: 9.297187\n",
      "Training Epoch: 93 [48896/50000]\tLoss: 4.8732\tLR: 9.297442\n",
      "Training Epoch: 93 [49024/50000]\tLoss: 4.7433\tLR: 9.297698\n",
      "Training Epoch: 93 [49152/50000]\tLoss: 4.8075\tLR: 9.297954\n",
      "Training Epoch: 93 [49280/50000]\tLoss: 4.6954\tLR: 9.298210\n",
      "Training Epoch: 93 [49408/50000]\tLoss: 4.7870\tLR: 9.298465\n",
      "Training Epoch: 93 [49536/50000]\tLoss: 4.7961\tLR: 9.298721\n",
      "Training Epoch: 93 [49664/50000]\tLoss: 4.7536\tLR: 9.298977\n",
      "Training Epoch: 93 [49792/50000]\tLoss: 4.8360\tLR: 9.299233\n",
      "Training Epoch: 93 [49920/50000]\tLoss: 4.7522\tLR: 9.299488\n",
      "Training Epoch: 93 [50000/50000]\tLoss: 4.7650\tLR: 9.299744\n",
      "epoch 93 training time consumed: 489.38s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  130379 GB |  130379 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  129979 GB |  129979 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     400 GB |     400 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  130379 GB |  130379 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  129979 GB |  129979 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     400 GB |     400 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  128547 GB |  128547 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  128147 GB |  128147 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     400 GB |     400 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   13824 K  |   13824 K  |\n",
      "|       from large pool |      24    |      65    |    5893 K  |    5893 K  |\n",
      "|       from small pool |     231    |     274    |    7931 K  |    7931 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   13824 K  |   13824 K  |\n",
      "|       from large pool |      24    |      65    |    5893 K  |    5893 K  |\n",
      "|       from small pool |     231    |     274    |    7931 K  |    7931 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      47    |    8013 K  |    8013 K  |\n",
      "|       from large pool |      10    |      23    |    2832 K  |    2832 K  |\n",
      "|       from small pool |      26    |      35    |    5180 K  |    5180 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 93, Average loss: 0.0376, Accuracy: 0.0100, Time consumed:31.40s\n",
      "\n",
      "Training Epoch: 94 [128/50000]\tLoss: 4.7893\tLR: 0.020000\n",
      "Training Epoch: 94 [256/50000]\tLoss: 4.7894\tLR: 9.300256\n",
      "Training Epoch: 94 [384/50000]\tLoss: 4.7845\tLR: 9.300512\n",
      "Training Epoch: 94 [512/50000]\tLoss: 4.7698\tLR: 9.300767\n",
      "Training Epoch: 94 [640/50000]\tLoss: 4.7852\tLR: 9.301023\n",
      "Training Epoch: 94 [768/50000]\tLoss: 4.7910\tLR: 9.301279\n",
      "Training Epoch: 94 [896/50000]\tLoss: 4.7831\tLR: 9.301535\n",
      "Training Epoch: 94 [1024/50000]\tLoss: 4.7448\tLR: 9.301790\n",
      "Training Epoch: 94 [1152/50000]\tLoss: 4.7871\tLR: 9.302046\n",
      "Training Epoch: 94 [1280/50000]\tLoss: 4.8823\tLR: 9.302302\n",
      "Training Epoch: 94 [1408/50000]\tLoss: 4.7818\tLR: 9.302558\n",
      "Training Epoch: 94 [1536/50000]\tLoss: 4.8268\tLR: 9.302813\n",
      "Training Epoch: 94 [1664/50000]\tLoss: 4.7479\tLR: 9.303069\n",
      "Training Epoch: 94 [1792/50000]\tLoss: 4.8118\tLR: 9.303325\n",
      "Training Epoch: 94 [1920/50000]\tLoss: 4.8709\tLR: 9.303581\n",
      "Training Epoch: 94 [2048/50000]\tLoss: 4.8756\tLR: 9.303836\n",
      "Training Epoch: 94 [2176/50000]\tLoss: 4.7405\tLR: 9.304092\n",
      "Training Epoch: 94 [2304/50000]\tLoss: 4.6794\tLR: 9.304348\n",
      "Training Epoch: 94 [2432/50000]\tLoss: 4.7092\tLR: 9.304604\n",
      "Training Epoch: 94 [2560/50000]\tLoss: 4.8277\tLR: 9.304859\n",
      "Training Epoch: 94 [2688/50000]\tLoss: 4.8953\tLR: 9.305115\n",
      "Training Epoch: 94 [2816/50000]\tLoss: 4.8224\tLR: 9.305371\n",
      "Training Epoch: 94 [2944/50000]\tLoss: 4.8143\tLR: 9.305627\n",
      "Training Epoch: 94 [3072/50000]\tLoss: 4.8311\tLR: 9.305882\n",
      "Training Epoch: 94 [3200/50000]\tLoss: 4.8752\tLR: 9.306138\n",
      "Training Epoch: 94 [3328/50000]\tLoss: 4.7691\tLR: 9.306394\n",
      "Training Epoch: 94 [3456/50000]\tLoss: 4.7822\tLR: 9.306650\n",
      "Training Epoch: 94 [3584/50000]\tLoss: 4.6357\tLR: 9.306905\n",
      "Training Epoch: 94 [3712/50000]\tLoss: 4.6938\tLR: 9.307161\n",
      "Training Epoch: 94 [3840/50000]\tLoss: 4.7118\tLR: 9.307417\n",
      "Training Epoch: 94 [3968/50000]\tLoss: 4.7584\tLR: 9.307673\n",
      "Training Epoch: 94 [4096/50000]\tLoss: 4.7360\tLR: 9.307928\n",
      "Training Epoch: 94 [4224/50000]\tLoss: 4.8836\tLR: 9.308184\n",
      "Training Epoch: 94 [4352/50000]\tLoss: 4.7457\tLR: 9.308440\n",
      "Training Epoch: 94 [4480/50000]\tLoss: 4.8889\tLR: 9.308696\n",
      "Training Epoch: 94 [4608/50000]\tLoss: 4.7804\tLR: 9.308951\n",
      "Training Epoch: 94 [4736/50000]\tLoss: 4.6869\tLR: 9.309207\n",
      "Training Epoch: 94 [4864/50000]\tLoss: 4.6906\tLR: 9.309463\n",
      "Training Epoch: 94 [4992/50000]\tLoss: 4.6999\tLR: 9.309719\n",
      "Training Epoch: 94 [5120/50000]\tLoss: 4.7183\tLR: 9.309974\n",
      "Training Epoch: 94 [5248/50000]\tLoss: 4.7247\tLR: 9.310230\n",
      "Training Epoch: 94 [5376/50000]\tLoss: 4.8216\tLR: 9.310486\n",
      "Training Epoch: 94 [5504/50000]\tLoss: 4.8373\tLR: 9.310742\n",
      "Training Epoch: 94 [5632/50000]\tLoss: 4.8351\tLR: 9.310997\n",
      "Training Epoch: 94 [5760/50000]\tLoss: 4.7792\tLR: 9.311253\n",
      "Training Epoch: 94 [5888/50000]\tLoss: 4.7744\tLR: 9.311509\n",
      "Training Epoch: 94 [6016/50000]\tLoss: 4.7925\tLR: 9.311765\n",
      "Training Epoch: 94 [6144/50000]\tLoss: 4.7215\tLR: 9.312020\n",
      "Training Epoch: 94 [6272/50000]\tLoss: 4.8452\tLR: 9.312276\n",
      "Training Epoch: 94 [6400/50000]\tLoss: 4.8034\tLR: 9.312532\n",
      "Training Epoch: 94 [6528/50000]\tLoss: 4.8451\tLR: 9.312788\n",
      "Training Epoch: 94 [6656/50000]\tLoss: 4.7375\tLR: 9.313043\n",
      "Training Epoch: 94 [6784/50000]\tLoss: 4.7782\tLR: 9.313299\n",
      "Training Epoch: 94 [6912/50000]\tLoss: 4.7870\tLR: 9.313555\n",
      "Training Epoch: 94 [7040/50000]\tLoss: 4.8221\tLR: 9.313811\n",
      "Training Epoch: 94 [7168/50000]\tLoss: 4.7632\tLR: 9.314066\n",
      "Training Epoch: 94 [7296/50000]\tLoss: 4.8644\tLR: 9.314322\n",
      "Training Epoch: 94 [7424/50000]\tLoss: 4.8004\tLR: 9.314578\n",
      "Training Epoch: 94 [7552/50000]\tLoss: 4.7341\tLR: 9.314834\n",
      "Training Epoch: 94 [7680/50000]\tLoss: 4.7023\tLR: 9.315090\n",
      "Training Epoch: 94 [7808/50000]\tLoss: 4.8438\tLR: 9.315345\n",
      "Training Epoch: 94 [7936/50000]\tLoss: 4.7358\tLR: 9.315601\n",
      "Training Epoch: 94 [8064/50000]\tLoss: 4.6524\tLR: 9.315857\n",
      "Training Epoch: 94 [8192/50000]\tLoss: 4.7351\tLR: 9.316113\n",
      "Training Epoch: 94 [8320/50000]\tLoss: 4.7502\tLR: 9.316368\n",
      "Training Epoch: 94 [8448/50000]\tLoss: 4.6792\tLR: 9.316624\n",
      "Training Epoch: 94 [8576/50000]\tLoss: 4.7581\tLR: 9.316880\n",
      "Training Epoch: 94 [8704/50000]\tLoss: 4.7817\tLR: 9.317136\n",
      "Training Epoch: 94 [8832/50000]\tLoss: 4.7283\tLR: 9.317391\n",
      "Training Epoch: 94 [8960/50000]\tLoss: 4.8154\tLR: 9.317647\n",
      "Training Epoch: 94 [9088/50000]\tLoss: 4.8670\tLR: 9.317903\n",
      "Training Epoch: 94 [9216/50000]\tLoss: 4.8623\tLR: 9.318159\n",
      "Training Epoch: 94 [9344/50000]\tLoss: 4.9338\tLR: 9.318414\n",
      "Training Epoch: 94 [9472/50000]\tLoss: 4.7442\tLR: 9.318670\n",
      "Training Epoch: 94 [9600/50000]\tLoss: 4.8464\tLR: 9.318926\n",
      "Training Epoch: 94 [9728/50000]\tLoss: 4.7918\tLR: 9.319182\n",
      "Training Epoch: 94 [9856/50000]\tLoss: 4.7311\tLR: 9.319437\n",
      "Training Epoch: 94 [9984/50000]\tLoss: 4.8629\tLR: 9.319693\n",
      "Training Epoch: 94 [10112/50000]\tLoss: 4.8506\tLR: 9.319949\n",
      "Training Epoch: 94 [10240/50000]\tLoss: 4.8835\tLR: 9.320205\n",
      "Training Epoch: 94 [10368/50000]\tLoss: 4.8947\tLR: 9.320460\n",
      "Training Epoch: 94 [10496/50000]\tLoss: 4.7905\tLR: 9.320716\n",
      "Training Epoch: 94 [10624/50000]\tLoss: 4.8798\tLR: 9.320972\n",
      "Training Epoch: 94 [10752/50000]\tLoss: 4.7690\tLR: 9.321228\n",
      "Training Epoch: 94 [10880/50000]\tLoss: 4.7538\tLR: 9.321483\n",
      "Training Epoch: 94 [11008/50000]\tLoss: 4.7237\tLR: 9.321739\n",
      "Training Epoch: 94 [11136/50000]\tLoss: 4.7776\tLR: 9.321995\n",
      "Training Epoch: 94 [11264/50000]\tLoss: 4.7880\tLR: 9.322251\n",
      "Training Epoch: 94 [11392/50000]\tLoss: 4.8030\tLR: 9.322506\n",
      "Training Epoch: 94 [11520/50000]\tLoss: 4.8017\tLR: 9.322762\n",
      "Training Epoch: 94 [11648/50000]\tLoss: 4.8857\tLR: 9.323018\n",
      "Training Epoch: 94 [11776/50000]\tLoss: 4.8263\tLR: 9.323274\n",
      "Training Epoch: 94 [11904/50000]\tLoss: 4.7311\tLR: 9.323529\n",
      "Training Epoch: 94 [12032/50000]\tLoss: 4.6655\tLR: 9.323785\n",
      "Training Epoch: 94 [12160/50000]\tLoss: 4.7121\tLR: 9.324041\n",
      "Training Epoch: 94 [12288/50000]\tLoss: 4.8413\tLR: 9.324297\n",
      "Training Epoch: 94 [12416/50000]\tLoss: 4.7214\tLR: 9.324552\n",
      "Training Epoch: 94 [12544/50000]\tLoss: 4.9546\tLR: 9.324808\n",
      "Training Epoch: 94 [12672/50000]\tLoss: 4.8887\tLR: 9.325064\n",
      "Training Epoch: 94 [12800/50000]\tLoss: 4.8487\tLR: 9.325320\n",
      "Training Epoch: 94 [12928/50000]\tLoss: 4.8003\tLR: 9.325575\n",
      "Training Epoch: 94 [13056/50000]\tLoss: 4.7597\tLR: 9.325831\n",
      "Training Epoch: 94 [13184/50000]\tLoss: 4.6780\tLR: 9.326087\n",
      "Training Epoch: 94 [13312/50000]\tLoss: 4.7672\tLR: 9.326343\n",
      "Training Epoch: 94 [13440/50000]\tLoss: 4.7507\tLR: 9.326598\n",
      "Training Epoch: 94 [13568/50000]\tLoss: 4.7831\tLR: 9.326854\n",
      "Training Epoch: 94 [13696/50000]\tLoss: 4.7693\tLR: 9.327110\n",
      "Training Epoch: 94 [13824/50000]\tLoss: 4.7731\tLR: 9.327366\n",
      "Training Epoch: 94 [13952/50000]\tLoss: 4.7727\tLR: 9.327621\n",
      "Training Epoch: 94 [14080/50000]\tLoss: 4.7347\tLR: 9.327877\n",
      "Training Epoch: 94 [14208/50000]\tLoss: 4.8180\tLR: 9.328133\n",
      "Training Epoch: 94 [14336/50000]\tLoss: 4.7391\tLR: 9.328389\n",
      "Training Epoch: 94 [14464/50000]\tLoss: 4.6582\tLR: 9.328645\n",
      "Training Epoch: 94 [14592/50000]\tLoss: 4.8089\tLR: 9.328900\n",
      "Training Epoch: 94 [14720/50000]\tLoss: 4.8268\tLR: 9.329156\n",
      "Training Epoch: 94 [14848/50000]\tLoss: 4.8623\tLR: 9.329412\n",
      "Training Epoch: 94 [14976/50000]\tLoss: 4.8311\tLR: 9.329668\n",
      "Training Epoch: 94 [15104/50000]\tLoss: 4.7402\tLR: 9.329923\n",
      "Training Epoch: 94 [15232/50000]\tLoss: 4.7521\tLR: 9.330179\n",
      "Training Epoch: 94 [15360/50000]\tLoss: 4.6887\tLR: 9.330435\n",
      "Training Epoch: 94 [15488/50000]\tLoss: 4.7667\tLR: 9.330691\n",
      "Training Epoch: 94 [15616/50000]\tLoss: 4.8938\tLR: 9.330946\n",
      "Training Epoch: 94 [15744/50000]\tLoss: 4.8121\tLR: 9.331202\n",
      "Training Epoch: 94 [15872/50000]\tLoss: 4.8243\tLR: 9.331458\n",
      "Training Epoch: 94 [16000/50000]\tLoss: 4.9063\tLR: 9.331714\n",
      "Training Epoch: 94 [16128/50000]\tLoss: 4.8137\tLR: 9.331969\n",
      "Training Epoch: 94 [16256/50000]\tLoss: 4.8114\tLR: 9.332225\n",
      "Training Epoch: 94 [16384/50000]\tLoss: 4.6534\tLR: 9.332481\n",
      "Training Epoch: 94 [16512/50000]\tLoss: 4.8125\tLR: 9.332737\n",
      "Training Epoch: 94 [16640/50000]\tLoss: 4.7708\tLR: 9.332992\n",
      "Training Epoch: 94 [16768/50000]\tLoss: 4.8330\tLR: 9.333248\n",
      "Training Epoch: 94 [16896/50000]\tLoss: 5.0951\tLR: 9.333504\n",
      "Training Epoch: 94 [17024/50000]\tLoss: 4.7528\tLR: 9.333760\n",
      "Training Epoch: 94 [17152/50000]\tLoss: 4.7808\tLR: 9.334015\n",
      "Training Epoch: 94 [17280/50000]\tLoss: 4.6956\tLR: 9.334271\n",
      "Training Epoch: 94 [17408/50000]\tLoss: 4.6980\tLR: 9.334527\n",
      "Training Epoch: 94 [17536/50000]\tLoss: 4.7070\tLR: 9.334783\n",
      "Training Epoch: 94 [17664/50000]\tLoss: 4.7676\tLR: 9.335038\n",
      "Training Epoch: 94 [17792/50000]\tLoss: 4.8217\tLR: 9.335294\n",
      "Training Epoch: 94 [17920/50000]\tLoss: 4.6921\tLR: 9.335550\n",
      "Training Epoch: 94 [18048/50000]\tLoss: 4.7874\tLR: 9.335806\n",
      "Training Epoch: 94 [18176/50000]\tLoss: 4.7409\tLR: 9.336061\n",
      "Training Epoch: 94 [18304/50000]\tLoss: 4.8079\tLR: 9.336317\n",
      "Training Epoch: 94 [18432/50000]\tLoss: 4.8469\tLR: 9.336573\n",
      "Training Epoch: 94 [18560/50000]\tLoss: 4.6878\tLR: 9.336829\n",
      "Training Epoch: 94 [18688/50000]\tLoss: 4.8104\tLR: 9.337084\n",
      "Training Epoch: 94 [18816/50000]\tLoss: 4.9098\tLR: 9.337340\n",
      "Training Epoch: 94 [18944/50000]\tLoss: 4.8417\tLR: 9.337596\n",
      "Training Epoch: 94 [19072/50000]\tLoss: 4.7168\tLR: 9.337852\n",
      "Training Epoch: 94 [19200/50000]\tLoss: 4.7688\tLR: 9.338107\n",
      "Training Epoch: 94 [19328/50000]\tLoss: 4.7623\tLR: 9.338363\n",
      "Training Epoch: 94 [19456/50000]\tLoss: 4.7762\tLR: 9.338619\n",
      "Training Epoch: 94 [19584/50000]\tLoss: 4.8259\tLR: 9.338875\n",
      "Training Epoch: 94 [19712/50000]\tLoss: 4.8260\tLR: 9.339130\n",
      "Training Epoch: 94 [19840/50000]\tLoss: 4.7347\tLR: 9.339386\n",
      "Training Epoch: 94 [19968/50000]\tLoss: 4.7835\tLR: 9.339642\n",
      "Training Epoch: 94 [20096/50000]\tLoss: 4.7683\tLR: 9.339898\n",
      "Training Epoch: 94 [20224/50000]\tLoss: 4.7564\tLR: 9.340153\n",
      "Training Epoch: 94 [20352/50000]\tLoss: 4.7571\tLR: 9.340409\n",
      "Training Epoch: 94 [20480/50000]\tLoss: 4.7733\tLR: 9.340665\n",
      "Training Epoch: 94 [20608/50000]\tLoss: 4.8977\tLR: 9.340921\n",
      "Training Epoch: 94 [20736/50000]\tLoss: 4.8338\tLR: 9.341176\n",
      "Training Epoch: 94 [20864/50000]\tLoss: 4.8005\tLR: 9.341432\n",
      "Training Epoch: 94 [20992/50000]\tLoss: 4.7397\tLR: 9.341688\n",
      "Training Epoch: 94 [21120/50000]\tLoss: 4.8358\tLR: 9.341944\n",
      "Training Epoch: 94 [21248/50000]\tLoss: 4.8186\tLR: 9.342199\n",
      "Training Epoch: 94 [21376/50000]\tLoss: 4.7419\tLR: 9.342455\n",
      "Training Epoch: 94 [21504/50000]\tLoss: 4.7405\tLR: 9.342711\n",
      "Training Epoch: 94 [21632/50000]\tLoss: 4.8411\tLR: 9.342967\n",
      "Training Epoch: 94 [21760/50000]\tLoss: 4.7795\tLR: 9.343223\n",
      "Training Epoch: 94 [21888/50000]\tLoss: 4.7658\tLR: 9.343478\n",
      "Training Epoch: 94 [22016/50000]\tLoss: 4.7500\tLR: 9.343734\n",
      "Training Epoch: 94 [22144/50000]\tLoss: 4.8262\tLR: 9.343990\n",
      "Training Epoch: 94 [22272/50000]\tLoss: 4.7677\tLR: 9.344246\n",
      "Training Epoch: 94 [22400/50000]\tLoss: 4.8133\tLR: 9.344501\n",
      "Training Epoch: 94 [22528/50000]\tLoss: 4.6898\tLR: 9.344757\n",
      "Training Epoch: 94 [22656/50000]\tLoss: 4.7893\tLR: 9.345013\n",
      "Training Epoch: 94 [22784/50000]\tLoss: 4.6691\tLR: 9.345269\n",
      "Training Epoch: 94 [22912/50000]\tLoss: 4.8652\tLR: 9.345524\n",
      "Training Epoch: 94 [23040/50000]\tLoss: 4.8077\tLR: 9.345780\n",
      "Training Epoch: 94 [23168/50000]\tLoss: 4.6838\tLR: 9.346036\n",
      "Training Epoch: 94 [23296/50000]\tLoss: 4.8096\tLR: 9.346292\n",
      "Training Epoch: 94 [23424/50000]\tLoss: 4.7215\tLR: 9.346547\n",
      "Training Epoch: 94 [23552/50000]\tLoss: 4.7968\tLR: 9.346803\n",
      "Training Epoch: 94 [23680/50000]\tLoss: 4.7815\tLR: 9.347059\n",
      "Training Epoch: 94 [23808/50000]\tLoss: 4.7268\tLR: 9.347315\n",
      "Training Epoch: 94 [23936/50000]\tLoss: 4.8158\tLR: 9.347570\n",
      "Training Epoch: 94 [24064/50000]\tLoss: 4.8811\tLR: 9.347826\n",
      "Training Epoch: 94 [24192/50000]\tLoss: 4.7373\tLR: 9.348082\n",
      "Training Epoch: 94 [24320/50000]\tLoss: 4.7454\tLR: 9.348338\n",
      "Training Epoch: 94 [24448/50000]\tLoss: 4.7115\tLR: 9.348593\n",
      "Training Epoch: 94 [24576/50000]\tLoss: 4.8497\tLR: 9.348849\n",
      "Training Epoch: 94 [24704/50000]\tLoss: 4.7070\tLR: 9.349105\n",
      "Training Epoch: 94 [24832/50000]\tLoss: 4.7250\tLR: 9.349361\n",
      "Training Epoch: 94 [24960/50000]\tLoss: 4.8259\tLR: 9.349616\n",
      "Training Epoch: 94 [25088/50000]\tLoss: 4.8670\tLR: 9.349872\n",
      "Training Epoch: 94 [25216/50000]\tLoss: 4.8447\tLR: 9.350128\n",
      "Training Epoch: 94 [25344/50000]\tLoss: 4.7823\tLR: 9.350384\n",
      "Training Epoch: 94 [25472/50000]\tLoss: 4.8192\tLR: 9.350639\n",
      "Training Epoch: 94 [25600/50000]\tLoss: 4.8027\tLR: 9.350895\n",
      "Training Epoch: 94 [25728/50000]\tLoss: 4.7691\tLR: 9.351151\n",
      "Training Epoch: 94 [25856/50000]\tLoss: 4.7649\tLR: 9.351407\n",
      "Training Epoch: 94 [25984/50000]\tLoss: 4.6921\tLR: 9.351662\n",
      "Training Epoch: 94 [26112/50000]\tLoss: 4.8207\tLR: 9.351918\n",
      "Training Epoch: 94 [26240/50000]\tLoss: 4.8122\tLR: 9.352174\n",
      "Training Epoch: 94 [26368/50000]\tLoss: 4.8762\tLR: 9.352430\n",
      "Training Epoch: 94 [26496/50000]\tLoss: 5.0137\tLR: 9.352685\n",
      "Training Epoch: 94 [26624/50000]\tLoss: 4.8600\tLR: 9.352941\n",
      "Training Epoch: 94 [26752/50000]\tLoss: 4.7083\tLR: 9.353197\n",
      "Training Epoch: 94 [26880/50000]\tLoss: 4.7155\tLR: 9.353453\n",
      "Training Epoch: 94 [27008/50000]\tLoss: 4.6401\tLR: 9.353708\n",
      "Training Epoch: 94 [27136/50000]\tLoss: 4.6974\tLR: 9.353964\n",
      "Training Epoch: 94 [27264/50000]\tLoss: 4.7629\tLR: 9.354220\n",
      "Training Epoch: 94 [27392/50000]\tLoss: 4.7545\tLR: 9.354476\n",
      "Training Epoch: 94 [27520/50000]\tLoss: 4.8527\tLR: 9.354731\n",
      "Training Epoch: 94 [27648/50000]\tLoss: 4.6741\tLR: 9.354987\n",
      "Training Epoch: 94 [27776/50000]\tLoss: 4.7614\tLR: 9.355243\n",
      "Training Epoch: 94 [27904/50000]\tLoss: 4.7375\tLR: 9.355499\n",
      "Training Epoch: 94 [28032/50000]\tLoss: 4.7867\tLR: 9.355754\n",
      "Training Epoch: 94 [28160/50000]\tLoss: 4.7024\tLR: 9.356010\n",
      "Training Epoch: 94 [28288/50000]\tLoss: 4.7513\tLR: 9.356266\n",
      "Training Epoch: 94 [28416/50000]\tLoss: 4.7953\tLR: 9.356522\n",
      "Training Epoch: 94 [28544/50000]\tLoss: 4.7287\tLR: 9.356777\n",
      "Training Epoch: 94 [28672/50000]\tLoss: 4.6991\tLR: 9.357033\n",
      "Training Epoch: 94 [28800/50000]\tLoss: 4.7080\tLR: 9.357289\n",
      "Training Epoch: 94 [28928/50000]\tLoss: 4.8233\tLR: 9.357545\n",
      "Training Epoch: 94 [29056/50000]\tLoss: 4.7826\tLR: 9.357801\n",
      "Training Epoch: 94 [29184/50000]\tLoss: 4.7553\tLR: 9.358056\n",
      "Training Epoch: 94 [29312/50000]\tLoss: 4.8238\tLR: 9.358312\n",
      "Training Epoch: 94 [29440/50000]\tLoss: 4.7494\tLR: 9.358568\n",
      "Training Epoch: 94 [29568/50000]\tLoss: 4.7509\tLR: 9.358824\n",
      "Training Epoch: 94 [29696/50000]\tLoss: 4.8653\tLR: 9.359079\n",
      "Training Epoch: 94 [29824/50000]\tLoss: 4.7645\tLR: 9.359335\n",
      "Training Epoch: 94 [29952/50000]\tLoss: 4.7555\tLR: 9.359591\n",
      "Training Epoch: 94 [30080/50000]\tLoss: 4.7583\tLR: 9.359847\n",
      "Training Epoch: 94 [30208/50000]\tLoss: 4.7417\tLR: 9.360102\n",
      "Training Epoch: 94 [30336/50000]\tLoss: 4.8142\tLR: 9.360358\n",
      "Training Epoch: 94 [30464/50000]\tLoss: 4.7480\tLR: 9.360614\n",
      "Training Epoch: 94 [30592/50000]\tLoss: 4.8649\tLR: 9.360870\n",
      "Training Epoch: 94 [30720/50000]\tLoss: 4.8647\tLR: 9.361125\n",
      "Training Epoch: 94 [30848/50000]\tLoss: 4.7578\tLR: 9.361381\n",
      "Training Epoch: 94 [30976/50000]\tLoss: 4.6499\tLR: 9.361637\n",
      "Training Epoch: 94 [31104/50000]\tLoss: 4.7644\tLR: 9.361893\n",
      "Training Epoch: 94 [31232/50000]\tLoss: 4.7393\tLR: 9.362148\n",
      "Training Epoch: 94 [31360/50000]\tLoss: 4.7727\tLR: 9.362404\n",
      "Training Epoch: 94 [31488/50000]\tLoss: 4.8071\tLR: 9.362660\n",
      "Training Epoch: 94 [31616/50000]\tLoss: 4.7754\tLR: 9.362916\n",
      "Training Epoch: 94 [31744/50000]\tLoss: 4.6963\tLR: 9.363171\n",
      "Training Epoch: 94 [31872/50000]\tLoss: 4.7822\tLR: 9.363427\n",
      "Training Epoch: 94 [32000/50000]\tLoss: 4.7100\tLR: 9.363683\n",
      "Training Epoch: 94 [32128/50000]\tLoss: 4.7781\tLR: 9.363939\n",
      "Training Epoch: 94 [32256/50000]\tLoss: 4.8166\tLR: 9.364194\n",
      "Training Epoch: 94 [32384/50000]\tLoss: 4.7916\tLR: 9.364450\n",
      "Training Epoch: 94 [32512/50000]\tLoss: 4.7104\tLR: 9.364706\n",
      "Training Epoch: 94 [32640/50000]\tLoss: 4.6844\tLR: 9.364962\n",
      "Training Epoch: 94 [32768/50000]\tLoss: 4.6652\tLR: 9.365217\n",
      "Training Epoch: 94 [32896/50000]\tLoss: 4.7647\tLR: 9.365473\n",
      "Training Epoch: 94 [33024/50000]\tLoss: 4.6946\tLR: 9.365729\n",
      "Training Epoch: 94 [33152/50000]\tLoss: 4.6723\tLR: 9.365985\n",
      "Training Epoch: 94 [33280/50000]\tLoss: 4.7381\tLR: 9.366240\n",
      "Training Epoch: 94 [33408/50000]\tLoss: 4.6767\tLR: 9.366496\n",
      "Training Epoch: 94 [33536/50000]\tLoss: 4.7760\tLR: 9.366752\n",
      "Training Epoch: 94 [33664/50000]\tLoss: 4.6818\tLR: 9.367008\n",
      "Training Epoch: 94 [33792/50000]\tLoss: 4.8300\tLR: 9.367263\n",
      "Training Epoch: 94 [33920/50000]\tLoss: 4.7582\tLR: 9.367519\n",
      "Training Epoch: 94 [34048/50000]\tLoss: 4.7375\tLR: 9.367775\n",
      "Training Epoch: 94 [34176/50000]\tLoss: 4.7999\tLR: 9.368031\n",
      "Training Epoch: 94 [34304/50000]\tLoss: 4.7902\tLR: 9.368286\n",
      "Training Epoch: 94 [34432/50000]\tLoss: 4.8422\tLR: 9.368542\n",
      "Training Epoch: 94 [34560/50000]\tLoss: 4.7013\tLR: 9.368798\n",
      "Training Epoch: 94 [34688/50000]\tLoss: 4.7721\tLR: 9.369054\n",
      "Training Epoch: 94 [34816/50000]\tLoss: 4.6649\tLR: 9.369309\n",
      "Training Epoch: 94 [34944/50000]\tLoss: 4.6932\tLR: 9.369565\n",
      "Training Epoch: 94 [35072/50000]\tLoss: 4.8317\tLR: 9.369821\n",
      "Training Epoch: 94 [35200/50000]\tLoss: 4.7498\tLR: 9.370077\n",
      "Training Epoch: 94 [35328/50000]\tLoss: 4.8928\tLR: 9.370332\n",
      "Training Epoch: 94 [35456/50000]\tLoss: 4.8418\tLR: 9.370588\n",
      "Training Epoch: 94 [35584/50000]\tLoss: 4.7555\tLR: 9.370844\n",
      "Training Epoch: 94 [35712/50000]\tLoss: 4.7385\tLR: 9.371100\n",
      "Training Epoch: 94 [35840/50000]\tLoss: 4.7190\tLR: 9.371355\n",
      "Training Epoch: 94 [35968/50000]\tLoss: 4.7750\tLR: 9.371611\n",
      "Training Epoch: 94 [36096/50000]\tLoss: 4.7868\tLR: 9.371867\n",
      "Training Epoch: 94 [36224/50000]\tLoss: 4.7806\tLR: 9.372123\n",
      "Training Epoch: 94 [36352/50000]\tLoss: 4.7355\tLR: 9.372379\n",
      "Training Epoch: 94 [36480/50000]\tLoss: 4.7449\tLR: 9.372634\n",
      "Training Epoch: 94 [36608/50000]\tLoss: 4.7041\tLR: 9.372890\n",
      "Training Epoch: 94 [36736/50000]\tLoss: 4.6871\tLR: 9.373146\n",
      "Training Epoch: 94 [36864/50000]\tLoss: 4.7899\tLR: 9.373402\n",
      "Training Epoch: 94 [36992/50000]\tLoss: 4.7787\tLR: 9.373657\n",
      "Training Epoch: 94 [37120/50000]\tLoss: 4.7079\tLR: 9.373913\n",
      "Training Epoch: 94 [37248/50000]\tLoss: 4.7819\tLR: 9.374169\n",
      "Training Epoch: 94 [37376/50000]\tLoss: 4.7819\tLR: 9.374425\n",
      "Training Epoch: 94 [37504/50000]\tLoss: 4.7787\tLR: 9.374680\n",
      "Training Epoch: 94 [37632/50000]\tLoss: 4.7515\tLR: 9.374936\n",
      "Training Epoch: 94 [37760/50000]\tLoss: 4.8412\tLR: 9.375192\n",
      "Training Epoch: 94 [37888/50000]\tLoss: 4.8744\tLR: 9.375448\n",
      "Training Epoch: 94 [38016/50000]\tLoss: 4.8150\tLR: 9.375703\n",
      "Training Epoch: 94 [38144/50000]\tLoss: 4.7297\tLR: 9.375959\n",
      "Training Epoch: 94 [38272/50000]\tLoss: 4.7057\tLR: 9.376215\n",
      "Training Epoch: 94 [38400/50000]\tLoss: 4.8304\tLR: 9.376471\n",
      "Training Epoch: 94 [38528/50000]\tLoss: 4.7125\tLR: 9.376726\n",
      "Training Epoch: 94 [38656/50000]\tLoss: 4.7466\tLR: 9.376982\n",
      "Training Epoch: 94 [38784/50000]\tLoss: 4.7223\tLR: 9.377238\n",
      "Training Epoch: 94 [38912/50000]\tLoss: 4.8509\tLR: 9.377494\n",
      "Training Epoch: 94 [39040/50000]\tLoss: 4.8453\tLR: 9.377749\n",
      "Training Epoch: 94 [39168/50000]\tLoss: 4.8265\tLR: 9.378005\n",
      "Training Epoch: 94 [39296/50000]\tLoss: 4.8225\tLR: 9.378261\n",
      "Training Epoch: 94 [39424/50000]\tLoss: 4.8331\tLR: 9.378517\n",
      "Training Epoch: 94 [39552/50000]\tLoss: 4.7640\tLR: 9.378772\n",
      "Training Epoch: 94 [39680/50000]\tLoss: 4.7711\tLR: 9.379028\n",
      "Training Epoch: 94 [39808/50000]\tLoss: 4.6805\tLR: 9.379284\n",
      "Training Epoch: 94 [39936/50000]\tLoss: 4.9073\tLR: 9.379540\n",
      "Training Epoch: 94 [40064/50000]\tLoss: 4.7477\tLR: 9.379795\n",
      "Training Epoch: 94 [40192/50000]\tLoss: 4.7996\tLR: 9.380051\n",
      "Training Epoch: 94 [40320/50000]\tLoss: 4.7884\tLR: 9.380307\n",
      "Training Epoch: 94 [40448/50000]\tLoss: 4.7756\tLR: 9.380563\n",
      "Training Epoch: 94 [40576/50000]\tLoss: 4.8481\tLR: 9.380818\n",
      "Training Epoch: 94 [40704/50000]\tLoss: 4.8281\tLR: 9.381074\n",
      "Training Epoch: 94 [40832/50000]\tLoss: 4.9598\tLR: 9.381330\n",
      "Training Epoch: 94 [40960/50000]\tLoss: 4.8454\tLR: 9.381586\n",
      "Training Epoch: 94 [41088/50000]\tLoss: 4.7499\tLR: 9.381841\n",
      "Training Epoch: 94 [41216/50000]\tLoss: 4.8068\tLR: 9.382097\n",
      "Training Epoch: 94 [41344/50000]\tLoss: 4.7865\tLR: 9.382353\n",
      "Training Epoch: 94 [41472/50000]\tLoss: 4.7677\tLR: 9.382609\n",
      "Training Epoch: 94 [41600/50000]\tLoss: 4.7789\tLR: 9.382864\n",
      "Training Epoch: 94 [41728/50000]\tLoss: 4.8530\tLR: 9.383120\n",
      "Training Epoch: 94 [41856/50000]\tLoss: 4.7645\tLR: 9.383376\n",
      "Training Epoch: 94 [41984/50000]\tLoss: 4.8661\tLR: 9.383632\n",
      "Training Epoch: 94 [42112/50000]\tLoss: 4.7383\tLR: 9.383887\n",
      "Training Epoch: 94 [42240/50000]\tLoss: 4.6729\tLR: 9.384143\n",
      "Training Epoch: 94 [42368/50000]\tLoss: 4.6917\tLR: 9.384399\n",
      "Training Epoch: 94 [42496/50000]\tLoss: 4.8735\tLR: 9.384655\n",
      "Training Epoch: 94 [42624/50000]\tLoss: 4.8014\tLR: 9.384910\n",
      "Training Epoch: 94 [42752/50000]\tLoss: 4.8088\tLR: 9.385166\n",
      "Training Epoch: 94 [42880/50000]\tLoss: 4.7949\tLR: 9.385422\n",
      "Training Epoch: 94 [43008/50000]\tLoss: 4.8503\tLR: 9.385678\n",
      "Training Epoch: 94 [43136/50000]\tLoss: 4.7737\tLR: 9.385934\n",
      "Training Epoch: 94 [43264/50000]\tLoss: 4.7171\tLR: 9.386189\n",
      "Training Epoch: 94 [43392/50000]\tLoss: 4.7985\tLR: 9.386445\n",
      "Training Epoch: 94 [43520/50000]\tLoss: 4.7919\tLR: 9.386701\n",
      "Training Epoch: 94 [43648/50000]\tLoss: 4.7581\tLR: 9.386957\n",
      "Training Epoch: 94 [43776/50000]\tLoss: 4.7718\tLR: 9.387212\n",
      "Training Epoch: 94 [43904/50000]\tLoss: 4.7736\tLR: 9.387468\n",
      "Training Epoch: 94 [44032/50000]\tLoss: 4.8751\tLR: 9.387724\n",
      "Training Epoch: 94 [44160/50000]\tLoss: 4.8516\tLR: 9.387980\n",
      "Training Epoch: 94 [44288/50000]\tLoss: 4.7970\tLR: 9.388235\n",
      "Training Epoch: 94 [44416/50000]\tLoss: 4.7624\tLR: 9.388491\n",
      "Training Epoch: 94 [44544/50000]\tLoss: 4.8101\tLR: 9.388747\n",
      "Training Epoch: 94 [44672/50000]\tLoss: 4.8835\tLR: 9.389003\n",
      "Training Epoch: 94 [44800/50000]\tLoss: 4.7805\tLR: 9.389258\n",
      "Training Epoch: 94 [44928/50000]\tLoss: 4.8321\tLR: 9.389514\n",
      "Training Epoch: 94 [45056/50000]\tLoss: 4.8512\tLR: 9.389770\n",
      "Training Epoch: 94 [45184/50000]\tLoss: 4.7843\tLR: 9.390026\n",
      "Training Epoch: 94 [45312/50000]\tLoss: 4.9155\tLR: 9.390281\n",
      "Training Epoch: 94 [45440/50000]\tLoss: 4.7510\tLR: 9.390537\n",
      "Training Epoch: 94 [45568/50000]\tLoss: 4.7616\tLR: 9.390793\n",
      "Training Epoch: 94 [45696/50000]\tLoss: 4.6858\tLR: 9.391049\n",
      "Training Epoch: 94 [45824/50000]\tLoss: 4.7309\tLR: 9.391304\n",
      "Training Epoch: 94 [45952/50000]\tLoss: 4.7250\tLR: 9.391560\n",
      "Training Epoch: 94 [46080/50000]\tLoss: 4.7026\tLR: 9.391816\n",
      "Training Epoch: 94 [46208/50000]\tLoss: 4.7808\tLR: 9.392072\n",
      "Training Epoch: 94 [46336/50000]\tLoss: 4.7491\tLR: 9.392327\n",
      "Training Epoch: 94 [46464/50000]\tLoss: 4.7350\tLR: 9.392583\n",
      "Training Epoch: 94 [46592/50000]\tLoss: 4.7937\tLR: 9.392839\n",
      "Training Epoch: 94 [46720/50000]\tLoss: 4.7010\tLR: 9.393095\n",
      "Training Epoch: 94 [46848/50000]\tLoss: 4.8171\tLR: 9.393350\n",
      "Training Epoch: 94 [46976/50000]\tLoss: 4.8079\tLR: 9.393606\n",
      "Training Epoch: 94 [47104/50000]\tLoss: 4.9195\tLR: 9.393862\n",
      "Training Epoch: 94 [47232/50000]\tLoss: 4.7442\tLR: 9.394118\n",
      "Training Epoch: 94 [47360/50000]\tLoss: 4.7046\tLR: 9.394373\n",
      "Training Epoch: 94 [47488/50000]\tLoss: 4.8058\tLR: 9.394629\n",
      "Training Epoch: 94 [47616/50000]\tLoss: 4.6932\tLR: 9.394885\n",
      "Training Epoch: 94 [47744/50000]\tLoss: 4.7021\tLR: 9.395141\n",
      "Training Epoch: 94 [47872/50000]\tLoss: 4.8107\tLR: 9.395396\n",
      "Training Epoch: 94 [48000/50000]\tLoss: 4.7250\tLR: 9.395652\n",
      "Training Epoch: 94 [48128/50000]\tLoss: 4.7199\tLR: 9.395908\n",
      "Training Epoch: 94 [48256/50000]\tLoss: 4.7133\tLR: 9.396164\n",
      "Training Epoch: 94 [48384/50000]\tLoss: 4.7345\tLR: 9.396419\n",
      "Training Epoch: 94 [48512/50000]\tLoss: 4.8161\tLR: 9.396675\n",
      "Training Epoch: 94 [48640/50000]\tLoss: 4.7367\tLR: 9.396931\n",
      "Training Epoch: 94 [48768/50000]\tLoss: 4.7062\tLR: 9.397187\n",
      "Training Epoch: 94 [48896/50000]\tLoss: 4.7951\tLR: 9.397442\n",
      "Training Epoch: 94 [49024/50000]\tLoss: 4.8007\tLR: 9.397698\n",
      "Training Epoch: 94 [49152/50000]\tLoss: 4.8496\tLR: 9.397954\n",
      "Training Epoch: 94 [49280/50000]\tLoss: 4.7064\tLR: 9.398210\n",
      "Training Epoch: 94 [49408/50000]\tLoss: 4.7843\tLR: 9.398465\n",
      "Training Epoch: 94 [49536/50000]\tLoss: 4.8581\tLR: 9.398721\n",
      "Training Epoch: 94 [49664/50000]\tLoss: 4.8455\tLR: 9.398977\n",
      "Training Epoch: 94 [49792/50000]\tLoss: 4.8199\tLR: 9.399233\n",
      "Training Epoch: 94 [49920/50000]\tLoss: 4.8243\tLR: 9.399488\n",
      "Training Epoch: 94 [50000/50000]\tLoss: 4.6753\tLR: 9.399744\n",
      "epoch 94 training time consumed: 489.22s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  131781 GB |  131781 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  131377 GB |  131376 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     404 GB |     404 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  131781 GB |  131781 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  131377 GB |  131376 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     404 GB |     404 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  129930 GB |  129930 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  129525 GB |  129525 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     404 GB |     404 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   13973 K  |   13973 K  |\n",
      "|       from large pool |      24    |      65    |    5956 K  |    5956 K  |\n",
      "|       from small pool |     231    |     274    |    8016 K  |    8016 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   13973 K  |   13973 K  |\n",
      "|       from large pool |      24    |      65    |    5956 K  |    5956 K  |\n",
      "|       from small pool |     231    |     274    |    8016 K  |    8016 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      47    |    8099 K  |    8099 K  |\n",
      "|       from large pool |      10    |      23    |    2863 K  |    2863 K  |\n",
      "|       from small pool |      27    |      35    |    5236 K  |    5236 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 94, Average loss: 0.0379, Accuracy: 0.0100, Time consumed:31.42s\n",
      "\n",
      "Training Epoch: 95 [128/50000]\tLoss: 4.7782\tLR: 0.020000\n",
      "Training Epoch: 95 [256/50000]\tLoss: 4.8984\tLR: 9.400256\n",
      "Training Epoch: 95 [384/50000]\tLoss: 4.8422\tLR: 9.400512\n",
      "Training Epoch: 95 [512/50000]\tLoss: 4.7654\tLR: 9.400767\n",
      "Training Epoch: 95 [640/50000]\tLoss: 4.7363\tLR: 9.401023\n",
      "Training Epoch: 95 [768/50000]\tLoss: 4.8048\tLR: 9.401279\n",
      "Training Epoch: 95 [896/50000]\tLoss: 4.7409\tLR: 9.401535\n",
      "Training Epoch: 95 [1024/50000]\tLoss: 4.7564\tLR: 9.401790\n",
      "Training Epoch: 95 [1152/50000]\tLoss: 4.8053\tLR: 9.402046\n",
      "Training Epoch: 95 [1280/50000]\tLoss: 4.7314\tLR: 9.402302\n",
      "Training Epoch: 95 [1408/50000]\tLoss: 4.9079\tLR: 9.402558\n",
      "Training Epoch: 95 [1536/50000]\tLoss: 4.7083\tLR: 9.402813\n",
      "Training Epoch: 95 [1664/50000]\tLoss: 4.8310\tLR: 9.403069\n",
      "Training Epoch: 95 [1792/50000]\tLoss: 4.7172\tLR: 9.403325\n",
      "Training Epoch: 95 [1920/50000]\tLoss: 4.7725\tLR: 9.403581\n",
      "Training Epoch: 95 [2048/50000]\tLoss: 4.7824\tLR: 9.403836\n",
      "Training Epoch: 95 [2176/50000]\tLoss: 4.7215\tLR: 9.404092\n",
      "Training Epoch: 95 [2304/50000]\tLoss: 4.7945\tLR: 9.404348\n",
      "Training Epoch: 95 [2432/50000]\tLoss: 4.8034\tLR: 9.404604\n",
      "Training Epoch: 95 [2560/50000]\tLoss: 4.7893\tLR: 9.404859\n",
      "Training Epoch: 95 [2688/50000]\tLoss: 4.7383\tLR: 9.405115\n",
      "Training Epoch: 95 [2816/50000]\tLoss: 4.7766\tLR: 9.405371\n",
      "Training Epoch: 95 [2944/50000]\tLoss: 4.8076\tLR: 9.405627\n",
      "Training Epoch: 95 [3072/50000]\tLoss: 4.7537\tLR: 9.405882\n",
      "Training Epoch: 95 [3200/50000]\tLoss: 4.7632\tLR: 9.406138\n",
      "Training Epoch: 95 [3328/50000]\tLoss: 4.8358\tLR: 9.406394\n",
      "Training Epoch: 95 [3456/50000]\tLoss: 4.9072\tLR: 9.406650\n",
      "Training Epoch: 95 [3584/50000]\tLoss: 4.7101\tLR: 9.406905\n",
      "Training Epoch: 95 [3712/50000]\tLoss: 4.7940\tLR: 9.407161\n",
      "Training Epoch: 95 [3840/50000]\tLoss: 4.7763\tLR: 9.407417\n",
      "Training Epoch: 95 [3968/50000]\tLoss: 4.8044\tLR: 9.407673\n",
      "Training Epoch: 95 [4096/50000]\tLoss: 4.7582\tLR: 9.407928\n",
      "Training Epoch: 95 [4224/50000]\tLoss: 4.7559\tLR: 9.408184\n",
      "Training Epoch: 95 [4352/50000]\tLoss: 4.8104\tLR: 9.408440\n",
      "Training Epoch: 95 [4480/50000]\tLoss: 4.7910\tLR: 9.408696\n",
      "Training Epoch: 95 [4608/50000]\tLoss: 4.8536\tLR: 9.408951\n",
      "Training Epoch: 95 [4736/50000]\tLoss: 4.8244\tLR: 9.409207\n",
      "Training Epoch: 95 [4864/50000]\tLoss: 4.7841\tLR: 9.409463\n",
      "Training Epoch: 95 [4992/50000]\tLoss: 4.7482\tLR: 9.409719\n",
      "Training Epoch: 95 [5120/50000]\tLoss: 4.8379\tLR: 9.409974\n",
      "Training Epoch: 95 [5248/50000]\tLoss: 4.8300\tLR: 9.410230\n",
      "Training Epoch: 95 [5376/50000]\tLoss: 4.8623\tLR: 9.410486\n",
      "Training Epoch: 95 [5504/50000]\tLoss: 4.8413\tLR: 9.410742\n",
      "Training Epoch: 95 [5632/50000]\tLoss: 4.8645\tLR: 9.410997\n",
      "Training Epoch: 95 [5760/50000]\tLoss: 4.8084\tLR: 9.411253\n",
      "Training Epoch: 95 [5888/50000]\tLoss: 4.7813\tLR: 9.411509\n",
      "Training Epoch: 95 [6016/50000]\tLoss: 4.6676\tLR: 9.411765\n",
      "Training Epoch: 95 [6144/50000]\tLoss: 4.8166\tLR: 9.412020\n",
      "Training Epoch: 95 [6272/50000]\tLoss: 4.7264\tLR: 9.412276\n",
      "Training Epoch: 95 [6400/50000]\tLoss: 4.8439\tLR: 9.412532\n",
      "Training Epoch: 95 [6528/50000]\tLoss: 4.7784\tLR: 9.412788\n",
      "Training Epoch: 95 [6656/50000]\tLoss: 4.8646\tLR: 9.413043\n",
      "Training Epoch: 95 [6784/50000]\tLoss: 4.8074\tLR: 9.413299\n",
      "Training Epoch: 95 [6912/50000]\tLoss: 4.7273\tLR: 9.413555\n",
      "Training Epoch: 95 [7040/50000]\tLoss: 4.8497\tLR: 9.413811\n",
      "Training Epoch: 95 [7168/50000]\tLoss: 4.7778\tLR: 9.414066\n",
      "Training Epoch: 95 [7296/50000]\tLoss: 4.8772\tLR: 9.414322\n",
      "Training Epoch: 95 [7424/50000]\tLoss: 4.8202\tLR: 9.414578\n",
      "Training Epoch: 95 [7552/50000]\tLoss: 4.7646\tLR: 9.414834\n",
      "Training Epoch: 95 [7680/50000]\tLoss: 4.8603\tLR: 9.415090\n",
      "Training Epoch: 95 [7808/50000]\tLoss: 4.8107\tLR: 9.415345\n",
      "Training Epoch: 95 [7936/50000]\tLoss: 4.8076\tLR: 9.415601\n",
      "Training Epoch: 95 [8064/50000]\tLoss: 4.8443\tLR: 9.415857\n",
      "Training Epoch: 95 [8192/50000]\tLoss: 4.7806\tLR: 9.416113\n",
      "Training Epoch: 95 [8320/50000]\tLoss: 4.8481\tLR: 9.416368\n",
      "Training Epoch: 95 [8448/50000]\tLoss: 4.7458\tLR: 9.416624\n",
      "Training Epoch: 95 [8576/50000]\tLoss: 4.7973\tLR: 9.416880\n",
      "Training Epoch: 95 [8704/50000]\tLoss: 4.8656\tLR: 9.417136\n",
      "Training Epoch: 95 [8832/50000]\tLoss: 4.8103\tLR: 9.417391\n",
      "Training Epoch: 95 [8960/50000]\tLoss: 4.8166\tLR: 9.417647\n",
      "Training Epoch: 95 [9088/50000]\tLoss: 4.8141\tLR: 9.417903\n",
      "Training Epoch: 95 [9216/50000]\tLoss: 4.7337\tLR: 9.418159\n",
      "Training Epoch: 95 [9344/50000]\tLoss: 4.7685\tLR: 9.418414\n",
      "Training Epoch: 95 [9472/50000]\tLoss: 4.8460\tLR: 9.418670\n",
      "Training Epoch: 95 [9600/50000]\tLoss: 4.6646\tLR: 9.418926\n",
      "Training Epoch: 95 [9728/50000]\tLoss: 4.8508\tLR: 9.419182\n",
      "Training Epoch: 95 [9856/50000]\tLoss: 4.7416\tLR: 9.419437\n",
      "Training Epoch: 95 [9984/50000]\tLoss: 4.8153\tLR: 9.419693\n",
      "Training Epoch: 95 [10112/50000]\tLoss: 4.9338\tLR: 9.419949\n",
      "Training Epoch: 95 [10240/50000]\tLoss: 4.8784\tLR: 9.420205\n",
      "Training Epoch: 95 [10368/50000]\tLoss: 4.7396\tLR: 9.420460\n",
      "Training Epoch: 95 [10496/50000]\tLoss: 4.7536\tLR: 9.420716\n",
      "Training Epoch: 95 [10624/50000]\tLoss: 4.7168\tLR: 9.420972\n",
      "Training Epoch: 95 [10752/50000]\tLoss: 4.8452\tLR: 9.421228\n",
      "Training Epoch: 95 [10880/50000]\tLoss: 4.8673\tLR: 9.421483\n",
      "Training Epoch: 95 [11008/50000]\tLoss: 4.7987\tLR: 9.421739\n",
      "Training Epoch: 95 [11136/50000]\tLoss: 4.8051\tLR: 9.421995\n",
      "Training Epoch: 95 [11264/50000]\tLoss: 4.7883\tLR: 9.422251\n",
      "Training Epoch: 95 [11392/50000]\tLoss: 4.7413\tLR: 9.422506\n",
      "Training Epoch: 95 [11520/50000]\tLoss: 4.7750\tLR: 9.422762\n",
      "Training Epoch: 95 [11648/50000]\tLoss: 4.7121\tLR: 9.423018\n",
      "Training Epoch: 95 [11776/50000]\tLoss: 4.7591\tLR: 9.423274\n",
      "Training Epoch: 95 [11904/50000]\tLoss: 4.7834\tLR: 9.423529\n",
      "Training Epoch: 95 [12032/50000]\tLoss: 4.7880\tLR: 9.423785\n",
      "Training Epoch: 95 [12160/50000]\tLoss: 4.7683\tLR: 9.424041\n",
      "Training Epoch: 95 [12288/50000]\tLoss: 4.8115\tLR: 9.424297\n",
      "Training Epoch: 95 [12416/50000]\tLoss: 4.7718\tLR: 9.424552\n",
      "Training Epoch: 95 [12544/50000]\tLoss: 4.9004\tLR: 9.424808\n",
      "Training Epoch: 95 [12672/50000]\tLoss: 4.7539\tLR: 9.425064\n",
      "Training Epoch: 95 [12800/50000]\tLoss: 4.7718\tLR: 9.425320\n",
      "Training Epoch: 95 [12928/50000]\tLoss: 4.7167\tLR: 9.425575\n",
      "Training Epoch: 95 [13056/50000]\tLoss: 4.8049\tLR: 9.425831\n",
      "Training Epoch: 95 [13184/50000]\tLoss: 4.7833\tLR: 9.426087\n",
      "Training Epoch: 95 [13312/50000]\tLoss: 4.8422\tLR: 9.426343\n",
      "Training Epoch: 95 [13440/50000]\tLoss: 5.0039\tLR: 9.426598\n",
      "Training Epoch: 95 [13568/50000]\tLoss: 4.9317\tLR: 9.426854\n",
      "Training Epoch: 95 [13696/50000]\tLoss: 4.8577\tLR: 9.427110\n",
      "Training Epoch: 95 [13824/50000]\tLoss: 4.6962\tLR: 9.427366\n",
      "Training Epoch: 95 [13952/50000]\tLoss: 4.7994\tLR: 9.427621\n",
      "Training Epoch: 95 [14080/50000]\tLoss: 4.7151\tLR: 9.427877\n",
      "Training Epoch: 95 [14208/50000]\tLoss: 4.8472\tLR: 9.428133\n",
      "Training Epoch: 95 [14336/50000]\tLoss: 4.8440\tLR: 9.428389\n",
      "Training Epoch: 95 [14464/50000]\tLoss: 4.9352\tLR: 9.428645\n",
      "Training Epoch: 95 [14592/50000]\tLoss: 4.8432\tLR: 9.428900\n",
      "Training Epoch: 95 [14720/50000]\tLoss: 4.8591\tLR: 9.429156\n",
      "Training Epoch: 95 [14848/50000]\tLoss: 4.7299\tLR: 9.429412\n",
      "Training Epoch: 95 [14976/50000]\tLoss: 4.7588\tLR: 9.429668\n",
      "Training Epoch: 95 [15104/50000]\tLoss: 4.9017\tLR: 9.429923\n",
      "Training Epoch: 95 [15232/50000]\tLoss: 4.7329\tLR: 9.430179\n",
      "Training Epoch: 95 [15360/50000]\tLoss: 4.7079\tLR: 9.430435\n",
      "Training Epoch: 95 [15488/50000]\tLoss: 4.7281\tLR: 9.430691\n",
      "Training Epoch: 95 [15616/50000]\tLoss: 4.7366\tLR: 9.430946\n",
      "Training Epoch: 95 [15744/50000]\tLoss: 4.6924\tLR: 9.431202\n",
      "Training Epoch: 95 [15872/50000]\tLoss: 4.7264\tLR: 9.431458\n",
      "Training Epoch: 95 [16000/50000]\tLoss: 4.7604\tLR: 9.431714\n",
      "Training Epoch: 95 [16128/50000]\tLoss: 4.6702\tLR: 9.431969\n",
      "Training Epoch: 95 [16256/50000]\tLoss: 4.7416\tLR: 9.432225\n",
      "Training Epoch: 95 [16384/50000]\tLoss: 4.7925\tLR: 9.432481\n",
      "Training Epoch: 95 [16512/50000]\tLoss: 4.8772\tLR: 9.432737\n",
      "Training Epoch: 95 [16640/50000]\tLoss: 4.7459\tLR: 9.432992\n",
      "Training Epoch: 95 [16768/50000]\tLoss: 4.7229\tLR: 9.433248\n",
      "Training Epoch: 95 [16896/50000]\tLoss: 4.7879\tLR: 9.433504\n",
      "Training Epoch: 95 [17024/50000]\tLoss: 4.7754\tLR: 9.433760\n",
      "Training Epoch: 95 [17152/50000]\tLoss: 4.7873\tLR: 9.434015\n",
      "Training Epoch: 95 [17280/50000]\tLoss: 4.7653\tLR: 9.434271\n",
      "Training Epoch: 95 [17408/50000]\tLoss: 4.7295\tLR: 9.434527\n",
      "Training Epoch: 95 [17536/50000]\tLoss: 4.7887\tLR: 9.434783\n",
      "Training Epoch: 95 [17664/50000]\tLoss: 4.7167\tLR: 9.435038\n",
      "Training Epoch: 95 [17792/50000]\tLoss: 4.7192\tLR: 9.435294\n",
      "Training Epoch: 95 [17920/50000]\tLoss: 4.7545\tLR: 9.435550\n",
      "Training Epoch: 95 [18048/50000]\tLoss: 4.7782\tLR: 9.435806\n",
      "Training Epoch: 95 [18176/50000]\tLoss: 4.6236\tLR: 9.436061\n",
      "Training Epoch: 95 [18304/50000]\tLoss: 4.6940\tLR: 9.436317\n",
      "Training Epoch: 95 [18432/50000]\tLoss: 4.8138\tLR: 9.436573\n",
      "Training Epoch: 95 [18560/50000]\tLoss: 4.8205\tLR: 9.436829\n",
      "Training Epoch: 95 [18688/50000]\tLoss: 4.6923\tLR: 9.437084\n",
      "Training Epoch: 95 [18816/50000]\tLoss: 4.8170\tLR: 9.437340\n",
      "Training Epoch: 95 [18944/50000]\tLoss: 4.7443\tLR: 9.437596\n",
      "Training Epoch: 95 [19072/50000]\tLoss: 4.8035\tLR: 9.437852\n",
      "Training Epoch: 95 [19200/50000]\tLoss: 4.7331\tLR: 9.438107\n",
      "Training Epoch: 95 [19328/50000]\tLoss: 4.7059\tLR: 9.438363\n",
      "Training Epoch: 95 [19456/50000]\tLoss: 4.7176\tLR: 9.438619\n",
      "Training Epoch: 95 [19584/50000]\tLoss: 4.7480\tLR: 9.438875\n",
      "Training Epoch: 95 [19712/50000]\tLoss: 4.8354\tLR: 9.439130\n",
      "Training Epoch: 95 [19840/50000]\tLoss: 4.8515\tLR: 9.439386\n",
      "Training Epoch: 95 [19968/50000]\tLoss: 4.9213\tLR: 9.439642\n",
      "Training Epoch: 95 [20096/50000]\tLoss: 4.8120\tLR: 9.439898\n",
      "Training Epoch: 95 [20224/50000]\tLoss: 4.7302\tLR: 9.440153\n",
      "Training Epoch: 95 [20352/50000]\tLoss: 4.7303\tLR: 9.440409\n",
      "Training Epoch: 95 [20480/50000]\tLoss: 4.7022\tLR: 9.440665\n",
      "Training Epoch: 95 [20608/50000]\tLoss: 4.7486\tLR: 9.440921\n",
      "Training Epoch: 95 [20736/50000]\tLoss: 4.7630\tLR: 9.441176\n",
      "Training Epoch: 95 [20864/50000]\tLoss: 4.6433\tLR: 9.441432\n",
      "Training Epoch: 95 [20992/50000]\tLoss: 4.7221\tLR: 9.441688\n",
      "Training Epoch: 95 [21120/50000]\tLoss: 4.7267\tLR: 9.441944\n",
      "Training Epoch: 95 [21248/50000]\tLoss: 4.7809\tLR: 9.442199\n",
      "Training Epoch: 95 [21376/50000]\tLoss: 4.8019\tLR: 9.442455\n",
      "Training Epoch: 95 [21504/50000]\tLoss: 4.8237\tLR: 9.442711\n",
      "Training Epoch: 95 [21632/50000]\tLoss: 4.7898\tLR: 9.442967\n",
      "Training Epoch: 95 [21760/50000]\tLoss: 4.7588\tLR: 9.443223\n",
      "Training Epoch: 95 [21888/50000]\tLoss: 4.6271\tLR: 9.443478\n",
      "Training Epoch: 95 [22016/50000]\tLoss: 4.7322\tLR: 9.443734\n",
      "Training Epoch: 95 [22144/50000]\tLoss: 4.7285\tLR: 9.443990\n",
      "Training Epoch: 95 [22272/50000]\tLoss: 4.6835\tLR: 9.444246\n",
      "Training Epoch: 95 [22400/50000]\tLoss: 4.7592\tLR: 9.444501\n",
      "Training Epoch: 95 [22528/50000]\tLoss: 4.7480\tLR: 9.444757\n",
      "Training Epoch: 95 [22656/50000]\tLoss: 4.8062\tLR: 9.445013\n",
      "Training Epoch: 95 [22784/50000]\tLoss: 4.7756\tLR: 9.445269\n",
      "Training Epoch: 95 [22912/50000]\tLoss: 4.7913\tLR: 9.445524\n",
      "Training Epoch: 95 [23040/50000]\tLoss: 4.7879\tLR: 9.445780\n",
      "Training Epoch: 95 [23168/50000]\tLoss: 4.7340\tLR: 9.446036\n",
      "Training Epoch: 95 [23296/50000]\tLoss: 4.7937\tLR: 9.446292\n",
      "Training Epoch: 95 [23424/50000]\tLoss: 4.6737\tLR: 9.446547\n",
      "Training Epoch: 95 [23552/50000]\tLoss: 4.7558\tLR: 9.446803\n",
      "Training Epoch: 95 [23680/50000]\tLoss: 4.8433\tLR: 9.447059\n",
      "Training Epoch: 95 [23808/50000]\tLoss: 4.7469\tLR: 9.447315\n",
      "Training Epoch: 95 [23936/50000]\tLoss: 4.6961\tLR: 9.447570\n",
      "Training Epoch: 95 [24064/50000]\tLoss: 4.8734\tLR: 9.447826\n",
      "Training Epoch: 95 [24192/50000]\tLoss: 4.8245\tLR: 9.448082\n",
      "Training Epoch: 95 [24320/50000]\tLoss: 4.6890\tLR: 9.448338\n",
      "Training Epoch: 95 [24448/50000]\tLoss: 4.7397\tLR: 9.448593\n",
      "Training Epoch: 95 [24576/50000]\tLoss: 4.7117\tLR: 9.448849\n",
      "Training Epoch: 95 [24704/50000]\tLoss: 4.7499\tLR: 9.449105\n",
      "Training Epoch: 95 [24832/50000]\tLoss: 4.8236\tLR: 9.449361\n",
      "Training Epoch: 95 [24960/50000]\tLoss: 4.7736\tLR: 9.449616\n",
      "Training Epoch: 95 [25088/50000]\tLoss: 4.7227\tLR: 9.449872\n",
      "Training Epoch: 95 [25216/50000]\tLoss: 4.7332\tLR: 9.450128\n",
      "Training Epoch: 95 [25344/50000]\tLoss: 4.7225\tLR: 9.450384\n",
      "Training Epoch: 95 [25472/50000]\tLoss: 4.7461\tLR: 9.450639\n",
      "Training Epoch: 95 [25600/50000]\tLoss: 4.8149\tLR: 9.450895\n",
      "Training Epoch: 95 [25728/50000]\tLoss: 4.7672\tLR: 9.451151\n",
      "Training Epoch: 95 [25856/50000]\tLoss: 4.7306\tLR: 9.451407\n",
      "Training Epoch: 95 [25984/50000]\tLoss: 4.8042\tLR: 9.451662\n",
      "Training Epoch: 95 [26112/50000]\tLoss: 4.7063\tLR: 9.451918\n",
      "Training Epoch: 95 [26240/50000]\tLoss: 4.7563\tLR: 9.452174\n",
      "Training Epoch: 95 [26368/50000]\tLoss: 4.8583\tLR: 9.452430\n",
      "Training Epoch: 95 [26496/50000]\tLoss: 4.7956\tLR: 9.452685\n",
      "Training Epoch: 95 [26624/50000]\tLoss: 4.7890\tLR: 9.452941\n",
      "Training Epoch: 95 [26752/50000]\tLoss: 4.7946\tLR: 9.453197\n",
      "Training Epoch: 95 [26880/50000]\tLoss: 4.8190\tLR: 9.453453\n",
      "Training Epoch: 95 [27008/50000]\tLoss: 4.8332\tLR: 9.453708\n",
      "Training Epoch: 95 [27136/50000]\tLoss: 4.7828\tLR: 9.453964\n",
      "Training Epoch: 95 [27264/50000]\tLoss: 4.7783\tLR: 9.454220\n",
      "Training Epoch: 95 [27392/50000]\tLoss: 4.7801\tLR: 9.454476\n",
      "Training Epoch: 95 [27520/50000]\tLoss: 4.7010\tLR: 9.454731\n",
      "Training Epoch: 95 [27648/50000]\tLoss: 4.8037\tLR: 9.454987\n",
      "Training Epoch: 95 [27776/50000]\tLoss: 4.8314\tLR: 9.455243\n",
      "Training Epoch: 95 [27904/50000]\tLoss: 4.8236\tLR: 9.455499\n",
      "Training Epoch: 95 [28032/50000]\tLoss: 4.7490\tLR: 9.455754\n",
      "Training Epoch: 95 [28160/50000]\tLoss: 4.8136\tLR: 9.456010\n",
      "Training Epoch: 95 [28288/50000]\tLoss: 4.6910\tLR: 9.456266\n",
      "Training Epoch: 95 [28416/50000]\tLoss: 4.8418\tLR: 9.456522\n",
      "Training Epoch: 95 [28544/50000]\tLoss: 4.7071\tLR: 9.456777\n",
      "Training Epoch: 95 [28672/50000]\tLoss: 4.7438\tLR: 9.457033\n",
      "Training Epoch: 95 [28800/50000]\tLoss: 4.7564\tLR: 9.457289\n",
      "Training Epoch: 95 [28928/50000]\tLoss: 4.7457\tLR: 9.457545\n",
      "Training Epoch: 95 [29056/50000]\tLoss: 4.7813\tLR: 9.457801\n",
      "Training Epoch: 95 [29184/50000]\tLoss: 4.7889\tLR: 9.458056\n",
      "Training Epoch: 95 [29312/50000]\tLoss: 4.8522\tLR: 9.458312\n",
      "Training Epoch: 95 [29440/50000]\tLoss: 4.7765\tLR: 9.458568\n",
      "Training Epoch: 95 [29568/50000]\tLoss: 4.7725\tLR: 9.458824\n",
      "Training Epoch: 95 [29696/50000]\tLoss: 4.8566\tLR: 9.459079\n",
      "Training Epoch: 95 [29824/50000]\tLoss: 4.7403\tLR: 9.459335\n",
      "Training Epoch: 95 [29952/50000]\tLoss: 4.7497\tLR: 9.459591\n",
      "Training Epoch: 95 [30080/50000]\tLoss: 4.8489\tLR: 9.459847\n",
      "Training Epoch: 95 [30208/50000]\tLoss: 4.7891\tLR: 9.460102\n",
      "Training Epoch: 95 [30336/50000]\tLoss: 4.7284\tLR: 9.460358\n",
      "Training Epoch: 95 [30464/50000]\tLoss: 4.6812\tLR: 9.460614\n",
      "Training Epoch: 95 [30592/50000]\tLoss: 4.7530\tLR: 9.460870\n",
      "Training Epoch: 95 [30720/50000]\tLoss: 4.7548\tLR: 9.461125\n",
      "Training Epoch: 95 [30848/50000]\tLoss: 4.6589\tLR: 9.461381\n",
      "Training Epoch: 95 [30976/50000]\tLoss: 4.7585\tLR: 9.461637\n",
      "Training Epoch: 95 [31104/50000]\tLoss: 4.6809\tLR: 9.461893\n",
      "Training Epoch: 95 [31232/50000]\tLoss: 4.6965\tLR: 9.462148\n",
      "Training Epoch: 95 [31360/50000]\tLoss: 4.7043\tLR: 9.462404\n",
      "Training Epoch: 95 [31488/50000]\tLoss: 4.7577\tLR: 9.462660\n",
      "Training Epoch: 95 [31616/50000]\tLoss: 4.7817\tLR: 9.462916\n",
      "Training Epoch: 95 [31744/50000]\tLoss: 4.7664\tLR: 9.463171\n",
      "Training Epoch: 95 [31872/50000]\tLoss: 4.7321\tLR: 9.463427\n",
      "Training Epoch: 95 [32000/50000]\tLoss: 4.7671\tLR: 9.463683\n",
      "Training Epoch: 95 [32128/50000]\tLoss: 4.8471\tLR: 9.463939\n",
      "Training Epoch: 95 [32256/50000]\tLoss: 4.7730\tLR: 9.464194\n",
      "Training Epoch: 95 [32384/50000]\tLoss: 4.7184\tLR: 9.464450\n",
      "Training Epoch: 95 [32512/50000]\tLoss: 4.8451\tLR: 9.464706\n",
      "Training Epoch: 95 [32640/50000]\tLoss: 4.6933\tLR: 9.464962\n",
      "Training Epoch: 95 [32768/50000]\tLoss: 4.8730\tLR: 9.465217\n",
      "Training Epoch: 95 [32896/50000]\tLoss: 4.7933\tLR: 9.465473\n",
      "Training Epoch: 95 [33024/50000]\tLoss: 4.7568\tLR: 9.465729\n",
      "Training Epoch: 95 [33152/50000]\tLoss: 4.7526\tLR: 9.465985\n",
      "Training Epoch: 95 [33280/50000]\tLoss: 4.8677\tLR: 9.466240\n",
      "Training Epoch: 95 [33408/50000]\tLoss: 4.9126\tLR: 9.466496\n",
      "Training Epoch: 95 [33536/50000]\tLoss: 4.7883\tLR: 9.466752\n",
      "Training Epoch: 95 [33664/50000]\tLoss: 4.8499\tLR: 9.467008\n",
      "Training Epoch: 95 [33792/50000]\tLoss: 4.7427\tLR: 9.467263\n",
      "Training Epoch: 95 [33920/50000]\tLoss: 4.8151\tLR: 9.467519\n",
      "Training Epoch: 95 [34048/50000]\tLoss: 4.7033\tLR: 9.467775\n",
      "Training Epoch: 95 [34176/50000]\tLoss: 4.7217\tLR: 9.468031\n",
      "Training Epoch: 95 [34304/50000]\tLoss: 4.6208\tLR: 9.468286\n",
      "Training Epoch: 95 [34432/50000]\tLoss: 4.6826\tLR: 9.468542\n",
      "Training Epoch: 95 [34560/50000]\tLoss: 4.7967\tLR: 9.468798\n",
      "Training Epoch: 95 [34688/50000]\tLoss: 4.7511\tLR: 9.469054\n",
      "Training Epoch: 95 [34816/50000]\tLoss: 4.7726\tLR: 9.469309\n",
      "Training Epoch: 95 [34944/50000]\tLoss: 4.7844\tLR: 9.469565\n",
      "Training Epoch: 95 [35072/50000]\tLoss: 4.7187\tLR: 9.469821\n",
      "Training Epoch: 95 [35200/50000]\tLoss: 4.7051\tLR: 9.470077\n",
      "Training Epoch: 95 [35328/50000]\tLoss: 4.8323\tLR: 9.470332\n",
      "Training Epoch: 95 [35456/50000]\tLoss: 4.8322\tLR: 9.470588\n",
      "Training Epoch: 95 [35584/50000]\tLoss: 4.8290\tLR: 9.470844\n",
      "Training Epoch: 95 [35712/50000]\tLoss: 4.7730\tLR: 9.471100\n",
      "Training Epoch: 95 [35840/50000]\tLoss: 4.7245\tLR: 9.471355\n",
      "Training Epoch: 95 [35968/50000]\tLoss: 4.7760\tLR: 9.471611\n",
      "Training Epoch: 95 [36096/50000]\tLoss: 4.7550\tLR: 9.471867\n",
      "Training Epoch: 95 [36224/50000]\tLoss: 4.6450\tLR: 9.472123\n",
      "Training Epoch: 95 [36352/50000]\tLoss: 4.8064\tLR: 9.472379\n",
      "Training Epoch: 95 [36480/50000]\tLoss: 4.7903\tLR: 9.472634\n",
      "Training Epoch: 95 [36608/50000]\tLoss: 4.8520\tLR: 9.472890\n",
      "Training Epoch: 95 [36736/50000]\tLoss: 4.7818\tLR: 9.473146\n",
      "Training Epoch: 95 [36864/50000]\tLoss: 4.8161\tLR: 9.473402\n",
      "Training Epoch: 95 [36992/50000]\tLoss: 4.7476\tLR: 9.473657\n",
      "Training Epoch: 95 [37120/50000]\tLoss: 4.7000\tLR: 9.473913\n",
      "Training Epoch: 95 [37248/50000]\tLoss: 4.8625\tLR: 9.474169\n",
      "Training Epoch: 95 [37376/50000]\tLoss: 4.8980\tLR: 9.474425\n",
      "Training Epoch: 95 [37504/50000]\tLoss: 4.7528\tLR: 9.474680\n",
      "Training Epoch: 95 [37632/50000]\tLoss: 4.7720\tLR: 9.474936\n",
      "Training Epoch: 95 [37760/50000]\tLoss: 4.8159\tLR: 9.475192\n",
      "Training Epoch: 95 [37888/50000]\tLoss: 4.6652\tLR: 9.475448\n",
      "Training Epoch: 95 [38016/50000]\tLoss: 4.8363\tLR: 9.475703\n",
      "Training Epoch: 95 [38144/50000]\tLoss: 4.7873\tLR: 9.475959\n",
      "Training Epoch: 95 [38272/50000]\tLoss: 4.8013\tLR: 9.476215\n",
      "Training Epoch: 95 [38400/50000]\tLoss: 4.8414\tLR: 9.476471\n",
      "Training Epoch: 95 [38528/50000]\tLoss: 4.7585\tLR: 9.476726\n",
      "Training Epoch: 95 [38656/50000]\tLoss: 4.7316\tLR: 9.476982\n",
      "Training Epoch: 95 [38784/50000]\tLoss: 4.8588\tLR: 9.477238\n",
      "Training Epoch: 95 [38912/50000]\tLoss: 4.8866\tLR: 9.477494\n",
      "Training Epoch: 95 [39040/50000]\tLoss: 4.7555\tLR: 9.477749\n",
      "Training Epoch: 95 [39168/50000]\tLoss: 4.7646\tLR: 9.478005\n",
      "Training Epoch: 95 [39296/50000]\tLoss: 4.8170\tLR: 9.478261\n",
      "Training Epoch: 95 [39424/50000]\tLoss: 4.7006\tLR: 9.478517\n",
      "Training Epoch: 95 [39552/50000]\tLoss: 4.7470\tLR: 9.478772\n",
      "Training Epoch: 95 [39680/50000]\tLoss: 4.8010\tLR: 9.479028\n",
      "Training Epoch: 95 [39808/50000]\tLoss: 4.8172\tLR: 9.479284\n",
      "Training Epoch: 95 [39936/50000]\tLoss: 4.6620\tLR: 9.479540\n",
      "Training Epoch: 95 [40064/50000]\tLoss: 4.8821\tLR: 9.479795\n",
      "Training Epoch: 95 [40192/50000]\tLoss: 4.6797\tLR: 9.480051\n",
      "Training Epoch: 95 [40320/50000]\tLoss: 4.7603\tLR: 9.480307\n",
      "Training Epoch: 95 [40448/50000]\tLoss: 4.6653\tLR: 9.480563\n",
      "Training Epoch: 95 [40576/50000]\tLoss: 4.7599\tLR: 9.480818\n",
      "Training Epoch: 95 [40704/50000]\tLoss: 4.8150\tLR: 9.481074\n",
      "Training Epoch: 95 [40832/50000]\tLoss: 4.7283\tLR: 9.481330\n",
      "Training Epoch: 95 [40960/50000]\tLoss: 4.7202\tLR: 9.481586\n",
      "Training Epoch: 95 [41088/50000]\tLoss: 4.7632\tLR: 9.481841\n",
      "Training Epoch: 95 [41216/50000]\tLoss: 4.7957\tLR: 9.482097\n",
      "Training Epoch: 95 [41344/50000]\tLoss: 4.8401\tLR: 9.482353\n",
      "Training Epoch: 95 [41472/50000]\tLoss: 4.8388\tLR: 9.482609\n",
      "Training Epoch: 95 [41600/50000]\tLoss: 4.8587\tLR: 9.482864\n",
      "Training Epoch: 95 [41728/50000]\tLoss: 4.7922\tLR: 9.483120\n",
      "Training Epoch: 95 [41856/50000]\tLoss: 4.8515\tLR: 9.483376\n",
      "Training Epoch: 95 [41984/50000]\tLoss: 4.8989\tLR: 9.483632\n",
      "Training Epoch: 95 [42112/50000]\tLoss: 4.7896\tLR: 9.483887\n",
      "Training Epoch: 95 [42240/50000]\tLoss: 4.8016\tLR: 9.484143\n",
      "Training Epoch: 95 [42368/50000]\tLoss: 4.7787\tLR: 9.484399\n",
      "Training Epoch: 95 [42496/50000]\tLoss: 4.8948\tLR: 9.484655\n",
      "Training Epoch: 95 [42624/50000]\tLoss: 4.7743\tLR: 9.484910\n",
      "Training Epoch: 95 [42752/50000]\tLoss: 4.8190\tLR: 9.485166\n",
      "Training Epoch: 95 [42880/50000]\tLoss: 4.8242\tLR: 9.485422\n",
      "Training Epoch: 95 [43008/50000]\tLoss: 4.6914\tLR: 9.485678\n",
      "Training Epoch: 95 [43136/50000]\tLoss: 4.7987\tLR: 9.485934\n",
      "Training Epoch: 95 [43264/50000]\tLoss: 4.8995\tLR: 9.486189\n",
      "Training Epoch: 95 [43392/50000]\tLoss: 4.8476\tLR: 9.486445\n",
      "Training Epoch: 95 [43520/50000]\tLoss: 4.9952\tLR: 9.486701\n",
      "Training Epoch: 95 [43648/50000]\tLoss: 4.8348\tLR: 9.486957\n",
      "Training Epoch: 95 [43776/50000]\tLoss: 4.6831\tLR: 9.487212\n",
      "Training Epoch: 95 [43904/50000]\tLoss: 4.7622\tLR: 9.487468\n",
      "Training Epoch: 95 [44032/50000]\tLoss: 4.7653\tLR: 9.487724\n",
      "Training Epoch: 95 [44160/50000]\tLoss: 4.6309\tLR: 9.487980\n",
      "Training Epoch: 95 [44288/50000]\tLoss: 4.8242\tLR: 9.488235\n",
      "Training Epoch: 95 [44416/50000]\tLoss: 4.7925\tLR: 9.488491\n",
      "Training Epoch: 95 [44544/50000]\tLoss: 4.7822\tLR: 9.488747\n",
      "Training Epoch: 95 [44672/50000]\tLoss: 4.7652\tLR: 9.489003\n",
      "Training Epoch: 95 [44800/50000]\tLoss: 4.7440\tLR: 9.489258\n",
      "Training Epoch: 95 [44928/50000]\tLoss: 4.7973\tLR: 9.489514\n",
      "Training Epoch: 95 [45056/50000]\tLoss: 4.7527\tLR: 9.489770\n",
      "Training Epoch: 95 [45184/50000]\tLoss: 4.7556\tLR: 9.490026\n",
      "Training Epoch: 95 [45312/50000]\tLoss: 4.8240\tLR: 9.490281\n",
      "Training Epoch: 95 [45440/50000]\tLoss: 4.8136\tLR: 9.490537\n",
      "Training Epoch: 95 [45568/50000]\tLoss: 4.8166\tLR: 9.490793\n",
      "Training Epoch: 95 [45696/50000]\tLoss: 4.8448\tLR: 9.491049\n",
      "Training Epoch: 95 [45824/50000]\tLoss: 4.7157\tLR: 9.491304\n",
      "Training Epoch: 95 [45952/50000]\tLoss: 4.7932\tLR: 9.491560\n",
      "Training Epoch: 95 [46080/50000]\tLoss: 4.7480\tLR: 9.491816\n",
      "Training Epoch: 95 [46208/50000]\tLoss: 4.6940\tLR: 9.492072\n",
      "Training Epoch: 95 [46336/50000]\tLoss: 4.8133\tLR: 9.492327\n",
      "Training Epoch: 95 [46464/50000]\tLoss: 4.7929\tLR: 9.492583\n",
      "Training Epoch: 95 [46592/50000]\tLoss: 4.8022\tLR: 9.492839\n",
      "Training Epoch: 95 [46720/50000]\tLoss: 4.9079\tLR: 9.493095\n",
      "Training Epoch: 95 [46848/50000]\tLoss: 4.7993\tLR: 9.493350\n",
      "Training Epoch: 95 [46976/50000]\tLoss: 4.7434\tLR: 9.493606\n",
      "Training Epoch: 95 [47104/50000]\tLoss: 4.8205\tLR: 9.493862\n",
      "Training Epoch: 95 [47232/50000]\tLoss: 4.8540\tLR: 9.494118\n",
      "Training Epoch: 95 [47360/50000]\tLoss: 4.8409\tLR: 9.494373\n",
      "Training Epoch: 95 [47488/50000]\tLoss: 4.7482\tLR: 9.494629\n",
      "Training Epoch: 95 [47616/50000]\tLoss: 4.8776\tLR: 9.494885\n",
      "Training Epoch: 95 [47744/50000]\tLoss: 4.7494\tLR: 9.495141\n",
      "Training Epoch: 95 [47872/50000]\tLoss: 4.7230\tLR: 9.495396\n",
      "Training Epoch: 95 [48000/50000]\tLoss: 4.7897\tLR: 9.495652\n",
      "Training Epoch: 95 [48128/50000]\tLoss: 4.8644\tLR: 9.495908\n",
      "Training Epoch: 95 [48256/50000]\tLoss: 4.8716\tLR: 9.496164\n",
      "Training Epoch: 95 [48384/50000]\tLoss: 4.8456\tLR: 9.496419\n",
      "Training Epoch: 95 [48512/50000]\tLoss: 4.7926\tLR: 9.496675\n",
      "Training Epoch: 95 [48640/50000]\tLoss: 4.8223\tLR: 9.496931\n",
      "Training Epoch: 95 [48768/50000]\tLoss: 4.8741\tLR: 9.497187\n",
      "Training Epoch: 95 [48896/50000]\tLoss: 4.7810\tLR: 9.497442\n",
      "Training Epoch: 95 [49024/50000]\tLoss: 4.7215\tLR: 9.497698\n",
      "Training Epoch: 95 [49152/50000]\tLoss: 4.7939\tLR: 9.497954\n",
      "Training Epoch: 95 [49280/50000]\tLoss: 4.8730\tLR: 9.498210\n",
      "Training Epoch: 95 [49408/50000]\tLoss: 4.8225\tLR: 9.498465\n",
      "Training Epoch: 95 [49536/50000]\tLoss: 4.8759\tLR: 9.498721\n",
      "Training Epoch: 95 [49664/50000]\tLoss: 4.8035\tLR: 9.498977\n",
      "Training Epoch: 95 [49792/50000]\tLoss: 4.7442\tLR: 9.499233\n",
      "Training Epoch: 95 [49920/50000]\tLoss: 4.8209\tLR: 9.499488\n",
      "Training Epoch: 95 [50000/50000]\tLoss: 4.7185\tLR: 9.499744\n",
      "epoch 95 training time consumed: 492.43s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  133183 GB |  133183 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  132774 GB |  132774 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     408 GB |     408 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  133183 GB |  133183 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  132774 GB |  132774 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     408 GB |     408 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  131312 GB |  131312 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  130903 GB |  130903 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     408 GB |     408 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   14122 K  |   14121 K  |\n",
      "|       from large pool |      24    |      65    |    6020 K  |    6020 K  |\n",
      "|       from small pool |     231    |     274    |    8101 K  |    8101 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   14122 K  |   14121 K  |\n",
      "|       from large pool |      24    |      65    |    6020 K  |    6020 K  |\n",
      "|       from small pool |     231    |     274    |    8101 K  |    8101 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      35    |      47    |    8185 K  |    8185 K  |\n",
      "|       from large pool |      10    |      23    |    2893 K  |    2893 K  |\n",
      "|       from small pool |      25    |      35    |    5291 K  |    5291 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 95, Average loss: 0.0379, Accuracy: 0.0100, Time consumed:31.33s\n",
      "\n",
      "Training Epoch: 96 [128/50000]\tLoss: 4.7981\tLR: 0.020000\n",
      "Training Epoch: 96 [256/50000]\tLoss: 4.7853\tLR: 9.500256\n",
      "Training Epoch: 96 [384/50000]\tLoss: 4.6587\tLR: 9.500512\n",
      "Training Epoch: 96 [512/50000]\tLoss: 4.7377\tLR: 9.500767\n",
      "Training Epoch: 96 [640/50000]\tLoss: 4.7425\tLR: 9.501023\n",
      "Training Epoch: 96 [768/50000]\tLoss: 4.8548\tLR: 9.501279\n",
      "Training Epoch: 96 [896/50000]\tLoss: 4.8129\tLR: 9.501535\n",
      "Training Epoch: 96 [1024/50000]\tLoss: 4.8051\tLR: 9.501790\n",
      "Training Epoch: 96 [1152/50000]\tLoss: 4.7502\tLR: 9.502046\n",
      "Training Epoch: 96 [1280/50000]\tLoss: 4.7871\tLR: 9.502302\n",
      "Training Epoch: 96 [1408/50000]\tLoss: 4.7364\tLR: 9.502558\n",
      "Training Epoch: 96 [1536/50000]\tLoss: 4.7633\tLR: 9.502813\n",
      "Training Epoch: 96 [1664/50000]\tLoss: 4.6804\tLR: 9.503069\n",
      "Training Epoch: 96 [1792/50000]\tLoss: 4.8132\tLR: 9.503325\n",
      "Training Epoch: 96 [1920/50000]\tLoss: 4.6951\tLR: 9.503581\n",
      "Training Epoch: 96 [2048/50000]\tLoss: 4.7872\tLR: 9.503836\n",
      "Training Epoch: 96 [2176/50000]\tLoss: 4.7418\tLR: 9.504092\n",
      "Training Epoch: 96 [2304/50000]\tLoss: 4.8280\tLR: 9.504348\n",
      "Training Epoch: 96 [2432/50000]\tLoss: 4.8772\tLR: 9.504604\n",
      "Training Epoch: 96 [2560/50000]\tLoss: 4.7680\tLR: 9.504859\n",
      "Training Epoch: 96 [2688/50000]\tLoss: 4.7438\tLR: 9.505115\n",
      "Training Epoch: 96 [2816/50000]\tLoss: 4.7928\tLR: 9.505371\n",
      "Training Epoch: 96 [2944/50000]\tLoss: 4.9487\tLR: 9.505627\n",
      "Training Epoch: 96 [3072/50000]\tLoss: 4.7842\tLR: 9.505882\n",
      "Training Epoch: 96 [3200/50000]\tLoss: 4.7773\tLR: 9.506138\n",
      "Training Epoch: 96 [3328/50000]\tLoss: 4.7838\tLR: 9.506394\n",
      "Training Epoch: 96 [3456/50000]\tLoss: 4.7724\tLR: 9.506650\n",
      "Training Epoch: 96 [3584/50000]\tLoss: 4.8387\tLR: 9.506905\n",
      "Training Epoch: 96 [3712/50000]\tLoss: 4.7529\tLR: 9.507161\n",
      "Training Epoch: 96 [3840/50000]\tLoss: 4.7288\tLR: 9.507417\n",
      "Training Epoch: 96 [3968/50000]\tLoss: 4.7948\tLR: 9.507673\n",
      "Training Epoch: 96 [4096/50000]\tLoss: 4.7988\tLR: 9.507928\n",
      "Training Epoch: 96 [4224/50000]\tLoss: 4.7389\tLR: 9.508184\n",
      "Training Epoch: 96 [4352/50000]\tLoss: 4.6958\tLR: 9.508440\n",
      "Training Epoch: 96 [4480/50000]\tLoss: 4.8250\tLR: 9.508696\n",
      "Training Epoch: 96 [4608/50000]\tLoss: 4.7706\tLR: 9.508951\n",
      "Training Epoch: 96 [4736/50000]\tLoss: 4.7333\tLR: 9.509207\n",
      "Training Epoch: 96 [4864/50000]\tLoss: 4.6929\tLR: 9.509463\n",
      "Training Epoch: 96 [4992/50000]\tLoss: 4.8062\tLR: 9.509719\n",
      "Training Epoch: 96 [5120/50000]\tLoss: 4.8472\tLR: 9.509974\n",
      "Training Epoch: 96 [5248/50000]\tLoss: 4.8141\tLR: 9.510230\n",
      "Training Epoch: 96 [5376/50000]\tLoss: 4.8156\tLR: 9.510486\n",
      "Training Epoch: 96 [5504/50000]\tLoss: 4.7676\tLR: 9.510742\n",
      "Training Epoch: 96 [5632/50000]\tLoss: 4.7177\tLR: 9.510997\n",
      "Training Epoch: 96 [5760/50000]\tLoss: 4.7654\tLR: 9.511253\n",
      "Training Epoch: 96 [5888/50000]\tLoss: 4.7799\tLR: 9.511509\n",
      "Training Epoch: 96 [6016/50000]\tLoss: 4.8058\tLR: 9.511765\n",
      "Training Epoch: 96 [6144/50000]\tLoss: 4.8000\tLR: 9.512020\n",
      "Training Epoch: 96 [6272/50000]\tLoss: 4.8131\tLR: 9.512276\n",
      "Training Epoch: 96 [6400/50000]\tLoss: 4.7821\tLR: 9.512532\n",
      "Training Epoch: 96 [6528/50000]\tLoss: 4.7910\tLR: 9.512788\n",
      "Training Epoch: 96 [6656/50000]\tLoss: 4.7522\tLR: 9.513043\n",
      "Training Epoch: 96 [6784/50000]\tLoss: 4.7959\tLR: 9.513299\n",
      "Training Epoch: 96 [6912/50000]\tLoss: 4.9078\tLR: 9.513555\n",
      "Training Epoch: 96 [7040/50000]\tLoss: 4.8703\tLR: 9.513811\n",
      "Training Epoch: 96 [7168/50000]\tLoss: 4.7746\tLR: 9.514066\n",
      "Training Epoch: 96 [7296/50000]\tLoss: 4.7179\tLR: 9.514322\n",
      "Training Epoch: 96 [7424/50000]\tLoss: 4.7506\tLR: 9.514578\n",
      "Training Epoch: 96 [7552/50000]\tLoss: 4.7771\tLR: 9.514834\n",
      "Training Epoch: 96 [7680/50000]\tLoss: 4.8062\tLR: 9.515090\n",
      "Training Epoch: 96 [7808/50000]\tLoss: 4.7115\tLR: 9.515345\n",
      "Training Epoch: 96 [7936/50000]\tLoss: 4.7511\tLR: 9.515601\n",
      "Training Epoch: 96 [8064/50000]\tLoss: 4.8032\tLR: 9.515857\n",
      "Training Epoch: 96 [8192/50000]\tLoss: 4.7630\tLR: 9.516113\n",
      "Training Epoch: 96 [8320/50000]\tLoss: 4.6984\tLR: 9.516368\n",
      "Training Epoch: 96 [8448/50000]\tLoss: 4.8747\tLR: 9.516624\n",
      "Training Epoch: 96 [8576/50000]\tLoss: 4.7608\tLR: 9.516880\n",
      "Training Epoch: 96 [8704/50000]\tLoss: 4.7203\tLR: 9.517136\n",
      "Training Epoch: 96 [8832/50000]\tLoss: 4.7970\tLR: 9.517391\n",
      "Training Epoch: 96 [8960/50000]\tLoss: 4.7404\tLR: 9.517647\n",
      "Training Epoch: 96 [9088/50000]\tLoss: 4.7072\tLR: 9.517903\n",
      "Training Epoch: 96 [9216/50000]\tLoss: 4.7193\tLR: 9.518159\n",
      "Training Epoch: 96 [9344/50000]\tLoss: 4.6847\tLR: 9.518414\n",
      "Training Epoch: 96 [9472/50000]\tLoss: 4.7044\tLR: 9.518670\n",
      "Training Epoch: 96 [9600/50000]\tLoss: 4.8297\tLR: 9.518926\n",
      "Training Epoch: 96 [9728/50000]\tLoss: 4.7008\tLR: 9.519182\n",
      "Training Epoch: 96 [9856/50000]\tLoss: 4.7134\tLR: 9.519437\n",
      "Training Epoch: 96 [9984/50000]\tLoss: 4.7454\tLR: 9.519693\n",
      "Training Epoch: 96 [10112/50000]\tLoss: 4.7541\tLR: 9.519949\n",
      "Training Epoch: 96 [10240/50000]\tLoss: 4.8355\tLR: 9.520205\n",
      "Training Epoch: 96 [10368/50000]\tLoss: 4.7044\tLR: 9.520460\n",
      "Training Epoch: 96 [10496/50000]\tLoss: 4.9126\tLR: 9.520716\n",
      "Training Epoch: 96 [10624/50000]\tLoss: 4.8742\tLR: 9.520972\n",
      "Training Epoch: 96 [10752/50000]\tLoss: 4.7269\tLR: 9.521228\n",
      "Training Epoch: 96 [10880/50000]\tLoss: 4.7558\tLR: 9.521483\n",
      "Training Epoch: 96 [11008/50000]\tLoss: 4.8087\tLR: 9.521739\n",
      "Training Epoch: 96 [11136/50000]\tLoss: 4.7917\tLR: 9.521995\n",
      "Training Epoch: 96 [11264/50000]\tLoss: 4.7941\tLR: 9.522251\n",
      "Training Epoch: 96 [11392/50000]\tLoss: 4.8519\tLR: 9.522506\n",
      "Training Epoch: 96 [11520/50000]\tLoss: 4.8630\tLR: 9.522762\n",
      "Training Epoch: 96 [11648/50000]\tLoss: 4.7774\tLR: 9.523018\n",
      "Training Epoch: 96 [11776/50000]\tLoss: 4.8476\tLR: 9.523274\n",
      "Training Epoch: 96 [11904/50000]\tLoss: 4.8345\tLR: 9.523529\n",
      "Training Epoch: 96 [12032/50000]\tLoss: 4.7320\tLR: 9.523785\n",
      "Training Epoch: 96 [12160/50000]\tLoss: 4.8649\tLR: 9.524041\n",
      "Training Epoch: 96 [12288/50000]\tLoss: 4.6978\tLR: 9.524297\n",
      "Training Epoch: 96 [12416/50000]\tLoss: 4.7289\tLR: 9.524552\n",
      "Training Epoch: 96 [12544/50000]\tLoss: 4.7964\tLR: 9.524808\n",
      "Training Epoch: 96 [12672/50000]\tLoss: 4.8629\tLR: 9.525064\n",
      "Training Epoch: 96 [12800/50000]\tLoss: 4.7072\tLR: 9.525320\n",
      "Training Epoch: 96 [12928/50000]\tLoss: 4.6976\tLR: 9.525575\n",
      "Training Epoch: 96 [13056/50000]\tLoss: 4.7185\tLR: 9.525831\n",
      "Training Epoch: 96 [13184/50000]\tLoss: 4.7421\tLR: 9.526087\n",
      "Training Epoch: 96 [13312/50000]\tLoss: 4.7869\tLR: 9.526343\n",
      "Training Epoch: 96 [13440/50000]\tLoss: 4.7530\tLR: 9.526598\n",
      "Training Epoch: 96 [13568/50000]\tLoss: 4.7066\tLR: 9.526854\n",
      "Training Epoch: 96 [13696/50000]\tLoss: 4.8656\tLR: 9.527110\n",
      "Training Epoch: 96 [13824/50000]\tLoss: 4.8105\tLR: 9.527366\n",
      "Training Epoch: 96 [13952/50000]\tLoss: 4.7239\tLR: 9.527621\n",
      "Training Epoch: 96 [14080/50000]\tLoss: 4.8015\tLR: 9.527877\n",
      "Training Epoch: 96 [14208/50000]\tLoss: 4.7687\tLR: 9.528133\n",
      "Training Epoch: 96 [14336/50000]\tLoss: 4.8549\tLR: 9.528389\n",
      "Training Epoch: 96 [14464/50000]\tLoss: 4.7668\tLR: 9.528645\n",
      "Training Epoch: 96 [14592/50000]\tLoss: 4.7767\tLR: 9.528900\n",
      "Training Epoch: 96 [14720/50000]\tLoss: 4.8081\tLR: 9.529156\n",
      "Training Epoch: 96 [14848/50000]\tLoss: 4.7988\tLR: 9.529412\n",
      "Training Epoch: 96 [14976/50000]\tLoss: 4.6539\tLR: 9.529668\n",
      "Training Epoch: 96 [15104/50000]\tLoss: 4.7027\tLR: 9.529923\n",
      "Training Epoch: 96 [15232/50000]\tLoss: 4.7326\tLR: 9.530179\n",
      "Training Epoch: 96 [15360/50000]\tLoss: 4.9369\tLR: 9.530435\n",
      "Training Epoch: 96 [15488/50000]\tLoss: 4.7765\tLR: 9.530691\n",
      "Training Epoch: 96 [15616/50000]\tLoss: 4.7804\tLR: 9.530946\n",
      "Training Epoch: 96 [15744/50000]\tLoss: 4.7828\tLR: 9.531202\n",
      "Training Epoch: 96 [15872/50000]\tLoss: 4.7092\tLR: 9.531458\n",
      "Training Epoch: 96 [16000/50000]\tLoss: 4.7424\tLR: 9.531714\n",
      "Training Epoch: 96 [16128/50000]\tLoss: 4.7932\tLR: 9.531969\n",
      "Training Epoch: 96 [16256/50000]\tLoss: 4.8079\tLR: 9.532225\n",
      "Training Epoch: 96 [16384/50000]\tLoss: 4.7818\tLR: 9.532481\n",
      "Training Epoch: 96 [16512/50000]\tLoss: 4.7107\tLR: 9.532737\n",
      "Training Epoch: 96 [16640/50000]\tLoss: 4.7043\tLR: 9.532992\n",
      "Training Epoch: 96 [16768/50000]\tLoss: 4.8507\tLR: 9.533248\n",
      "Training Epoch: 96 [16896/50000]\tLoss: 4.8115\tLR: 9.533504\n",
      "Training Epoch: 96 [17024/50000]\tLoss: 4.7826\tLR: 9.533760\n",
      "Training Epoch: 96 [17152/50000]\tLoss: 4.8158\tLR: 9.534015\n",
      "Training Epoch: 96 [17280/50000]\tLoss: 4.8617\tLR: 9.534271\n",
      "Training Epoch: 96 [17408/50000]\tLoss: 4.7471\tLR: 9.534527\n",
      "Training Epoch: 96 [17536/50000]\tLoss: 4.8351\tLR: 9.534783\n",
      "Training Epoch: 96 [17664/50000]\tLoss: 4.7783\tLR: 9.535038\n",
      "Training Epoch: 96 [17792/50000]\tLoss: 4.8288\tLR: 9.535294\n",
      "Training Epoch: 96 [17920/50000]\tLoss: 4.7538\tLR: 9.535550\n",
      "Training Epoch: 96 [18048/50000]\tLoss: 4.8537\tLR: 9.535806\n",
      "Training Epoch: 96 [18176/50000]\tLoss: 4.6978\tLR: 9.536061\n",
      "Training Epoch: 96 [18304/50000]\tLoss: 4.6964\tLR: 9.536317\n",
      "Training Epoch: 96 [18432/50000]\tLoss: 4.7796\tLR: 9.536573\n",
      "Training Epoch: 96 [18560/50000]\tLoss: 4.7633\tLR: 9.536829\n",
      "Training Epoch: 96 [18688/50000]\tLoss: 4.6974\tLR: 9.537084\n",
      "Training Epoch: 96 [18816/50000]\tLoss: 4.7786\tLR: 9.537340\n",
      "Training Epoch: 96 [18944/50000]\tLoss: 4.7252\tLR: 9.537596\n",
      "Training Epoch: 96 [19072/50000]\tLoss: 4.8328\tLR: 9.537852\n",
      "Training Epoch: 96 [19200/50000]\tLoss: 4.8061\tLR: 9.538107\n",
      "Training Epoch: 96 [19328/50000]\tLoss: 4.8127\tLR: 9.538363\n",
      "Training Epoch: 96 [19456/50000]\tLoss: 4.7275\tLR: 9.538619\n",
      "Training Epoch: 96 [19584/50000]\tLoss: 4.8127\tLR: 9.538875\n",
      "Training Epoch: 96 [19712/50000]\tLoss: 4.8559\tLR: 9.539130\n",
      "Training Epoch: 96 [19840/50000]\tLoss: 4.7973\tLR: 9.539386\n",
      "Training Epoch: 96 [19968/50000]\tLoss: 4.8003\tLR: 9.539642\n",
      "Training Epoch: 96 [20096/50000]\tLoss: 4.8630\tLR: 9.539898\n",
      "Training Epoch: 96 [20224/50000]\tLoss: 4.7076\tLR: 9.540153\n",
      "Training Epoch: 96 [20352/50000]\tLoss: 4.8401\tLR: 9.540409\n",
      "Training Epoch: 96 [20480/50000]\tLoss: 4.7238\tLR: 9.540665\n",
      "Training Epoch: 96 [20608/50000]\tLoss: 4.7683\tLR: 9.540921\n",
      "Training Epoch: 96 [20736/50000]\tLoss: 4.8124\tLR: 9.541176\n",
      "Training Epoch: 96 [20864/50000]\tLoss: 4.7436\tLR: 9.541432\n",
      "Training Epoch: 96 [20992/50000]\tLoss: 4.8419\tLR: 9.541688\n",
      "Training Epoch: 96 [21120/50000]\tLoss: 4.8169\tLR: 9.541944\n",
      "Training Epoch: 96 [21248/50000]\tLoss: 4.8423\tLR: 9.542199\n",
      "Training Epoch: 96 [21376/50000]\tLoss: 4.7273\tLR: 9.542455\n",
      "Training Epoch: 96 [21504/50000]\tLoss: 4.6830\tLR: 9.542711\n",
      "Training Epoch: 96 [21632/50000]\tLoss: 4.9566\tLR: 9.542967\n",
      "Training Epoch: 96 [21760/50000]\tLoss: 4.7025\tLR: 9.543223\n",
      "Training Epoch: 96 [21888/50000]\tLoss: 4.7486\tLR: 9.543478\n",
      "Training Epoch: 96 [22016/50000]\tLoss: 4.6716\tLR: 9.543734\n",
      "Training Epoch: 96 [22144/50000]\tLoss: 4.7474\tLR: 9.543990\n",
      "Training Epoch: 96 [22272/50000]\tLoss: 4.7940\tLR: 9.544246\n",
      "Training Epoch: 96 [22400/50000]\tLoss: 4.7364\tLR: 9.544501\n",
      "Training Epoch: 96 [22528/50000]\tLoss: 4.7777\tLR: 9.544757\n",
      "Training Epoch: 96 [22656/50000]\tLoss: 4.8456\tLR: 9.545013\n",
      "Training Epoch: 96 [22784/50000]\tLoss: 4.7533\tLR: 9.545269\n",
      "Training Epoch: 96 [22912/50000]\tLoss: 4.8261\tLR: 9.545524\n",
      "Training Epoch: 96 [23040/50000]\tLoss: 4.8744\tLR: 9.545780\n",
      "Training Epoch: 96 [23168/50000]\tLoss: 4.7974\tLR: 9.546036\n",
      "Training Epoch: 96 [23296/50000]\tLoss: 4.8238\tLR: 9.546292\n",
      "Training Epoch: 96 [23424/50000]\tLoss: 4.8729\tLR: 9.546547\n",
      "Training Epoch: 96 [23552/50000]\tLoss: 4.7852\tLR: 9.546803\n",
      "Training Epoch: 96 [23680/50000]\tLoss: 4.7635\tLR: 9.547059\n",
      "Training Epoch: 96 [23808/50000]\tLoss: 4.8613\tLR: 9.547315\n",
      "Training Epoch: 96 [23936/50000]\tLoss: 4.6907\tLR: 9.547570\n",
      "Training Epoch: 96 [24064/50000]\tLoss: 4.8902\tLR: 9.547826\n",
      "Training Epoch: 96 [24192/50000]\tLoss: 4.7248\tLR: 9.548082\n",
      "Training Epoch: 96 [24320/50000]\tLoss: 4.8837\tLR: 9.548338\n",
      "Training Epoch: 96 [24448/50000]\tLoss: 4.7903\tLR: 9.548593\n",
      "Training Epoch: 96 [24576/50000]\tLoss: 4.8123\tLR: 9.548849\n",
      "Training Epoch: 96 [24704/50000]\tLoss: 4.7581\tLR: 9.549105\n",
      "Training Epoch: 96 [24832/50000]\tLoss: 4.7288\tLR: 9.549361\n",
      "Training Epoch: 96 [24960/50000]\tLoss: 4.7979\tLR: 9.549616\n",
      "Training Epoch: 96 [25088/50000]\tLoss: 4.9253\tLR: 9.549872\n",
      "Training Epoch: 96 [25216/50000]\tLoss: 4.8389\tLR: 9.550128\n",
      "Training Epoch: 96 [25344/50000]\tLoss: 4.7617\tLR: 9.550384\n",
      "Training Epoch: 96 [25472/50000]\tLoss: 4.7556\tLR: 9.550639\n",
      "Training Epoch: 96 [25600/50000]\tLoss: 4.9251\tLR: 9.550895\n",
      "Training Epoch: 96 [25728/50000]\tLoss: 4.7585\tLR: 9.551151\n",
      "Training Epoch: 96 [25856/50000]\tLoss: 4.7325\tLR: 9.551407\n",
      "Training Epoch: 96 [25984/50000]\tLoss: 4.7152\tLR: 9.551662\n",
      "Training Epoch: 96 [26112/50000]\tLoss: 4.8725\tLR: 9.551918\n",
      "Training Epoch: 96 [26240/50000]\tLoss: 4.8309\tLR: 9.552174\n",
      "Training Epoch: 96 [26368/50000]\tLoss: 4.8484\tLR: 9.552430\n",
      "Training Epoch: 96 [26496/50000]\tLoss: 4.7466\tLR: 9.552685\n",
      "Training Epoch: 96 [26624/50000]\tLoss: 4.7985\tLR: 9.552941\n",
      "Training Epoch: 96 [26752/50000]\tLoss: 4.8087\tLR: 9.553197\n",
      "Training Epoch: 96 [26880/50000]\tLoss: 4.9238\tLR: 9.553453\n",
      "Training Epoch: 96 [27008/50000]\tLoss: 4.8065\tLR: 9.553708\n",
      "Training Epoch: 96 [27136/50000]\tLoss: 4.7247\tLR: 9.553964\n",
      "Training Epoch: 96 [27264/50000]\tLoss: 4.8599\tLR: 9.554220\n",
      "Training Epoch: 96 [27392/50000]\tLoss: 4.8031\tLR: 9.554476\n",
      "Training Epoch: 96 [27520/50000]\tLoss: 4.6969\tLR: 9.554731\n",
      "Training Epoch: 96 [27648/50000]\tLoss: 5.1176\tLR: 9.554987\n",
      "Training Epoch: 96 [27776/50000]\tLoss: 4.8773\tLR: 9.555243\n",
      "Training Epoch: 96 [27904/50000]\tLoss: 4.7216\tLR: 9.555499\n",
      "Training Epoch: 96 [28032/50000]\tLoss: 4.6577\tLR: 9.555754\n",
      "Training Epoch: 96 [28160/50000]\tLoss: 4.7335\tLR: 9.556010\n",
      "Training Epoch: 96 [28288/50000]\tLoss: 4.7828\tLR: 9.556266\n",
      "Training Epoch: 96 [28416/50000]\tLoss: 4.6932\tLR: 9.556522\n",
      "Training Epoch: 96 [28544/50000]\tLoss: 4.6842\tLR: 9.556777\n",
      "Training Epoch: 96 [28672/50000]\tLoss: 4.7640\tLR: 9.557033\n",
      "Training Epoch: 96 [28800/50000]\tLoss: 4.8336\tLR: 9.557289\n",
      "Training Epoch: 96 [28928/50000]\tLoss: 4.7664\tLR: 9.557545\n",
      "Training Epoch: 96 [29056/50000]\tLoss: 4.7423\tLR: 9.557801\n",
      "Training Epoch: 96 [29184/50000]\tLoss: 4.7762\tLR: 9.558056\n",
      "Training Epoch: 96 [29312/50000]\tLoss: 4.6630\tLR: 9.558312\n",
      "Training Epoch: 96 [29440/50000]\tLoss: 4.8940\tLR: 9.558568\n",
      "Training Epoch: 96 [29568/50000]\tLoss: 4.7918\tLR: 9.558824\n",
      "Training Epoch: 96 [29696/50000]\tLoss: 4.8015\tLR: 9.559079\n",
      "Training Epoch: 96 [29824/50000]\tLoss: 4.8342\tLR: 9.559335\n",
      "Training Epoch: 96 [29952/50000]\tLoss: 4.7607\tLR: 9.559591\n",
      "Training Epoch: 96 [30080/50000]\tLoss: 4.7338\tLR: 9.559847\n",
      "Training Epoch: 96 [30208/50000]\tLoss: 4.8463\tLR: 9.560102\n",
      "Training Epoch: 96 [30336/50000]\tLoss: 4.9084\tLR: 9.560358\n",
      "Training Epoch: 96 [30464/50000]\tLoss: 4.7609\tLR: 9.560614\n",
      "Training Epoch: 96 [30592/50000]\tLoss: 4.8069\tLR: 9.560870\n",
      "Training Epoch: 96 [30720/50000]\tLoss: 4.7869\tLR: 9.561125\n",
      "Training Epoch: 96 [30848/50000]\tLoss: 4.8226\tLR: 9.561381\n",
      "Training Epoch: 96 [30976/50000]\tLoss: 4.8506\tLR: 9.561637\n",
      "Training Epoch: 96 [31104/50000]\tLoss: 4.7793\tLR: 9.561893\n",
      "Training Epoch: 96 [31232/50000]\tLoss: 4.7189\tLR: 9.562148\n",
      "Training Epoch: 96 [31360/50000]\tLoss: 4.8956\tLR: 9.562404\n",
      "Training Epoch: 96 [31488/50000]\tLoss: 4.7036\tLR: 9.562660\n",
      "Training Epoch: 96 [31616/50000]\tLoss: 4.7124\tLR: 9.562916\n",
      "Training Epoch: 96 [31744/50000]\tLoss: 4.7549\tLR: 9.563171\n",
      "Training Epoch: 96 [31872/50000]\tLoss: 4.7894\tLR: 9.563427\n",
      "Training Epoch: 96 [32000/50000]\tLoss: 4.7155\tLR: 9.563683\n",
      "Training Epoch: 96 [32128/50000]\tLoss: 4.8533\tLR: 9.563939\n",
      "Training Epoch: 96 [32256/50000]\tLoss: 4.6669\tLR: 9.564194\n",
      "Training Epoch: 96 [32384/50000]\tLoss: 4.7449\tLR: 9.564450\n",
      "Training Epoch: 96 [32512/50000]\tLoss: 4.7219\tLR: 9.564706\n",
      "Training Epoch: 96 [32640/50000]\tLoss: 4.7452\tLR: 9.564962\n",
      "Training Epoch: 96 [32768/50000]\tLoss: 4.8187\tLR: 9.565217\n",
      "Training Epoch: 96 [32896/50000]\tLoss: 4.7199\tLR: 9.565473\n",
      "Training Epoch: 96 [33024/50000]\tLoss: 4.8429\tLR: 9.565729\n",
      "Training Epoch: 96 [33152/50000]\tLoss: 4.8131\tLR: 9.565985\n",
      "Training Epoch: 96 [33280/50000]\tLoss: 4.8443\tLR: 9.566240\n",
      "Training Epoch: 96 [33408/50000]\tLoss: 4.8069\tLR: 9.566496\n",
      "Training Epoch: 96 [33536/50000]\tLoss: 4.8357\tLR: 9.566752\n",
      "Training Epoch: 96 [33664/50000]\tLoss: 4.7151\tLR: 9.567008\n",
      "Training Epoch: 96 [33792/50000]\tLoss: 4.7451\tLR: 9.567263\n",
      "Training Epoch: 96 [33920/50000]\tLoss: 4.8589\tLR: 9.567519\n",
      "Training Epoch: 96 [34048/50000]\tLoss: 4.8689\tLR: 9.567775\n",
      "Training Epoch: 96 [34176/50000]\tLoss: 4.7688\tLR: 9.568031\n",
      "Training Epoch: 96 [34304/50000]\tLoss: 4.7017\tLR: 9.568286\n",
      "Training Epoch: 96 [34432/50000]\tLoss: 4.8209\tLR: 9.568542\n",
      "Training Epoch: 96 [34560/50000]\tLoss: 4.7312\tLR: 9.568798\n",
      "Training Epoch: 96 [34688/50000]\tLoss: 4.8011\tLR: 9.569054\n",
      "Training Epoch: 96 [34816/50000]\tLoss: 4.7500\tLR: 9.569309\n",
      "Training Epoch: 96 [34944/50000]\tLoss: 4.9002\tLR: 9.569565\n",
      "Training Epoch: 96 [35072/50000]\tLoss: 4.8171\tLR: 9.569821\n",
      "Training Epoch: 96 [35200/50000]\tLoss: 4.6884\tLR: 9.570077\n",
      "Training Epoch: 96 [35328/50000]\tLoss: 4.7888\tLR: 9.570332\n",
      "Training Epoch: 96 [35456/50000]\tLoss: 4.8175\tLR: 9.570588\n",
      "Training Epoch: 96 [35584/50000]\tLoss: 4.8445\tLR: 9.570844\n",
      "Training Epoch: 96 [35712/50000]\tLoss: 4.7477\tLR: 9.571100\n",
      "Training Epoch: 96 [35840/50000]\tLoss: 4.7802\tLR: 9.571355\n",
      "Training Epoch: 96 [35968/50000]\tLoss: 4.7593\tLR: 9.571611\n",
      "Training Epoch: 96 [36096/50000]\tLoss: 4.8285\tLR: 9.571867\n",
      "Training Epoch: 96 [36224/50000]\tLoss: 4.8930\tLR: 9.572123\n",
      "Training Epoch: 96 [36352/50000]\tLoss: 4.7510\tLR: 9.572379\n",
      "Training Epoch: 96 [36480/50000]\tLoss: 4.7543\tLR: 9.572634\n",
      "Training Epoch: 96 [36608/50000]\tLoss: 4.7874\tLR: 9.572890\n",
      "Training Epoch: 96 [36736/50000]\tLoss: 4.7506\tLR: 9.573146\n",
      "Training Epoch: 96 [36864/50000]\tLoss: 4.8072\tLR: 9.573402\n",
      "Training Epoch: 96 [36992/50000]\tLoss: 4.8279\tLR: 9.573657\n",
      "Training Epoch: 96 [37120/50000]\tLoss: 4.7702\tLR: 9.573913\n",
      "Training Epoch: 96 [37248/50000]\tLoss: 4.7040\tLR: 9.574169\n",
      "Training Epoch: 96 [37376/50000]\tLoss: 4.6651\tLR: 9.574425\n",
      "Training Epoch: 96 [37504/50000]\tLoss: 4.7982\tLR: 9.574680\n",
      "Training Epoch: 96 [37632/50000]\tLoss: 4.7525\tLR: 9.574936\n",
      "Training Epoch: 96 [37760/50000]\tLoss: 4.7798\tLR: 9.575192\n",
      "Training Epoch: 96 [37888/50000]\tLoss: 4.7941\tLR: 9.575448\n",
      "Training Epoch: 96 [38016/50000]\tLoss: 4.7523\tLR: 9.575703\n",
      "Training Epoch: 96 [38144/50000]\tLoss: 4.8091\tLR: 9.575959\n",
      "Training Epoch: 96 [38272/50000]\tLoss: 4.6863\tLR: 9.576215\n",
      "Training Epoch: 96 [38400/50000]\tLoss: 4.8597\tLR: 9.576471\n",
      "Training Epoch: 96 [38528/50000]\tLoss: 4.6660\tLR: 9.576726\n",
      "Training Epoch: 96 [38656/50000]\tLoss: 4.6897\tLR: 9.576982\n",
      "Training Epoch: 96 [38784/50000]\tLoss: 4.6430\tLR: 9.577238\n",
      "Training Epoch: 96 [38912/50000]\tLoss: 4.8816\tLR: 9.577494\n",
      "Training Epoch: 96 [39040/50000]\tLoss: 4.7412\tLR: 9.577749\n",
      "Training Epoch: 96 [39168/50000]\tLoss: 4.7499\tLR: 9.578005\n",
      "Training Epoch: 96 [39296/50000]\tLoss: 4.7508\tLR: 9.578261\n",
      "Training Epoch: 96 [39424/50000]\tLoss: 4.7836\tLR: 9.578517\n",
      "Training Epoch: 96 [39552/50000]\tLoss: 4.7382\tLR: 9.578772\n",
      "Training Epoch: 96 [39680/50000]\tLoss: 4.7267\tLR: 9.579028\n",
      "Training Epoch: 96 [39808/50000]\tLoss: 4.8336\tLR: 9.579284\n",
      "Training Epoch: 96 [39936/50000]\tLoss: 4.7872\tLR: 9.579540\n",
      "Training Epoch: 96 [40064/50000]\tLoss: 4.8918\tLR: 9.579795\n",
      "Training Epoch: 96 [40192/50000]\tLoss: 4.8177\tLR: 9.580051\n",
      "Training Epoch: 96 [40320/50000]\tLoss: 4.7071\tLR: 9.580307\n",
      "Training Epoch: 96 [40448/50000]\tLoss: 5.1097\tLR: 9.580563\n",
      "Training Epoch: 96 [40576/50000]\tLoss: 4.7876\tLR: 9.580818\n",
      "Training Epoch: 96 [40704/50000]\tLoss: 4.7074\tLR: 9.581074\n",
      "Training Epoch: 96 [40832/50000]\tLoss: 4.7432\tLR: 9.581330\n",
      "Training Epoch: 96 [40960/50000]\tLoss: 4.7610\tLR: 9.581586\n",
      "Training Epoch: 96 [41088/50000]\tLoss: 4.7624\tLR: 9.581841\n",
      "Training Epoch: 96 [41216/50000]\tLoss: 4.7101\tLR: 9.582097\n",
      "Training Epoch: 96 [41344/50000]\tLoss: 4.7382\tLR: 9.582353\n",
      "Training Epoch: 96 [41472/50000]\tLoss: 4.7431\tLR: 9.582609\n",
      "Training Epoch: 96 [41600/50000]\tLoss: 4.7893\tLR: 9.582864\n",
      "Training Epoch: 96 [41728/50000]\tLoss: 4.7250\tLR: 9.583120\n",
      "Training Epoch: 96 [41856/50000]\tLoss: 4.8321\tLR: 9.583376\n",
      "Training Epoch: 96 [41984/50000]\tLoss: 4.8238\tLR: 9.583632\n",
      "Training Epoch: 96 [42112/50000]\tLoss: 4.8031\tLR: 9.583887\n",
      "Training Epoch: 96 [42240/50000]\tLoss: 4.7841\tLR: 9.584143\n",
      "Training Epoch: 96 [42368/50000]\tLoss: 4.9302\tLR: 9.584399\n",
      "Training Epoch: 96 [42496/50000]\tLoss: 4.6899\tLR: 9.584655\n",
      "Training Epoch: 96 [42624/50000]\tLoss: 4.8313\tLR: 9.584910\n",
      "Training Epoch: 96 [42752/50000]\tLoss: 4.7291\tLR: 9.585166\n",
      "Training Epoch: 96 [42880/50000]\tLoss: 4.8160\tLR: 9.585422\n",
      "Training Epoch: 96 [43008/50000]\tLoss: 4.7499\tLR: 9.585678\n",
      "Training Epoch: 96 [43136/50000]\tLoss: 4.7526\tLR: 9.585934\n",
      "Training Epoch: 96 [43264/50000]\tLoss: 4.7428\tLR: 9.586189\n",
      "Training Epoch: 96 [43392/50000]\tLoss: 4.7411\tLR: 9.586445\n",
      "Training Epoch: 96 [43520/50000]\tLoss: 4.7495\tLR: 9.586701\n",
      "Training Epoch: 96 [43648/50000]\tLoss: 4.7834\tLR: 9.586957\n",
      "Training Epoch: 96 [43776/50000]\tLoss: 4.7540\tLR: 9.587212\n",
      "Training Epoch: 96 [43904/50000]\tLoss: 4.8447\tLR: 9.587468\n",
      "Training Epoch: 96 [44032/50000]\tLoss: 4.7244\tLR: 9.587724\n",
      "Training Epoch: 96 [44160/50000]\tLoss: 4.7242\tLR: 9.587980\n",
      "Training Epoch: 96 [44288/50000]\tLoss: 4.7725\tLR: 9.588235\n",
      "Training Epoch: 96 [44416/50000]\tLoss: 4.6608\tLR: 9.588491\n",
      "Training Epoch: 96 [44544/50000]\tLoss: 4.6788\tLR: 9.588747\n",
      "Training Epoch: 96 [44672/50000]\tLoss: 4.8988\tLR: 9.589003\n",
      "Training Epoch: 96 [44800/50000]\tLoss: 4.7851\tLR: 9.589258\n",
      "Training Epoch: 96 [44928/50000]\tLoss: 4.8579\tLR: 9.589514\n",
      "Training Epoch: 96 [45056/50000]\tLoss: 4.8643\tLR: 9.589770\n",
      "Training Epoch: 96 [45184/50000]\tLoss: 4.7760\tLR: 9.590026\n",
      "Training Epoch: 96 [45312/50000]\tLoss: 4.7220\tLR: 9.590281\n",
      "Training Epoch: 96 [45440/50000]\tLoss: 4.7407\tLR: 9.590537\n",
      "Training Epoch: 96 [45568/50000]\tLoss: 4.7082\tLR: 9.590793\n",
      "Training Epoch: 96 [45696/50000]\tLoss: 4.8276\tLR: 9.591049\n",
      "Training Epoch: 96 [45824/50000]\tLoss: 4.7972\tLR: 9.591304\n",
      "Training Epoch: 96 [45952/50000]\tLoss: 4.7423\tLR: 9.591560\n",
      "Training Epoch: 96 [46080/50000]\tLoss: 4.9370\tLR: 9.591816\n",
      "Training Epoch: 96 [46208/50000]\tLoss: 4.9360\tLR: 9.592072\n",
      "Training Epoch: 96 [46336/50000]\tLoss: 4.7421\tLR: 9.592327\n",
      "Training Epoch: 96 [46464/50000]\tLoss: 4.7378\tLR: 9.592583\n",
      "Training Epoch: 96 [46592/50000]\tLoss: 4.7132\tLR: 9.592839\n",
      "Training Epoch: 96 [46720/50000]\tLoss: 4.7404\tLR: 9.593095\n",
      "Training Epoch: 96 [46848/50000]\tLoss: 4.7743\tLR: 9.593350\n",
      "Training Epoch: 96 [46976/50000]\tLoss: 4.7202\tLR: 9.593606\n",
      "Training Epoch: 96 [47104/50000]\tLoss: 4.7391\tLR: 9.593862\n",
      "Training Epoch: 96 [47232/50000]\tLoss: 4.7843\tLR: 9.594118\n",
      "Training Epoch: 96 [47360/50000]\tLoss: 4.7592\tLR: 9.594373\n",
      "Training Epoch: 96 [47488/50000]\tLoss: 4.7493\tLR: 9.594629\n",
      "Training Epoch: 96 [47616/50000]\tLoss: 4.7716\tLR: 9.594885\n",
      "Training Epoch: 96 [47744/50000]\tLoss: 4.7802\tLR: 9.595141\n",
      "Training Epoch: 96 [47872/50000]\tLoss: 4.7908\tLR: 9.595396\n",
      "Training Epoch: 96 [48000/50000]\tLoss: 4.6897\tLR: 9.595652\n",
      "Training Epoch: 96 [48128/50000]\tLoss: 4.8069\tLR: 9.595908\n",
      "Training Epoch: 96 [48256/50000]\tLoss: 4.8774\tLR: 9.596164\n",
      "Training Epoch: 96 [48384/50000]\tLoss: 4.8275\tLR: 9.596419\n",
      "Training Epoch: 96 [48512/50000]\tLoss: 4.8241\tLR: 9.596675\n",
      "Training Epoch: 96 [48640/50000]\tLoss: 4.8324\tLR: 9.596931\n",
      "Training Epoch: 96 [48768/50000]\tLoss: 4.7574\tLR: 9.597187\n",
      "Training Epoch: 96 [48896/50000]\tLoss: 4.6992\tLR: 9.597442\n",
      "Training Epoch: 96 [49024/50000]\tLoss: 4.7799\tLR: 9.597698\n",
      "Training Epoch: 96 [49152/50000]\tLoss: 4.7661\tLR: 9.597954\n",
      "Training Epoch: 96 [49280/50000]\tLoss: 4.6958\tLR: 9.598210\n",
      "Training Epoch: 96 [49408/50000]\tLoss: 4.8278\tLR: 9.598465\n",
      "Training Epoch: 96 [49536/50000]\tLoss: 4.7704\tLR: 9.598721\n",
      "Training Epoch: 96 [49664/50000]\tLoss: 4.7693\tLR: 9.598977\n",
      "Training Epoch: 96 [49792/50000]\tLoss: 4.7172\tLR: 9.599233\n",
      "Training Epoch: 96 [49920/50000]\tLoss: 4.7862\tLR: 9.599488\n",
      "Training Epoch: 96 [50000/50000]\tLoss: 4.9306\tLR: 9.599744\n",
      "epoch 96 training time consumed: 496.99s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  134585 GB |  134585 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  134172 GB |  134172 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     413 GB |     413 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  134585 GB |  134585 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  134172 GB |  134172 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     413 GB |     413 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  132694 GB |  132694 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  132281 GB |  132281 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     413 GB |     413 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   14270 K  |   14270 K  |\n",
      "|       from large pool |      24    |      65    |    6083 K  |    6083 K  |\n",
      "|       from small pool |     231    |     274    |    8187 K  |    8186 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   14270 K  |   14270 K  |\n",
      "|       from large pool |      24    |      65    |    6083 K  |    6083 K  |\n",
      "|       from small pool |     231    |     274    |    8187 K  |    8186 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      47    |    8271 K  |    8271 K  |\n",
      "|       from large pool |      10    |      23    |    2924 K  |    2924 K  |\n",
      "|       from small pool |      27    |      35    |    5347 K  |    5347 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 96, Average loss: 0.0377, Accuracy: 0.0100, Time consumed:31.42s\n",
      "\n",
      "Training Epoch: 97 [128/50000]\tLoss: 4.7070\tLR: 0.020000\n",
      "Training Epoch: 97 [256/50000]\tLoss: 4.8111\tLR: 9.600256\n",
      "Training Epoch: 97 [384/50000]\tLoss: 4.6828\tLR: 9.600512\n",
      "Training Epoch: 97 [512/50000]\tLoss: 4.6953\tLR: 9.600767\n",
      "Training Epoch: 97 [640/50000]\tLoss: 4.7697\tLR: 9.601023\n",
      "Training Epoch: 97 [768/50000]\tLoss: 4.7316\tLR: 9.601279\n",
      "Training Epoch: 97 [896/50000]\tLoss: 4.8133\tLR: 9.601535\n",
      "Training Epoch: 97 [1024/50000]\tLoss: 4.7427\tLR: 9.601790\n",
      "Training Epoch: 97 [1152/50000]\tLoss: 4.7338\tLR: 9.602046\n",
      "Training Epoch: 97 [1280/50000]\tLoss: 4.8237\tLR: 9.602302\n",
      "Training Epoch: 97 [1408/50000]\tLoss: 4.8784\tLR: 9.602558\n",
      "Training Epoch: 97 [1536/50000]\tLoss: 4.8044\tLR: 9.602813\n",
      "Training Epoch: 97 [1664/50000]\tLoss: 4.7513\tLR: 9.603069\n",
      "Training Epoch: 97 [1792/50000]\tLoss: 4.7751\tLR: 9.603325\n",
      "Training Epoch: 97 [1920/50000]\tLoss: 4.7212\tLR: 9.603581\n",
      "Training Epoch: 97 [2048/50000]\tLoss: 4.9347\tLR: 9.603836\n",
      "Training Epoch: 97 [2176/50000]\tLoss: 4.8285\tLR: 9.604092\n",
      "Training Epoch: 97 [2304/50000]\tLoss: 4.7968\tLR: 9.604348\n",
      "Training Epoch: 97 [2432/50000]\tLoss: 4.7624\tLR: 9.604604\n",
      "Training Epoch: 97 [2560/50000]\tLoss: 4.7354\tLR: 9.604859\n",
      "Training Epoch: 97 [2688/50000]\tLoss: 4.7338\tLR: 9.605115\n",
      "Training Epoch: 97 [2816/50000]\tLoss: 4.8241\tLR: 9.605371\n",
      "Training Epoch: 97 [2944/50000]\tLoss: 4.8238\tLR: 9.605627\n",
      "Training Epoch: 97 [3072/50000]\tLoss: 4.8914\tLR: 9.605882\n",
      "Training Epoch: 97 [3200/50000]\tLoss: 4.8411\tLR: 9.606138\n",
      "Training Epoch: 97 [3328/50000]\tLoss: 4.6690\tLR: 9.606394\n",
      "Training Epoch: 97 [3456/50000]\tLoss: 4.8279\tLR: 9.606650\n",
      "Training Epoch: 97 [3584/50000]\tLoss: 4.8844\tLR: 9.606905\n",
      "Training Epoch: 97 [3712/50000]\tLoss: 4.7610\tLR: 9.607161\n",
      "Training Epoch: 97 [3840/50000]\tLoss: 4.7585\tLR: 9.607417\n",
      "Training Epoch: 97 [3968/50000]\tLoss: 4.7735\tLR: 9.607673\n",
      "Training Epoch: 97 [4096/50000]\tLoss: 4.7738\tLR: 9.607928\n",
      "Training Epoch: 97 [4224/50000]\tLoss: 4.8237\tLR: 9.608184\n",
      "Training Epoch: 97 [4352/50000]\tLoss: 4.9154\tLR: 9.608440\n",
      "Training Epoch: 97 [4480/50000]\tLoss: 4.8504\tLR: 9.608696\n",
      "Training Epoch: 97 [4608/50000]\tLoss: 4.7407\tLR: 9.608951\n",
      "Training Epoch: 97 [4736/50000]\tLoss: 4.7868\tLR: 9.609207\n",
      "Training Epoch: 97 [4864/50000]\tLoss: 4.8927\tLR: 9.609463\n",
      "Training Epoch: 97 [4992/50000]\tLoss: 4.8057\tLR: 9.609719\n",
      "Training Epoch: 97 [5120/50000]\tLoss: 4.7786\tLR: 9.609974\n",
      "Training Epoch: 97 [5248/50000]\tLoss: 4.8265\tLR: 9.610230\n",
      "Training Epoch: 97 [5376/50000]\tLoss: 4.7768\tLR: 9.610486\n",
      "Training Epoch: 97 [5504/50000]\tLoss: 4.8436\tLR: 9.610742\n",
      "Training Epoch: 97 [5632/50000]\tLoss: 4.8694\tLR: 9.610997\n",
      "Training Epoch: 97 [5760/50000]\tLoss: 4.8678\tLR: 9.611253\n",
      "Training Epoch: 97 [5888/50000]\tLoss: 4.6767\tLR: 9.611509\n",
      "Training Epoch: 97 [6016/50000]\tLoss: 4.8749\tLR: 9.611765\n",
      "Training Epoch: 97 [6144/50000]\tLoss: 4.8080\tLR: 9.612020\n",
      "Training Epoch: 97 [6272/50000]\tLoss: 4.8062\tLR: 9.612276\n",
      "Training Epoch: 97 [6400/50000]\tLoss: 4.7978\tLR: 9.612532\n",
      "Training Epoch: 97 [6528/50000]\tLoss: 4.8191\tLR: 9.612788\n",
      "Training Epoch: 97 [6656/50000]\tLoss: 4.7472\tLR: 9.613043\n",
      "Training Epoch: 97 [6784/50000]\tLoss: 4.7425\tLR: 9.613299\n",
      "Training Epoch: 97 [6912/50000]\tLoss: 4.8615\tLR: 9.613555\n",
      "Training Epoch: 97 [7040/50000]\tLoss: 4.8700\tLR: 9.613811\n",
      "Training Epoch: 97 [7168/50000]\tLoss: 4.8089\tLR: 9.614066\n",
      "Training Epoch: 97 [7296/50000]\tLoss: 4.8186\tLR: 9.614322\n",
      "Training Epoch: 97 [7424/50000]\tLoss: 4.9091\tLR: 9.614578\n",
      "Training Epoch: 97 [7552/50000]\tLoss: 4.8193\tLR: 9.614834\n",
      "Training Epoch: 97 [7680/50000]\tLoss: 4.6880\tLR: 9.615090\n",
      "Training Epoch: 97 [7808/50000]\tLoss: 4.7466\tLR: 9.615345\n",
      "Training Epoch: 97 [7936/50000]\tLoss: 4.7295\tLR: 9.615601\n",
      "Training Epoch: 97 [8064/50000]\tLoss: 4.8655\tLR: 9.615857\n",
      "Training Epoch: 97 [8192/50000]\tLoss: 4.7662\tLR: 9.616113\n",
      "Training Epoch: 97 [8320/50000]\tLoss: 4.8077\tLR: 9.616368\n",
      "Training Epoch: 97 [8448/50000]\tLoss: 4.8452\tLR: 9.616624\n",
      "Training Epoch: 97 [8576/50000]\tLoss: 4.8015\tLR: 9.616880\n",
      "Training Epoch: 97 [8704/50000]\tLoss: 4.8412\tLR: 9.617136\n",
      "Training Epoch: 97 [8832/50000]\tLoss: 4.7883\tLR: 9.617391\n",
      "Training Epoch: 97 [8960/50000]\tLoss: 4.8082\tLR: 9.617647\n",
      "Training Epoch: 97 [9088/50000]\tLoss: 4.8732\tLR: 9.617903\n",
      "Training Epoch: 97 [9216/50000]\tLoss: 4.8376\tLR: 9.618159\n",
      "Training Epoch: 97 [9344/50000]\tLoss: 4.7172\tLR: 9.618414\n",
      "Training Epoch: 97 [9472/50000]\tLoss: 4.6963\tLR: 9.618670\n",
      "Training Epoch: 97 [9600/50000]\tLoss: 4.6738\tLR: 9.618926\n",
      "Training Epoch: 97 [9728/50000]\tLoss: 4.7418\tLR: 9.619182\n",
      "Training Epoch: 97 [9856/50000]\tLoss: 4.7310\tLR: 9.619437\n",
      "Training Epoch: 97 [9984/50000]\tLoss: 4.7091\tLR: 9.619693\n",
      "Training Epoch: 97 [10112/50000]\tLoss: 4.8262\tLR: 9.619949\n",
      "Training Epoch: 97 [10240/50000]\tLoss: 4.7897\tLR: 9.620205\n",
      "Training Epoch: 97 [10368/50000]\tLoss: 4.7895\tLR: 9.620460\n",
      "Training Epoch: 97 [10496/50000]\tLoss: 4.8725\tLR: 9.620716\n",
      "Training Epoch: 97 [10624/50000]\tLoss: 4.7173\tLR: 9.620972\n",
      "Training Epoch: 97 [10752/50000]\tLoss: 4.7422\tLR: 9.621228\n",
      "Training Epoch: 97 [10880/50000]\tLoss: 4.7538\tLR: 9.621483\n",
      "Training Epoch: 97 [11008/50000]\tLoss: 4.7435\tLR: 9.621739\n",
      "Training Epoch: 97 [11136/50000]\tLoss: 4.7078\tLR: 9.621995\n",
      "Training Epoch: 97 [11264/50000]\tLoss: 4.7118\tLR: 9.622251\n",
      "Training Epoch: 97 [11392/50000]\tLoss: 4.7169\tLR: 9.622506\n",
      "Training Epoch: 97 [11520/50000]\tLoss: 4.7345\tLR: 9.622762\n",
      "Training Epoch: 97 [11648/50000]\tLoss: 4.8326\tLR: 9.623018\n",
      "Training Epoch: 97 [11776/50000]\tLoss: 4.7203\tLR: 9.623274\n",
      "Training Epoch: 97 [11904/50000]\tLoss: 4.8472\tLR: 9.623529\n",
      "Training Epoch: 97 [12032/50000]\tLoss: 4.7477\tLR: 9.623785\n",
      "Training Epoch: 97 [12160/50000]\tLoss: 4.8000\tLR: 9.624041\n",
      "Training Epoch: 97 [12288/50000]\tLoss: 4.7627\tLR: 9.624297\n",
      "Training Epoch: 97 [12416/50000]\tLoss: 4.6853\tLR: 9.624552\n",
      "Training Epoch: 97 [12544/50000]\tLoss: 4.6887\tLR: 9.624808\n",
      "Training Epoch: 97 [12672/50000]\tLoss: 4.8184\tLR: 9.625064\n",
      "Training Epoch: 97 [12800/50000]\tLoss: 4.8140\tLR: 9.625320\n",
      "Training Epoch: 97 [12928/50000]\tLoss: 4.8296\tLR: 9.625575\n",
      "Training Epoch: 97 [13056/50000]\tLoss: 4.8680\tLR: 9.625831\n",
      "Training Epoch: 97 [13184/50000]\tLoss: 4.7933\tLR: 9.626087\n",
      "Training Epoch: 97 [13312/50000]\tLoss: 4.7038\tLR: 9.626343\n",
      "Training Epoch: 97 [13440/50000]\tLoss: 4.6375\tLR: 9.626598\n",
      "Training Epoch: 97 [13568/50000]\tLoss: 4.7704\tLR: 9.626854\n",
      "Training Epoch: 97 [13696/50000]\tLoss: 4.7475\tLR: 9.627110\n",
      "Training Epoch: 97 [13824/50000]\tLoss: 4.8592\tLR: 9.627366\n",
      "Training Epoch: 97 [13952/50000]\tLoss: 4.8541\tLR: 9.627621\n",
      "Training Epoch: 97 [14080/50000]\tLoss: 4.7186\tLR: 9.627877\n",
      "Training Epoch: 97 [14208/50000]\tLoss: 4.7585\tLR: 9.628133\n",
      "Training Epoch: 97 [14336/50000]\tLoss: 4.7025\tLR: 9.628389\n",
      "Training Epoch: 97 [14464/50000]\tLoss: 4.7372\tLR: 9.628645\n",
      "Training Epoch: 97 [14592/50000]\tLoss: 4.8588\tLR: 9.628900\n",
      "Training Epoch: 97 [14720/50000]\tLoss: 4.8739\tLR: 9.629156\n",
      "Training Epoch: 97 [14848/50000]\tLoss: 4.8984\tLR: 9.629412\n",
      "Training Epoch: 97 [14976/50000]\tLoss: 4.8196\tLR: 9.629668\n",
      "Training Epoch: 97 [15104/50000]\tLoss: 4.7521\tLR: 9.629923\n",
      "Training Epoch: 97 [15232/50000]\tLoss: 4.7961\tLR: 9.630179\n",
      "Training Epoch: 97 [15360/50000]\tLoss: 4.7218\tLR: 9.630435\n",
      "Training Epoch: 97 [15488/50000]\tLoss: 4.7224\tLR: 9.630691\n",
      "Training Epoch: 97 [15616/50000]\tLoss: 4.7025\tLR: 9.630946\n",
      "Training Epoch: 97 [15744/50000]\tLoss: 4.6456\tLR: 9.631202\n",
      "Training Epoch: 97 [15872/50000]\tLoss: 4.6613\tLR: 9.631458\n",
      "Training Epoch: 97 [16000/50000]\tLoss: 4.8183\tLR: 9.631714\n",
      "Training Epoch: 97 [16128/50000]\tLoss: 4.8276\tLR: 9.631969\n",
      "Training Epoch: 97 [16256/50000]\tLoss: 4.7621\tLR: 9.632225\n",
      "Training Epoch: 97 [16384/50000]\tLoss: 4.8294\tLR: 9.632481\n",
      "Training Epoch: 97 [16512/50000]\tLoss: 4.6983\tLR: 9.632737\n",
      "Training Epoch: 97 [16640/50000]\tLoss: 4.7438\tLR: 9.632992\n",
      "Training Epoch: 97 [16768/50000]\tLoss: 4.7761\tLR: 9.633248\n",
      "Training Epoch: 97 [16896/50000]\tLoss: 4.7923\tLR: 9.633504\n",
      "Training Epoch: 97 [17024/50000]\tLoss: 4.8650\tLR: 9.633760\n",
      "Training Epoch: 97 [17152/50000]\tLoss: 4.8200\tLR: 9.634015\n",
      "Training Epoch: 97 [17280/50000]\tLoss: 4.7367\tLR: 9.634271\n",
      "Training Epoch: 97 [17408/50000]\tLoss: 4.7622\tLR: 9.634527\n",
      "Training Epoch: 97 [17536/50000]\tLoss: 4.7044\tLR: 9.634783\n",
      "Training Epoch: 97 [17664/50000]\tLoss: 4.7171\tLR: 9.635038\n",
      "Training Epoch: 97 [17792/50000]\tLoss: 4.8745\tLR: 9.635294\n",
      "Training Epoch: 97 [17920/50000]\tLoss: 4.7449\tLR: 9.635550\n",
      "Training Epoch: 97 [18048/50000]\tLoss: 4.7641\tLR: 9.635806\n",
      "Training Epoch: 97 [18176/50000]\tLoss: 4.8947\tLR: 9.636061\n",
      "Training Epoch: 97 [18304/50000]\tLoss: 4.8454\tLR: 9.636317\n",
      "Training Epoch: 97 [18432/50000]\tLoss: 4.8028\tLR: 9.636573\n",
      "Training Epoch: 97 [18560/50000]\tLoss: 4.7179\tLR: 9.636829\n",
      "Training Epoch: 97 [18688/50000]\tLoss: 4.7184\tLR: 9.637084\n",
      "Training Epoch: 97 [18816/50000]\tLoss: 4.7121\tLR: 9.637340\n",
      "Training Epoch: 97 [18944/50000]\tLoss: 4.8169\tLR: 9.637596\n",
      "Training Epoch: 97 [19072/50000]\tLoss: 4.7271\tLR: 9.637852\n",
      "Training Epoch: 97 [19200/50000]\tLoss: 4.7612\tLR: 9.638107\n",
      "Training Epoch: 97 [19328/50000]\tLoss: 4.7985\tLR: 9.638363\n",
      "Training Epoch: 97 [19456/50000]\tLoss: 4.8262\tLR: 9.638619\n",
      "Training Epoch: 97 [19584/50000]\tLoss: 4.7831\tLR: 9.638875\n",
      "Training Epoch: 97 [19712/50000]\tLoss: 4.7481\tLR: 9.639130\n",
      "Training Epoch: 97 [19840/50000]\tLoss: 4.8178\tLR: 9.639386\n",
      "Training Epoch: 97 [19968/50000]\tLoss: 4.6935\tLR: 9.639642\n",
      "Training Epoch: 97 [20096/50000]\tLoss: 4.8254\tLR: 9.639898\n",
      "Training Epoch: 97 [20224/50000]\tLoss: 4.7893\tLR: 9.640153\n",
      "Training Epoch: 97 [20352/50000]\tLoss: 4.7206\tLR: 9.640409\n",
      "Training Epoch: 97 [20480/50000]\tLoss: 4.5884\tLR: 9.640665\n",
      "Training Epoch: 97 [20608/50000]\tLoss: 4.8294\tLR: 9.640921\n",
      "Training Epoch: 97 [20736/50000]\tLoss: 4.7782\tLR: 9.641176\n",
      "Training Epoch: 97 [20864/50000]\tLoss: 4.6346\tLR: 9.641432\n",
      "Training Epoch: 97 [20992/50000]\tLoss: 4.8420\tLR: 9.641688\n",
      "Training Epoch: 97 [21120/50000]\tLoss: 4.7732\tLR: 9.641944\n",
      "Training Epoch: 97 [21248/50000]\tLoss: 4.8210\tLR: 9.642199\n",
      "Training Epoch: 97 [21376/50000]\tLoss: 4.8166\tLR: 9.642455\n",
      "Training Epoch: 97 [21504/50000]\tLoss: 4.7968\tLR: 9.642711\n",
      "Training Epoch: 97 [21632/50000]\tLoss: 4.7039\tLR: 9.642967\n",
      "Training Epoch: 97 [21760/50000]\tLoss: 4.7341\tLR: 9.643223\n",
      "Training Epoch: 97 [21888/50000]\tLoss: 4.8022\tLR: 9.643478\n",
      "Training Epoch: 97 [22016/50000]\tLoss: 4.8425\tLR: 9.643734\n",
      "Training Epoch: 97 [22144/50000]\tLoss: 4.8160\tLR: 9.643990\n",
      "Training Epoch: 97 [22272/50000]\tLoss: 4.9147\tLR: 9.644246\n",
      "Training Epoch: 97 [22400/50000]\tLoss: 4.8025\tLR: 9.644501\n",
      "Training Epoch: 97 [22528/50000]\tLoss: 4.8817\tLR: 9.644757\n",
      "Training Epoch: 97 [22656/50000]\tLoss: 4.7926\tLR: 9.645013\n",
      "Training Epoch: 97 [22784/50000]\tLoss: 4.8075\tLR: 9.645269\n",
      "Training Epoch: 97 [22912/50000]\tLoss: 4.8376\tLR: 9.645524\n",
      "Training Epoch: 97 [23040/50000]\tLoss: 4.8238\tLR: 9.645780\n",
      "Training Epoch: 97 [23168/50000]\tLoss: 4.7898\tLR: 9.646036\n",
      "Training Epoch: 97 [23296/50000]\tLoss: 4.7926\tLR: 9.646292\n",
      "Training Epoch: 97 [23424/50000]\tLoss: 4.7339\tLR: 9.646547\n",
      "Training Epoch: 97 [23552/50000]\tLoss: 4.7172\tLR: 9.646803\n",
      "Training Epoch: 97 [23680/50000]\tLoss: 4.9074\tLR: 9.647059\n",
      "Training Epoch: 97 [23808/50000]\tLoss: 4.8455\tLR: 9.647315\n",
      "Training Epoch: 97 [23936/50000]\tLoss: 4.8118\tLR: 9.647570\n",
      "Training Epoch: 97 [24064/50000]\tLoss: 4.8724\tLR: 9.647826\n",
      "Training Epoch: 97 [24192/50000]\tLoss: 4.7164\tLR: 9.648082\n",
      "Training Epoch: 97 [24320/50000]\tLoss: 4.7611\tLR: 9.648338\n",
      "Training Epoch: 97 [24448/50000]\tLoss: 4.7425\tLR: 9.648593\n",
      "Training Epoch: 97 [24576/50000]\tLoss: 4.7256\tLR: 9.648849\n",
      "Training Epoch: 97 [24704/50000]\tLoss: 4.9143\tLR: 9.649105\n",
      "Training Epoch: 97 [24832/50000]\tLoss: 4.8341\tLR: 9.649361\n",
      "Training Epoch: 97 [24960/50000]\tLoss: 4.7632\tLR: 9.649616\n",
      "Training Epoch: 97 [25088/50000]\tLoss: 4.7795\tLR: 9.649872\n",
      "Training Epoch: 97 [25216/50000]\tLoss: 4.9603\tLR: 9.650128\n",
      "Training Epoch: 97 [25344/50000]\tLoss: 4.8521\tLR: 9.650384\n",
      "Training Epoch: 97 [25472/50000]\tLoss: 4.8025\tLR: 9.650639\n",
      "Training Epoch: 97 [25600/50000]\tLoss: 4.7427\tLR: 9.650895\n",
      "Training Epoch: 97 [25728/50000]\tLoss: 4.9462\tLR: 9.651151\n",
      "Training Epoch: 97 [25856/50000]\tLoss: 4.7261\tLR: 9.651407\n",
      "Training Epoch: 97 [25984/50000]\tLoss: 4.8184\tLR: 9.651662\n",
      "Training Epoch: 97 [26112/50000]\tLoss: 4.8535\tLR: 9.651918\n",
      "Training Epoch: 97 [26240/50000]\tLoss: 4.8387\tLR: 9.652174\n",
      "Training Epoch: 97 [26368/50000]\tLoss: 4.7743\tLR: 9.652430\n",
      "Training Epoch: 97 [26496/50000]\tLoss: 4.7280\tLR: 9.652685\n",
      "Training Epoch: 97 [26624/50000]\tLoss: 4.7406\tLR: 9.652941\n",
      "Training Epoch: 97 [26752/50000]\tLoss: 4.7907\tLR: 9.653197\n",
      "Training Epoch: 97 [26880/50000]\tLoss: 4.8049\tLR: 9.653453\n",
      "Training Epoch: 97 [27008/50000]\tLoss: 4.8700\tLR: 9.653708\n",
      "Training Epoch: 97 [27136/50000]\tLoss: 4.7553\tLR: 9.653964\n",
      "Training Epoch: 97 [27264/50000]\tLoss: 4.8530\tLR: 9.654220\n",
      "Training Epoch: 97 [27392/50000]\tLoss: 4.7460\tLR: 9.654476\n",
      "Training Epoch: 97 [27520/50000]\tLoss: 4.7398\tLR: 9.654731\n",
      "Training Epoch: 97 [27648/50000]\tLoss: 4.7744\tLR: 9.654987\n",
      "Training Epoch: 97 [27776/50000]\tLoss: 4.7340\tLR: 9.655243\n",
      "Training Epoch: 97 [27904/50000]\tLoss: 4.7129\tLR: 9.655499\n",
      "Training Epoch: 97 [28032/50000]\tLoss: 4.6393\tLR: 9.655754\n",
      "Training Epoch: 97 [28160/50000]\tLoss: 4.7945\tLR: 9.656010\n",
      "Training Epoch: 97 [28288/50000]\tLoss: 4.7847\tLR: 9.656266\n",
      "Training Epoch: 97 [28416/50000]\tLoss: 4.8078\tLR: 9.656522\n",
      "Training Epoch: 97 [28544/50000]\tLoss: 4.8147\tLR: 9.656777\n",
      "Training Epoch: 97 [28672/50000]\tLoss: 4.7351\tLR: 9.657033\n",
      "Training Epoch: 97 [28800/50000]\tLoss: 4.7513\tLR: 9.657289\n",
      "Training Epoch: 97 [28928/50000]\tLoss: 4.7604\tLR: 9.657545\n",
      "Training Epoch: 97 [29056/50000]\tLoss: 4.7808\tLR: 9.657801\n",
      "Training Epoch: 97 [29184/50000]\tLoss: 4.6649\tLR: 9.658056\n",
      "Training Epoch: 97 [29312/50000]\tLoss: 4.7171\tLR: 9.658312\n",
      "Training Epoch: 97 [29440/50000]\tLoss: 4.9062\tLR: 9.658568\n",
      "Training Epoch: 97 [29568/50000]\tLoss: 4.6959\tLR: 9.658824\n",
      "Training Epoch: 97 [29696/50000]\tLoss: 4.8538\tLR: 9.659079\n",
      "Training Epoch: 97 [29824/50000]\tLoss: 4.7790\tLR: 9.659335\n",
      "Training Epoch: 97 [29952/50000]\tLoss: 4.7372\tLR: 9.659591\n",
      "Training Epoch: 97 [30080/50000]\tLoss: 4.7737\tLR: 9.659847\n",
      "Training Epoch: 97 [30208/50000]\tLoss: 4.6845\tLR: 9.660102\n",
      "Training Epoch: 97 [30336/50000]\tLoss: 4.8304\tLR: 9.660358\n",
      "Training Epoch: 97 [30464/50000]\tLoss: 4.7239\tLR: 9.660614\n",
      "Training Epoch: 97 [30592/50000]\tLoss: 4.7236\tLR: 9.660870\n",
      "Training Epoch: 97 [30720/50000]\tLoss: 4.7716\tLR: 9.661125\n",
      "Training Epoch: 97 [30848/50000]\tLoss: 4.7564\tLR: 9.661381\n",
      "Training Epoch: 97 [30976/50000]\tLoss: 4.9521\tLR: 9.661637\n",
      "Training Epoch: 97 [31104/50000]\tLoss: 4.8805\tLR: 9.661893\n",
      "Training Epoch: 97 [31232/50000]\tLoss: 4.8323\tLR: 9.662148\n",
      "Training Epoch: 97 [31360/50000]\tLoss: 4.7287\tLR: 9.662404\n",
      "Training Epoch: 97 [31488/50000]\tLoss: 4.8581\tLR: 9.662660\n",
      "Training Epoch: 97 [31616/50000]\tLoss: 4.6938\tLR: 9.662916\n",
      "Training Epoch: 97 [31744/50000]\tLoss: 4.7549\tLR: 9.663171\n",
      "Training Epoch: 97 [31872/50000]\tLoss: 4.8083\tLR: 9.663427\n",
      "Training Epoch: 97 [32000/50000]\tLoss: 4.8460\tLR: 9.663683\n",
      "Training Epoch: 97 [32128/50000]\tLoss: 4.8734\tLR: 9.663939\n",
      "Training Epoch: 97 [32256/50000]\tLoss: 4.7947\tLR: 9.664194\n",
      "Training Epoch: 97 [32384/50000]\tLoss: 4.7965\tLR: 9.664450\n",
      "Training Epoch: 97 [32512/50000]\tLoss: 4.7369\tLR: 9.664706\n",
      "Training Epoch: 97 [32640/50000]\tLoss: 4.6551\tLR: 9.664962\n",
      "Training Epoch: 97 [32768/50000]\tLoss: 4.7967\tLR: 9.665217\n",
      "Training Epoch: 97 [32896/50000]\tLoss: 4.7649\tLR: 9.665473\n",
      "Training Epoch: 97 [33024/50000]\tLoss: 4.9192\tLR: 9.665729\n",
      "Training Epoch: 97 [33152/50000]\tLoss: 4.7610\tLR: 9.665985\n",
      "Training Epoch: 97 [33280/50000]\tLoss: 4.8501\tLR: 9.666240\n",
      "Training Epoch: 97 [33408/50000]\tLoss: 4.8377\tLR: 9.666496\n",
      "Training Epoch: 97 [33536/50000]\tLoss: 4.7577\tLR: 9.666752\n",
      "Training Epoch: 97 [33664/50000]\tLoss: 4.7407\tLR: 9.667008\n",
      "Training Epoch: 97 [33792/50000]\tLoss: 4.7311\tLR: 9.667263\n",
      "Training Epoch: 97 [33920/50000]\tLoss: 4.7552\tLR: 9.667519\n",
      "Training Epoch: 97 [34048/50000]\tLoss: 4.8513\tLR: 9.667775\n",
      "Training Epoch: 97 [34176/50000]\tLoss: 4.7662\tLR: 9.668031\n",
      "Training Epoch: 97 [34304/50000]\tLoss: 4.6852\tLR: 9.668286\n",
      "Training Epoch: 97 [34432/50000]\tLoss: 4.7821\tLR: 9.668542\n",
      "Training Epoch: 97 [34560/50000]\tLoss: 4.7844\tLR: 9.668798\n",
      "Training Epoch: 97 [34688/50000]\tLoss: 4.7550\tLR: 9.669054\n",
      "Training Epoch: 97 [34816/50000]\tLoss: 4.9065\tLR: 9.669309\n",
      "Training Epoch: 97 [34944/50000]\tLoss: 4.7421\tLR: 9.669565\n",
      "Training Epoch: 97 [35072/50000]\tLoss: 4.7287\tLR: 9.669821\n",
      "Training Epoch: 97 [35200/50000]\tLoss: 4.7635\tLR: 9.670077\n",
      "Training Epoch: 97 [35328/50000]\tLoss: 4.7359\tLR: 9.670332\n",
      "Training Epoch: 97 [35456/50000]\tLoss: 4.7236\tLR: 9.670588\n",
      "Training Epoch: 97 [35584/50000]\tLoss: 4.8597\tLR: 9.670844\n",
      "Training Epoch: 97 [35712/50000]\tLoss: 4.8250\tLR: 9.671100\n",
      "Training Epoch: 97 [35840/50000]\tLoss: 4.7527\tLR: 9.671355\n",
      "Training Epoch: 97 [35968/50000]\tLoss: 4.6606\tLR: 9.671611\n",
      "Training Epoch: 97 [36096/50000]\tLoss: 4.7229\tLR: 9.671867\n",
      "Training Epoch: 97 [36224/50000]\tLoss: 4.8569\tLR: 9.672123\n",
      "Training Epoch: 97 [36352/50000]\tLoss: 4.7756\tLR: 9.672379\n",
      "Training Epoch: 97 [36480/50000]\tLoss: 4.7476\tLR: 9.672634\n",
      "Training Epoch: 97 [36608/50000]\tLoss: 4.8618\tLR: 9.672890\n",
      "Training Epoch: 97 [36736/50000]\tLoss: 4.8114\tLR: 9.673146\n",
      "Training Epoch: 97 [36864/50000]\tLoss: 4.8605\tLR: 9.673402\n",
      "Training Epoch: 97 [36992/50000]\tLoss: 4.7569\tLR: 9.673657\n",
      "Training Epoch: 97 [37120/50000]\tLoss: 4.8001\tLR: 9.673913\n",
      "Training Epoch: 97 [37248/50000]\tLoss: 4.8166\tLR: 9.674169\n",
      "Training Epoch: 97 [37376/50000]\tLoss: 4.7278\tLR: 9.674425\n",
      "Training Epoch: 97 [37504/50000]\tLoss: 4.7713\tLR: 9.674680\n",
      "Training Epoch: 97 [37632/50000]\tLoss: 4.7960\tLR: 9.674936\n",
      "Training Epoch: 97 [37760/50000]\tLoss: 4.9244\tLR: 9.675192\n",
      "Training Epoch: 97 [37888/50000]\tLoss: 4.7916\tLR: 9.675448\n",
      "Training Epoch: 97 [38016/50000]\tLoss: 4.8562\tLR: 9.675703\n",
      "Training Epoch: 97 [38144/50000]\tLoss: 4.7849\tLR: 9.675959\n",
      "Training Epoch: 97 [38272/50000]\tLoss: 4.8443\tLR: 9.676215\n",
      "Training Epoch: 97 [38400/50000]\tLoss: 4.7393\tLR: 9.676471\n",
      "Training Epoch: 97 [38528/50000]\tLoss: 4.8169\tLR: 9.676726\n",
      "Training Epoch: 97 [38656/50000]\tLoss: 4.9067\tLR: 9.676982\n",
      "Training Epoch: 97 [38784/50000]\tLoss: 4.8203\tLR: 9.677238\n",
      "Training Epoch: 97 [38912/50000]\tLoss: 4.8750\tLR: 9.677494\n",
      "Training Epoch: 97 [39040/50000]\tLoss: 4.7746\tLR: 9.677749\n",
      "Training Epoch: 97 [39168/50000]\tLoss: 4.7184\tLR: 9.678005\n",
      "Training Epoch: 97 [39296/50000]\tLoss: 4.7958\tLR: 9.678261\n",
      "Training Epoch: 97 [39424/50000]\tLoss: 4.8249\tLR: 9.678517\n",
      "Training Epoch: 97 [39552/50000]\tLoss: 4.9035\tLR: 9.678772\n",
      "Training Epoch: 97 [39680/50000]\tLoss: 4.7928\tLR: 9.679028\n",
      "Training Epoch: 97 [39808/50000]\tLoss: 4.7307\tLR: 9.679284\n",
      "Training Epoch: 97 [39936/50000]\tLoss: 4.8938\tLR: 9.679540\n",
      "Training Epoch: 97 [40064/50000]\tLoss: 4.8394\tLR: 9.679795\n",
      "Training Epoch: 97 [40192/50000]\tLoss: 4.7726\tLR: 9.680051\n",
      "Training Epoch: 97 [40320/50000]\tLoss: 4.7458\tLR: 9.680307\n",
      "Training Epoch: 97 [40448/50000]\tLoss: 4.7882\tLR: 9.680563\n",
      "Training Epoch: 97 [40576/50000]\tLoss: 4.8295\tLR: 9.680818\n",
      "Training Epoch: 97 [40704/50000]\tLoss: 4.8449\tLR: 9.681074\n",
      "Training Epoch: 97 [40832/50000]\tLoss: 4.7246\tLR: 9.681330\n",
      "Training Epoch: 97 [40960/50000]\tLoss: 4.8231\tLR: 9.681586\n",
      "Training Epoch: 97 [41088/50000]\tLoss: 4.7990\tLR: 9.681841\n",
      "Training Epoch: 97 [41216/50000]\tLoss: 4.8097\tLR: 9.682097\n",
      "Training Epoch: 97 [41344/50000]\tLoss: 4.7610\tLR: 9.682353\n",
      "Training Epoch: 97 [41472/50000]\tLoss: 4.9422\tLR: 9.682609\n",
      "Training Epoch: 97 [41600/50000]\tLoss: 4.8555\tLR: 9.682864\n",
      "Training Epoch: 97 [41728/50000]\tLoss: 4.8000\tLR: 9.683120\n",
      "Training Epoch: 97 [41856/50000]\tLoss: 4.7651\tLR: 9.683376\n",
      "Training Epoch: 97 [41984/50000]\tLoss: 4.8723\tLR: 9.683632\n",
      "Training Epoch: 97 [42112/50000]\tLoss: 4.7428\tLR: 9.683887\n",
      "Training Epoch: 97 [42240/50000]\tLoss: 4.7112\tLR: 9.684143\n",
      "Training Epoch: 97 [42368/50000]\tLoss: 4.8237\tLR: 9.684399\n",
      "Training Epoch: 97 [42496/50000]\tLoss: 4.9463\tLR: 9.684655\n",
      "Training Epoch: 97 [42624/50000]\tLoss: 4.7982\tLR: 9.684910\n",
      "Training Epoch: 97 [42752/50000]\tLoss: 4.7856\tLR: 9.685166\n",
      "Training Epoch: 97 [42880/50000]\tLoss: 4.8303\tLR: 9.685422\n",
      "Training Epoch: 97 [43008/50000]\tLoss: 4.7124\tLR: 9.685678\n",
      "Training Epoch: 97 [43136/50000]\tLoss: 4.7930\tLR: 9.685934\n",
      "Training Epoch: 97 [43264/50000]\tLoss: 4.8670\tLR: 9.686189\n",
      "Training Epoch: 97 [43392/50000]\tLoss: 4.8247\tLR: 9.686445\n",
      "Training Epoch: 97 [43520/50000]\tLoss: 4.7480\tLR: 9.686701\n",
      "Training Epoch: 97 [43648/50000]\tLoss: 4.8407\tLR: 9.686957\n",
      "Training Epoch: 97 [43776/50000]\tLoss: 4.7275\tLR: 9.687212\n",
      "Training Epoch: 97 [43904/50000]\tLoss: 4.7786\tLR: 9.687468\n",
      "Training Epoch: 97 [44032/50000]\tLoss: 4.7412\tLR: 9.687724\n",
      "Training Epoch: 97 [44160/50000]\tLoss: 4.7937\tLR: 9.687980\n",
      "Training Epoch: 97 [44288/50000]\tLoss: 4.6627\tLR: 9.688235\n",
      "Training Epoch: 97 [44416/50000]\tLoss: 4.8516\tLR: 9.688491\n",
      "Training Epoch: 97 [44544/50000]\tLoss: 4.6860\tLR: 9.688747\n",
      "Training Epoch: 97 [44672/50000]\tLoss: 4.8511\tLR: 9.689003\n",
      "Training Epoch: 97 [44800/50000]\tLoss: 4.7599\tLR: 9.689258\n",
      "Training Epoch: 97 [44928/50000]\tLoss: 4.7063\tLR: 9.689514\n",
      "Training Epoch: 97 [45056/50000]\tLoss: 4.6960\tLR: 9.689770\n",
      "Training Epoch: 97 [45184/50000]\tLoss: 4.7929\tLR: 9.690026\n",
      "Training Epoch: 97 [45312/50000]\tLoss: 4.8544\tLR: 9.690281\n",
      "Training Epoch: 97 [45440/50000]\tLoss: 4.8361\tLR: 9.690537\n",
      "Training Epoch: 97 [45568/50000]\tLoss: 4.7704\tLR: 9.690793\n",
      "Training Epoch: 97 [45696/50000]\tLoss: 4.9225\tLR: 9.691049\n",
      "Training Epoch: 97 [45824/50000]\tLoss: 4.7690\tLR: 9.691304\n",
      "Training Epoch: 97 [45952/50000]\tLoss: 4.7479\tLR: 9.691560\n",
      "Training Epoch: 97 [46080/50000]\tLoss: 4.7597\tLR: 9.691816\n",
      "Training Epoch: 97 [46208/50000]\tLoss: 4.6910\tLR: 9.692072\n",
      "Training Epoch: 97 [46336/50000]\tLoss: 4.7862\tLR: 9.692327\n",
      "Training Epoch: 97 [46464/50000]\tLoss: 4.7109\tLR: 9.692583\n",
      "Training Epoch: 97 [46592/50000]\tLoss: 4.8288\tLR: 9.692839\n",
      "Training Epoch: 97 [46720/50000]\tLoss: 4.7213\tLR: 9.693095\n",
      "Training Epoch: 97 [46848/50000]\tLoss: 4.7659\tLR: 9.693350\n",
      "Training Epoch: 97 [46976/50000]\tLoss: 4.7832\tLR: 9.693606\n",
      "Training Epoch: 97 [47104/50000]\tLoss: 4.7057\tLR: 9.693862\n",
      "Training Epoch: 97 [47232/50000]\tLoss: 4.7648\tLR: 9.694118\n",
      "Training Epoch: 97 [47360/50000]\tLoss: 4.8455\tLR: 9.694373\n",
      "Training Epoch: 97 [47488/50000]\tLoss: 4.7864\tLR: 9.694629\n",
      "Training Epoch: 97 [47616/50000]\tLoss: 4.8356\tLR: 9.694885\n",
      "Training Epoch: 97 [47744/50000]\tLoss: 4.8275\tLR: 9.695141\n",
      "Training Epoch: 97 [47872/50000]\tLoss: 4.8140\tLR: 9.695396\n",
      "Training Epoch: 97 [48000/50000]\tLoss: 4.8571\tLR: 9.695652\n",
      "Training Epoch: 97 [48128/50000]\tLoss: 4.7663\tLR: 9.695908\n",
      "Training Epoch: 97 [48256/50000]\tLoss: 4.7638\tLR: 9.696164\n",
      "Training Epoch: 97 [48384/50000]\tLoss: 4.7431\tLR: 9.696419\n",
      "Training Epoch: 97 [48512/50000]\tLoss: 4.6936\tLR: 9.696675\n",
      "Training Epoch: 97 [48640/50000]\tLoss: 4.7787\tLR: 9.696931\n",
      "Training Epoch: 97 [48768/50000]\tLoss: 4.8280\tLR: 9.697187\n",
      "Training Epoch: 97 [48896/50000]\tLoss: 4.7988\tLR: 9.697442\n",
      "Training Epoch: 97 [49024/50000]\tLoss: 4.7100\tLR: 9.697698\n",
      "Training Epoch: 97 [49152/50000]\tLoss: 4.8638\tLR: 9.697954\n",
      "Training Epoch: 97 [49280/50000]\tLoss: 4.9455\tLR: 9.698210\n",
      "Training Epoch: 97 [49408/50000]\tLoss: 4.7395\tLR: 9.698465\n",
      "Training Epoch: 97 [49536/50000]\tLoss: 4.7150\tLR: 9.698721\n",
      "Training Epoch: 97 [49664/50000]\tLoss: 4.7255\tLR: 9.698977\n",
      "Training Epoch: 97 [49792/50000]\tLoss: 4.7339\tLR: 9.699233\n",
      "Training Epoch: 97 [49920/50000]\tLoss: 4.9383\tLR: 9.699488\n",
      "Training Epoch: 97 [50000/50000]\tLoss: 4.7634\tLR: 9.699744\n",
      "epoch 97 training time consumed: 496.61s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  135987 GB |  135987 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  135569 GB |  135569 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     417 GB |     417 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  135987 GB |  135987 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  135569 GB |  135569 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     417 GB |     417 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  134076 GB |  134076 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  133659 GB |  133659 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     417 GB |     417 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   14419 K  |   14419 K  |\n",
      "|       from large pool |      24    |      65    |    6146 K  |    6146 K  |\n",
      "|       from small pool |     231    |     274    |    8272 K  |    8272 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   14419 K  |   14419 K  |\n",
      "|       from large pool |      24    |      65    |    6146 K  |    6146 K  |\n",
      "|       from small pool |     231    |     274    |    8272 K  |    8272 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      37    |      47    |    8358 K  |    8358 K  |\n",
      "|       from large pool |      10    |      23    |    2954 K  |    2954 K  |\n",
      "|       from small pool |      27    |      35    |    5403 K  |    5403 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 97, Average loss: 0.0380, Accuracy: 0.0100, Time consumed:31.18s\n",
      "\n",
      "Training Epoch: 98 [128/50000]\tLoss: 4.7572\tLR: 0.020000\n",
      "Training Epoch: 98 [256/50000]\tLoss: 4.7847\tLR: 9.700256\n",
      "Training Epoch: 98 [384/50000]\tLoss: 4.7994\tLR: 9.700512\n",
      "Training Epoch: 98 [512/50000]\tLoss: 4.7975\tLR: 9.700767\n",
      "Training Epoch: 98 [640/50000]\tLoss: 4.8491\tLR: 9.701023\n",
      "Training Epoch: 98 [768/50000]\tLoss: 4.7629\tLR: 9.701279\n",
      "Training Epoch: 98 [896/50000]\tLoss: 4.7242\tLR: 9.701535\n",
      "Training Epoch: 98 [1024/50000]\tLoss: 4.7615\tLR: 9.701790\n",
      "Training Epoch: 98 [1152/50000]\tLoss: 4.8857\tLR: 9.702046\n",
      "Training Epoch: 98 [1280/50000]\tLoss: 4.7917\tLR: 9.702302\n",
      "Training Epoch: 98 [1408/50000]\tLoss: 4.8561\tLR: 9.702558\n",
      "Training Epoch: 98 [1536/50000]\tLoss: 4.8548\tLR: 9.702813\n",
      "Training Epoch: 98 [1664/50000]\tLoss: 4.8466\tLR: 9.703069\n",
      "Training Epoch: 98 [1792/50000]\tLoss: 4.7295\tLR: 9.703325\n",
      "Training Epoch: 98 [1920/50000]\tLoss: 4.8128\tLR: 9.703581\n",
      "Training Epoch: 98 [2048/50000]\tLoss: 4.8461\tLR: 9.703836\n",
      "Training Epoch: 98 [2176/50000]\tLoss: 4.8580\tLR: 9.704092\n",
      "Training Epoch: 98 [2304/50000]\tLoss: 4.7823\tLR: 9.704348\n",
      "Training Epoch: 98 [2432/50000]\tLoss: 4.9053\tLR: 9.704604\n",
      "Training Epoch: 98 [2560/50000]\tLoss: 4.9133\tLR: 9.704859\n",
      "Training Epoch: 98 [2688/50000]\tLoss: 4.7566\tLR: 9.705115\n",
      "Training Epoch: 98 [2816/50000]\tLoss: 4.8350\tLR: 9.705371\n",
      "Training Epoch: 98 [2944/50000]\tLoss: 4.8502\tLR: 9.705627\n",
      "Training Epoch: 98 [3072/50000]\tLoss: 4.8340\tLR: 9.705882\n",
      "Training Epoch: 98 [3200/50000]\tLoss: 4.7987\tLR: 9.706138\n",
      "Training Epoch: 98 [3328/50000]\tLoss: 4.7781\tLR: 9.706394\n",
      "Training Epoch: 98 [3456/50000]\tLoss: 5.0177\tLR: 9.706650\n",
      "Training Epoch: 98 [3584/50000]\tLoss: 4.8453\tLR: 9.706905\n",
      "Training Epoch: 98 [3712/50000]\tLoss: 4.7906\tLR: 9.707161\n",
      "Training Epoch: 98 [3840/50000]\tLoss: 4.7897\tLR: 9.707417\n",
      "Training Epoch: 98 [3968/50000]\tLoss: 4.7175\tLR: 9.707673\n",
      "Training Epoch: 98 [4096/50000]\tLoss: 4.7666\tLR: 9.707928\n",
      "Training Epoch: 98 [4224/50000]\tLoss: 4.7300\tLR: 9.708184\n",
      "Training Epoch: 98 [4352/50000]\tLoss: 4.7774\tLR: 9.708440\n",
      "Training Epoch: 98 [4480/50000]\tLoss: 4.8098\tLR: 9.708696\n",
      "Training Epoch: 98 [4608/50000]\tLoss: 4.7099\tLR: 9.708951\n",
      "Training Epoch: 98 [4736/50000]\tLoss: 4.8593\tLR: 9.709207\n",
      "Training Epoch: 98 [4864/50000]\tLoss: 4.7439\tLR: 9.709463\n",
      "Training Epoch: 98 [4992/50000]\tLoss: 4.8921\tLR: 9.709719\n",
      "Training Epoch: 98 [5120/50000]\tLoss: 4.8056\tLR: 9.709974\n",
      "Training Epoch: 98 [5248/50000]\tLoss: 4.8567\tLR: 9.710230\n",
      "Training Epoch: 98 [5376/50000]\tLoss: 4.7355\tLR: 9.710486\n",
      "Training Epoch: 98 [5504/50000]\tLoss: 4.7869\tLR: 9.710742\n",
      "Training Epoch: 98 [5632/50000]\tLoss: 4.8014\tLR: 9.710997\n",
      "Training Epoch: 98 [5760/50000]\tLoss: 4.6836\tLR: 9.711253\n",
      "Training Epoch: 98 [5888/50000]\tLoss: 4.7692\tLR: 9.711509\n",
      "Training Epoch: 98 [6016/50000]\tLoss: 4.7535\tLR: 9.711765\n",
      "Training Epoch: 98 [6144/50000]\tLoss: 4.7612\tLR: 9.712020\n",
      "Training Epoch: 98 [6272/50000]\tLoss: 4.7947\tLR: 9.712276\n",
      "Training Epoch: 98 [6400/50000]\tLoss: 4.7828\tLR: 9.712532\n",
      "Training Epoch: 98 [6528/50000]\tLoss: 4.8652\tLR: 9.712788\n",
      "Training Epoch: 98 [6656/50000]\tLoss: 4.7737\tLR: 9.713043\n",
      "Training Epoch: 98 [6784/50000]\tLoss: 4.7114\tLR: 9.713299\n",
      "Training Epoch: 98 [6912/50000]\tLoss: 4.7004\tLR: 9.713555\n",
      "Training Epoch: 98 [7040/50000]\tLoss: 4.7278\tLR: 9.713811\n",
      "Training Epoch: 98 [7168/50000]\tLoss: 4.7619\tLR: 9.714066\n",
      "Training Epoch: 98 [7296/50000]\tLoss: 4.7967\tLR: 9.714322\n",
      "Training Epoch: 98 [7424/50000]\tLoss: 4.9209\tLR: 9.714578\n",
      "Training Epoch: 98 [7552/50000]\tLoss: 4.8234\tLR: 9.714834\n",
      "Training Epoch: 98 [7680/50000]\tLoss: 4.7916\tLR: 9.715090\n",
      "Training Epoch: 98 [7808/50000]\tLoss: 4.7613\tLR: 9.715345\n",
      "Training Epoch: 98 [7936/50000]\tLoss: 4.7169\tLR: 9.715601\n",
      "Training Epoch: 98 [8064/50000]\tLoss: 4.8685\tLR: 9.715857\n",
      "Training Epoch: 98 [8192/50000]\tLoss: 4.8897\tLR: 9.716113\n",
      "Training Epoch: 98 [8320/50000]\tLoss: 4.7695\tLR: 9.716368\n",
      "Training Epoch: 98 [8448/50000]\tLoss: 4.7839\tLR: 9.716624\n",
      "Training Epoch: 98 [8576/50000]\tLoss: 4.7841\tLR: 9.716880\n",
      "Training Epoch: 98 [8704/50000]\tLoss: 4.7262\tLR: 9.717136\n",
      "Training Epoch: 98 [8832/50000]\tLoss: 4.8065\tLR: 9.717391\n",
      "Training Epoch: 98 [8960/50000]\tLoss: 4.7455\tLR: 9.717647\n",
      "Training Epoch: 98 [9088/50000]\tLoss: 4.7412\tLR: 9.717903\n",
      "Training Epoch: 98 [9216/50000]\tLoss: 4.7241\tLR: 9.718159\n",
      "Training Epoch: 98 [9344/50000]\tLoss: 4.7541\tLR: 9.718414\n",
      "Training Epoch: 98 [9472/50000]\tLoss: 4.7629\tLR: 9.718670\n",
      "Training Epoch: 98 [9600/50000]\tLoss: 4.9093\tLR: 9.718926\n",
      "Training Epoch: 98 [9728/50000]\tLoss: 4.9876\tLR: 9.719182\n",
      "Training Epoch: 98 [9856/50000]\tLoss: 4.8152\tLR: 9.719437\n",
      "Training Epoch: 98 [9984/50000]\tLoss: 4.7668\tLR: 9.719693\n",
      "Training Epoch: 98 [10112/50000]\tLoss: 4.7615\tLR: 9.719949\n",
      "Training Epoch: 98 [10240/50000]\tLoss: 4.7675\tLR: 9.720205\n",
      "Training Epoch: 98 [10368/50000]\tLoss: 4.8961\tLR: 9.720460\n",
      "Training Epoch: 98 [10496/50000]\tLoss: 4.8301\tLR: 9.720716\n",
      "Training Epoch: 98 [10624/50000]\tLoss: 4.8089\tLR: 9.720972\n",
      "Training Epoch: 98 [10752/50000]\tLoss: 4.7634\tLR: 9.721228\n",
      "Training Epoch: 98 [10880/50000]\tLoss: 4.7442\tLR: 9.721483\n",
      "Training Epoch: 98 [11008/50000]\tLoss: 4.7247\tLR: 9.721739\n",
      "Training Epoch: 98 [11136/50000]\tLoss: 4.7794\tLR: 9.721995\n",
      "Training Epoch: 98 [11264/50000]\tLoss: 4.7476\tLR: 9.722251\n",
      "Training Epoch: 98 [11392/50000]\tLoss: 4.7453\tLR: 9.722506\n",
      "Training Epoch: 98 [11520/50000]\tLoss: 4.7797\tLR: 9.722762\n",
      "Training Epoch: 98 [11648/50000]\tLoss: 4.7986\tLR: 9.723018\n",
      "Training Epoch: 98 [11776/50000]\tLoss: 4.9426\tLR: 9.723274\n",
      "Training Epoch: 98 [11904/50000]\tLoss: 4.8636\tLR: 9.723529\n",
      "Training Epoch: 98 [12032/50000]\tLoss: 4.8139\tLR: 9.723785\n",
      "Training Epoch: 98 [12160/50000]\tLoss: 4.8230\tLR: 9.724041\n",
      "Training Epoch: 98 [12288/50000]\tLoss: 4.7175\tLR: 9.724297\n",
      "Training Epoch: 98 [12416/50000]\tLoss: 4.8806\tLR: 9.724552\n",
      "Training Epoch: 98 [12544/50000]\tLoss: 4.8669\tLR: 9.724808\n",
      "Training Epoch: 98 [12672/50000]\tLoss: 4.8253\tLR: 9.725064\n",
      "Training Epoch: 98 [12800/50000]\tLoss: 4.7341\tLR: 9.725320\n",
      "Training Epoch: 98 [12928/50000]\tLoss: 4.7129\tLR: 9.725575\n",
      "Training Epoch: 98 [13056/50000]\tLoss: 4.7178\tLR: 9.725831\n",
      "Training Epoch: 98 [13184/50000]\tLoss: 4.9000\tLR: 9.726087\n",
      "Training Epoch: 98 [13312/50000]\tLoss: 4.7901\tLR: 9.726343\n",
      "Training Epoch: 98 [13440/50000]\tLoss: 4.8378\tLR: 9.726598\n",
      "Training Epoch: 98 [13568/50000]\tLoss: 4.8569\tLR: 9.726854\n",
      "Training Epoch: 98 [13696/50000]\tLoss: 4.8594\tLR: 9.727110\n",
      "Training Epoch: 98 [13824/50000]\tLoss: 4.8165\tLR: 9.727366\n",
      "Training Epoch: 98 [13952/50000]\tLoss: 4.7657\tLR: 9.727621\n",
      "Training Epoch: 98 [14080/50000]\tLoss: 4.7279\tLR: 9.727877\n",
      "Training Epoch: 98 [14208/50000]\tLoss: 4.7552\tLR: 9.728133\n",
      "Training Epoch: 98 [14336/50000]\tLoss: 4.8076\tLR: 9.728389\n",
      "Training Epoch: 98 [14464/50000]\tLoss: 4.8683\tLR: 9.728645\n",
      "Training Epoch: 98 [14592/50000]\tLoss: 4.7952\tLR: 9.728900\n",
      "Training Epoch: 98 [14720/50000]\tLoss: 4.8566\tLR: 9.729156\n",
      "Training Epoch: 98 [14848/50000]\tLoss: 4.8572\tLR: 9.729412\n",
      "Training Epoch: 98 [14976/50000]\tLoss: 4.7729\tLR: 9.729668\n",
      "Training Epoch: 98 [15104/50000]\tLoss: 4.7850\tLR: 9.729923\n",
      "Training Epoch: 98 [15232/50000]\tLoss: 4.7770\tLR: 9.730179\n",
      "Training Epoch: 98 [15360/50000]\tLoss: 4.6998\tLR: 9.730435\n",
      "Training Epoch: 98 [15488/50000]\tLoss: 4.7231\tLR: 9.730691\n",
      "Training Epoch: 98 [15616/50000]\tLoss: 4.7998\tLR: 9.730946\n",
      "Training Epoch: 98 [15744/50000]\tLoss: 4.6535\tLR: 9.731202\n",
      "Training Epoch: 98 [15872/50000]\tLoss: 4.8149\tLR: 9.731458\n",
      "Training Epoch: 98 [16000/50000]\tLoss: 4.8808\tLR: 9.731714\n",
      "Training Epoch: 98 [16128/50000]\tLoss: 4.7827\tLR: 9.731969\n",
      "Training Epoch: 98 [16256/50000]\tLoss: 4.8070\tLR: 9.732225\n",
      "Training Epoch: 98 [16384/50000]\tLoss: 4.8889\tLR: 9.732481\n",
      "Training Epoch: 98 [16512/50000]\tLoss: 4.7059\tLR: 9.732737\n",
      "Training Epoch: 98 [16640/50000]\tLoss: 4.7273\tLR: 9.732992\n",
      "Training Epoch: 98 [16768/50000]\tLoss: 4.7727\tLR: 9.733248\n",
      "Training Epoch: 98 [16896/50000]\tLoss: 4.7532\tLR: 9.733504\n",
      "Training Epoch: 98 [17024/50000]\tLoss: 4.7917\tLR: 9.733760\n",
      "Training Epoch: 98 [17152/50000]\tLoss: 4.8558\tLR: 9.734015\n",
      "Training Epoch: 98 [17280/50000]\tLoss: 4.8122\tLR: 9.734271\n",
      "Training Epoch: 98 [17408/50000]\tLoss: 4.8570\tLR: 9.734527\n",
      "Training Epoch: 98 [17536/50000]\tLoss: 4.7051\tLR: 9.734783\n",
      "Training Epoch: 98 [17664/50000]\tLoss: 4.7838\tLR: 9.735038\n",
      "Training Epoch: 98 [17792/50000]\tLoss: 4.8057\tLR: 9.735294\n",
      "Training Epoch: 98 [17920/50000]\tLoss: 4.8704\tLR: 9.735550\n",
      "Training Epoch: 98 [18048/50000]\tLoss: 4.7562\tLR: 9.735806\n",
      "Training Epoch: 98 [18176/50000]\tLoss: 4.7813\tLR: 9.736061\n",
      "Training Epoch: 98 [18304/50000]\tLoss: 4.8423\tLR: 9.736317\n",
      "Training Epoch: 98 [18432/50000]\tLoss: 4.8309\tLR: 9.736573\n",
      "Training Epoch: 98 [18560/50000]\tLoss: 4.8494\tLR: 9.736829\n",
      "Training Epoch: 98 [18688/50000]\tLoss: 4.8910\tLR: 9.737084\n",
      "Training Epoch: 98 [18816/50000]\tLoss: 4.8809\tLR: 9.737340\n",
      "Training Epoch: 98 [18944/50000]\tLoss: 4.8189\tLR: 9.737596\n",
      "Training Epoch: 98 [19072/50000]\tLoss: 4.8296\tLR: 9.737852\n",
      "Training Epoch: 98 [19200/50000]\tLoss: 4.8431\tLR: 9.738107\n",
      "Training Epoch: 98 [19328/50000]\tLoss: 4.8492\tLR: 9.738363\n",
      "Training Epoch: 98 [19456/50000]\tLoss: 4.8064\tLR: 9.738619\n",
      "Training Epoch: 98 [19584/50000]\tLoss: 4.8016\tLR: 9.738875\n",
      "Training Epoch: 98 [19712/50000]\tLoss: 4.7803\tLR: 9.739130\n",
      "Training Epoch: 98 [19840/50000]\tLoss: 4.7821\tLR: 9.739386\n",
      "Training Epoch: 98 [19968/50000]\tLoss: 4.7838\tLR: 9.739642\n",
      "Training Epoch: 98 [20096/50000]\tLoss: 4.8253\tLR: 9.739898\n",
      "Training Epoch: 98 [20224/50000]\tLoss: 4.8029\tLR: 9.740153\n",
      "Training Epoch: 98 [20352/50000]\tLoss: 4.7760\tLR: 9.740409\n",
      "Training Epoch: 98 [20480/50000]\tLoss: 4.7571\tLR: 9.740665\n",
      "Training Epoch: 98 [20608/50000]\tLoss: 4.8738\tLR: 9.740921\n",
      "Training Epoch: 98 [20736/50000]\tLoss: 4.7797\tLR: 9.741176\n",
      "Training Epoch: 98 [20864/50000]\tLoss: 4.8310\tLR: 9.741432\n",
      "Training Epoch: 98 [20992/50000]\tLoss: 4.8131\tLR: 9.741688\n",
      "Training Epoch: 98 [21120/50000]\tLoss: 4.7928\tLR: 9.741944\n",
      "Training Epoch: 98 [21248/50000]\tLoss: 4.7859\tLR: 9.742199\n",
      "Training Epoch: 98 [21376/50000]\tLoss: 4.7946\tLR: 9.742455\n",
      "Training Epoch: 98 [21504/50000]\tLoss: 4.6873\tLR: 9.742711\n",
      "Training Epoch: 98 [21632/50000]\tLoss: 4.7361\tLR: 9.742967\n",
      "Training Epoch: 98 [21760/50000]\tLoss: 4.8198\tLR: 9.743223\n",
      "Training Epoch: 98 [21888/50000]\tLoss: 4.7820\tLR: 9.743478\n",
      "Training Epoch: 98 [22016/50000]\tLoss: 4.7771\tLR: 9.743734\n",
      "Training Epoch: 98 [22144/50000]\tLoss: 4.7511\tLR: 9.743990\n",
      "Training Epoch: 98 [22272/50000]\tLoss: 4.6820\tLR: 9.744246\n",
      "Training Epoch: 98 [22400/50000]\tLoss: 4.7558\tLR: 9.744501\n",
      "Training Epoch: 98 [22528/50000]\tLoss: 4.7799\tLR: 9.744757\n",
      "Training Epoch: 98 [22656/50000]\tLoss: 4.7166\tLR: 9.745013\n",
      "Training Epoch: 98 [22784/50000]\tLoss: 4.6782\tLR: 9.745269\n",
      "Training Epoch: 98 [22912/50000]\tLoss: 4.7072\tLR: 9.745524\n",
      "Training Epoch: 98 [23040/50000]\tLoss: 4.8371\tLR: 9.745780\n",
      "Training Epoch: 98 [23168/50000]\tLoss: 4.8082\tLR: 9.746036\n",
      "Training Epoch: 98 [23296/50000]\tLoss: 4.7380\tLR: 9.746292\n",
      "Training Epoch: 98 [23424/50000]\tLoss: 4.7946\tLR: 9.746547\n",
      "Training Epoch: 98 [23552/50000]\tLoss: 4.6799\tLR: 9.746803\n",
      "Training Epoch: 98 [23680/50000]\tLoss: 4.8262\tLR: 9.747059\n",
      "Training Epoch: 98 [23808/50000]\tLoss: 4.7179\tLR: 9.747315\n",
      "Training Epoch: 98 [23936/50000]\tLoss: 4.6886\tLR: 9.747570\n",
      "Training Epoch: 98 [24064/50000]\tLoss: 4.6099\tLR: 9.747826\n",
      "Training Epoch: 98 [24192/50000]\tLoss: 4.7156\tLR: 9.748082\n",
      "Training Epoch: 98 [24320/50000]\tLoss: 4.8260\tLR: 9.748338\n",
      "Training Epoch: 98 [24448/50000]\tLoss: 4.7140\tLR: 9.748593\n",
      "Training Epoch: 98 [24576/50000]\tLoss: 4.7965\tLR: 9.748849\n",
      "Training Epoch: 98 [24704/50000]\tLoss: 4.8518\tLR: 9.749105\n",
      "Training Epoch: 98 [24832/50000]\tLoss: 4.7868\tLR: 9.749361\n",
      "Training Epoch: 98 [24960/50000]\tLoss: 4.7628\tLR: 9.749616\n",
      "Training Epoch: 98 [25088/50000]\tLoss: 4.8007\tLR: 9.749872\n",
      "Training Epoch: 98 [25216/50000]\tLoss: 4.7447\tLR: 9.750128\n",
      "Training Epoch: 98 [25344/50000]\tLoss: 4.6917\tLR: 9.750384\n",
      "Training Epoch: 98 [25472/50000]\tLoss: 4.6569\tLR: 9.750639\n",
      "Training Epoch: 98 [25600/50000]\tLoss: 4.8057\tLR: 9.750895\n",
      "Training Epoch: 98 [25728/50000]\tLoss: 4.8190\tLR: 9.751151\n",
      "Training Epoch: 98 [25856/50000]\tLoss: 4.7529\tLR: 9.751407\n",
      "Training Epoch: 98 [25984/50000]\tLoss: 4.8529\tLR: 9.751662\n",
      "Training Epoch: 98 [26112/50000]\tLoss: 4.8055\tLR: 9.751918\n",
      "Training Epoch: 98 [26240/50000]\tLoss: 4.7792\tLR: 9.752174\n",
      "Training Epoch: 98 [26368/50000]\tLoss: 4.7557\tLR: 9.752430\n",
      "Training Epoch: 98 [26496/50000]\tLoss: 4.7691\tLR: 9.752685\n",
      "Training Epoch: 98 [26624/50000]\tLoss: 4.7931\tLR: 9.752941\n",
      "Training Epoch: 98 [26752/50000]\tLoss: 4.7556\tLR: 9.753197\n",
      "Training Epoch: 98 [26880/50000]\tLoss: 4.7217\tLR: 9.753453\n",
      "Training Epoch: 98 [27008/50000]\tLoss: 4.7957\tLR: 9.753708\n",
      "Training Epoch: 98 [27136/50000]\tLoss: 4.8303\tLR: 9.753964\n",
      "Training Epoch: 98 [27264/50000]\tLoss: 4.7968\tLR: 9.754220\n",
      "Training Epoch: 98 [27392/50000]\tLoss: 4.9173\tLR: 9.754476\n",
      "Training Epoch: 98 [27520/50000]\tLoss: 4.8588\tLR: 9.754731\n",
      "Training Epoch: 98 [27648/50000]\tLoss: 4.7455\tLR: 9.754987\n",
      "Training Epoch: 98 [27776/50000]\tLoss: 4.7685\tLR: 9.755243\n",
      "Training Epoch: 98 [27904/50000]\tLoss: 4.7293\tLR: 9.755499\n",
      "Training Epoch: 98 [28032/50000]\tLoss: 4.7330\tLR: 9.755754\n",
      "Training Epoch: 98 [28160/50000]\tLoss: 4.7779\tLR: 9.756010\n",
      "Training Epoch: 98 [28288/50000]\tLoss: 4.7818\tLR: 9.756266\n",
      "Training Epoch: 98 [28416/50000]\tLoss: 4.9047\tLR: 9.756522\n",
      "Training Epoch: 98 [28544/50000]\tLoss: 4.7950\tLR: 9.756777\n",
      "Training Epoch: 98 [28672/50000]\tLoss: 4.7621\tLR: 9.757033\n",
      "Training Epoch: 98 [28800/50000]\tLoss: 4.8099\tLR: 9.757289\n",
      "Training Epoch: 98 [28928/50000]\tLoss: 4.7465\tLR: 9.757545\n",
      "Training Epoch: 98 [29056/50000]\tLoss: 4.7907\tLR: 9.757801\n",
      "Training Epoch: 98 [29184/50000]\tLoss: 4.8325\tLR: 9.758056\n",
      "Training Epoch: 98 [29312/50000]\tLoss: 4.7170\tLR: 9.758312\n",
      "Training Epoch: 98 [29440/50000]\tLoss: 4.8080\tLR: 9.758568\n",
      "Training Epoch: 98 [29568/50000]\tLoss: 4.8066\tLR: 9.758824\n",
      "Training Epoch: 98 [29696/50000]\tLoss: 4.8401\tLR: 9.759079\n",
      "Training Epoch: 98 [29824/50000]\tLoss: 4.7438\tLR: 9.759335\n",
      "Training Epoch: 98 [29952/50000]\tLoss: 4.7759\tLR: 9.759591\n",
      "Training Epoch: 98 [30080/50000]\tLoss: 4.7844\tLR: 9.759847\n",
      "Training Epoch: 98 [30208/50000]\tLoss: 4.8335\tLR: 9.760102\n",
      "Training Epoch: 98 [30336/50000]\tLoss: 4.8053\tLR: 9.760358\n",
      "Training Epoch: 98 [30464/50000]\tLoss: 4.8410\tLR: 9.760614\n",
      "Training Epoch: 98 [30592/50000]\tLoss: 4.7850\tLR: 9.760870\n",
      "Training Epoch: 98 [30720/50000]\tLoss: 4.8241\tLR: 9.761125\n",
      "Training Epoch: 98 [30848/50000]\tLoss: 4.6923\tLR: 9.761381\n",
      "Training Epoch: 98 [30976/50000]\tLoss: 4.7235\tLR: 9.761637\n",
      "Training Epoch: 98 [31104/50000]\tLoss: 4.8397\tLR: 9.761893\n",
      "Training Epoch: 98 [31232/50000]\tLoss: 4.7641\tLR: 9.762148\n",
      "Training Epoch: 98 [31360/50000]\tLoss: 4.7067\tLR: 9.762404\n",
      "Training Epoch: 98 [31488/50000]\tLoss: 4.7763\tLR: 9.762660\n",
      "Training Epoch: 98 [31616/50000]\tLoss: 4.7800\tLR: 9.762916\n",
      "Training Epoch: 98 [31744/50000]\tLoss: 4.7886\tLR: 9.763171\n",
      "Training Epoch: 98 [31872/50000]\tLoss: 4.6921\tLR: 9.763427\n",
      "Training Epoch: 98 [32000/50000]\tLoss: 4.8059\tLR: 9.763683\n",
      "Training Epoch: 98 [32128/50000]\tLoss: 4.7363\tLR: 9.763939\n",
      "Training Epoch: 98 [32256/50000]\tLoss: 4.7932\tLR: 9.764194\n",
      "Training Epoch: 98 [32384/50000]\tLoss: 4.7522\tLR: 9.764450\n",
      "Training Epoch: 98 [32512/50000]\tLoss: 4.7647\tLR: 9.764706\n",
      "Training Epoch: 98 [32640/50000]\tLoss: 4.7356\tLR: 9.764962\n",
      "Training Epoch: 98 [32768/50000]\tLoss: 4.6989\tLR: 9.765217\n",
      "Training Epoch: 98 [32896/50000]\tLoss: 4.7651\tLR: 9.765473\n",
      "Training Epoch: 98 [33024/50000]\tLoss: 4.7533\tLR: 9.765729\n",
      "Training Epoch: 98 [33152/50000]\tLoss: 4.7813\tLR: 9.765985\n",
      "Training Epoch: 98 [33280/50000]\tLoss: 4.7789\tLR: 9.766240\n",
      "Training Epoch: 98 [33408/50000]\tLoss: 4.7421\tLR: 9.766496\n",
      "Training Epoch: 98 [33536/50000]\tLoss: 4.7431\tLR: 9.766752\n",
      "Training Epoch: 98 [33664/50000]\tLoss: 4.7846\tLR: 9.767008\n",
      "Training Epoch: 98 [33792/50000]\tLoss: 4.7182\tLR: 9.767263\n",
      "Training Epoch: 98 [33920/50000]\tLoss: 4.6508\tLR: 9.767519\n",
      "Training Epoch: 98 [34048/50000]\tLoss: 4.6818\tLR: 9.767775\n",
      "Training Epoch: 98 [34176/50000]\tLoss: 4.6699\tLR: 9.768031\n",
      "Training Epoch: 98 [34304/50000]\tLoss: 4.8486\tLR: 9.768286\n",
      "Training Epoch: 98 [34432/50000]\tLoss: 4.7856\tLR: 9.768542\n",
      "Training Epoch: 98 [34560/50000]\tLoss: 4.8373\tLR: 9.768798\n",
      "Training Epoch: 98 [34688/50000]\tLoss: 4.7478\tLR: 9.769054\n",
      "Training Epoch: 98 [34816/50000]\tLoss: 4.8233\tLR: 9.769309\n",
      "Training Epoch: 98 [34944/50000]\tLoss: 4.8139\tLR: 9.769565\n",
      "Training Epoch: 98 [35072/50000]\tLoss: 4.7920\tLR: 9.769821\n",
      "Training Epoch: 98 [35200/50000]\tLoss: 4.7108\tLR: 9.770077\n",
      "Training Epoch: 98 [35328/50000]\tLoss: 4.8037\tLR: 9.770332\n",
      "Training Epoch: 98 [35456/50000]\tLoss: 4.8659\tLR: 9.770588\n",
      "Training Epoch: 98 [35584/50000]\tLoss: 4.7573\tLR: 9.770844\n",
      "Training Epoch: 98 [35712/50000]\tLoss: 4.7812\tLR: 9.771100\n",
      "Training Epoch: 98 [35840/50000]\tLoss: 4.8149\tLR: 9.771355\n",
      "Training Epoch: 98 [35968/50000]\tLoss: 4.8456\tLR: 9.771611\n",
      "Training Epoch: 98 [36096/50000]\tLoss: 4.8678\tLR: 9.771867\n",
      "Training Epoch: 98 [36224/50000]\tLoss: 4.7445\tLR: 9.772123\n",
      "Training Epoch: 98 [36352/50000]\tLoss: 4.8460\tLR: 9.772379\n",
      "Training Epoch: 98 [36480/50000]\tLoss: 4.7853\tLR: 9.772634\n",
      "Training Epoch: 98 [36608/50000]\tLoss: 4.7778\tLR: 9.772890\n",
      "Training Epoch: 98 [36736/50000]\tLoss: 4.7886\tLR: 9.773146\n",
      "Training Epoch: 98 [36864/50000]\tLoss: 4.8409\tLR: 9.773402\n",
      "Training Epoch: 98 [36992/50000]\tLoss: 4.7493\tLR: 9.773657\n",
      "Training Epoch: 98 [37120/50000]\tLoss: 4.6768\tLR: 9.773913\n",
      "Training Epoch: 98 [37248/50000]\tLoss: 4.8563\tLR: 9.774169\n",
      "Training Epoch: 98 [37376/50000]\tLoss: 4.7769\tLR: 9.774425\n",
      "Training Epoch: 98 [37504/50000]\tLoss: 4.9355\tLR: 9.774680\n",
      "Training Epoch: 98 [37632/50000]\tLoss: 4.8693\tLR: 9.774936\n",
      "Training Epoch: 98 [37760/50000]\tLoss: 4.7567\tLR: 9.775192\n",
      "Training Epoch: 98 [37888/50000]\tLoss: 4.8157\tLR: 9.775448\n",
      "Training Epoch: 98 [38016/50000]\tLoss: 4.6524\tLR: 9.775703\n",
      "Training Epoch: 98 [38144/50000]\tLoss: 4.7611\tLR: 9.775959\n",
      "Training Epoch: 98 [38272/50000]\tLoss: 4.7850\tLR: 9.776215\n",
      "Training Epoch: 98 [38400/50000]\tLoss: 4.8607\tLR: 9.776471\n",
      "Training Epoch: 98 [38528/50000]\tLoss: 4.8715\tLR: 9.776726\n",
      "Training Epoch: 98 [38656/50000]\tLoss: 4.8693\tLR: 9.776982\n",
      "Training Epoch: 98 [38784/50000]\tLoss: 4.7628\tLR: 9.777238\n",
      "Training Epoch: 98 [38912/50000]\tLoss: 4.8103\tLR: 9.777494\n",
      "Training Epoch: 98 [39040/50000]\tLoss: 4.7990\tLR: 9.777749\n",
      "Training Epoch: 98 [39168/50000]\tLoss: 4.9512\tLR: 9.778005\n",
      "Training Epoch: 98 [39296/50000]\tLoss: 4.6976\tLR: 9.778261\n",
      "Training Epoch: 98 [39424/50000]\tLoss: 4.8860\tLR: 9.778517\n",
      "Training Epoch: 98 [39552/50000]\tLoss: 4.9098\tLR: 9.778772\n",
      "Training Epoch: 98 [39680/50000]\tLoss: 4.8190\tLR: 9.779028\n",
      "Training Epoch: 98 [39808/50000]\tLoss: 4.8880\tLR: 9.779284\n",
      "Training Epoch: 98 [39936/50000]\tLoss: 4.8066\tLR: 9.779540\n",
      "Training Epoch: 98 [40064/50000]\tLoss: 4.6899\tLR: 9.779795\n",
      "Training Epoch: 98 [40192/50000]\tLoss: 4.7665\tLR: 9.780051\n",
      "Training Epoch: 98 [40320/50000]\tLoss: 4.8614\tLR: 9.780307\n",
      "Training Epoch: 98 [40448/50000]\tLoss: 4.8863\tLR: 9.780563\n",
      "Training Epoch: 98 [40576/50000]\tLoss: 4.9266\tLR: 9.780818\n",
      "Training Epoch: 98 [40704/50000]\tLoss: 4.8154\tLR: 9.781074\n",
      "Training Epoch: 98 [40832/50000]\tLoss: 4.7738\tLR: 9.781330\n",
      "Training Epoch: 98 [40960/50000]\tLoss: 4.7233\tLR: 9.781586\n",
      "Training Epoch: 98 [41088/50000]\tLoss: 4.7421\tLR: 9.781841\n",
      "Training Epoch: 98 [41216/50000]\tLoss: 4.7186\tLR: 9.782097\n",
      "Training Epoch: 98 [41344/50000]\tLoss: 4.8030\tLR: 9.782353\n",
      "Training Epoch: 98 [41472/50000]\tLoss: 4.7862\tLR: 9.782609\n",
      "Training Epoch: 98 [41600/50000]\tLoss: 4.7123\tLR: 9.782864\n",
      "Training Epoch: 98 [41728/50000]\tLoss: 4.7620\tLR: 9.783120\n",
      "Training Epoch: 98 [41856/50000]\tLoss: 4.9901\tLR: 9.783376\n",
      "Training Epoch: 98 [41984/50000]\tLoss: 4.7506\tLR: 9.783632\n",
      "Training Epoch: 98 [42112/50000]\tLoss: 4.8558\tLR: 9.783887\n",
      "Training Epoch: 98 [42240/50000]\tLoss: 4.8292\tLR: 9.784143\n",
      "Training Epoch: 98 [42368/50000]\tLoss: 4.9777\tLR: 9.784399\n",
      "Training Epoch: 98 [42496/50000]\tLoss: 4.7839\tLR: 9.784655\n",
      "Training Epoch: 98 [42624/50000]\tLoss: 4.7024\tLR: 9.784910\n",
      "Training Epoch: 98 [42752/50000]\tLoss: 4.7367\tLR: 9.785166\n",
      "Training Epoch: 98 [42880/50000]\tLoss: 4.7273\tLR: 9.785422\n",
      "Training Epoch: 98 [43008/50000]\tLoss: 4.7037\tLR: 9.785678\n",
      "Training Epoch: 98 [43136/50000]\tLoss: 4.6578\tLR: 9.785934\n",
      "Training Epoch: 98 [43264/50000]\tLoss: 4.7663\tLR: 9.786189\n",
      "Training Epoch: 98 [43392/50000]\tLoss: 4.7838\tLR: 9.786445\n",
      "Training Epoch: 98 [43520/50000]\tLoss: 4.7211\tLR: 9.786701\n",
      "Training Epoch: 98 [43648/50000]\tLoss: 4.6499\tLR: 9.786957\n",
      "Training Epoch: 98 [43776/50000]\tLoss: 4.7036\tLR: 9.787212\n",
      "Training Epoch: 98 [43904/50000]\tLoss: 4.7667\tLR: 9.787468\n",
      "Training Epoch: 98 [44032/50000]\tLoss: 4.7582\tLR: 9.787724\n",
      "Training Epoch: 98 [44160/50000]\tLoss: 4.6989\tLR: 9.787980\n",
      "Training Epoch: 98 [44288/50000]\tLoss: 4.8205\tLR: 9.788235\n",
      "Training Epoch: 98 [44416/50000]\tLoss: 4.7734\tLR: 9.788491\n",
      "Training Epoch: 98 [44544/50000]\tLoss: 4.7660\tLR: 9.788747\n",
      "Training Epoch: 98 [44672/50000]\tLoss: 4.6553\tLR: 9.789003\n",
      "Training Epoch: 98 [44800/50000]\tLoss: 4.7023\tLR: 9.789258\n",
      "Training Epoch: 98 [44928/50000]\tLoss: 4.7331\tLR: 9.789514\n",
      "Training Epoch: 98 [45056/50000]\tLoss: 4.7150\tLR: 9.789770\n",
      "Training Epoch: 98 [45184/50000]\tLoss: 4.8547\tLR: 9.790026\n",
      "Training Epoch: 98 [45312/50000]\tLoss: 4.7463\tLR: 9.790281\n",
      "Training Epoch: 98 [45440/50000]\tLoss: 4.7730\tLR: 9.790537\n",
      "Training Epoch: 98 [45568/50000]\tLoss: 4.7244\tLR: 9.790793\n",
      "Training Epoch: 98 [45696/50000]\tLoss: 4.7298\tLR: 9.791049\n",
      "Training Epoch: 98 [45824/50000]\tLoss: 4.6949\tLR: 9.791304\n",
      "Training Epoch: 98 [45952/50000]\tLoss: 4.7891\tLR: 9.791560\n",
      "Training Epoch: 98 [46080/50000]\tLoss: 4.7239\tLR: 9.791816\n",
      "Training Epoch: 98 [46208/50000]\tLoss: 4.8693\tLR: 9.792072\n",
      "Training Epoch: 98 [46336/50000]\tLoss: 4.7301\tLR: 9.792327\n",
      "Training Epoch: 98 [46464/50000]\tLoss: 4.6790\tLR: 9.792583\n",
      "Training Epoch: 98 [46592/50000]\tLoss: 4.7037\tLR: 9.792839\n",
      "Training Epoch: 98 [46720/50000]\tLoss: 4.8828\tLR: 9.793095\n",
      "Training Epoch: 98 [46848/50000]\tLoss: 4.7493\tLR: 9.793350\n",
      "Training Epoch: 98 [46976/50000]\tLoss: 4.7315\tLR: 9.793606\n",
      "Training Epoch: 98 [47104/50000]\tLoss: 5.0452\tLR: 9.793862\n",
      "Training Epoch: 98 [47232/50000]\tLoss: 4.8034\tLR: 9.794118\n",
      "Training Epoch: 98 [47360/50000]\tLoss: 4.8335\tLR: 9.794373\n",
      "Training Epoch: 98 [47488/50000]\tLoss: 4.7734\tLR: 9.794629\n",
      "Training Epoch: 98 [47616/50000]\tLoss: 4.7633\tLR: 9.794885\n",
      "Training Epoch: 98 [47744/50000]\tLoss: 4.8141\tLR: 9.795141\n",
      "Training Epoch: 98 [47872/50000]\tLoss: 4.8143\tLR: 9.795396\n",
      "Training Epoch: 98 [48000/50000]\tLoss: 4.8880\tLR: 9.795652\n",
      "Training Epoch: 98 [48128/50000]\tLoss: 4.7748\tLR: 9.795908\n",
      "Training Epoch: 98 [48256/50000]\tLoss: 4.7533\tLR: 9.796164\n",
      "Training Epoch: 98 [48384/50000]\tLoss: 4.8718\tLR: 9.796419\n",
      "Training Epoch: 98 [48512/50000]\tLoss: 4.7521\tLR: 9.796675\n",
      "Training Epoch: 98 [48640/50000]\tLoss: 4.8913\tLR: 9.796931\n",
      "Training Epoch: 98 [48768/50000]\tLoss: 4.7736\tLR: 9.797187\n",
      "Training Epoch: 98 [48896/50000]\tLoss: 4.7009\tLR: 9.797442\n",
      "Training Epoch: 98 [49024/50000]\tLoss: 4.8135\tLR: 9.797698\n",
      "Training Epoch: 98 [49152/50000]\tLoss: 4.7806\tLR: 9.797954\n",
      "Training Epoch: 98 [49280/50000]\tLoss: 4.7428\tLR: 9.798210\n",
      "Training Epoch: 98 [49408/50000]\tLoss: 4.8440\tLR: 9.798465\n",
      "Training Epoch: 98 [49536/50000]\tLoss: 4.7693\tLR: 9.798721\n",
      "Training Epoch: 98 [49664/50000]\tLoss: 4.7425\tLR: 9.798977\n",
      "Training Epoch: 98 [49792/50000]\tLoss: 4.7044\tLR: 9.799233\n",
      "Training Epoch: 98 [49920/50000]\tLoss: 4.7280\tLR: 9.799488\n",
      "Training Epoch: 98 [50000/50000]\tLoss: 4.7617\tLR: 9.799744\n",
      "epoch 98 training time consumed: 494.53s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  137389 GB |  137389 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  136967 GB |  136967 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     421 GB |     421 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  137389 GB |  137389 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  136967 GB |  136967 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     421 GB |     421 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  135459 GB |  135459 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  135037 GB |  135037 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     421 GB |     421 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   14567 K  |   14567 K  |\n",
      "|       from large pool |      24    |      65    |    6210 K  |    6210 K  |\n",
      "|       from small pool |     231    |     274    |    8357 K  |    8357 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   14567 K  |   14567 K  |\n",
      "|       from large pool |      24    |      65    |    6210 K  |    6210 K  |\n",
      "|       from small pool |     231    |     274    |    8357 K  |    8357 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      47    |    8444 K  |    8444 K  |\n",
      "|       from large pool |      10    |      23    |    2985 K  |    2985 K  |\n",
      "|       from small pool |      26    |      35    |    5459 K  |    5459 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 98, Average loss: 0.0373, Accuracy: 0.0100, Time consumed:31.28s\n",
      "\n",
      "Training Epoch: 99 [128/50000]\tLoss: 4.6674\tLR: 0.020000\n",
      "Training Epoch: 99 [256/50000]\tLoss: 4.6267\tLR: 9.800256\n",
      "Training Epoch: 99 [384/50000]\tLoss: 4.7178\tLR: 9.800512\n",
      "Training Epoch: 99 [512/50000]\tLoss: 4.7897\tLR: 9.800767\n",
      "Training Epoch: 99 [640/50000]\tLoss: 4.8545\tLR: 9.801023\n",
      "Training Epoch: 99 [768/50000]\tLoss: 4.7677\tLR: 9.801279\n",
      "Training Epoch: 99 [896/50000]\tLoss: 4.8641\tLR: 9.801535\n",
      "Training Epoch: 99 [1024/50000]\tLoss: 4.8409\tLR: 9.801790\n",
      "Training Epoch: 99 [1152/50000]\tLoss: 4.7535\tLR: 9.802046\n",
      "Training Epoch: 99 [1280/50000]\tLoss: 4.7223\tLR: 9.802302\n",
      "Training Epoch: 99 [1408/50000]\tLoss: 4.7217\tLR: 9.802558\n",
      "Training Epoch: 99 [1536/50000]\tLoss: 4.9110\tLR: 9.802813\n",
      "Training Epoch: 99 [1664/50000]\tLoss: 4.7756\tLR: 9.803069\n",
      "Training Epoch: 99 [1792/50000]\tLoss: 4.7417\tLR: 9.803325\n",
      "Training Epoch: 99 [1920/50000]\tLoss: 4.7363\tLR: 9.803581\n",
      "Training Epoch: 99 [2048/50000]\tLoss: 4.7449\tLR: 9.803836\n",
      "Training Epoch: 99 [2176/50000]\tLoss: 4.7711\tLR: 9.804092\n",
      "Training Epoch: 99 [2304/50000]\tLoss: 4.8015\tLR: 9.804348\n",
      "Training Epoch: 99 [2432/50000]\tLoss: 4.9273\tLR: 9.804604\n",
      "Training Epoch: 99 [2560/50000]\tLoss: 4.7350\tLR: 9.804859\n",
      "Training Epoch: 99 [2688/50000]\tLoss: 4.7863\tLR: 9.805115\n",
      "Training Epoch: 99 [2816/50000]\tLoss: 4.8510\tLR: 9.805371\n",
      "Training Epoch: 99 [2944/50000]\tLoss: 4.8383\tLR: 9.805627\n",
      "Training Epoch: 99 [3072/50000]\tLoss: 4.7856\tLR: 9.805882\n",
      "Training Epoch: 99 [3200/50000]\tLoss: 4.8177\tLR: 9.806138\n",
      "Training Epoch: 99 [3328/50000]\tLoss: 4.7363\tLR: 9.806394\n",
      "Training Epoch: 99 [3456/50000]\tLoss: 4.7749\tLR: 9.806650\n",
      "Training Epoch: 99 [3584/50000]\tLoss: 4.8513\tLR: 9.806905\n",
      "Training Epoch: 99 [3712/50000]\tLoss: 4.7263\tLR: 9.807161\n",
      "Training Epoch: 99 [3840/50000]\tLoss: 4.8401\tLR: 9.807417\n",
      "Training Epoch: 99 [3968/50000]\tLoss: 4.6956\tLR: 9.807673\n",
      "Training Epoch: 99 [4096/50000]\tLoss: 4.9037\tLR: 9.807928\n",
      "Training Epoch: 99 [4224/50000]\tLoss: 4.8077\tLR: 9.808184\n",
      "Training Epoch: 99 [4352/50000]\tLoss: 4.9650\tLR: 9.808440\n",
      "Training Epoch: 99 [4480/50000]\tLoss: 4.9009\tLR: 9.808696\n",
      "Training Epoch: 99 [4608/50000]\tLoss: 4.8244\tLR: 9.808951\n",
      "Training Epoch: 99 [4736/50000]\tLoss: 4.8273\tLR: 9.809207\n",
      "Training Epoch: 99 [4864/50000]\tLoss: 4.7958\tLR: 9.809463\n",
      "Training Epoch: 99 [4992/50000]\tLoss: 4.7577\tLR: 9.809719\n",
      "Training Epoch: 99 [5120/50000]\tLoss: 4.7499\tLR: 9.809974\n",
      "Training Epoch: 99 [5248/50000]\tLoss: 4.7990\tLR: 9.810230\n",
      "Training Epoch: 99 [5376/50000]\tLoss: 4.8269\tLR: 9.810486\n",
      "Training Epoch: 99 [5504/50000]\tLoss: 4.8680\tLR: 9.810742\n",
      "Training Epoch: 99 [5632/50000]\tLoss: 4.8680\tLR: 9.810997\n",
      "Training Epoch: 99 [5760/50000]\tLoss: 5.0018\tLR: 9.811253\n",
      "Training Epoch: 99 [5888/50000]\tLoss: 4.8115\tLR: 9.811509\n",
      "Training Epoch: 99 [6016/50000]\tLoss: 4.9170\tLR: 9.811765\n",
      "Training Epoch: 99 [6144/50000]\tLoss: 4.8238\tLR: 9.812020\n",
      "Training Epoch: 99 [6272/50000]\tLoss: 4.7611\tLR: 9.812276\n",
      "Training Epoch: 99 [6400/50000]\tLoss: 4.8743\tLR: 9.812532\n",
      "Training Epoch: 99 [6528/50000]\tLoss: 4.7723\tLR: 9.812788\n",
      "Training Epoch: 99 [6656/50000]\tLoss: 4.8907\tLR: 9.813043\n",
      "Training Epoch: 99 [6784/50000]\tLoss: 4.8475\tLR: 9.813299\n",
      "Training Epoch: 99 [6912/50000]\tLoss: 4.9234\tLR: 9.813555\n",
      "Training Epoch: 99 [7040/50000]\tLoss: 4.8722\tLR: 9.813811\n",
      "Training Epoch: 99 [7168/50000]\tLoss: 4.7577\tLR: 9.814066\n",
      "Training Epoch: 99 [7296/50000]\tLoss: 4.7455\tLR: 9.814322\n",
      "Training Epoch: 99 [7424/50000]\tLoss: 4.7399\tLR: 9.814578\n",
      "Training Epoch: 99 [7552/50000]\tLoss: 4.8521\tLR: 9.814834\n",
      "Training Epoch: 99 [7680/50000]\tLoss: 4.8278\tLR: 9.815090\n",
      "Training Epoch: 99 [7808/50000]\tLoss: 4.8601\tLR: 9.815345\n",
      "Training Epoch: 99 [7936/50000]\tLoss: 4.6407\tLR: 9.815601\n",
      "Training Epoch: 99 [8064/50000]\tLoss: 4.7301\tLR: 9.815857\n",
      "Training Epoch: 99 [8192/50000]\tLoss: 4.6980\tLR: 9.816113\n",
      "Training Epoch: 99 [8320/50000]\tLoss: 4.8609\tLR: 9.816368\n",
      "Training Epoch: 99 [8448/50000]\tLoss: 4.7806\tLR: 9.816624\n",
      "Training Epoch: 99 [8576/50000]\tLoss: 4.7052\tLR: 9.816880\n",
      "Training Epoch: 99 [8704/50000]\tLoss: 4.7486\tLR: 9.817136\n",
      "Training Epoch: 99 [8832/50000]\tLoss: 4.8295\tLR: 9.817391\n",
      "Training Epoch: 99 [8960/50000]\tLoss: 4.8000\tLR: 9.817647\n",
      "Training Epoch: 99 [9088/50000]\tLoss: 4.8859\tLR: 9.817903\n",
      "Training Epoch: 99 [9216/50000]\tLoss: 4.8132\tLR: 9.818159\n",
      "Training Epoch: 99 [9344/50000]\tLoss: 4.8477\tLR: 9.818414\n",
      "Training Epoch: 99 [9472/50000]\tLoss: 4.8041\tLR: 9.818670\n",
      "Training Epoch: 99 [9600/50000]\tLoss: 4.8137\tLR: 9.818926\n",
      "Training Epoch: 99 [9728/50000]\tLoss: 4.8417\tLR: 9.819182\n",
      "Training Epoch: 99 [9856/50000]\tLoss: 4.8695\tLR: 9.819437\n",
      "Training Epoch: 99 [9984/50000]\tLoss: 4.7955\tLR: 9.819693\n",
      "Training Epoch: 99 [10112/50000]\tLoss: 4.8074\tLR: 9.819949\n",
      "Training Epoch: 99 [10240/50000]\tLoss: 4.7634\tLR: 9.820205\n",
      "Training Epoch: 99 [10368/50000]\tLoss: 4.8811\tLR: 9.820460\n",
      "Training Epoch: 99 [10496/50000]\tLoss: 4.8362\tLR: 9.820716\n",
      "Training Epoch: 99 [10624/50000]\tLoss: 4.7652\tLR: 9.820972\n",
      "Training Epoch: 99 [10752/50000]\tLoss: 4.8410\tLR: 9.821228\n",
      "Training Epoch: 99 [10880/50000]\tLoss: 4.7967\tLR: 9.821483\n",
      "Training Epoch: 99 [11008/50000]\tLoss: 4.7021\tLR: 9.821739\n",
      "Training Epoch: 99 [11136/50000]\tLoss: 4.8214\tLR: 9.821995\n",
      "Training Epoch: 99 [11264/50000]\tLoss: 4.7896\tLR: 9.822251\n",
      "Training Epoch: 99 [11392/50000]\tLoss: 4.8500\tLR: 9.822506\n",
      "Training Epoch: 99 [11520/50000]\tLoss: 4.7296\tLR: 9.822762\n",
      "Training Epoch: 99 [11648/50000]\tLoss: 4.8487\tLR: 9.823018\n",
      "Training Epoch: 99 [11776/50000]\tLoss: 4.7774\tLR: 9.823274\n",
      "Training Epoch: 99 [11904/50000]\tLoss: 4.9202\tLR: 9.823529\n",
      "Training Epoch: 99 [12032/50000]\tLoss: 4.9204\tLR: 9.823785\n",
      "Training Epoch: 99 [12160/50000]\tLoss: 4.7536\tLR: 9.824041\n",
      "Training Epoch: 99 [12288/50000]\tLoss: 4.7487\tLR: 9.824297\n",
      "Training Epoch: 99 [12416/50000]\tLoss: 4.6734\tLR: 9.824552\n",
      "Training Epoch: 99 [12544/50000]\tLoss: 4.9031\tLR: 9.824808\n",
      "Training Epoch: 99 [12672/50000]\tLoss: 4.7842\tLR: 9.825064\n",
      "Training Epoch: 99 [12800/50000]\tLoss: 4.8479\tLR: 9.825320\n",
      "Training Epoch: 99 [12928/50000]\tLoss: 4.8042\tLR: 9.825575\n",
      "Training Epoch: 99 [13056/50000]\tLoss: 4.8915\tLR: 9.825831\n",
      "Training Epoch: 99 [13184/50000]\tLoss: 4.8448\tLR: 9.826087\n",
      "Training Epoch: 99 [13312/50000]\tLoss: 4.8188\tLR: 9.826343\n",
      "Training Epoch: 99 [13440/50000]\tLoss: 4.8016\tLR: 9.826598\n",
      "Training Epoch: 99 [13568/50000]\tLoss: 4.7789\tLR: 9.826854\n",
      "Training Epoch: 99 [13696/50000]\tLoss: 4.7417\tLR: 9.827110\n",
      "Training Epoch: 99 [13824/50000]\tLoss: 4.7171\tLR: 9.827366\n",
      "Training Epoch: 99 [13952/50000]\tLoss: 4.6973\tLR: 9.827621\n",
      "Training Epoch: 99 [14080/50000]\tLoss: 4.6943\tLR: 9.827877\n",
      "Training Epoch: 99 [14208/50000]\tLoss: 4.8030\tLR: 9.828133\n",
      "Training Epoch: 99 [14336/50000]\tLoss: 4.7409\tLR: 9.828389\n",
      "Training Epoch: 99 [14464/50000]\tLoss: 4.8255\tLR: 9.828645\n",
      "Training Epoch: 99 [14592/50000]\tLoss: 4.8031\tLR: 9.828900\n",
      "Training Epoch: 99 [14720/50000]\tLoss: 4.7129\tLR: 9.829156\n",
      "Training Epoch: 99 [14848/50000]\tLoss: 4.7365\tLR: 9.829412\n",
      "Training Epoch: 99 [14976/50000]\tLoss: 4.8632\tLR: 9.829668\n",
      "Training Epoch: 99 [15104/50000]\tLoss: 4.8155\tLR: 9.829923\n",
      "Training Epoch: 99 [15232/50000]\tLoss: 4.8432\tLR: 9.830179\n",
      "Training Epoch: 99 [15360/50000]\tLoss: 4.7841\tLR: 9.830435\n",
      "Training Epoch: 99 [15488/50000]\tLoss: 4.8069\tLR: 9.830691\n",
      "Training Epoch: 99 [15616/50000]\tLoss: 4.7761\tLR: 9.830946\n",
      "Training Epoch: 99 [15744/50000]\tLoss: 4.7561\tLR: 9.831202\n",
      "Training Epoch: 99 [15872/50000]\tLoss: 4.9476\tLR: 9.831458\n",
      "Training Epoch: 99 [16000/50000]\tLoss: 4.8408\tLR: 9.831714\n",
      "Training Epoch: 99 [16128/50000]\tLoss: 4.7659\tLR: 9.831969\n",
      "Training Epoch: 99 [16256/50000]\tLoss: 4.9001\tLR: 9.832225\n",
      "Training Epoch: 99 [16384/50000]\tLoss: 4.8385\tLR: 9.832481\n",
      "Training Epoch: 99 [16512/50000]\tLoss: 4.8017\tLR: 9.832737\n",
      "Training Epoch: 99 [16640/50000]\tLoss: 4.8864\tLR: 9.832992\n",
      "Training Epoch: 99 [16768/50000]\tLoss: 4.8622\tLR: 9.833248\n",
      "Training Epoch: 99 [16896/50000]\tLoss: 4.8333\tLR: 9.833504\n",
      "Training Epoch: 99 [17024/50000]\tLoss: 4.8667\tLR: 9.833760\n",
      "Training Epoch: 99 [17152/50000]\tLoss: 4.8326\tLR: 9.834015\n",
      "Training Epoch: 99 [17280/50000]\tLoss: 4.6801\tLR: 9.834271\n",
      "Training Epoch: 99 [17408/50000]\tLoss: 4.7931\tLR: 9.834527\n",
      "Training Epoch: 99 [17536/50000]\tLoss: 4.8527\tLR: 9.834783\n",
      "Training Epoch: 99 [17664/50000]\tLoss: 4.6887\tLR: 9.835038\n",
      "Training Epoch: 99 [17792/50000]\tLoss: 4.8145\tLR: 9.835294\n",
      "Training Epoch: 99 [17920/50000]\tLoss: 4.8121\tLR: 9.835550\n",
      "Training Epoch: 99 [18048/50000]\tLoss: 4.8137\tLR: 9.835806\n",
      "Training Epoch: 99 [18176/50000]\tLoss: 4.8395\tLR: 9.836061\n",
      "Training Epoch: 99 [18304/50000]\tLoss: 4.7772\tLR: 9.836317\n",
      "Training Epoch: 99 [18432/50000]\tLoss: 4.8230\tLR: 9.836573\n",
      "Training Epoch: 99 [18560/50000]\tLoss: 4.8931\tLR: 9.836829\n",
      "Training Epoch: 99 [18688/50000]\tLoss: 4.8505\tLR: 9.837084\n",
      "Training Epoch: 99 [18816/50000]\tLoss: 4.8245\tLR: 9.837340\n",
      "Training Epoch: 99 [18944/50000]\tLoss: 4.9176\tLR: 9.837596\n",
      "Training Epoch: 99 [19072/50000]\tLoss: 4.8061\tLR: 9.837852\n",
      "Training Epoch: 99 [19200/50000]\tLoss: 4.7995\tLR: 9.838107\n",
      "Training Epoch: 99 [19328/50000]\tLoss: 4.7494\tLR: 9.838363\n",
      "Training Epoch: 99 [19456/50000]\tLoss: 4.8584\tLR: 9.838619\n",
      "Training Epoch: 99 [19584/50000]\tLoss: 4.7838\tLR: 9.838875\n",
      "Training Epoch: 99 [19712/50000]\tLoss: 4.7652\tLR: 9.839130\n",
      "Training Epoch: 99 [19840/50000]\tLoss: 4.9383\tLR: 9.839386\n",
      "Training Epoch: 99 [19968/50000]\tLoss: 4.7999\tLR: 9.839642\n",
      "Training Epoch: 99 [20096/50000]\tLoss: 4.7318\tLR: 9.839898\n",
      "Training Epoch: 99 [20224/50000]\tLoss: 4.9273\tLR: 9.840153\n",
      "Training Epoch: 99 [20352/50000]\tLoss: 4.7947\tLR: 9.840409\n",
      "Training Epoch: 99 [20480/50000]\tLoss: 4.8009\tLR: 9.840665\n",
      "Training Epoch: 99 [20608/50000]\tLoss: 4.6893\tLR: 9.840921\n",
      "Training Epoch: 99 [20736/50000]\tLoss: 4.7253\tLR: 9.841176\n",
      "Training Epoch: 99 [20864/50000]\tLoss: 4.8660\tLR: 9.841432\n",
      "Training Epoch: 99 [20992/50000]\tLoss: 4.7647\tLR: 9.841688\n",
      "Training Epoch: 99 [21120/50000]\tLoss: 4.8072\tLR: 9.841944\n",
      "Training Epoch: 99 [21248/50000]\tLoss: 4.7073\tLR: 9.842199\n",
      "Training Epoch: 99 [21376/50000]\tLoss: 4.6710\tLR: 9.842455\n",
      "Training Epoch: 99 [21504/50000]\tLoss: 4.7630\tLR: 9.842711\n",
      "Training Epoch: 99 [21632/50000]\tLoss: 4.8188\tLR: 9.842967\n",
      "Training Epoch: 99 [21760/50000]\tLoss: 4.7681\tLR: 9.843223\n",
      "Training Epoch: 99 [21888/50000]\tLoss: 4.6813\tLR: 9.843478\n",
      "Training Epoch: 99 [22016/50000]\tLoss: 4.7545\tLR: 9.843734\n",
      "Training Epoch: 99 [22144/50000]\tLoss: 4.7133\tLR: 9.843990\n",
      "Training Epoch: 99 [22272/50000]\tLoss: 4.7929\tLR: 9.844246\n",
      "Training Epoch: 99 [22400/50000]\tLoss: 4.7575\tLR: 9.844501\n",
      "Training Epoch: 99 [22528/50000]\tLoss: 4.8044\tLR: 9.844757\n",
      "Training Epoch: 99 [22656/50000]\tLoss: 4.7909\tLR: 9.845013\n",
      "Training Epoch: 99 [22784/50000]\tLoss: 4.8239\tLR: 9.845269\n",
      "Training Epoch: 99 [22912/50000]\tLoss: 4.8022\tLR: 9.845524\n",
      "Training Epoch: 99 [23040/50000]\tLoss: 4.7962\tLR: 9.845780\n",
      "Training Epoch: 99 [23168/50000]\tLoss: 4.8725\tLR: 9.846036\n",
      "Training Epoch: 99 [23296/50000]\tLoss: 4.7962\tLR: 9.846292\n",
      "Training Epoch: 99 [23424/50000]\tLoss: 4.7431\tLR: 9.846547\n",
      "Training Epoch: 99 [23552/50000]\tLoss: 4.8314\tLR: 9.846803\n",
      "Training Epoch: 99 [23680/50000]\tLoss: 4.8789\tLR: 9.847059\n",
      "Training Epoch: 99 [23808/50000]\tLoss: 4.8431\tLR: 9.847315\n",
      "Training Epoch: 99 [23936/50000]\tLoss: 4.8028\tLR: 9.847570\n",
      "Training Epoch: 99 [24064/50000]\tLoss: 4.8383\tLR: 9.847826\n",
      "Training Epoch: 99 [24192/50000]\tLoss: 4.8186\tLR: 9.848082\n",
      "Training Epoch: 99 [24320/50000]\tLoss: 4.7329\tLR: 9.848338\n",
      "Training Epoch: 99 [24448/50000]\tLoss: 4.8438\tLR: 9.848593\n",
      "Training Epoch: 99 [24576/50000]\tLoss: 4.7771\tLR: 9.848849\n",
      "Training Epoch: 99 [24704/50000]\tLoss: 4.8839\tLR: 9.849105\n",
      "Training Epoch: 99 [24832/50000]\tLoss: 4.7048\tLR: 9.849361\n",
      "Training Epoch: 99 [24960/50000]\tLoss: 4.7867\tLR: 9.849616\n",
      "Training Epoch: 99 [25088/50000]\tLoss: 4.7991\tLR: 9.849872\n",
      "Training Epoch: 99 [25216/50000]\tLoss: 4.7751\tLR: 9.850128\n",
      "Training Epoch: 99 [25344/50000]\tLoss: 4.7598\tLR: 9.850384\n",
      "Training Epoch: 99 [25472/50000]\tLoss: 4.7835\tLR: 9.850639\n",
      "Training Epoch: 99 [25600/50000]\tLoss: 4.8297\tLR: 9.850895\n",
      "Training Epoch: 99 [25728/50000]\tLoss: 4.9030\tLR: 9.851151\n",
      "Training Epoch: 99 [25856/50000]\tLoss: 4.8082\tLR: 9.851407\n",
      "Training Epoch: 99 [25984/50000]\tLoss: 4.8742\tLR: 9.851662\n",
      "Training Epoch: 99 [26112/50000]\tLoss: 4.7028\tLR: 9.851918\n",
      "Training Epoch: 99 [26240/50000]\tLoss: 4.8599\tLR: 9.852174\n",
      "Training Epoch: 99 [26368/50000]\tLoss: 4.7200\tLR: 9.852430\n",
      "Training Epoch: 99 [26496/50000]\tLoss: 4.7876\tLR: 9.852685\n",
      "Training Epoch: 99 [26624/50000]\tLoss: 4.7737\tLR: 9.852941\n",
      "Training Epoch: 99 [26752/50000]\tLoss: 4.9375\tLR: 9.853197\n",
      "Training Epoch: 99 [26880/50000]\tLoss: 4.7508\tLR: 9.853453\n",
      "Training Epoch: 99 [27008/50000]\tLoss: 4.6971\tLR: 9.853708\n",
      "Training Epoch: 99 [27136/50000]\tLoss: 4.7735\tLR: 9.853964\n",
      "Training Epoch: 99 [27264/50000]\tLoss: 4.7572\tLR: 9.854220\n",
      "Training Epoch: 99 [27392/50000]\tLoss: 4.7548\tLR: 9.854476\n",
      "Training Epoch: 99 [27520/50000]\tLoss: 4.7051\tLR: 9.854731\n",
      "Training Epoch: 99 [27648/50000]\tLoss: 4.7834\tLR: 9.854987\n",
      "Training Epoch: 99 [27776/50000]\tLoss: 4.7678\tLR: 9.855243\n",
      "Training Epoch: 99 [27904/50000]\tLoss: 4.7776\tLR: 9.855499\n",
      "Training Epoch: 99 [28032/50000]\tLoss: 4.8736\tLR: 9.855754\n",
      "Training Epoch: 99 [28160/50000]\tLoss: 4.8146\tLR: 9.856010\n",
      "Training Epoch: 99 [28288/50000]\tLoss: 4.7336\tLR: 9.856266\n",
      "Training Epoch: 99 [28416/50000]\tLoss: 4.8082\tLR: 9.856522\n",
      "Training Epoch: 99 [28544/50000]\tLoss: 4.7074\tLR: 9.856777\n",
      "Training Epoch: 99 [28672/50000]\tLoss: 4.8383\tLR: 9.857033\n",
      "Training Epoch: 99 [28800/50000]\tLoss: 4.7519\tLR: 9.857289\n",
      "Training Epoch: 99 [28928/50000]\tLoss: 4.7952\tLR: 9.857545\n",
      "Training Epoch: 99 [29056/50000]\tLoss: 4.7658\tLR: 9.857801\n",
      "Training Epoch: 99 [29184/50000]\tLoss: 4.7461\tLR: 9.858056\n",
      "Training Epoch: 99 [29312/50000]\tLoss: 4.7951\tLR: 9.858312\n",
      "Training Epoch: 99 [29440/50000]\tLoss: 4.8645\tLR: 9.858568\n",
      "Training Epoch: 99 [29568/50000]\tLoss: 4.7460\tLR: 9.858824\n",
      "Training Epoch: 99 [29696/50000]\tLoss: 4.7161\tLR: 9.859079\n",
      "Training Epoch: 99 [29824/50000]\tLoss: 4.6973\tLR: 9.859335\n",
      "Training Epoch: 99 [29952/50000]\tLoss: 4.7327\tLR: 9.859591\n",
      "Training Epoch: 99 [30080/50000]\tLoss: 4.7905\tLR: 9.859847\n",
      "Training Epoch: 99 [30208/50000]\tLoss: 4.8827\tLR: 9.860102\n",
      "Training Epoch: 99 [30336/50000]\tLoss: 4.8018\tLR: 9.860358\n",
      "Training Epoch: 99 [30464/50000]\tLoss: 4.7244\tLR: 9.860614\n",
      "Training Epoch: 99 [30592/50000]\tLoss: 4.8526\tLR: 9.860870\n",
      "Training Epoch: 99 [30720/50000]\tLoss: 4.9123\tLR: 9.861125\n",
      "Training Epoch: 99 [30848/50000]\tLoss: 4.7848\tLR: 9.861381\n",
      "Training Epoch: 99 [30976/50000]\tLoss: 4.9642\tLR: 9.861637\n",
      "Training Epoch: 99 [31104/50000]\tLoss: 4.8466\tLR: 9.861893\n",
      "Training Epoch: 99 [31232/50000]\tLoss: 4.7906\tLR: 9.862148\n",
      "Training Epoch: 99 [31360/50000]\tLoss: 4.7568\tLR: 9.862404\n",
      "Training Epoch: 99 [31488/50000]\tLoss: 4.7292\tLR: 9.862660\n",
      "Training Epoch: 99 [31616/50000]\tLoss: 4.7573\tLR: 9.862916\n",
      "Training Epoch: 99 [31744/50000]\tLoss: 4.7772\tLR: 9.863171\n",
      "Training Epoch: 99 [31872/50000]\tLoss: 4.7417\tLR: 9.863427\n",
      "Training Epoch: 99 [32000/50000]\tLoss: 4.7829\tLR: 9.863683\n",
      "Training Epoch: 99 [32128/50000]\tLoss: 4.8199\tLR: 9.863939\n",
      "Training Epoch: 99 [32256/50000]\tLoss: 4.6562\tLR: 9.864194\n",
      "Training Epoch: 99 [32384/50000]\tLoss: 4.8028\tLR: 9.864450\n",
      "Training Epoch: 99 [32512/50000]\tLoss: 4.8099\tLR: 9.864706\n",
      "Training Epoch: 99 [32640/50000]\tLoss: 4.8150\tLR: 9.864962\n",
      "Training Epoch: 99 [32768/50000]\tLoss: 4.7846\tLR: 9.865217\n",
      "Training Epoch: 99 [32896/50000]\tLoss: 4.7402\tLR: 9.865473\n",
      "Training Epoch: 99 [33024/50000]\tLoss: 4.7615\tLR: 9.865729\n",
      "Training Epoch: 99 [33152/50000]\tLoss: 4.7561\tLR: 9.865985\n",
      "Training Epoch: 99 [33280/50000]\tLoss: 4.7061\tLR: 9.866240\n",
      "Training Epoch: 99 [33408/50000]\tLoss: 4.7588\tLR: 9.866496\n",
      "Training Epoch: 99 [33536/50000]\tLoss: 4.7742\tLR: 9.866752\n",
      "Training Epoch: 99 [33664/50000]\tLoss: 4.8304\tLR: 9.867008\n",
      "Training Epoch: 99 [33792/50000]\tLoss: 4.7580\tLR: 9.867263\n",
      "Training Epoch: 99 [33920/50000]\tLoss: 4.7090\tLR: 9.867519\n",
      "Training Epoch: 99 [34048/50000]\tLoss: 4.6694\tLR: 9.867775\n",
      "Training Epoch: 99 [34176/50000]\tLoss: 4.7682\tLR: 9.868031\n",
      "Training Epoch: 99 [34304/50000]\tLoss: 4.7416\tLR: 9.868286\n",
      "Training Epoch: 99 [34432/50000]\tLoss: 4.8243\tLR: 9.868542\n",
      "Training Epoch: 99 [34560/50000]\tLoss: 4.7909\tLR: 9.868798\n",
      "Training Epoch: 99 [34688/50000]\tLoss: 4.7565\tLR: 9.869054\n",
      "Training Epoch: 99 [34816/50000]\tLoss: 4.7848\tLR: 9.869309\n",
      "Training Epoch: 99 [34944/50000]\tLoss: 4.8118\tLR: 9.869565\n",
      "Training Epoch: 99 [35072/50000]\tLoss: 4.7903\tLR: 9.869821\n",
      "Training Epoch: 99 [35200/50000]\tLoss: 4.8855\tLR: 9.870077\n",
      "Training Epoch: 99 [35328/50000]\tLoss: 4.7843\tLR: 9.870332\n",
      "Training Epoch: 99 [35456/50000]\tLoss: 4.8630\tLR: 9.870588\n",
      "Training Epoch: 99 [35584/50000]\tLoss: 4.6898\tLR: 9.870844\n",
      "Training Epoch: 99 [35712/50000]\tLoss: 4.7034\tLR: 9.871100\n",
      "Training Epoch: 99 [35840/50000]\tLoss: 4.8906\tLR: 9.871355\n",
      "Training Epoch: 99 [35968/50000]\tLoss: 4.7638\tLR: 9.871611\n",
      "Training Epoch: 99 [36096/50000]\tLoss: 4.8726\tLR: 9.871867\n",
      "Training Epoch: 99 [36224/50000]\tLoss: 4.7238\tLR: 9.872123\n",
      "Training Epoch: 99 [36352/50000]\tLoss: 4.8022\tLR: 9.872379\n",
      "Training Epoch: 99 [36480/50000]\tLoss: 4.7590\tLR: 9.872634\n",
      "Training Epoch: 99 [36608/50000]\tLoss: 4.7647\tLR: 9.872890\n",
      "Training Epoch: 99 [36736/50000]\tLoss: 4.8404\tLR: 9.873146\n",
      "Training Epoch: 99 [36864/50000]\tLoss: 4.8690\tLR: 9.873402\n",
      "Training Epoch: 99 [36992/50000]\tLoss: 4.7706\tLR: 9.873657\n",
      "Training Epoch: 99 [37120/50000]\tLoss: 4.7490\tLR: 9.873913\n",
      "Training Epoch: 99 [37248/50000]\tLoss: 4.7153\tLR: 9.874169\n",
      "Training Epoch: 99 [37376/50000]\tLoss: 4.7440\tLR: 9.874425\n",
      "Training Epoch: 99 [37504/50000]\tLoss: 4.7920\tLR: 9.874680\n",
      "Training Epoch: 99 [37632/50000]\tLoss: 4.7191\tLR: 9.874936\n",
      "Training Epoch: 99 [37760/50000]\tLoss: 4.6794\tLR: 9.875192\n",
      "Training Epoch: 99 [37888/50000]\tLoss: 4.7935\tLR: 9.875448\n",
      "Training Epoch: 99 [38016/50000]\tLoss: 4.7597\tLR: 9.875703\n",
      "Training Epoch: 99 [38144/50000]\tLoss: 4.6838\tLR: 9.875959\n",
      "Training Epoch: 99 [38272/50000]\tLoss: 4.7762\tLR: 9.876215\n",
      "Training Epoch: 99 [38400/50000]\tLoss: 4.7728\tLR: 9.876471\n",
      "Training Epoch: 99 [38528/50000]\tLoss: 4.7924\tLR: 9.876726\n",
      "Training Epoch: 99 [38656/50000]\tLoss: 4.7129\tLR: 9.876982\n",
      "Training Epoch: 99 [38784/50000]\tLoss: 4.6582\tLR: 9.877238\n",
      "Training Epoch: 99 [38912/50000]\tLoss: 4.7739\tLR: 9.877494\n",
      "Training Epoch: 99 [39040/50000]\tLoss: 4.7969\tLR: 9.877749\n",
      "Training Epoch: 99 [39168/50000]\tLoss: 4.7411\tLR: 9.878005\n",
      "Training Epoch: 99 [39296/50000]\tLoss: 4.6882\tLR: 9.878261\n",
      "Training Epoch: 99 [39424/50000]\tLoss: 4.7890\tLR: 9.878517\n",
      "Training Epoch: 99 [39552/50000]\tLoss: 4.8614\tLR: 9.878772\n",
      "Training Epoch: 99 [39680/50000]\tLoss: 4.7973\tLR: 9.879028\n",
      "Training Epoch: 99 [39808/50000]\tLoss: 4.7972\tLR: 9.879284\n",
      "Training Epoch: 99 [39936/50000]\tLoss: 4.7848\tLR: 9.879540\n",
      "Training Epoch: 99 [40064/50000]\tLoss: 4.8629\tLR: 9.879795\n",
      "Training Epoch: 99 [40192/50000]\tLoss: 4.7394\tLR: 9.880051\n",
      "Training Epoch: 99 [40320/50000]\tLoss: 4.7423\tLR: 9.880307\n",
      "Training Epoch: 99 [40448/50000]\tLoss: 4.7526\tLR: 9.880563\n",
      "Training Epoch: 99 [40576/50000]\tLoss: 4.7572\tLR: 9.880818\n",
      "Training Epoch: 99 [40704/50000]\tLoss: 4.8205\tLR: 9.881074\n",
      "Training Epoch: 99 [40832/50000]\tLoss: 4.8502\tLR: 9.881330\n",
      "Training Epoch: 99 [40960/50000]\tLoss: 4.8639\tLR: 9.881586\n",
      "Training Epoch: 99 [41088/50000]\tLoss: 4.7692\tLR: 9.881841\n",
      "Training Epoch: 99 [41216/50000]\tLoss: 4.8048\tLR: 9.882097\n",
      "Training Epoch: 99 [41344/50000]\tLoss: 4.7746\tLR: 9.882353\n",
      "Training Epoch: 99 [41472/50000]\tLoss: 4.7167\tLR: 9.882609\n",
      "Training Epoch: 99 [41600/50000]\tLoss: 4.7261\tLR: 9.882864\n",
      "Training Epoch: 99 [41728/50000]\tLoss: 4.8161\tLR: 9.883120\n",
      "Training Epoch: 99 [41856/50000]\tLoss: 4.8809\tLR: 9.883376\n",
      "Training Epoch: 99 [41984/50000]\tLoss: 4.7904\tLR: 9.883632\n",
      "Training Epoch: 99 [42112/50000]\tLoss: 4.8866\tLR: 9.883887\n",
      "Training Epoch: 99 [42240/50000]\tLoss: 4.9193\tLR: 9.884143\n",
      "Training Epoch: 99 [42368/50000]\tLoss: 4.7324\tLR: 9.884399\n",
      "Training Epoch: 99 [42496/50000]\tLoss: 4.7178\tLR: 9.884655\n",
      "Training Epoch: 99 [42624/50000]\tLoss: 4.7494\tLR: 9.884910\n",
      "Training Epoch: 99 [42752/50000]\tLoss: 4.7635\tLR: 9.885166\n",
      "Training Epoch: 99 [42880/50000]\tLoss: 4.7448\tLR: 9.885422\n",
      "Training Epoch: 99 [43008/50000]\tLoss: 4.7367\tLR: 9.885678\n",
      "Training Epoch: 99 [43136/50000]\tLoss: 4.9877\tLR: 9.885934\n",
      "Training Epoch: 99 [43264/50000]\tLoss: 4.8704\tLR: 9.886189\n",
      "Training Epoch: 99 [43392/50000]\tLoss: 4.8566\tLR: 9.886445\n",
      "Training Epoch: 99 [43520/50000]\tLoss: 4.8189\tLR: 9.886701\n",
      "Training Epoch: 99 [43648/50000]\tLoss: 4.7684\tLR: 9.886957\n",
      "Training Epoch: 99 [43776/50000]\tLoss: 4.7571\tLR: 9.887212\n",
      "Training Epoch: 99 [43904/50000]\tLoss: 4.8117\tLR: 9.887468\n",
      "Training Epoch: 99 [44032/50000]\tLoss: 4.7020\tLR: 9.887724\n",
      "Training Epoch: 99 [44160/50000]\tLoss: 4.8000\tLR: 9.887980\n",
      "Training Epoch: 99 [44288/50000]\tLoss: 4.8246\tLR: 9.888235\n",
      "Training Epoch: 99 [44416/50000]\tLoss: 4.8130\tLR: 9.888491\n",
      "Training Epoch: 99 [44544/50000]\tLoss: 4.7630\tLR: 9.888747\n",
      "Training Epoch: 99 [44672/50000]\tLoss: 4.8329\tLR: 9.889003\n",
      "Training Epoch: 99 [44800/50000]\tLoss: 4.7271\tLR: 9.889258\n",
      "Training Epoch: 99 [44928/50000]\tLoss: 4.6729\tLR: 9.889514\n",
      "Training Epoch: 99 [45056/50000]\tLoss: 4.8387\tLR: 9.889770\n",
      "Training Epoch: 99 [45184/50000]\tLoss: 4.8213\tLR: 9.890026\n",
      "Training Epoch: 99 [45312/50000]\tLoss: 4.8659\tLR: 9.890281\n",
      "Training Epoch: 99 [45440/50000]\tLoss: 4.9292\tLR: 9.890537\n",
      "Training Epoch: 99 [45568/50000]\tLoss: 4.8572\tLR: 9.890793\n",
      "Training Epoch: 99 [45696/50000]\tLoss: 4.7902\tLR: 9.891049\n",
      "Training Epoch: 99 [45824/50000]\tLoss: 4.8242\tLR: 9.891304\n",
      "Training Epoch: 99 [45952/50000]\tLoss: 4.7617\tLR: 9.891560\n",
      "Training Epoch: 99 [46080/50000]\tLoss: 4.6832\tLR: 9.891816\n",
      "Training Epoch: 99 [46208/50000]\tLoss: 4.6749\tLR: 9.892072\n",
      "Training Epoch: 99 [46336/50000]\tLoss: 4.8069\tLR: 9.892327\n",
      "Training Epoch: 99 [46464/50000]\tLoss: 4.7305\tLR: 9.892583\n",
      "Training Epoch: 99 [46592/50000]\tLoss: 4.7661\tLR: 9.892839\n",
      "Training Epoch: 99 [46720/50000]\tLoss: 4.8491\tLR: 9.893095\n",
      "Training Epoch: 99 [46848/50000]\tLoss: 4.7898\tLR: 9.893350\n",
      "Training Epoch: 99 [46976/50000]\tLoss: 4.7408\tLR: 9.893606\n",
      "Training Epoch: 99 [47104/50000]\tLoss: 4.9097\tLR: 9.893862\n",
      "Training Epoch: 99 [47232/50000]\tLoss: 4.7635\tLR: 9.894118\n",
      "Training Epoch: 99 [47360/50000]\tLoss: 4.8154\tLR: 9.894373\n",
      "Training Epoch: 99 [47488/50000]\tLoss: 4.7161\tLR: 9.894629\n",
      "Training Epoch: 99 [47616/50000]\tLoss: 4.8168\tLR: 9.894885\n",
      "Training Epoch: 99 [47744/50000]\tLoss: 4.8465\tLR: 9.895141\n",
      "Training Epoch: 99 [47872/50000]\tLoss: 4.8095\tLR: 9.895396\n",
      "Training Epoch: 99 [48000/50000]\tLoss: 4.8270\tLR: 9.895652\n",
      "Training Epoch: 99 [48128/50000]\tLoss: 4.8470\tLR: 9.895908\n",
      "Training Epoch: 99 [48256/50000]\tLoss: 4.7328\tLR: 9.896164\n",
      "Training Epoch: 99 [48384/50000]\tLoss: 4.7487\tLR: 9.896419\n",
      "Training Epoch: 99 [48512/50000]\tLoss: 4.7363\tLR: 9.896675\n",
      "Training Epoch: 99 [48640/50000]\tLoss: 4.7673\tLR: 9.896931\n",
      "Training Epoch: 99 [48768/50000]\tLoss: 4.6829\tLR: 9.897187\n",
      "Training Epoch: 99 [48896/50000]\tLoss: 4.7517\tLR: 9.897442\n",
      "Training Epoch: 99 [49024/50000]\tLoss: 4.7763\tLR: 9.897698\n",
      "Training Epoch: 99 [49152/50000]\tLoss: 4.8476\tLR: 9.897954\n",
      "Training Epoch: 99 [49280/50000]\tLoss: 4.8954\tLR: 9.898210\n",
      "Training Epoch: 99 [49408/50000]\tLoss: 4.7351\tLR: 9.898465\n",
      "Training Epoch: 99 [49536/50000]\tLoss: 4.7835\tLR: 9.898721\n",
      "Training Epoch: 99 [49664/50000]\tLoss: 4.7675\tLR: 9.898977\n",
      "Training Epoch: 99 [49792/50000]\tLoss: 4.7348\tLR: 9.899233\n",
      "Training Epoch: 99 [49920/50000]\tLoss: 4.7504\tLR: 9.899488\n",
      "Training Epoch: 99 [50000/50000]\tLoss: 4.8501\tLR: 9.899744\n",
      "epoch 99 training time consumed: 492.57s\n",
      "GPU INFO.....\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  132782 KB |     854 MB |  138791 GB |  138791 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  138365 GB |  138365 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     426 GB |     426 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  132782 KB |     854 MB |  138791 GB |  138791 GB |\n",
      "|       from large pool |  121984 KB |     849 MB |  138365 GB |  138365 GB |\n",
      "|       from small pool |   10798 KB |      13 MB |     426 GB |     426 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     970 MB |    1002 MB |    1300 MB |  337920 KB |\n",
      "|       from large pool |     954 MB |     994 MB |    1282 MB |  335872 KB |\n",
      "|       from small pool |      16 MB |      16 MB |      18 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   45394 KB |  412127 KB |  136841 GB |  136841 GB |\n",
      "|       from large pool |   43904 KB |  410496 KB |  136415 GB |  136415 GB |\n",
      "|       from small pool |    1490 KB |    3494 KB |     426 GB |     426 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     255    |     335    |   14716 K  |   14716 K  |\n",
      "|       from large pool |      24    |      65    |    6273 K  |    6273 K  |\n",
      "|       from small pool |     231    |     274    |    8443 K  |    8442 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     255    |     335    |   14716 K  |   14716 K  |\n",
      "|       from large pool |      24    |      65    |    6273 K  |    6273 K  |\n",
      "|       from small pool |     231    |     274    |    8443 K  |    8442 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      27    |      27    |      30    |       3    |\n",
      "|       from large pool |      19    |      20    |      21    |       2    |\n",
      "|       from small pool |       8    |       8    |       9    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      36    |      47    |    8530 K  |    8530 K  |\n",
      "|       from large pool |      10    |      23    |    3015 K  |    3015 K  |\n",
      "|       from small pool |      26    |      35    |    5514 K  |    5514 K  |\n",
      "|===========================================================================|\n",
      "Evaluating Network.....\n",
      "Test set: Epoch: 99, Average loss: 0.0377, Accuracy: 0.0100, Time consumed:33.94s\n",
      "\n",
      "Training Epoch: 100 [128/50000]\tLoss: 4.7547\tLR: 0.020000\n",
      "Training Epoch: 100 [256/50000]\tLoss: 4.7439\tLR: 9.900256\n",
      "Training Epoch: 100 [384/50000]\tLoss: 4.6821\tLR: 9.900512\n",
      "Training Epoch: 100 [512/50000]\tLoss: 4.7443\tLR: 9.900767\n",
      "Training Epoch: 100 [640/50000]\tLoss: 4.8328\tLR: 9.901023\n",
      "Training Epoch: 100 [768/50000]\tLoss: 4.7154\tLR: 9.901279\n",
      "Training Epoch: 100 [896/50000]\tLoss: 4.7921\tLR: 9.901535\n",
      "Training Epoch: 100 [1024/50000]\tLoss: 4.7432\tLR: 9.901790\n",
      "Training Epoch: 100 [1152/50000]\tLoss: 4.7568\tLR: 9.902046\n",
      "Training Epoch: 100 [1280/50000]\tLoss: 4.8388\tLR: 9.902302\n",
      "Training Epoch: 100 [1408/50000]\tLoss: 4.7877\tLR: 9.902558\n",
      "Training Epoch: 100 [1536/50000]\tLoss: 4.7620\tLR: 9.902813\n",
      "Training Epoch: 100 [1664/50000]\tLoss: 4.7845\tLR: 9.903069\n",
      "Training Epoch: 100 [1792/50000]\tLoss: 4.8536\tLR: 9.903325\n",
      "Training Epoch: 100 [1920/50000]\tLoss: 4.7922\tLR: 9.903581\n",
      "Training Epoch: 100 [2048/50000]\tLoss: 4.8218\tLR: 9.903836\n",
      "Training Epoch: 100 [2176/50000]\tLoss: 4.6882\tLR: 9.904092\n",
      "Training Epoch: 100 [2304/50000]\tLoss: 4.7680\tLR: 9.904348\n",
      "Training Epoch: 100 [2432/50000]\tLoss: 4.7925\tLR: 9.904604\n",
      "Training Epoch: 100 [2560/50000]\tLoss: 4.7952\tLR: 9.904859\n",
      "Training Epoch: 100 [2688/50000]\tLoss: 4.7085\tLR: 9.905115\n",
      "Training Epoch: 100 [2816/50000]\tLoss: 4.9149\tLR: 9.905371\n",
      "Training Epoch: 100 [2944/50000]\tLoss: 4.8360\tLR: 9.905627\n",
      "Training Epoch: 100 [3072/50000]\tLoss: 4.6909\tLR: 9.905882\n",
      "Training Epoch: 100 [3200/50000]\tLoss: 4.8258\tLR: 9.906138\n",
      "Training Epoch: 100 [3328/50000]\tLoss: 4.7664\tLR: 9.906394\n",
      "Training Epoch: 100 [3456/50000]\tLoss: 4.7914\tLR: 9.906650\n",
      "Training Epoch: 100 [3584/50000]\tLoss: 4.7918\tLR: 9.906905\n",
      "Training Epoch: 100 [3712/50000]\tLoss: 4.8080\tLR: 9.907161\n",
      "Training Epoch: 100 [3840/50000]\tLoss: 4.8171\tLR: 9.907417\n",
      "Training Epoch: 100 [3968/50000]\tLoss: 4.7577\tLR: 9.907673\n",
      "Training Epoch: 100 [4096/50000]\tLoss: 4.7887\tLR: 9.907928\n",
      "Training Epoch: 100 [4224/50000]\tLoss: 4.8659\tLR: 9.908184\n",
      "Training Epoch: 100 [4352/50000]\tLoss: 4.7257\tLR: 9.908440\n",
      "Training Epoch: 100 [4480/50000]\tLoss: 4.8790\tLR: 9.908696\n",
      "Training Epoch: 100 [4608/50000]\tLoss: 4.7539\tLR: 9.908951\n",
      "Training Epoch: 100 [4736/50000]\tLoss: 4.8750\tLR: 9.909207\n",
      "Training Epoch: 100 [4864/50000]\tLoss: 4.7155\tLR: 9.909463\n",
      "Training Epoch: 100 [4992/50000]\tLoss: 5.4047\tLR: 9.909719\n",
      "Training Epoch: 100 [5120/50000]\tLoss: 4.8852\tLR: 9.909974\n",
      "Training Epoch: 100 [5248/50000]\tLoss: 4.6940\tLR: 9.910230\n",
      "Training Epoch: 100 [5376/50000]\tLoss: 4.7583\tLR: 9.910486\n",
      "Training Epoch: 100 [5504/50000]\tLoss: 4.7497\tLR: 9.910742\n",
      "Training Epoch: 100 [5632/50000]\tLoss: 4.7133\tLR: 9.910997\n",
      "Training Epoch: 100 [5760/50000]\tLoss: 4.7686\tLR: 9.911253\n",
      "Training Epoch: 100 [5888/50000]\tLoss: 4.8775\tLR: 9.911509\n",
      "Training Epoch: 100 [6016/50000]\tLoss: 4.7467\tLR: 9.911765\n",
      "Training Epoch: 100 [6144/50000]\tLoss: 4.8758\tLR: 9.912020\n",
      "Training Epoch: 100 [6272/50000]\tLoss: 4.7518\tLR: 9.912276\n",
      "Training Epoch: 100 [6400/50000]\tLoss: 4.6904\tLR: 9.912532\n",
      "Training Epoch: 100 [6528/50000]\tLoss: 4.8127\tLR: 9.912788\n",
      "Training Epoch: 100 [6656/50000]\tLoss: 4.6771\tLR: 9.913043\n",
      "Training Epoch: 100 [6784/50000]\tLoss: 4.7093\tLR: 9.913299\n",
      "Training Epoch: 100 [6912/50000]\tLoss: 4.7294\tLR: 9.913555\n",
      "Training Epoch: 100 [7040/50000]\tLoss: 4.7398\tLR: 9.913811\n",
      "Training Epoch: 100 [7168/50000]\tLoss: 4.7856\tLR: 9.914066\n",
      "Training Epoch: 100 [7296/50000]\tLoss: 4.8747\tLR: 9.914322\n",
      "Training Epoch: 100 [7424/50000]\tLoss: 4.8391\tLR: 9.914578\n",
      "Training Epoch: 100 [7552/50000]\tLoss: 4.7311\tLR: 9.914834\n",
      "Training Epoch: 100 [7680/50000]\tLoss: 4.7917\tLR: 9.915090\n",
      "Training Epoch: 100 [7808/50000]\tLoss: 4.6978\tLR: 9.915345\n",
      "Training Epoch: 100 [7936/50000]\tLoss: 4.7457\tLR: 9.915601\n",
      "Training Epoch: 100 [8064/50000]\tLoss: 4.8367\tLR: 9.915857\n",
      "Training Epoch: 100 [8192/50000]\tLoss: 4.8563\tLR: 9.916113\n",
      "Training Epoch: 100 [8320/50000]\tLoss: 4.8122\tLR: 9.916368\n",
      "Training Epoch: 100 [8448/50000]\tLoss: 4.7936\tLR: 9.916624\n",
      "Training Epoch: 100 [8576/50000]\tLoss: 4.7450\tLR: 9.916880\n",
      "Training Epoch: 100 [8704/50000]\tLoss: 4.7667\tLR: 9.917136\n",
      "Training Epoch: 100 [8832/50000]\tLoss: 4.6647\tLR: 9.917391\n",
      "Training Epoch: 100 [8960/50000]\tLoss: 4.8224\tLR: 9.917647\n",
      "Training Epoch: 100 [9088/50000]\tLoss: 4.6856\tLR: 9.917903\n",
      "Training Epoch: 100 [9216/50000]\tLoss: 4.8370\tLR: 9.918159\n",
      "Training Epoch: 100 [9344/50000]\tLoss: 4.7430\tLR: 9.918414\n",
      "Training Epoch: 100 [9472/50000]\tLoss: 4.8249\tLR: 9.918670\n",
      "Training Epoch: 100 [9600/50000]\tLoss: 4.8326\tLR: 9.918926\n",
      "Training Epoch: 100 [9728/50000]\tLoss: 4.7668\tLR: 9.919182\n",
      "Training Epoch: 100 [9856/50000]\tLoss: 4.7560\tLR: 9.919437\n",
      "Training Epoch: 100 [9984/50000]\tLoss: 4.7963\tLR: 9.919693\n",
      "Training Epoch: 100 [10112/50000]\tLoss: 4.8678\tLR: 9.919949\n",
      "Training Epoch: 100 [10240/50000]\tLoss: 4.8535\tLR: 9.920205\n",
      "Training Epoch: 100 [10368/50000]\tLoss: 4.8152\tLR: 9.920460\n",
      "Training Epoch: 100 [10496/50000]\tLoss: 4.6114\tLR: 9.920716\n",
      "Training Epoch: 100 [10624/50000]\tLoss: 4.8754\tLR: 9.920972\n",
      "Training Epoch: 100 [10752/50000]\tLoss: 4.7284\tLR: 9.921228\n",
      "Training Epoch: 100 [10880/50000]\tLoss: 4.7644\tLR: 9.921483\n",
      "Training Epoch: 100 [11008/50000]\tLoss: 4.8009\tLR: 9.921739\n",
      "Training Epoch: 100 [11136/50000]\tLoss: 4.9006\tLR: 9.921995\n",
      "Training Epoch: 100 [11264/50000]\tLoss: 4.8330\tLR: 9.922251\n",
      "Training Epoch: 100 [11392/50000]\tLoss: 4.8696\tLR: 9.922506\n",
      "Training Epoch: 100 [11520/50000]\tLoss: 4.9574\tLR: 9.922762\n",
      "Training Epoch: 100 [11648/50000]\tLoss: 4.8423\tLR: 9.923018\n",
      "Training Epoch: 100 [11776/50000]\tLoss: 4.7435\tLR: 9.923274\n",
      "Training Epoch: 100 [11904/50000]\tLoss: 4.7931\tLR: 9.923529\n",
      "Training Epoch: 100 [12032/50000]\tLoss: 4.8006\tLR: 9.923785\n",
      "Training Epoch: 100 [12160/50000]\tLoss: 4.7215\tLR: 9.924041\n",
      "Training Epoch: 100 [12288/50000]\tLoss: 4.8343\tLR: 9.924297\n",
      "Training Epoch: 100 [12416/50000]\tLoss: 4.7886\tLR: 9.924552\n",
      "Training Epoch: 100 [12544/50000]\tLoss: 4.7174\tLR: 9.924808\n",
      "Training Epoch: 100 [12672/50000]\tLoss: 4.8650\tLR: 9.925064\n",
      "Training Epoch: 100 [12800/50000]\tLoss: 4.8480\tLR: 9.925320\n",
      "Training Epoch: 100 [12928/50000]\tLoss: 4.7826\tLR: 9.925575\n",
      "Training Epoch: 100 [13056/50000]\tLoss: 4.7577\tLR: 9.925831\n",
      "Training Epoch: 100 [13184/50000]\tLoss: 4.7931\tLR: 9.926087\n",
      "Training Epoch: 100 [13312/50000]\tLoss: 4.7543\tLR: 9.926343\n",
      "Training Epoch: 100 [13440/50000]\tLoss: 4.7514\tLR: 9.926598\n",
      "Training Epoch: 100 [13568/50000]\tLoss: 4.7407\tLR: 9.926854\n",
      "Training Epoch: 100 [13696/50000]\tLoss: 4.7413\tLR: 9.927110\n",
      "Training Epoch: 100 [13824/50000]\tLoss: 4.7129\tLR: 9.927366\n",
      "Training Epoch: 100 [13952/50000]\tLoss: 4.7848\tLR: 9.927621\n",
      "Training Epoch: 100 [14080/50000]\tLoss: 4.6593\tLR: 9.927877\n",
      "Training Epoch: 100 [14208/50000]\tLoss: 4.7195\tLR: 9.928133\n",
      "Training Epoch: 100 [14336/50000]\tLoss: 4.6763\tLR: 9.928389\n",
      "Training Epoch: 100 [14464/50000]\tLoss: 4.6814\tLR: 9.928645\n",
      "Training Epoch: 100 [14592/50000]\tLoss: 4.8196\tLR: 9.928900\n",
      "Training Epoch: 100 [14720/50000]\tLoss: 4.7978\tLR: 9.929156\n",
      "Training Epoch: 100 [14848/50000]\tLoss: 4.7371\tLR: 9.929412\n",
      "Training Epoch: 100 [14976/50000]\tLoss: 4.6691\tLR: 9.929668\n",
      "Training Epoch: 100 [15104/50000]\tLoss: 4.7183\tLR: 9.929923\n",
      "Training Epoch: 100 [15232/50000]\tLoss: 4.7105\tLR: 9.930179\n",
      "Training Epoch: 100 [15360/50000]\tLoss: 4.9443\tLR: 9.930435\n",
      "Training Epoch: 100 [15488/50000]\tLoss: 4.8557\tLR: 9.930691\n",
      "Training Epoch: 100 [15616/50000]\tLoss: 4.7471\tLR: 9.930946\n",
      "Training Epoch: 100 [15744/50000]\tLoss: 4.6531\tLR: 9.931202\n",
      "Training Epoch: 100 [15872/50000]\tLoss: 5.0078\tLR: 9.931458\n",
      "Training Epoch: 100 [16000/50000]\tLoss: 4.8209\tLR: 9.931714\n",
      "Training Epoch: 100 [16128/50000]\tLoss: 4.7601\tLR: 9.931969\n",
      "Training Epoch: 100 [16256/50000]\tLoss: 4.8358\tLR: 9.932225\n",
      "Training Epoch: 100 [16384/50000]\tLoss: 4.7146\tLR: 9.932481\n",
      "Training Epoch: 100 [16512/50000]\tLoss: 4.7051\tLR: 9.932737\n",
      "Training Epoch: 100 [16640/50000]\tLoss: 4.8037\tLR: 9.932992\n",
      "Training Epoch: 100 [16768/50000]\tLoss: 4.8244\tLR: 9.933248\n",
      "Training Epoch: 100 [16896/50000]\tLoss: 4.7584\tLR: 9.933504\n",
      "Training Epoch: 100 [17024/50000]\tLoss: 4.8122\tLR: 9.933760\n",
      "Training Epoch: 100 [17152/50000]\tLoss: 4.8241\tLR: 9.934015\n",
      "Training Epoch: 100 [17280/50000]\tLoss: 4.8251\tLR: 9.934271\n",
      "Training Epoch: 100 [17408/50000]\tLoss: 4.7920\tLR: 9.934527\n",
      "Training Epoch: 100 [17536/50000]\tLoss: 4.7981\tLR: 9.934783\n",
      "Training Epoch: 100 [17664/50000]\tLoss: 4.9115\tLR: 9.935038\n",
      "Training Epoch: 100 [17792/50000]\tLoss: 4.7246\tLR: 9.935294\n",
      "Training Epoch: 100 [17920/50000]\tLoss: 4.7878\tLR: 9.935550\n",
      "Training Epoch: 100 [18048/50000]\tLoss: 4.7179\tLR: 9.935806\n",
      "Training Epoch: 100 [18176/50000]\tLoss: 4.7349\tLR: 9.936061\n",
      "Training Epoch: 100 [18304/50000]\tLoss: 4.8298\tLR: 9.936317\n",
      "Training Epoch: 100 [18432/50000]\tLoss: 4.8078\tLR: 9.936573\n",
      "Training Epoch: 100 [18560/50000]\tLoss: 4.7691\tLR: 9.936829\n",
      "Training Epoch: 100 [18688/50000]\tLoss: 4.7623\tLR: 9.937084\n",
      "Training Epoch: 100 [18816/50000]\tLoss: 4.7839\tLR: 9.937340\n",
      "Training Epoch: 100 [18944/50000]\tLoss: 4.8637\tLR: 9.937596\n",
      "Training Epoch: 100 [19072/50000]\tLoss: 4.7788\tLR: 9.937852\n",
      "Training Epoch: 100 [19200/50000]\tLoss: 4.8599\tLR: 9.938107\n",
      "Training Epoch: 100 [19328/50000]\tLoss: 4.7665\tLR: 9.938363\n",
      "Training Epoch: 100 [19456/50000]\tLoss: 4.8067\tLR: 9.938619\n",
      "Training Epoch: 100 [19584/50000]\tLoss: 4.7214\tLR: 9.938875\n",
      "Training Epoch: 100 [19712/50000]\tLoss: 4.7705\tLR: 9.939130\n",
      "Training Epoch: 100 [19840/50000]\tLoss: 4.8289\tLR: 9.939386\n",
      "Training Epoch: 100 [19968/50000]\tLoss: 4.7755\tLR: 9.939642\n",
      "Training Epoch: 100 [20096/50000]\tLoss: 4.8493\tLR: 9.939898\n",
      "Training Epoch: 100 [20224/50000]\tLoss: 4.7662\tLR: 9.940153\n",
      "Training Epoch: 100 [20352/50000]\tLoss: 4.7762\tLR: 9.940409\n",
      "Training Epoch: 100 [20480/50000]\tLoss: 4.7853\tLR: 9.940665\n",
      "Training Epoch: 100 [20608/50000]\tLoss: 4.7275\tLR: 9.940921\n",
      "Training Epoch: 100 [20736/50000]\tLoss: 4.8378\tLR: 9.941176\n",
      "Training Epoch: 100 [20864/50000]\tLoss: 4.7989\tLR: 9.941432\n",
      "Training Epoch: 100 [20992/50000]\tLoss: 4.6814\tLR: 9.941688\n",
      "Training Epoch: 100 [21120/50000]\tLoss: 4.7323\tLR: 9.941944\n",
      "Training Epoch: 100 [21248/50000]\tLoss: 4.8131\tLR: 9.942199\n",
      "Training Epoch: 100 [21376/50000]\tLoss: 4.8238\tLR: 9.942455\n",
      "Training Epoch: 100 [21504/50000]\tLoss: 4.8225\tLR: 9.942711\n",
      "Training Epoch: 100 [21632/50000]\tLoss: 4.6678\tLR: 9.942967\n",
      "Training Epoch: 100 [21760/50000]\tLoss: 4.7269\tLR: 9.943223\n",
      "Training Epoch: 100 [21888/50000]\tLoss: 4.7913\tLR: 9.943478\n",
      "Training Epoch: 100 [22016/50000]\tLoss: 4.7369\tLR: 9.943734\n",
      "Training Epoch: 100 [22144/50000]\tLoss: 4.8465\tLR: 9.943990\n",
      "Training Epoch: 100 [22272/50000]\tLoss: 4.7645\tLR: 9.944246\n",
      "Training Epoch: 100 [22400/50000]\tLoss: 4.7063\tLR: 9.944501\n",
      "Training Epoch: 100 [22528/50000]\tLoss: 4.7769\tLR: 9.944757\n",
      "Training Epoch: 100 [22656/50000]\tLoss: 4.9231\tLR: 9.945013\n",
      "Training Epoch: 100 [22784/50000]\tLoss: 4.6785\tLR: 9.945269\n",
      "Training Epoch: 100 [22912/50000]\tLoss: 4.7195\tLR: 9.945524\n",
      "Training Epoch: 100 [23040/50000]\tLoss: 4.7532\tLR: 9.945780\n",
      "Training Epoch: 100 [23168/50000]\tLoss: 4.8124\tLR: 9.946036\n",
      "Training Epoch: 100 [23296/50000]\tLoss: 4.6795\tLR: 9.946292\n",
      "Training Epoch: 100 [23424/50000]\tLoss: 4.7086\tLR: 9.946547\n",
      "Training Epoch: 100 [23552/50000]\tLoss: 4.7875\tLR: 9.946803\n",
      "Training Epoch: 100 [23680/50000]\tLoss: 4.6841\tLR: 9.947059\n",
      "Training Epoch: 100 [23808/50000]\tLoss: 4.7834\tLR: 9.947315\n",
      "Training Epoch: 100 [23936/50000]\tLoss: 4.8408\tLR: 9.947570\n",
      "Training Epoch: 100 [24064/50000]\tLoss: 4.8383\tLR: 9.947826\n",
      "Training Epoch: 100 [24192/50000]\tLoss: 4.8205\tLR: 9.948082\n",
      "Training Epoch: 100 [24320/50000]\tLoss: 4.7905\tLR: 9.948338\n",
      "Training Epoch: 100 [24448/50000]\tLoss: 4.6860\tLR: 9.948593\n",
      "Training Epoch: 100 [24576/50000]\tLoss: 4.7869\tLR: 9.948849\n",
      "Training Epoch: 100 [24704/50000]\tLoss: 4.8280\tLR: 9.949105\n",
      "Training Epoch: 100 [24832/50000]\tLoss: 4.7709\tLR: 9.949361\n",
      "Training Epoch: 100 [24960/50000]\tLoss: 4.8823\tLR: 9.949616\n",
      "Training Epoch: 100 [25088/50000]\tLoss: 4.7255\tLR: 9.949872\n",
      "Training Epoch: 100 [25216/50000]\tLoss: 4.7351\tLR: 9.950128\n",
      "Training Epoch: 100 [25344/50000]\tLoss: 4.7291\tLR: 9.950384\n",
      "Training Epoch: 100 [25472/50000]\tLoss: 4.7997\tLR: 9.950639\n",
      "Training Epoch: 100 [25600/50000]\tLoss: 4.7726\tLR: 9.950895\n",
      "Training Epoch: 100 [25728/50000]\tLoss: 4.7779\tLR: 9.951151\n",
      "Training Epoch: 100 [25856/50000]\tLoss: 4.7533\tLR: 9.951407\n",
      "Training Epoch: 100 [25984/50000]\tLoss: 4.7522\tLR: 9.951662\n",
      "Training Epoch: 100 [26112/50000]\tLoss: 4.7846\tLR: 9.951918\n",
      "Training Epoch: 100 [26240/50000]\tLoss: 4.7827\tLR: 9.952174\n",
      "Training Epoch: 100 [26368/50000]\tLoss: 4.7484\tLR: 9.952430\n",
      "Training Epoch: 100 [26496/50000]\tLoss: 4.8268\tLR: 9.952685\n",
      "Training Epoch: 100 [26624/50000]\tLoss: 4.7235\tLR: 9.952941\n",
      "Training Epoch: 100 [26752/50000]\tLoss: 4.7474\tLR: 9.953197\n",
      "Training Epoch: 100 [26880/50000]\tLoss: 4.7138\tLR: 9.953453\n",
      "Training Epoch: 100 [27008/50000]\tLoss: 4.8498\tLR: 9.953708\n",
      "Training Epoch: 100 [27136/50000]\tLoss: 4.7887\tLR: 9.953964\n",
      "Training Epoch: 100 [27264/50000]\tLoss: 4.7883\tLR: 9.954220\n",
      "Training Epoch: 100 [27392/50000]\tLoss: 4.8035\tLR: 9.954476\n",
      "Training Epoch: 100 [27520/50000]\tLoss: 4.8447\tLR: 9.954731\n",
      "Training Epoch: 100 [27648/50000]\tLoss: 4.7244\tLR: 9.954987\n",
      "Training Epoch: 100 [27776/50000]\tLoss: 4.8269\tLR: 9.955243\n",
      "Training Epoch: 100 [27904/50000]\tLoss: 4.7874\tLR: 9.955499\n",
      "Training Epoch: 100 [28032/50000]\tLoss: 4.8383\tLR: 9.955754\n",
      "Training Epoch: 100 [28160/50000]\tLoss: 4.8471\tLR: 9.956010\n",
      "Training Epoch: 100 [28288/50000]\tLoss: 4.7772\tLR: 9.956266\n",
      "Training Epoch: 100 [28416/50000]\tLoss: 4.8278\tLR: 9.956522\n",
      "Training Epoch: 100 [28544/50000]\tLoss: 4.8278\tLR: 9.956777\n",
      "Training Epoch: 100 [28672/50000]\tLoss: 4.8287\tLR: 9.957033\n",
      "Training Epoch: 100 [28800/50000]\tLoss: 4.7218\tLR: 9.957289\n",
      "Training Epoch: 100 [28928/50000]\tLoss: 4.8480\tLR: 9.957545\n",
      "Training Epoch: 100 [29056/50000]\tLoss: 4.8086\tLR: 9.957801\n",
      "Training Epoch: 100 [29184/50000]\tLoss: 4.8419\tLR: 9.958056\n",
      "Training Epoch: 100 [29312/50000]\tLoss: 4.7574\tLR: 9.958312\n",
      "Training Epoch: 100 [29440/50000]\tLoss: 4.6971\tLR: 9.958568\n",
      "Training Epoch: 100 [29568/50000]\tLoss: 4.7569\tLR: 9.958824\n",
      "Training Epoch: 100 [29696/50000]\tLoss: 4.6728\tLR: 9.959079\n",
      "Training Epoch: 100 [29824/50000]\tLoss: 4.8183\tLR: 9.959335\n",
      "Training Epoch: 100 [29952/50000]\tLoss: 4.8178\tLR: 9.959591\n",
      "Training Epoch: 100 [30080/50000]\tLoss: 4.9362\tLR: 9.959847\n",
      "Training Epoch: 100 [30208/50000]\tLoss: 4.8071\tLR: 9.960102\n",
      "Training Epoch: 100 [30336/50000]\tLoss: 4.7880\tLR: 9.960358\n",
      "Training Epoch: 100 [30464/50000]\tLoss: 4.6979\tLR: 9.960614\n",
      "Training Epoch: 100 [30592/50000]\tLoss: 4.8077\tLR: 9.960870\n",
      "Training Epoch: 100 [30720/50000]\tLoss: 4.8698\tLR: 9.961125\n",
      "Training Epoch: 100 [30848/50000]\tLoss: 4.7892\tLR: 9.961381\n",
      "Training Epoch: 100 [30976/50000]\tLoss: 4.8645\tLR: 9.961637\n",
      "Training Epoch: 100 [31104/50000]\tLoss: 4.7626\tLR: 9.961893\n",
      "Training Epoch: 100 [31232/50000]\tLoss: 4.8415\tLR: 9.962148\n",
      "Training Epoch: 100 [31360/50000]\tLoss: 4.8159\tLR: 9.962404\n",
      "Training Epoch: 100 [31488/50000]\tLoss: 4.7202\tLR: 9.962660\n",
      "Training Epoch: 100 [31616/50000]\tLoss: 4.7733\tLR: 9.962916\n",
      "Training Epoch: 100 [31744/50000]\tLoss: 4.7277\tLR: 9.963171\n",
      "Training Epoch: 100 [31872/50000]\tLoss: 4.8052\tLR: 9.963427\n",
      "Training Epoch: 100 [32000/50000]\tLoss: 4.5710\tLR: 9.963683\n",
      "Training Epoch: 100 [32128/50000]\tLoss: 4.7907\tLR: 9.963939\n",
      "Training Epoch: 100 [32256/50000]\tLoss: 4.7346\tLR: 9.964194\n",
      "Training Epoch: 100 [32384/50000]\tLoss: 4.8134\tLR: 9.964450\n",
      "Training Epoch: 100 [32512/50000]\tLoss: 4.8224\tLR: 9.964706\n",
      "Training Epoch: 100 [32640/50000]\tLoss: 4.7734\tLR: 9.964962\n",
      "Training Epoch: 100 [32768/50000]\tLoss: 4.6447\tLR: 9.965217\n",
      "Training Epoch: 100 [32896/50000]\tLoss: 4.8218\tLR: 9.965473\n",
      "Training Epoch: 100 [33024/50000]\tLoss: 4.8194\tLR: 9.965729\n",
      "Training Epoch: 100 [33152/50000]\tLoss: 4.8558\tLR: 9.965985\n",
      "Training Epoch: 100 [33280/50000]\tLoss: 4.8156\tLR: 9.966240\n",
      "Training Epoch: 100 [33408/50000]\tLoss: 4.7090\tLR: 9.966496\n",
      "Training Epoch: 100 [33536/50000]\tLoss: 4.6920\tLR: 9.966752\n",
      "Training Epoch: 100 [33664/50000]\tLoss: 4.7549\tLR: 9.967008\n",
      "Training Epoch: 100 [33792/50000]\tLoss: 4.7568\tLR: 9.967263\n",
      "Training Epoch: 100 [33920/50000]\tLoss: 4.8844\tLR: 9.967519\n",
      "Training Epoch: 100 [34048/50000]\tLoss: 4.7705\tLR: 9.967775\n",
      "Training Epoch: 100 [34176/50000]\tLoss: 4.7957\tLR: 9.968031\n",
      "Training Epoch: 100 [34304/50000]\tLoss: 4.8597\tLR: 9.968286\n",
      "Training Epoch: 100 [34432/50000]\tLoss: 4.7815\tLR: 9.968542\n",
      "Training Epoch: 100 [34560/50000]\tLoss: 4.8076\tLR: 9.968798\n",
      "Training Epoch: 100 [34688/50000]\tLoss: 4.6656\tLR: 9.969054\n",
      "Training Epoch: 100 [34816/50000]\tLoss: 4.6608\tLR: 9.969309\n",
      "Training Epoch: 100 [34944/50000]\tLoss: 4.7495\tLR: 9.969565\n",
      "Training Epoch: 100 [35072/50000]\tLoss: 4.8915\tLR: 9.969821\n",
      "Training Epoch: 100 [35200/50000]\tLoss: 4.8192\tLR: 9.970077\n",
      "Training Epoch: 100 [35328/50000]\tLoss: 4.8901\tLR: 9.970332\n",
      "Training Epoch: 100 [35456/50000]\tLoss: 4.7785\tLR: 9.970588\n",
      "Training Epoch: 100 [35584/50000]\tLoss: 4.8164\tLR: 9.970844\n",
      "Training Epoch: 100 [35712/50000]\tLoss: 4.8133\tLR: 9.971100\n",
      "Training Epoch: 100 [35840/50000]\tLoss: 4.7521\tLR: 9.971355\n",
      "Training Epoch: 100 [35968/50000]\tLoss: 4.7558\tLR: 9.971611\n",
      "Training Epoch: 100 [36096/50000]\tLoss: 4.7986\tLR: 9.971867\n",
      "Training Epoch: 100 [36224/50000]\tLoss: 4.7786\tLR: 9.972123\n",
      "Training Epoch: 100 [36352/50000]\tLoss: 4.8314\tLR: 9.972379\n",
      "Training Epoch: 100 [36480/50000]\tLoss: 4.7938\tLR: 9.972634\n",
      "Training Epoch: 100 [36608/50000]\tLoss: 4.7874\tLR: 9.972890\n",
      "Training Epoch: 100 [36736/50000]\tLoss: 4.8369\tLR: 9.973146\n",
      "Training Epoch: 100 [36864/50000]\tLoss: 4.8219\tLR: 9.973402\n",
      "Training Epoch: 100 [36992/50000]\tLoss: 4.7454\tLR: 9.973657\n",
      "Training Epoch: 100 [37120/50000]\tLoss: 4.7497\tLR: 9.973913\n",
      "Training Epoch: 100 [37248/50000]\tLoss: 4.7408\tLR: 9.974169\n",
      "Training Epoch: 100 [37376/50000]\tLoss: 4.8053\tLR: 9.974425\n",
      "Training Epoch: 100 [37504/50000]\tLoss: 4.7082\tLR: 9.974680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\program\\conda\\envs\\pytorch0\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, settings.EPOCH + 1):\n",
    "    if epoch > settings.WARMUP:\n",
    "        train_scheduler.step(epoch)\n",
    "    if settings.RESUME:\n",
    "        if epoch <= resume_epoch:\n",
    "            continue\n",
    "    train(epoch)\n",
    "    acc = eval_training(epoch)\n",
    "    #start to save best performance model after learning rate decay to 0.01\n",
    "    if epoch > settings.MILESTONES[1] and best_acc < acc:\n",
    "        weights_path = checkpoint_path.format(net=settings.NET, epoch=epoch, type='best')\n",
    "        print('saving weights file to {}'.format(weights_path))\n",
    "        torch.save(net.state_dict(), weights_path)\n",
    "        best_acc = acc\n",
    "        continue\n",
    "\n",
    "    if not epoch % settings.SAVE_EPOCH:\n",
    "        weights_path = checkpoint_path.format(net=settings.NET, epoch=epoch, type='regular')\n",
    "        print('saving weights file to {}'.format(weights_path))\n",
    "        torch.save(net.state_dict(), weights_path)\n",
    "\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data\\cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14.5%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "19.4%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "20.3%"
     ]
    }
   ],
   "source": [
    "'''\n",
    "测试阶段\n",
    "'''\n",
    "net = resnet18()\n",
    "\n",
    "cifar100_test_loader = get_test_dataloader(\n",
    "    settings.CIFAR100_TRAIN_MEAN,\n",
    "    settings.CIFAR100_TRAIN_STD,\n",
    "    #settings.CIFAR100_PATH,\n",
    "    num_workers=4,\n",
    "    batch_size=settings.BATCH_SIZE,\n",
    ")\n",
    "\n",
    "net.load_state_dict(torch.load('checkpoint\\resnet18\\Tuesday_26_July_2022_06h_54m_48s\\resnet18-121-best.pth'))\n",
    "print(net)\n",
    "net.eval()\n",
    "\n",
    "correct_1 = 0.0\n",
    "correct_5 = 0.0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for n_iter, (image, label) in enumerate(cifar100_test_loader):\n",
    "        print(\"iteration: {}\\ttotal {} iterations\".format(n_iter + 1, len(cifar100_test_loader)))\n",
    "\n",
    "        if settings.GPU:\n",
    "            image = image.cuda()\n",
    "            label = label.cuda()\n",
    "            print('GPU INFO.....')\n",
    "            print(torch.cuda.memory_summary(), end='')\n",
    "\n",
    "\n",
    "        output = net(image)\n",
    "        _, pred = output.topk(5, 1, largest=True, sorted=True)\n",
    "\n",
    "        label = label.view(label.size(0), -1).expand_as(pred)\n",
    "        correct = pred.eq(label).float()\n",
    "\n",
    "        #compute top 5\n",
    "        correct_5 += correct[:, :5].sum()\n",
    "\n",
    "        #compute top1\n",
    "        correct_1 += correct[:, :1].sum()\n",
    "\n",
    "if settings.GPU:\n",
    "    print('GPU INFO.....')\n",
    "    print(torch.cuda.memory_summary(), end='')\n",
    "\n",
    "print()\n",
    "print(\"Top 1 err: \", 1 - correct_1 / len(cifar100_test_loader.dataset))\n",
    "print(\"Top 5 err: \", 1 - correct_5 / len(cifar100_test_loader.dataset))\n",
    "print(\"Parameter numbers: {}\".format(sum(p.numel() for p in net.parameters())))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}